[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anas Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jrme Damet"
                    },
                    {
                        "name": "Luca Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Luca Gallego Manzano"
                },
                "author": "Luca Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19260v2",
                "updated": "2025-09-29T22:56:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    22,
                    56,
                    52,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-23T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "title": "Reconstruction of a potential parameter in subdiffusion via a\n  Kohn--Vogelius type functional: Theory and computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of a potential parameter in subdiffusion via a\n  Kohn--Vogelius type functional: Theory and computation"
                },
                "summary": "This work considers the reconstruction of a space-dependent potential from\nboundary observations in subdiffusion by a stable and robust recovery method.\nSpecifically, we develop an algorithm to minimize the Kohn-Vogelius cost\nfunction, which measures the difference between the solutions of two\nexcitations. The inverse potential problem is recast into an optimization\nproblem, where the objective is to minimize a Kohn-Vogelius-type functional\nwithin a set of admissible potentials. We establish the well-posedness of this\noptimization problem by proving the existence and uniqueness of a minimizer and\ndemonstrating its stability with respect to perturbations in the boundary data.\nFurthermore, we analyze the Fr\\'echet differentiability of the Kohn-Vogelius\nfunctional and prove the Lipschitz continuity of its gradient. These\ntheoretical results enable the development of a convergent conjugate gradient\nalgorithm for numerical reconstruction. The effectiveness and robustness of the\nproposed method are confirmed through several numerical examples in both one\nand two dimensions, including cases with noisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work considers the reconstruction of a space-dependent potential from\nboundary observations in subdiffusion by a stable and robust recovery method.\nSpecifically, we develop an algorithm to minimize the Kohn-Vogelius cost\nfunction, which measures the difference between the solutions of two\nexcitations. The inverse potential problem is recast into an optimization\nproblem, where the objective is to minimize a Kohn-Vogelius-type functional\nwithin a set of admissible potentials. We establish the well-posedness of this\noptimization problem by proving the existence and uniqueness of a minimizer and\ndemonstrating its stability with respect to perturbations in the boundary data.\nFurthermore, we analyze the Fr\\'echet differentiability of the Kohn-Vogelius\nfunctional and prove the Lipschitz continuity of its gradient. These\ntheoretical results enable the development of a convergent conjugate gradient\nalgorithm for numerical reconstruction. The effectiveness and robustness of the\nproposed method are confirmed through several numerical examples in both one\nand two dimensions, including cases with noisy data."
                },
                "authors": [
                    {
                        "name": "Hamza Kahlaoui"
                    },
                    {
                        "name": "Mourad Hrizi"
                    },
                    {
                        "name": "Abdessamad Oulmelk"
                    },
                    {
                        "name": "Xiangcheng Zheng"
                    },
                    {
                        "name": "Mahmoud A. Zaky"
                    },
                    {
                        "name": "Ahmed Hendy"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hendy"
                },
                "author": "Ahmed Hendy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Pntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21354v1",
                "updated": "2025-09-20T02:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T02:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "title": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache"
                },
                "summary": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic."
                },
                "authors": [
                    {
                        "name": "Wanshun Xu"
                    },
                    {
                        "name": "Long Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Long Zhuang"
                },
                "author": "Long Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazn"
                    },
                    {
                        "name": "V. lvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodrguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Crcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cosso"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Daz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. Garca-Barrena"
                    },
                    {
                        "name": "J. J. Gmez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gmez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervs Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. Lpez-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martn-Albo"
                    },
                    {
                        "name": "G. Martnez-Lema"
                    },
                    {
                        "name": "M. Martnez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Prez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simn"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnel"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usn"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.25148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25148v1",
                "updated": "2025-09-29T17:53:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    53,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    53,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following"
                },
                "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
                },
                "authors": [
                    {
                        "name": "FaQiang Qian"
                    },
                    {
                        "name": "WeiKun Zhang"
                    },
                    {
                        "name": "Ziliang Wang"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Xuhui Zheng"
                    },
                    {
                        "name": "Liangjian Wen"
                    },
                    {
                        "name": "Mengya Gao"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Yichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Wu"
                },
                "author": "Yichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25144v1",
                "updated": "2025-09-29T17:51:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:51:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation"
                },
                "summary": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis."
                },
                "authors": [
                    {
                        "name": "Yen-Ju Lu"
                    },
                    {
                        "name": "Thomas Thebaud"
                    },
                    {
                        "name": "Laureano Moro-Velazquez"
                    },
                    {
                        "name": "Najim Dehak"
                    },
                    {
                        "name": "Jesus Villalba"
                    }
                ],
                "author_detail": {
                    "name": "Jesus Villalba"
                },
                "author": "Jesus Villalba",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25139v1",
                "updated": "2025-09-29T17:51:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:51:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs"
                },
                "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25138v1",
                "updated": "2025-09-29T17:50:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    32,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:50:32Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    32,
                    0,
                    272,
                    0
                ],
                "title": "Investigating Language and Retrieval Bias in Multilingual Previously\n  Fact-Checked Claim Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Language and Retrieval Bias in Multilingual Previously\n  Fact-Checked Claim Detection"
                },
                "summary": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Antonia Karamolegkou"
                    },
                    {
                        "name": "Jaroslav Kopan"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Tom Javrek"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Marin imko"
                    }
                ],
                "author_detail": {
                    "name": "Marin imko"
                },
                "author": "Marin imko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25136v1",
                "updated": "2025-09-29T17:50:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:50:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression"
                },
                "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop."
                },
                "authors": [
                    {
                        "name": "David Gonzlez Martnez"
                    }
                ],
                "author_detail": {
                    "name": "David Gonzlez Martnez"
                },
                "author": "David Gonzlez Martnez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25131v1",
                "updated": "2025-09-29T17:48:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    48,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:48:28Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    48,
                    28,
                    0,
                    272,
                    0
                ],
                "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech"
                },
                "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation."
                },
                "authors": [
                    {
                        "name": "Chengyao Wang"
                    },
                    {
                        "name": "Zhisheng Zhong"
                    },
                    {
                        "name": "Bohao Peng"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Haokun Gui"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code is available at https://github.com/dvlab-research/MGM-Omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25123v1",
                "updated": "2025-09-29T17:44:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones"
                },
                "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25121v1",
                "updated": "2025-09-29T17:43:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    43,
                    32,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:43:32Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    43,
                    32,
                    0,
                    272,
                    0
                ],
                "title": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs"
                },
                "summary": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as\nunstructured graphs, achieving state of the art performance in computer vision\ntasks such as image classification, object detection, and instance\nsegmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by\nconnecting patches (nodes) based on feature similarity, and is dynamically\nrepeated in each ViG layer following GNN based patch (node) feature updates.\nHowever, DIGC constitutes over 50% of end to end ViG inference latency, rising\nto 95% at high image resolutions, making it the dominant computational\nbottleneck. While hardware acceleration holds promise, prior works primarily\noptimize graph construction algorithmically, often compromising DIGC\nflexibility, accuracy, or generality. To address these limitations, we propose\na streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip\nbuffers that process input features in small, uniform blocks. Our design\nminimizes external memory traffic via localized computation and performs\nefficient parallel sorting with local merge sort and global k way merging\ndirectly on streaming input blocks via heap insertion. This modular\narchitecture scales seamlessly across image resolutions, ViG layer types, and\nmodel sizes and variants, and supports DIGC across diverse ViG based vision\nbackbones. The design achieves high clock frequencies post place and route due\nto the statically configured parallelism minimizing critical path delay and\ndelivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as\nunstructured graphs, achieving state of the art performance in computer vision\ntasks such as image classification, object detection, and instance\nsegmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by\nconnecting patches (nodes) based on feature similarity, and is dynamically\nrepeated in each ViG layer following GNN based patch (node) feature updates.\nHowever, DIGC constitutes over 50% of end to end ViG inference latency, rising\nto 95% at high image resolutions, making it the dominant computational\nbottleneck. While hardware acceleration holds promise, prior works primarily\noptimize graph construction algorithmically, often compromising DIGC\nflexibility, accuracy, or generality. To address these limitations, we propose\na streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip\nbuffers that process input features in small, uniform blocks. Our design\nminimizes external memory traffic via localized computation and performs\nefficient parallel sorting with local merge sort and global k way merging\ndirectly on streaming input blocks via heap insertion. This modular\narchitecture scales seamlessly across image resolutions, ViG layer types, and\nmodel sizes and variants, and supports DIGC across diverse ViG based vision\nbackbones. The design achieves high clock frequencies post place and route due\nto the statically configured parallelism minimizing critical path delay and\ndelivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC\nbaselines."
                },
                "authors": [
                    {
                        "name": "Anvitha Ramachandran"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HPEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04633v2",
                "updated": "2025-09-29T17:40:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    40,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-04T19:51:00Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    19,
                    51,
                    0,
                    3,
                    247,
                    0
                ],
                "title": "The Physical Basis of Prediction: World Model Formation in Neural\n  Organoids via an LLM-Generated Curriculum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Physical Basis of Prediction: World Model Formation in Neural\n  Organoids via an LLM-Generated Curriculum"
                },
                "summary": "The capacity of an embodied agent to understand, predict, and interact with\nits environment is fundamentally contingent on an internal world model. This\npaper introduces a novel framework for investigating the formation and\nadaptation of such world models within a biological substrate: human neural\norganoids. We present a curriculum of three scalable, closed-loop virtual\nenvironments designed to train these biological agents and probe the underlying\nsynaptic mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments that demand progressively more sophisticated world models for\nsuccessful decision-making: (1) a conditional avoidance task for learning\nstatic state-action contingencies, (2) a one-dimensional predator-prey scenario\nfor goal-directed interaction, and (3) a replication of the classic Pong game\nfor modeling dynamic, continuous-time systems. For each environment, we\nformalize the state and action spaces, the sensory encoding and motor decoding\nmechanisms, and the feedback protocols based on predictable (reward) and\nunpredictable (punishment) stimulation, which serve to drive model refinement.\nIn a significant methodological advance, we propose a meta-learning approach\nwhere a Large Language Model automates the generative design and optimization\nof experimental protocols, thereby scaling the process of environment and\ncurriculum design. Finally, we outline a multi-modal evaluation strategy that\nmoves beyond task performance to directly measure the physical correlates of\nthe learned world model by quantifying synaptic plasticity at\nelectrophysiological, cellular, and molecular levels. This work bridges the gap\nbetween model-based reinforcement learning and computational neuroscience,\noffering a unique platform for studying embodiment, decision-making, and the\nphysical basis of intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of an embodied agent to understand, predict, and interact with\nits environment is fundamentally contingent on an internal world model. This\npaper introduces a novel framework for investigating the formation and\nadaptation of such world models within a biological substrate: human neural\norganoids. We present a curriculum of three scalable, closed-loop virtual\nenvironments designed to train these biological agents and probe the underlying\nsynaptic mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments that demand progressively more sophisticated world models for\nsuccessful decision-making: (1) a conditional avoidance task for learning\nstatic state-action contingencies, (2) a one-dimensional predator-prey scenario\nfor goal-directed interaction, and (3) a replication of the classic Pong game\nfor modeling dynamic, continuous-time systems. For each environment, we\nformalize the state and action spaces, the sensory encoding and motor decoding\nmechanisms, and the feedback protocols based on predictable (reward) and\nunpredictable (punishment) stimulation, which serve to drive model refinement.\nIn a significant methodological advance, we propose a meta-learning approach\nwhere a Large Language Model automates the generative design and optimization\nof experimental protocols, thereby scaling the process of environment and\ncurriculum design. Finally, we outline a multi-modal evaluation strategy that\nmoves beyond task performance to directly measure the physical correlates of\nthe learned world model by quantifying synaptic plasticity at\nelectrophysiological, cellular, and molecular levels. This work bridges the gap\nbetween model-based reinforcement learning and computational neuroscience,\noffering a unique platform for studying embodiment, decision-making, and the\nphysical basis of intelligence."
                },
                "authors": [
                    {
                        "name": "Brennen Hill"
                    }
                ],
                "author_detail": {
                    "name": "Brennen Hill"
                },
                "author": "Brennen Hill",
                "arxiv_comment": "Published in the proceedings of the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments\n  for Agents (SEA). Additionally accepted for presentation in NeurIPS 2025\n  Workshop: Embodied World Models for Decision Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T05, 92C20, 93E35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.3; I.6.8; D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25110v1",
                "updated": "2025-09-29T17:39:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:39:51Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    51,
                    0,
                    272,
                    0
                ],
                "title": "gCAMB: A GPU-accelerated Boltzmann solver for next-generation\n  cosmological surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gCAMB: A GPU-accelerated Boltzmann solver for next-generation\n  cosmological surveys"
                },
                "summary": "Inferring cosmological parameters from Cosmic Microwave Background (CMB) data\nrequires repeated and computationally expensive calculations of theoretical\nangular power spectra using Boltzmann solvers like CAMB. This creates a\nsignificant bottleneck, particularly for non-standard cosmological models and\nthe high-accuracy demands of future surveys. While emulators based on deep\nneural networks can accelerate this process by several orders of magnitude,\nthey first require large, pre-computed training datasets, which are costly to\ngenerate and model-specific. To address this challenge, we introduce gCAMB, a\nversion of the CAMB code ported to GPUs, which preserves all the features of\nthe original CPU-only code. By offloading the most computationally intensive\nmodules to the GPU, gCAMB significantly accelerates the generation of power\nspectra, saving massive computational time, halving the power consumption in\nhigh-accuracy settings and, among other purposes, facilitating the creation of\nextensive training sets needed for robust cosmological analyses. We make the\ngCAMB software available to the community at\nhttps://github.com/lstorchi/CAMB/tree/gpuport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring cosmological parameters from Cosmic Microwave Background (CMB) data\nrequires repeated and computationally expensive calculations of theoretical\nangular power spectra using Boltzmann solvers like CAMB. This creates a\nsignificant bottleneck, particularly for non-standard cosmological models and\nthe high-accuracy demands of future surveys. While emulators based on deep\nneural networks can accelerate this process by several orders of magnitude,\nthey first require large, pre-computed training datasets, which are costly to\ngenerate and model-specific. To address this challenge, we introduce gCAMB, a\nversion of the CAMB code ported to GPUs, which preserves all the features of\nthe original CPU-only code. By offloading the most computationally intensive\nmodules to the GPU, gCAMB significantly accelerates the generation of power\nspectra, saving massive computational time, halving the power consumption in\nhigh-accuracy settings and, among other purposes, facilitating the creation of\nextensive training sets needed for robust cosmological analyses. We make the\ngCAMB software available to the community at\nhttps://github.com/lstorchi/CAMB/tree/gpuport."
                },
                "authors": [
                    {
                        "name": "L. Storchi"
                    },
                    {
                        "name": "P. Campeti"
                    },
                    {
                        "name": "M. Lattanzi"
                    },
                    {
                        "name": "N. Antonini"
                    },
                    {
                        "name": "E. Calore"
                    },
                    {
                        "name": "P. Lubrano"
                    }
                ],
                "author_detail": {
                    "name": "P. Lubrano"
                },
                "author": "P. Lubrano",
                "arxiv_comment": "Code available at https://github.com/lstorchi/CAMB/tree/gpuport.\n  Submitted to Astronomy & Computing. 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25107v1",
                "updated": "2025-09-29T17:39:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    19,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:39:19Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    19,
                    0,
                    272,
                    0
                ],
                "title": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant\n  for Question Answering in the Era of LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant\n  for Question Answering in the Era of LLMs?"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings."
                },
                "authors": [
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yin Huang"
                    },
                    {
                        "name": "Srishti Mehra"
                    },
                    {
                        "name": "Mohammad Kachuee"
                    },
                    {
                        "name": "Xilun Chen"
                    },
                    {
                        "name": "Renjie Tao"
                    },
                    {
                        "name": "Zhaojiang Lin"
                    },
                    {
                        "name": "Andrea Jessee"
                    },
                    {
                        "name": "Nirav Shah"
                    },
                    {
                        "name": "Alex Betty"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xin Luna Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Luna Dong"
                },
                "author": "Xin Luna Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04731v2",
                "updated": "2025-09-29T17:38:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    38,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-05T01:03:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    1,
                    3,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "Hierarchical Task Environments as the Next Frontier for Embodied World\n  Models in Robot Soccer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Task Environments as the Next Frontier for Embodied World\n  Models in Robot Soccer"
                },
                "summary": "Recent advances in agent development have focused on scaling model size and\nraw interaction data, mirroring the successes seen in large language models.\nHowever, for complex, long-horizon multi-agent tasks such as robotic soccer,\nthis end-to-end approach often fails due to intractable exploration spaces and\nsparse rewards. This position paper argues that the next frontier in developing\nembodied world models is not merely increasing the fidelity or size of\nenvironments, but scaling their structural complexity through explicit\nhierarchical scaffolding. We posit that an effective world model for\ndecision-making must model not only the world's physics but also its task\nsemantics. Drawing from a systematic review of 2024 research in low-resource\nmulti-agent soccer, we identify a clear trend towards integrating symbolic and\nhierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian\nStrategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These\nmethods decompose complex goals into manageable subgoals, creating an intrinsic\ncurriculum that shapes agent learning. We propose that such structured\nenvironments are essential for bridging the gap between simple, reactive\nbehaviors and sophisticated, strategic team play. We further extend this\nprinciple, proposing that this scaffolding can be generalized to other complex\ndomains and dynamically generated by Large Language Models (LLMs), which act as\ngenerative world models of tasks. By building environments with explicit,\ncomposable task layers, we can guide agent exploration more efficiently,\ngenerate meaningful learning signals, and ultimately train more capable and\ngeneral-purpose agents with fewer resources than purely end-to-end approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in agent development have focused on scaling model size and\nraw interaction data, mirroring the successes seen in large language models.\nHowever, for complex, long-horizon multi-agent tasks such as robotic soccer,\nthis end-to-end approach often fails due to intractable exploration spaces and\nsparse rewards. This position paper argues that the next frontier in developing\nembodied world models is not merely increasing the fidelity or size of\nenvironments, but scaling their structural complexity through explicit\nhierarchical scaffolding. We posit that an effective world model for\ndecision-making must model not only the world's physics but also its task\nsemantics. Drawing from a systematic review of 2024 research in low-resource\nmulti-agent soccer, we identify a clear trend towards integrating symbolic and\nhierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian\nStrategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These\nmethods decompose complex goals into manageable subgoals, creating an intrinsic\ncurriculum that shapes agent learning. We propose that such structured\nenvironments are essential for bridging the gap between simple, reactive\nbehaviors and sophisticated, strategic team play. We further extend this\nprinciple, proposing that this scaffolding can be generalized to other complex\ndomains and dynamically generated by Large Language Models (LLMs), which act as\ngenerative world models of tasks. By building environments with explicit,\ncomposable task layers, we can guide agent exploration more efficiently,\ngenerate meaningful learning signals, and ultimately train more capable and\ngeneral-purpose agents with fewer resources than purely end-to-end approaches."
                },
                "authors": [
                    {
                        "name": "Brennen Hill"
                    }
                ],
                "author_detail": {
                    "name": "Brennen Hill"
                },
                "author": "Brennen Hill",
                "arxiv_comment": "In the 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: Embodied World Models for Decision Making (EWM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 90C40, 91A26, 68T42, 93E35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25100v1",
                "updated": "2025-09-29T17:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    34,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    34,
                    2,
                    0,
                    272,
                    0
                ],
                "title": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation"
                },
                "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines."
                },
                "authors": [
                    {
                        "name": "Aasheesh Singh"
                    },
                    {
                        "name": "Vishal Vaddina"
                    },
                    {
                        "name": "Dagnachew Birru"
                    }
                ],
                "author_detail": {
                    "name": "Dagnachew Birru"
                },
                "author": "Dagnachew Birru",
                "arxiv_comment": "Accepted at NeurIPS 2025, Efficient Reasoning Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25096v1",
                "updated": "2025-09-29T17:30:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    30,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:30:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    30,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "Estimating high-resolution albedo for urban applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating high-resolution albedo for urban applications"
                },
                "summary": "Implementation of cool roofs is a high-impact pathway for mitigating heat at\nboth global and city scales. However, while albedo estimates derived from\nSentinel-2 are free and globally-available, the 10 m resolution is insufficient\nto resolve individual roofs. We present methods for increasing the resolution\nof Sentinel-2 albedo using high-resolution satellite imagery to produce albedo\ninferences at a 30-cm scale. Validating against high-resolution aerial albedo\nmeasurements over Boulder, CO we find improved precision and accuracy relative\nto Sentinel-2 with an RMSE of 0.04. Applying these methods to 12 global cities,\nwe evaluate the impacts of three cool roof implementation scenarios. We find\nthat cities can see up to a 0.5{\\deg}C cooling effect from full scale\nimplementation of cool roofs and prioritizing the largest buildings for\nimplementation is a highly effective policy pathway. While Sentinel-2 produces\naccurate estimates of albedo change at larger scales, high-resolution\ninferences are required for prioritizing buildings based on their solar\nradiation management potential. This research demonstrates a scalable\nimplementation of targeted cool roof interventions in neighborhoods with the\ngreatest potential for heat mitigation by enabling actionable, building-level\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementation of cool roofs is a high-impact pathway for mitigating heat at\nboth global and city scales. However, while albedo estimates derived from\nSentinel-2 are free and globally-available, the 10 m resolution is insufficient\nto resolve individual roofs. We present methods for increasing the resolution\nof Sentinel-2 albedo using high-resolution satellite imagery to produce albedo\ninferences at a 30-cm scale. Validating against high-resolution aerial albedo\nmeasurements over Boulder, CO we find improved precision and accuracy relative\nto Sentinel-2 with an RMSE of 0.04. Applying these methods to 12 global cities,\nwe evaluate the impacts of three cool roof implementation scenarios. We find\nthat cities can see up to a 0.5{\\deg}C cooling effect from full scale\nimplementation of cool roofs and prioritizing the largest buildings for\nimplementation is a highly effective policy pathway. While Sentinel-2 produces\naccurate estimates of albedo change at larger scales, high-resolution\ninferences are required for prioritizing buildings based on their solar\nradiation management potential. This research demonstrates a scalable\nimplementation of targeted cool roof interventions in neighborhoods with the\ngreatest potential for heat mitigation by enabling actionable, building-level\ninsights."
                },
                "authors": [
                    {
                        "name": "David Fork"
                    },
                    {
                        "name": "Elizabeth Jane Wesley"
                    },
                    {
                        "name": "Salil Banerjee"
                    },
                    {
                        "name": "Vishal Batchu"
                    },
                    {
                        "name": "Aniruddh Chennapragada"
                    },
                    {
                        "name": "Kevin Crossan"
                    },
                    {
                        "name": "Bryce Cronkite-Ratcliff"
                    },
                    {
                        "name": "Ellie Delich"
                    },
                    {
                        "name": "Tristan Goulden"
                    },
                    {
                        "name": "Mansi Kansal"
                    },
                    {
                        "name": "Jonas Kemp"
                    },
                    {
                        "name": "Eric Mackres"
                    },
                    {
                        "name": "Yael Mayer"
                    },
                    {
                        "name": "Becca Milman"
                    },
                    {
                        "name": "John C. Platt"
                    },
                    {
                        "name": "Shruthi Prabhakara"
                    },
                    {
                        "name": "Gautam Prasad"
                    },
                    {
                        "name": "Shravya Shetty"
                    },
                    {
                        "name": "Charlotte Stanton"
                    },
                    {
                        "name": "Wayne Sun"
                    },
                    {
                        "name": "Lucy R. Hutyra"
                    }
                ],
                "author_detail": {
                    "name": "Lucy R. Hutyra"
                },
                "author": "Lucy R. Hutyra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25087v1",
                "updated": "2025-09-29T17:26:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    26,
                    11,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:26:11Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    26,
                    11,
                    0,
                    272,
                    0
                ],
                "title": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families"
                },
                "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs."
                },
                "authors": [
                    {
                        "name": "Shane Bergsma"
                    },
                    {
                        "name": "Bin Claire Zhang"
                    },
                    {
                        "name": "Nolan Dey"
                    },
                    {
                        "name": "Shaheer Muhammad"
                    },
                    {
                        "name": "Gurpreet Gosal"
                    },
                    {
                        "name": "Joel Hestness"
                    }
                ],
                "author_detail": {
                    "name": "Joel Hestness"
                },
                "author": "Joel Hestness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25086v1",
                "updated": "2025-09-29T17:25:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    25,
                    56,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:25:56Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    25,
                    56,
                    0,
                    272,
                    0
                ],
                "title": "Towards Trustworthy Lexical Simplification: Exploring Safety and\n  Efficiency with Small LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy Lexical Simplification: Exploring Safety and\n  Efficiency with Small LLMs"
                },
                "summary": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment."
                },
                "authors": [
                    {
                        "name": "Akio Hayakawa"
                    },
                    {
                        "name": "Stefan Bott"
                    },
                    {
                        "name": "Horacio Saggion"
                    }
                ],
                "author_detail": {
                    "name": "Horacio Saggion"
                },
                "author": "Horacio Saggion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25072v1",
                "updated": "2025-09-29T17:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    16,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    16,
                    51,
                    0,
                    272,
                    0
                ],
                "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications"
                },
                "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference."
                },
                "authors": [
                    {
                        "name": "Yaman Jandali"
                    },
                    {
                        "name": "Ruisi Zhang"
                    },
                    {
                        "name": "Nojan Sheybani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25063v1",
                "updated": "2025-09-29T17:12:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    12,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:12:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    12,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for\n  Survey Non-response in the German Longitudinal Election Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for\n  Survey Non-response in the German Longitudinal Election Study"
                },
                "summary": "Survey researchers face two key challenges: the rising costs of probability\nsamples and missing data (e.g., non-response or attrition), which can undermine\ninference and increase the use of convenience samples. Recent work explores\nusing large language models (LLMs) to simulate respondents via persona-based\nprompts, often without labeled data. We study a more practical setting where\npartial survey responses exist: we fine-tune LLMs on available data to impute\nself-reported vote choice under both random and systematic nonresponse, using\nthe German Longitudinal Election Study. We compare zero-shot prompting and\nsupervised fine-tuning against tabular classifiers (e.g., CatBoost) and test\nhow different convenience samples (e.g., students) used for fine-tuning affect\ngeneralization.\n  Our results show that when data are missing completely at random, fine-tuned\nLLMs match tabular classifiers but outperform zero-shot approaches. When only\nbiased convenience samples are available, fine-tuning small (3B to 8B)\nopen-source LLMs can recover both individual-level predictions and\npopulation-level distributions more accurately than zero-shot and often better\nthan tabular methods. This suggests fine-tuned LLMs offer a promising strategy\nfor researchers working with non-probability samples or systematic missingness,\nand may enable new survey designs requiring only easily accessible\nsubpopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey researchers face two key challenges: the rising costs of probability\nsamples and missing data (e.g., non-response or attrition), which can undermine\ninference and increase the use of convenience samples. Recent work explores\nusing large language models (LLMs) to simulate respondents via persona-based\nprompts, often without labeled data. We study a more practical setting where\npartial survey responses exist: we fine-tune LLMs on available data to impute\nself-reported vote choice under both random and systematic nonresponse, using\nthe German Longitudinal Election Study. We compare zero-shot prompting and\nsupervised fine-tuning against tabular classifiers (e.g., CatBoost) and test\nhow different convenience samples (e.g., students) used for fine-tuning affect\ngeneralization.\n  Our results show that when data are missing completely at random, fine-tuned\nLLMs match tabular classifiers but outperform zero-shot approaches. When only\nbiased convenience samples are available, fine-tuning small (3B to 8B)\nopen-source LLMs can recover both individual-level predictions and\npopulation-level distributions more accurately than zero-shot and often better\nthan tabular methods. This suggests fine-tuned LLMs offer a promising strategy\nfor researchers working with non-probability samples or systematic missingness,\nand may enable new survey designs requiring only easily accessible\nsubpopulations."
                },
                "authors": [
                    {
                        "name": "Tobias Holtdirk"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    },
                    {
                        "name": "Arnim Bleier"
                    },
                    {
                        "name": "Claudia Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Wagner"
                },
                "author": "Claudia Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21874v2",
                "updated": "2025-09-29T17:02:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    48,
                    0,
                    272,
                    0
                ],
                "published": "2025-07-29T14:43:46Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    14,
                    43,
                    46,
                    1,
                    210,
                    0
                ],
                "title": "Bayesian Predictive Inference Beyond Martingales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Predictive Inference Beyond Martingales"
                },
                "summary": "There is a growing interest in the so-called Bayesian Predictive Inference\napproach, which allows to perform Bayesian inference without specifying the\nlikelihood and prior of the model, or the need of any MCMC. Instead, only a\nsequence of predictive distributions for the observations is required, and\ninference on the unknown estimand can be performed, cheaply in parallel, using\nbootstrap-type schemes. Understanding which classes of predictive distributions\ncan be used within this framework, is still a key open question. We relax\ncommonly used probabilistic assumptions on the observations, namely\nexchangeability and conditional identical distribution, and on their predictive\ndistributions, being measure-valued martingales, by introducing the new class\nof Almost Conditional Identically Distributed (a.c.i.d.) random variables. This\nclass assumes that the predictive distributions are measure-valued almost\nsupermartingales, and is parametrized by a sequence of parameters\n$(\\xi_n)_{n>0}$, which regulate the decay of conditional dependence among\nfuture observations. Under mild summability assumptions on $(\\xi_n)_{n>0}$, the\nresulting sequence of observations is shown to be asymptotically exchangeable,\nhence amenable to Bayesian Predictive Inference techniques. A.c.i.d. random\nvariables arise naturally in recursive algorithms, and include classic\napproaches in Statistics and Learning Theory, such as kernel estimators, and\nmore novel ones, such as the parametric Bayesian bootstraps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in the so-called Bayesian Predictive Inference\napproach, which allows to perform Bayesian inference without specifying the\nlikelihood and prior of the model, or the need of any MCMC. Instead, only a\nsequence of predictive distributions for the observations is required, and\ninference on the unknown estimand can be performed, cheaply in parallel, using\nbootstrap-type schemes. Understanding which classes of predictive distributions\ncan be used within this framework, is still a key open question. We relax\ncommonly used probabilistic assumptions on the observations, namely\nexchangeability and conditional identical distribution, and on their predictive\ndistributions, being measure-valued martingales, by introducing the new class\nof Almost Conditional Identically Distributed (a.c.i.d.) random variables. This\nclass assumes that the predictive distributions are measure-valued almost\nsupermartingales, and is parametrized by a sequence of parameters\n$(\\xi_n)_{n>0}$, which regulate the decay of conditional dependence among\nfuture observations. Under mild summability assumptions on $(\\xi_n)_{n>0}$, the\nresulting sequence of observations is shown to be asymptotically exchangeable,\nhence amenable to Bayesian Predictive Inference techniques. A.c.i.d. random\nvariables arise naturally in recursive algorithms, and include classic\napproaches in Statistics and Learning Theory, such as kernel estimators, and\nmore novel ones, such as the parametric Bayesian bootstraps."
                },
                "authors": [
                    {
                        "name": "Marco Battiston"
                    },
                    {
                        "name": "Lorenzo Cappello"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cappello"
                },
                "author": "Lorenzo Cappello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25052v1",
                "updated": "2025-09-29T17:02:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    31,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:02:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning"
                },
                "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience."
                },
                "authors": [
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zhongwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongwen Xu"
                },
                "author": "Zhongwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25050v1",
                "updated": "2025-09-29T17:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    20,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:02:20Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    20,
                    0,
                    272,
                    0
                ],
                "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models"
                },
                "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching."
                },
                "authors": [
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Chongjian Ge"
                    },
                    {
                        "name": "Shilong Zhang"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Zhi-Ming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Ming Ma"
                },
                "author": "Zhi-Ming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25048v1",
                "updated": "2025-09-29T17:00:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    0,
                    38,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:00:38Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    0,
                    38,
                    0,
                    272,
                    0
                ],
                "title": "Confidence-Guided Error Correction for Disordered Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Guided Error Correction for Disordered Speech Recognition"
                },
                "summary": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech."
                },
                "authors": [
                    {
                        "name": "Abner Hernandez"
                    },
                    {
                        "name": "Toms Arias Vergara"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Paula Andrea Prez-Toro"
                    }
                ],
                "author_detail": {
                    "name": "Paula Andrea Prez-Toro"
                },
                "author": "Paula Andrea Prez-Toro",
                "arxiv_comment": "Preprint submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25045v1",
                "updated": "2025-09-29T16:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    59,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    59,
                    7,
                    0,
                    272,
                    0
                ],
                "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures"
                },
                "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations."
                },
                "authors": [
                    {
                        "name": "Marco Bronzini"
                    },
                    {
                        "name": "Carlo Nicolini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Andrea Passerini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Passerini"
                },
                "author": "Andrea Passerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25043v1",
                "updated": "2025-09-29T16:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    21,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    21,
                    0,
                    272,
                    0
                ],
                "title": "Large Language Models for Software Testing: A Research Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Software Testing: A Research Roadmap"
                },
                "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field."
                },
                "authors": [
                    {
                        "name": "Cristian Augusto"
                    },
                    {
                        "name": "Antonia Bertolino"
                    },
                    {
                        "name": "Guglielmo De Angelis"
                    },
                    {
                        "name": "Francesca Lonetti"
                    },
                    {
                        "name": "Jess Morn"
                    }
                ],
                "author_detail": {
                    "name": "Jess Morn"
                },
                "author": "Jess Morn",
                "arxiv_comment": "40 pages & 10 figures Submitted on 29th September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02536v2",
                "updated": "2025-09-29T16:58:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T07:20:54Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    7,
                    20,
                    54,
                    1,
                    154,
                    0
                ],
                "title": "Answer Convergence as a Signal for Early Stopping in Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Convergence as a Signal for Early Stopping in Reasoning"
                },
                "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13316v2",
                "updated": "2025-09-29T16:57:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    58,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?"
                },
                "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey can succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets may not be ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the knowledge of the target LLM whose activations are\ndecoded. Taken together, our results indicate a need for targeted benchmarks\nand experimental controls to rigorously assess whether verbalization methods\nprovide meaningful insights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey can succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets may not be ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the knowledge of the target LLM whose activations are\ndecoded. Taken together, our results indicate a need for targeted benchmarks\nand experimental controls to rigorously assess whether verbalization methods\nprovide meaningful insights into the operations of LLMs."
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "37 pages, 6 figures. Updated content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25041v1",
                "updated": "2025-09-29T16:57:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    33,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:57:33Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    33,
                    0,
                    272,
                    0
                ],
                "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference"
                },
                "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Lehan Pan"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Ziyang Tao"
                    },
                    {
                        "name": "Wuyang Zhang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25040v1",
                "updated": "2025-09-29T16:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    4,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    4,
                    0,
                    272,
                    0
                ],
                "title": "A multiscale analysis of mean-field transformers in the moderate\n  interaction regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A multiscale analysis of mean-field transformers in the moderate\n  interaction regime"
                },
                "summary": "In this paper, we study the evolution of tokens through the depth of\nencoder-only transformer models at inference time by modeling them as a system\nof particles interacting in a mean-field way and studying the corresponding\ndynamics. More specifically, we consider this problem in the moderate\ninteraction regime, where the number $N$ of tokens is large and the inverse\ntemperature parameter $\\beta$ of the model scales together with $N$. In this\nregime, the dynamics of the system displays a multiscale behavior: a fast\nphase, where the token empirical measure collapses on a low-dimensional space,\nan intermediate phase, where the measure further collapses into clusters, and a\nslow one, where such clusters sequentially merge into a single one. We provide\na rigorous characterization of the limiting dynamics in each of these phases\nand prove convergence in the above mentioned limit, exemplifying our results\nwith some simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the evolution of tokens through the depth of\nencoder-only transformer models at inference time by modeling them as a system\nof particles interacting in a mean-field way and studying the corresponding\ndynamics. More specifically, we consider this problem in the moderate\ninteraction regime, where the number $N$ of tokens is large and the inverse\ntemperature parameter $\\beta$ of the model scales together with $N$. In this\nregime, the dynamics of the system displays a multiscale behavior: a fast\nphase, where the token empirical measure collapses on a low-dimensional space,\nan intermediate phase, where the measure further collapses into clusters, and a\nslow one, where such clusters sequentially merge into a single one. We provide\na rigorous characterization of the limiting dynamics in each of these phases\nand prove convergence in the above mentioned limit, exemplifying our results\nwith some simulations."
                },
                "authors": [
                    {
                        "name": "Giuseppe Bruno"
                    },
                    {
                        "name": "Federico Pasqualotto"
                    },
                    {
                        "name": "Andrea Agazzi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Agazzi"
                },
                "author": "Andrea Agazzi",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v3",
                "updated": "2025-09-29T16:56:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    56,
                    56,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25035v1",
                "updated": "2025-09-29T16:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    55,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:55:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    55,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct"
                },
                "summary": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct."
                },
                "authors": [
                    {
                        "name": "Haoyang Zheng"
                    },
                    {
                        "name": "Xinyang Liu"
                    },
                    {
                        "name": "Cindy Xiangrui Kong"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Zheyuan Hu"
                    },
                    {
                        "name": "Weijian Luo"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "arxiv_comment": "56 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25034v1",
                "updated": "2025-09-29T16:53:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    53,
                    24,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:53:24Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    53,
                    24,
                    0,
                    272,
                    0
                ],
                "title": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence\n  and LLM Guidance for Reservoir Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence\n  and LLM Guidance for Reservoir Management"
                },
                "summary": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management."
                },
                "authors": [
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Shan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shan Lin"
                },
                "author": "Shan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25033v1",
                "updated": "2025-09-29T16:52:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    52,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:52:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    52,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"
                },
                "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qiangchang Wang"
                    },
                    {
                        "name": "Xianjing Meng"
                    },
                    {
                        "name": "Zhibin Wu"
                    },
                    {
                        "name": "Yilong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Yin"
                },
                "author": "Yilong Yin",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25029v1",
                "updated": "2025-09-29T16:50:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    50,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:50:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    50,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Probing the \\ion{He}{2} re-Ionization ERa via Absorbing \\ion{C}{4}\n  Historical Yield (HIERACHY) IV: A complex redshifted absorption system\n  intrinsic to quasar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the \\ion{He}{2} re-Ionization ERa via Absorbing \\ion{C}{4}\n  Historical Yield (HIERACHY) IV: A complex redshifted absorption system\n  intrinsic to quasar"
                },
                "summary": "High-resolution spectra provide a powerful tool in studying the associated\nabsorption lines (AALs) in quasars. We present a case study of the quasar\nJ014741-030247 at $z \\sim$ 4.75, which hosts complex intrinsic absorption lines\nrevealed by the high-resolution Magellan/MIKE spectrum obtained from the\nHIERACHY program. We focus on one of the strongest absorption systems ($z$\n$\\sim$ 4.7804) and determine the column densities of multiple ionization\nspecies. We find that the Apparent Optical Depth method may significantly\nunderestimate the column densities of high ions. Decomposing the absorption\ninto multiple components yields a better fit and reveals clear evidence of\npartial coverage. The variation in covering fractions among different ions\nsuggests that high ions are distributed more extensively in this system. We\nestimate electron densities of different components ($630 - 4070 \\\n\\mathrm{cm}^{-3}$), these are based on the column densities of \\ion{Si}{2}* and\n\\ion{C}{2}*. By combining these with the hydrogen number density and ionization\nparameter derived from photoionization modeling, we infer that the different\ncomponents are located at distances of 2.3 to 9.5 kpc from the quasar. The\nderived $N_{\\mathrm H} / n_{\\mathrm e}$ and the partial coverage observed in\nlow ions all require cloud sizes smaller than 1 pc, even down to 0.01 pc.\nFinally, the low kinetic luminosity of the gas ($< 0.5\\% L_\\mathrm{bol}$)\nindicates that it is insufficient to drive significant AGN feedback and may\nonly suppress star formation via `multistage' mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution spectra provide a powerful tool in studying the associated\nabsorption lines (AALs) in quasars. We present a case study of the quasar\nJ014741-030247 at $z \\sim$ 4.75, which hosts complex intrinsic absorption lines\nrevealed by the high-resolution Magellan/MIKE spectrum obtained from the\nHIERACHY program. We focus on one of the strongest absorption systems ($z$\n$\\sim$ 4.7804) and determine the column densities of multiple ionization\nspecies. We find that the Apparent Optical Depth method may significantly\nunderestimate the column densities of high ions. Decomposing the absorption\ninto multiple components yields a better fit and reveals clear evidence of\npartial coverage. The variation in covering fractions among different ions\nsuggests that high ions are distributed more extensively in this system. We\nestimate electron densities of different components ($630 - 4070 \\\n\\mathrm{cm}^{-3}$), these are based on the column densities of \\ion{Si}{2}* and\n\\ion{C}{2}*. By combining these with the hydrogen number density and ionization\nparameter derived from photoionization modeling, we infer that the different\ncomponents are located at distances of 2.3 to 9.5 kpc from the quasar. The\nderived $N_{\\mathrm H} / n_{\\mathrm e}$ and the partial coverage observed in\nlow ions all require cloud sizes smaller than 1 pc, even down to 0.01 pc.\nFinally, the low kinetic luminosity of the gas ($< 0.5\\% L_\\mathrm{bol}$)\nindicates that it is insufficient to drive significant AGN feedback and may\nonly suppress star formation via `multistage' mechanism."
                },
                "authors": [
                    {
                        "name": "Huiyang Mao"
                    },
                    {
                        "name": "Jiang-Tao Li"
                    },
                    {
                        "name": "Xiaodi Yu"
                    },
                    {
                        "name": "Zhijie Qu"
                    },
                    {
                        "name": "Weizhe Liu"
                    },
                    {
                        "name": "Li Ji"
                    }
                ],
                "author_detail": {
                    "name": "Li Ji"
                },
                "author": "Li Ji",
                "arxiv_comment": "18 pages, 19 figures, accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00228v2",
                "updated": "2025-09-29T16:47:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    47,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-29T20:32:08Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    32,
                    8,
                    4,
                    241,
                    0
                ],
                "title": "On the Use of Weighting for Personalized and Transparent Evidence\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Use of Weighting for Personalized and Transparent Evidence\n  Synthesis"
                },
                "summary": "Over the past few decades, statistical methods for causal inference have made\nimpressive strides, enabling progress across a range of scientific fields.\nHowever, much of this methodological development has been confined to\nindividual studies, limiting its ability to draw more generalizable\nconclusions. Achieving a thorough understanding of cause and effect typically\nrelies on the integration, reconciliation, and synthesis from diverse study\ndesigns and multiple data sources. Furthermore, it is crucial to direct this\nsynthesis effort toward understanding the effect of treatments for specific\npatient populations. To address these challenges, we present a weighting\nframework for evidence synthesis that handles both individual- and\naggregate-level data, encompassing and extending conventional regression-based\nmeta-analysis methods. We use this approach to tailor meta-analyses, targeting\nthe covariate profiles of patients in a target population in a sample-bounded\nmanner, thereby enhancing their personalization and robustness. We propose a\ntechnique to detect studies that meaningfully deviate from the target\npopulation, suggesting when it might be prudent to exclude them from the\nanalysis. We establish multiple consistency conditions and demonstrate\nasymptotic normality for the proposed estimator. We demonstrate the\neffectiveness of the method through a simulation study and two real-world case\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few decades, statistical methods for causal inference have made\nimpressive strides, enabling progress across a range of scientific fields.\nHowever, much of this methodological development has been confined to\nindividual studies, limiting its ability to draw more generalizable\nconclusions. Achieving a thorough understanding of cause and effect typically\nrelies on the integration, reconciliation, and synthesis from diverse study\ndesigns and multiple data sources. Furthermore, it is crucial to direct this\nsynthesis effort toward understanding the effect of treatments for specific\npatient populations. To address these challenges, we present a weighting\nframework for evidence synthesis that handles both individual- and\naggregate-level data, encompassing and extending conventional regression-based\nmeta-analysis methods. We use this approach to tailor meta-analyses, targeting\nthe covariate profiles of patients in a target population in a sample-bounded\nmanner, thereby enhancing their personalization and robustness. We propose a\ntechnique to detect studies that meaningfully deviate from the target\npopulation, suggesting when it might be prudent to exclude them from the\nanalysis. We establish multiple consistency conditions and demonstrate\nasymptotic normality for the proposed estimator. We demonstrate the\neffectiveness of the method through a simulation study and two real-world case\nstudies."
                },
                "authors": [
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Jos R. Zubizarreta"
                    }
                ],
                "author_detail": {
                    "name": "Jos R. Zubizarreta"
                },
                "author": "Jos R. Zubizarreta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00031v2",
                "updated": "2025-09-29T16:45:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    45,
                    41,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-21T01:18:27Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    1,
                    18,
                    27,
                    3,
                    233,
                    0
                ],
                "title": "End-to-End On-Device Quantization-Aware Training for LLMs at Inference\n  Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End On-Device Quantization-Aware Training for LLMs at Inference\n  Cost"
                },
                "summary": "Quantization is an effective technique to reduce the deployment cost of large\nlanguage models (LLMs), and post-training quantization (PTQ) has been widely\nstudied due to its efficiency. However, existing PTQ methods are limited by\ntheir inability to fine-tune model parameters and often suffer significant\naccuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides\na more principled solution, but its reliance on backpropagation incurs\nprohibitive memory costs, limiting its practicality for LLM deployment. To\naddress these challenges, we propose ZeroQAT, a zeroth-order optimization-based\nQAT framework that supports both weight and activation quantization. ZeroQAT\nleverages forward-only gradient estimation to eliminate backpropagation,\nsubstantially reducing computational and memory overhead while retaining the\nbenefits of end-to-end optimization. We further introduce a lightweight variant\nof ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most\nparameters to further cut memory usage. Experiments show that ZeroQAT\nconsistently outperforms representative PTQ and QAT baselines while requiring\nsignificantly less memory. For example, ZeroQAT enables fine-tuning of a 13B\nmodel at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and\neven allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating\nits practicality for end-to-end QAT on resource-limited edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is an effective technique to reduce the deployment cost of large\nlanguage models (LLMs), and post-training quantization (PTQ) has been widely\nstudied due to its efficiency. However, existing PTQ methods are limited by\ntheir inability to fine-tune model parameters and often suffer significant\naccuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides\na more principled solution, but its reliance on backpropagation incurs\nprohibitive memory costs, limiting its practicality for LLM deployment. To\naddress these challenges, we propose ZeroQAT, a zeroth-order optimization-based\nQAT framework that supports both weight and activation quantization. ZeroQAT\nleverages forward-only gradient estimation to eliminate backpropagation,\nsubstantially reducing computational and memory overhead while retaining the\nbenefits of end-to-end optimization. We further introduce a lightweight variant\nof ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most\nparameters to further cut memory usage. Experiments show that ZeroQAT\nconsistently outperforms representative PTQ and QAT baselines while requiring\nsignificantly less memory. For example, ZeroQAT enables fine-tuning of a 13B\nmodel at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and\neven allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating\nits practicality for end-to-end QAT on resource-limited edge devices."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Xiaoying Song"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Lingzi Hong"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25020v1",
                "updated": "2025-09-29T16:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    44,
                    22,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    44,
                    22,
                    0,
                    272,
                    0
                ],
                "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts"
                },
                "summary": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Ning Miao"
                    }
                ],
                "author_detail": {
                    "name": "Ning Miao"
                },
                "author": "Ning Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25009v1",
                "updated": "2025-09-29T16:35:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    35,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:35:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    35,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "Efficient Difference-in-Differences Estimation when Outcomes are Missing\n  at Random",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Difference-in-Differences Estimation when Outcomes are Missing\n  at Random"
                },
                "summary": "The Difference-in-Differences (DiD) method is a fundamental tool for causal\ninference, yet its application is often complicated by missing data. Although\nrecent work has developed robust DiD estimators for complex settings like\nstaggered treatment adoption, these methods typically assume complete data and\nfail to address the critical challenge of outcomes that are missing at random\n(MAR) -- a common problem that invalidates standard estimators. We develop a\nrigorous framework, rooted in semiparametric theory, for identifying and\nefficiently estimating the Average Treatment Effect on the Treated (ATT) when\neither pre- or post-treatment (or both) outcomes are missing at random. We\nfirst establish nonparametric identification of the ATT under two minimal sets\nof sufficient conditions. For each, we derive the semiparametric efficiency\nbound, which provides a formal benchmark for asymptotic optimality. We then\npropose novel estimators that are asymptotically efficient, achieving this\ntheoretical bound. A key feature of our estimators is their multiple\nrobustness, which ensures consistency even if some nuisance function models are\nmisspecified. We validate the properties of our estimators and showcase their\nbroad applicability through an extensive simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Difference-in-Differences (DiD) method is a fundamental tool for causal\ninference, yet its application is often complicated by missing data. Although\nrecent work has developed robust DiD estimators for complex settings like\nstaggered treatment adoption, these methods typically assume complete data and\nfail to address the critical challenge of outcomes that are missing at random\n(MAR) -- a common problem that invalidates standard estimators. We develop a\nrigorous framework, rooted in semiparametric theory, for identifying and\nefficiently estimating the Average Treatment Effect on the Treated (ATT) when\neither pre- or post-treatment (or both) outcomes are missing at random. We\nfirst establish nonparametric identification of the ATT under two minimal sets\nof sufficient conditions. For each, we derive the semiparametric efficiency\nbound, which provides a formal benchmark for asymptotic optimality. We then\npropose novel estimators that are asymptotically efficient, achieving this\ntheoretical bound. A key feature of our estimators is their multiple\nrobustness, which ensures consistency even if some nuisance function models are\nmisspecified. We validate the properties of our estimators and showcase their\nbroad applicability through an extensive simulation study."
                },
                "authors": [
                    {
                        "name": "Lorenzo Testa"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Matthew Reimherr"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Reimherr"
                },
                "author": "Matthew Reimherr",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20086v2",
                "updated": "2025-09-29T16:30:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    30,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-27T17:54:15Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    54,
                    15,
                    2,
                    239,
                    0
                ],
                "title": "Smart Contract Intent Detection with Pre-trained Programming Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Contract Intent Detection with Pre-trained Programming Language\n  Model"
                },
                "summary": "Malicious developer intents in smart contracts constitute a significant\nsecurity threat in decentralized applications (DApps), leading to substantial\neconomic losses. To address this, SmartIntentNN was previously introduced as a\ndeep learning model for detecting unsafe developer intents. It integrates the\nUniversal Sentence Encoder, K-means clustering-based intent highlighting, and a\nBidirectional Long Short-Term Memory (BiLSTM) network for multi-label\nclassification, achieving an F1 score of 0.8633.\n  In this study, we present an enhanced version of this model, SmartIntentNN2\n(Smart Contract Intent Neural Network V2). The primary enhancement is the\nintegration of a BERT-based pre-trained programming language model, which we\ndomain-adaptively pre-train on a dataset of 16,000 real-world smart contracts\nusing a Masked Language Modeling (MLM) objective. SmartIntentNN2 retains the\nBiLSTM-based multi-label classification network for downstream tasks.\nExperimental results demonstrate that SmartIntentNN2 achieves superior overall\nperformance with an accuracy of 0.9789, precision of 0.9090, recall of 0.9476,\nand an F1 score of 0.9279, substantially outperforming its predecessor and\nother baseline models. Notably, SmartIntentNN2 also shows significant\nadvantages over large language models (LLMs), achieving a 65.5% relative\nimprovement in F1 score over GPT-4.1 on this specialized task. These results\nestablish SmartIntentNN2 as the new state-of-the-art model for smart contract\nintent detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious developer intents in smart contracts constitute a significant\nsecurity threat in decentralized applications (DApps), leading to substantial\neconomic losses. To address this, SmartIntentNN was previously introduced as a\ndeep learning model for detecting unsafe developer intents. It integrates the\nUniversal Sentence Encoder, K-means clustering-based intent highlighting, and a\nBidirectional Long Short-Term Memory (BiLSTM) network for multi-label\nclassification, achieving an F1 score of 0.8633.\n  In this study, we present an enhanced version of this model, SmartIntentNN2\n(Smart Contract Intent Neural Network V2). The primary enhancement is the\nintegration of a BERT-based pre-trained programming language model, which we\ndomain-adaptively pre-train on a dataset of 16,000 real-world smart contracts\nusing a Masked Language Modeling (MLM) objective. SmartIntentNN2 retains the\nBiLSTM-based multi-label classification network for downstream tasks.\nExperimental results demonstrate that SmartIntentNN2 achieves superior overall\nperformance with an accuracy of 0.9789, precision of 0.9090, recall of 0.9476,\nand an F1 score of 0.9279, substantially outperforming its predecessor and\nother baseline models. Notably, SmartIntentNN2 also shows significant\nadvantages over large language models (LLMs), achieving a 65.5% relative\nimprovement in F1 score over GPT-4.1 on this specialized task. These results\nestablish SmartIntentNN2 as the new state-of-the-art model for smart contract\nintent detection."
                },
                "authors": [
                    {
                        "name": "Youwei Huang"
                    },
                    {
                        "name": "Jianwen Li"
                    },
                    {
                        "name": "Sen Fang"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "arxiv_comment": "11 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25004v1",
                "updated": "2025-09-29T16:29:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    29,
                    4,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:29:04Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    29,
                    4,
                    0,
                    272,
                    0
                ],
                "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning"
                },
                "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models."
                },
                "authors": [
                    {
                        "name": "Shijie Zhang"
                    },
                    {
                        "name": "Guohao Sun"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Xiang Guo"
                    },
                    {
                        "name": "Rujun Guo"
                    }
                ],
                "author_detail": {
                    "name": "Rujun Guo"
                },
                "author": "Rujun Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25003v1",
                "updated": "2025-09-29T16:28:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    28,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:28:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    28,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Score-based Membership Inference on Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score-based Membership Inference on Diffusion Models"
                },
                "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess"
                },
                "authors": [
                    {
                        "name": "Mingxing Rao"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Daniel Moyer"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Moyer"
                },
                "author": "Daniel Moyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13257v2",
                "updated": "2025-09-29T16:23:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    23,
                    6,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-19T15:39:48Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    39,
                    48,
                    0,
                    139,
                    0
                ],
                "title": "Is Active Persona Inference Necessary for Aligning Small Models to\n  Personal Preferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Active Persona Inference Necessary for Aligning Small Models to\n  Personal Preferences?"
                },
                "summary": "A prominent issue in aligning language models (LMs) to personalized\npreferences is underspecification -- the lack of information from users about\ntheir preferences. A popular trend of injecting such specification is adding a\nprefix (e.g. prior relevant conversations) to the current user's conversation\nto steer preference distribution. Most methods passively model personal\npreferences with prior example preferences pairs. We ask whether models benefit\nfrom actively inferring preference descriptions, and address this question by\ncreating a synthetic personalized alignment dataset based on famous people with\nknown public preferences. We then test how effective finetuned 1-8B size models\nare at inferring and aligning to personal preferences. Results show that\nhigher-quality active prefixes lead to better generalization, more contextually\nfaithful models, and less systematic biases across different protected\nattributes. All our results suggest active alignment can lead to a more\ncontrollable and efficient path for personalized alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prominent issue in aligning language models (LMs) to personalized\npreferences is underspecification -- the lack of information from users about\ntheir preferences. A popular trend of injecting such specification is adding a\nprefix (e.g. prior relevant conversations) to the current user's conversation\nto steer preference distribution. Most methods passively model personal\npreferences with prior example preferences pairs. We ask whether models benefit\nfrom actively inferring preference descriptions, and address this question by\ncreating a synthetic personalized alignment dataset based on famous people with\nknown public preferences. We then test how effective finetuned 1-8B size models\nare at inferring and aligning to personal preferences. Results show that\nhigher-quality active prefixes lead to better generalization, more contextually\nfaithful models, and less systematic biases across different protected\nattributes. All our results suggest active alignment can lead to a more\ncontrollable and efficient path for personalized alignment."
                },
                "authors": [
                    {
                        "name": "Zilu Tang"
                    },
                    {
                        "name": "Afra Feyza Akyrek"
                    },
                    {
                        "name": "Ekin Akyrek"
                    },
                    {
                        "name": "Derry Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Derry Wijaya"
                },
                "author": "Derry Wijaya",
                "arxiv_comment": "9 pages, EMNLP PALS workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04690v2",
                "updated": "2025-09-29T16:20:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    20,
                    58,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-05T07:13:59Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    7,
                    13,
                    59,
                    3,
                    156,
                    0
                ],
                "title": "Towards Better Generalization via Distributional Input Projection\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Generalization via Distributional Input Projection\n  Network"
                },
                "summary": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning."
                },
                "authors": [
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yanxin Lu"
                    },
                    {
                        "name": "Hanning Zhang"
                    },
                    {
                        "name": "Xinwei Shen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24993v1",
                "updated": "2025-09-29T16:19:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:19:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "Unified laboratory-frame analysis of atomic gravitational-wave sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified laboratory-frame analysis of atomic gravitational-wave sensors"
                },
                "summary": "Atomic sensors using light-matter interactions, in particular atomic clocks\nand atom interferometers, have the potential to complement optical\ngravitational-wave detectors in the mid-frequency regime. Although both rely on\ninterference, the interfering components of clocks are spatially colocated,\nwhereas atom interferometers are based on spatial superpositions. Both the\nelectromagnetic fields that drive the transitions and generate superpositions,\nwhile propagating through spacetime, as well as the atoms themselves as massive\nparticles are influenced by gravitational waves, leading to effective\npotentials that induce phase differences inferred by the sensor. In this work,\nwe analyze the effects of these potentials on atomic clocks and atom\ninterferometers in the laboratory frame. We show that spatial superpositions in\natom interferometers, both light-pulse and guided ones, give rise to a\ngravitational-wave signal. Although these spatial superpositions are suppressed\nfor clocks, we show that the light pulses driving internal transitions measure\nthe spatial distance between the centers of two separate clocks. We highlight\nthat this mechanism only yields a sensitivity if both clocks, including\npossible trapping setups, move on geodesics given by the gravitational wave.\nWhile such configurations are natural for satellite free-fliers, terrestrial\noptical clocks normally rely on stationary traps, rendering them insensitive to\nleading order. Moreover, we show that both sensors can be enhanced by composite\ninterrogation protocols in a common framework. To this end, we propose a pulse\nsequence that can be used for large-momentum-transfer atom interferometers and\nfor hyper-echo atomic clocks, leading to a signal enhancement and noise\nsuppression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic sensors using light-matter interactions, in particular atomic clocks\nand atom interferometers, have the potential to complement optical\ngravitational-wave detectors in the mid-frequency regime. Although both rely on\ninterference, the interfering components of clocks are spatially colocated,\nwhereas atom interferometers are based on spatial superpositions. Both the\nelectromagnetic fields that drive the transitions and generate superpositions,\nwhile propagating through spacetime, as well as the atoms themselves as massive\nparticles are influenced by gravitational waves, leading to effective\npotentials that induce phase differences inferred by the sensor. In this work,\nwe analyze the effects of these potentials on atomic clocks and atom\ninterferometers in the laboratory frame. We show that spatial superpositions in\natom interferometers, both light-pulse and guided ones, give rise to a\ngravitational-wave signal. Although these spatial superpositions are suppressed\nfor clocks, we show that the light pulses driving internal transitions measure\nthe spatial distance between the centers of two separate clocks. We highlight\nthat this mechanism only yields a sensitivity if both clocks, including\npossible trapping setups, move on geodesics given by the gravitational wave.\nWhile such configurations are natural for satellite free-fliers, terrestrial\noptical clocks normally rely on stationary traps, rendering them insensitive to\nleading order. Moreover, we show that both sensors can be enhanced by composite\ninterrogation protocols in a common framework. To this end, we propose a pulse\nsequence that can be used for large-momentum-transfer atom interferometers and\nfor hyper-echo atomic clocks, leading to a signal enhancement and noise\nsuppression."
                },
                "authors": [
                    {
                        "name": "Simon Schaffrath"
                    },
                    {
                        "name": "Daniel Strk"
                    },
                    {
                        "name": "Fabio Di Pumpo"
                    },
                    {
                        "name": "Enno Giese"
                    }
                ],
                "author_detail": {
                    "name": "Enno Giese"
                },
                "author": "Enno Giese",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24988v1",
                "updated": "2025-09-29T16:19:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:19:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns"
                },
                "summary": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection."
                },
                "authors": [
                    {
                        "name": "Hanqi Xiao"
                    },
                    {
                        "name": "Vaidehi Patil"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Code:\n  https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08753v2",
                "updated": "2025-09-29T16:17:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    17,
                    12,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-10T16:43:01Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    43,
                    1,
                    2,
                    253,
                    0
                ],
                "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"
                },
                "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling"
                },
                "authors": [
                    {
                        "name": "Neil Zeghidour"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Manu Orsini"
                    },
                    {
                        "name": "Vclav Volhejn"
                    },
                    {
                        "name": "Gabriel de Marmiesse"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Patrick Prez"
                    },
                    {
                        "name": "Laurent Mazar"
                    },
                    {
                        "name": "Alexandre Dfossez"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Dfossez"
                },
                "author": "Alexandre Dfossez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19580v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19580v3",
                "updated": "2025-09-29T16:13:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    13,
                    52,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-23T21:09:24Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    9,
                    24,
                    1,
                    266,
                    0
                ],
                "title": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines"
                },
                "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Shifu Hou"
                    },
                    {
                        "name": "Weixiang Sun"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Yijun Ma"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Steven Corcelli"
                    },
                    {
                        "name": "Robert Goulding"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "John Lalor"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Tengfei Luo"
                    },
                    {
                        "name": "Ed Maginn"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Jason Rohr"
                    },
                    {
                        "name": "Brett Savoie"
                    },
                    {
                        "name": "Daniel Slate"
                    },
                    {
                        "name": "Tom Stapleford"
                    },
                    {
                        "name": "Matthew Webber"
                    },
                    {
                        "name": "Olaf Wiest"
                    },
                    {
                        "name": "Johnny Zhang"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19580v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19580v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24981v1",
                "updated": "2025-09-29T16:09:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    9,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:09:07Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    9,
                    7,
                    0,
                    272,
                    0
                ],
                "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards"
                },
                "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods."
                },
                "authors": [
                    {
                        "name": "Haoran He"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Ling Pan"
                    }
                ],
                "author_detail": {
                    "name": "Ling Pan"
                },
                "author": "Ling Pan",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24978v2",
                "updated": "2025-09-30T08:50:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    8,
                    50,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T16:07:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    7,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Agentic Exploration of Physics Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Exploration of Physics Models"
                },
                "summary": "The process of scientific discovery relies on an interplay of observations,\nanalysis, and hypothesis generation. Machine learning is increasingly being\nadopted to address individual aspects of this process. However, it remains an\nopen challenge to fully automate the open-ended, heuristic, iterative loop\nrequired to discover the laws of an unknown system by exploring it through\nexperiments and analysis, without tailoring the approach to the specifics of a\ngiven task. Here, we introduce SciExplorer, an agent that leverages large\nlanguage model tool-use capabilities to enable free-form exploration of systems\nwithout any domain-specific blueprints, and apply it to the exploration of\nphysical systems that are initially unknown to the agent. We test SciExplorer\non a broad set of models spanning mechanical dynamical systems, wave evolution,\nand quantum many-body physics. Despite using a minimal set of tools, primarily\nbased on code execution, we observe impressive performance on tasks such as\nrecovering equations of motion from observed dynamics and inferring\nHamiltonians from expectation values. The demonstrated effectiveness of this\nsetup opens the door towards similar scientific exploration in other domains,\nwithout the need for finetuning or task-specific instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of scientific discovery relies on an interplay of observations,\nanalysis, and hypothesis generation. Machine learning is increasingly being\nadopted to address individual aspects of this process. However, it remains an\nopen challenge to fully automate the open-ended, heuristic, iterative loop\nrequired to discover the laws of an unknown system by exploring it through\nexperiments and analysis, without tailoring the approach to the specifics of a\ngiven task. Here, we introduce SciExplorer, an agent that leverages large\nlanguage model tool-use capabilities to enable free-form exploration of systems\nwithout any domain-specific blueprints, and apply it to the exploration of\nphysical systems that are initially unknown to the agent. We test SciExplorer\non a broad set of models spanning mechanical dynamical systems, wave evolution,\nand quantum many-body physics. Despite using a minimal set of tools, primarily\nbased on code execution, we observe impressive performance on tasks such as\nrecovering equations of motion from observed dynamics and inferring\nHamiltonians from expectation values. The demonstrated effectiveness of this\nsetup opens the door towards similar scientific exploration in other domains,\nwithout the need for finetuning or task-specific instructions."
                },
                "authors": [
                    {
                        "name": "Maximilian Ngele"
                    },
                    {
                        "name": "Florian Marquardt"
                    }
                ],
                "author_detail": {
                    "name": "Florian Marquardt"
                },
                "author": "Florian Marquardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24977v1",
                "updated": "2025-09-29T16:06:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    6,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:06:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    6,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "The Limits of Inference in Complex Systems: When Stochastic Models\n  Become Indistinguishable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Inference in Complex Systems: When Stochastic Models\n  Become Indistinguishable"
                },
                "summary": "Robust inference methods are essential for parameter estimation and model\nselection in stochastic modeling approaches, which provide interpretable\nframeworks for describing time series of complex phenomena. However, applying\nsuch methods is often challenging, as they typically demand either\nhigh-frequency observations or access to the model's analytical solution,\nresources that are rarely available in practice. Here, we address these\nlimitations by designing a novel Monte Carlo method based on full-path\nstatistics and bridge processes, which optimize sampling efforts and\nperformance even under coarse sampling. We systematically investigate how\nexperimental design -- particularly sampling frequency and dataset size --\nshapes inference accuracy, revealing optimal sampling regimes and the\nfundamental limits of model distinguishability. We validate our approach on\nfour datasets -- optical tweezers, human microbiome, topic mentions in social\nmedia, and forest population dynamics -- where resolution-dependent limits to\ninference emerge, offering fresh insight into ongoing debates about the\ndominant sources of noise in these systems. Overall, this work shows how\ncombining minimal stochastic models with path-inference tools and model\nselection can guide the experimental design of optimized strategies in data\ncollection and clarify the boundaries of model-based understanding in complex\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust inference methods are essential for parameter estimation and model\nselection in stochastic modeling approaches, which provide interpretable\nframeworks for describing time series of complex phenomena. However, applying\nsuch methods is often challenging, as they typically demand either\nhigh-frequency observations or access to the model's analytical solution,\nresources that are rarely available in practice. Here, we address these\nlimitations by designing a novel Monte Carlo method based on full-path\nstatistics and bridge processes, which optimize sampling efforts and\nperformance even under coarse sampling. We systematically investigate how\nexperimental design -- particularly sampling frequency and dataset size --\nshapes inference accuracy, revealing optimal sampling regimes and the\nfundamental limits of model distinguishability. We validate our approach on\nfour datasets -- optical tweezers, human microbiome, topic mentions in social\nmedia, and forest population dynamics -- where resolution-dependent limits to\ninference emerge, offering fresh insight into ongoing debates about the\ndominant sources of noise in these systems. Overall, this work shows how\ncombining minimal stochastic models with path-inference tools and model\nselection can guide the experimental design of optimized strategies in data\ncollection and clarify the boundaries of model-based understanding in complex\nsystems."
                },
                "authors": [
                    {
                        "name": "Javier Aguilar"
                    },
                    {
                        "name": "Miguel A. Muoz"
                    },
                    {
                        "name": "Sandro Azaele"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Azaele"
                },
                "author": "Sandro Azaele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24975v1",
                "updated": "2025-09-29T16:04:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:04:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern"
                },
                "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open ."
                },
                "authors": [
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Yuetong Liu"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19923v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19923v3",
                "updated": "2025-09-29T16:01:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    1,
                    54,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-24T18:01:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    18,
                    1,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs"
                },
                "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas. These auxiliary\nlemmas are not limited to subgoals in the formal proof but can also include\nspecial cases or potentially useful facts derived from the assumptions, which\nhelp in discovering a viable proof strategy. It achieves an 88.1% success rate\non the MiniF2F benchmark, establishing a new state-of-the-art among methods\nusing small language models (SLMs) with a much lower sample budget than\nprevious approaches. We also present theoretical analyses and case studies that\nillustrate how these generated lemmas contribute to solving challenging\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas. These auxiliary\nlemmas are not limited to subgoals in the formal proof but can also include\nspecial cases or potentially useful facts derived from the assumptions, which\nhelp in discovering a viable proof strategy. It achieves an 88.1% success rate\non the MiniF2F benchmark, establishing a new state-of-the-art among methods\nusing small language models (SLMs) with a much lower sample budget than\nprevious approaches. We also present theoretical analyses and case studies that\nillustrate how these generated lemmas contribute to solving challenging\nproblems."
                },
                "authors": [
                    {
                        "name": "Kaito Baba"
                    },
                    {
                        "name": "Chaoran Liu"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Akiyoshi Sannai"
                    }
                ],
                "author_detail": {
                    "name": "Akiyoshi Sannai"
                },
                "author": "Akiyoshi Sannai",
                "arxiv_comment": "36 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19923v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19923v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24967v1",
                "updated": "2025-09-29T16:00:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    0,
                    41,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:00:41Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    0,
                    41,
                    0,
                    272,
                    0
                ],
                "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecInfer: Preventing Prompt Injection via Inference-time Scaling"
                },
                "summary": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24962v1",
                "updated": "2025-09-29T15:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    56,
                    24,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:56:24Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    56,
                    24,
                    0,
                    272,
                    0
                ],
                "title": "Overlap-Adaptive Regularization for Conditional Average Treatment Effect\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlap-Adaptive Regularization for Conditional Average Treatment Effect\n  Estimation"
                },
                "summary": "The conditional average treatment effect (CATE) is widely used in\npersonalized medicine to inform therapeutic decisions. However,\nstate-of-the-art methods for CATE estimation (so-called meta-learners) often\nperform poorly in the presence of low overlap. In this work, we introduce a new\napproach to tackle this issue and improve the performance of existing\nmeta-learners in the low-overlap regions. Specifically, we introduce\nOverlap-Adaptive Regularization (OAR) that regularizes target models\nproportionally to overlap weights so that, informally, the regularization is\nhigher in regions with low overlap. To the best of our knowledge, our OAR is\nthe first approach to leverage overlap weights in the regularization terms of\nthe meta-learners. Our OAR approach is flexible and works with any existing\nCATE meta-learner: we demonstrate how OAR can be applied to both parametric and\nnon-parametric second-stage models. Furthermore, we propose debiased versions\nof our OAR that preserve the Neyman-orthogonality of existing meta-learners and\nthus ensure more robust inference. Through a series of (semi-)synthetic\nexperiments, we demonstrate that our OAR significantly improves CATE estimation\nin low-overlap settings in comparison to constant regularization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conditional average treatment effect (CATE) is widely used in\npersonalized medicine to inform therapeutic decisions. However,\nstate-of-the-art methods for CATE estimation (so-called meta-learners) often\nperform poorly in the presence of low overlap. In this work, we introduce a new\napproach to tackle this issue and improve the performance of existing\nmeta-learners in the low-overlap regions. Specifically, we introduce\nOverlap-Adaptive Regularization (OAR) that regularizes target models\nproportionally to overlap weights so that, informally, the regularization is\nhigher in regions with low overlap. To the best of our knowledge, our OAR is\nthe first approach to leverage overlap weights in the regularization terms of\nthe meta-learners. Our OAR approach is flexible and works with any existing\nCATE meta-learner: we demonstrate how OAR can be applied to both parametric and\nnon-parametric second-stage models. Furthermore, we propose debiased versions\nof our OAR that preserve the Neyman-orthogonality of existing meta-learners and\nthus ensure more robust inference. Through a series of (semi-)synthetic\nexperiments, we demonstrate that our OAR significantly improves CATE estimation\nin low-overlap settings in comparison to constant regularization."
                },
                "authors": [
                    {
                        "name": "Valentyn Melnychuk"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04952v2",
                "updated": "2025-09-29T15:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    55,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-07-07T12:53:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation"
                },
                "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24961v1",
                "updated": "2025-09-29T15:53:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    53,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:53:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    53,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in\n  Recommender Systems"
                },
                "summary": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield."
                },
                "authors": [
                    {
                        "name": "Kaihong Li"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Fangjun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fangjun Huang"
                },
                "author": "Fangjun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24958v1",
                "updated": "2025-09-29T15:52:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    36,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:52:36Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    36,
                    0,
                    272,
                    0
                ],
                "title": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents'\n  Inquiry Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents'\n  Inquiry Capability"
                },
                "summary": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings."
                },
                "authors": [
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Yunghwei Lai"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24957v1",
                "updated": "2025-09-29T15:52:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    8,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:52:08Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    8,
                    0,
                    272,
                    0
                ],
                "title": "Intra-request branch orchestration for efficient LLM reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-request branch orchestration for efficient LLM reasoning"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates."
                },
                "authors": [
                    {
                        "name": "Weifan Jiang"
                    },
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24956v1",
                "updated": "2025-09-29T15:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    50,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:50:51Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    50,
                    51,
                    0,
                    272,
                    0
                ],
                "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic\n  Manipulation"
                },
                "summary": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de."
                },
                "authors": [
                    {
                        "name": "Jan Ole von Hartz"
                    },
                    {
                        "name": "Lukas Schweizer"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10774v2",
                "updated": "2025-09-29T15:46:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    46,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-14T15:58:59Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    58,
                    59,
                    3,
                    226,
                    0
                ],
                "title": "BLADE: Block-Sparse Attention Meets Step Distillation for Efficient\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLADE: Block-Sparse Attention Meets Step Distillation for Efficient\n  Video Generation"
                },
                "summary": "Diffusion Transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm, built upon Trajectory Distribution Matching (TDM),\ndirectly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step and features fast convergence. We\nvalidate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B, and\nour framework demonstrates remarkable efficiency gains across different scales.\nOn Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over\na 50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Project is available at\nhttp://ziplab.co/BLADE-Homepage/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm, built upon Trajectory Distribution Matching (TDM),\ndirectly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step and features fast convergence. We\nvalidate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B, and\nour framework demonstrates remarkable efficiency gains across different scales.\nOn Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over\na 50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Project is available at\nhttp://ziplab.co/BLADE-Homepage/."
                },
                "authors": [
                    {
                        "name": "Youping Gu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Minqi Chen"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24952v1",
                "updated": "2025-09-29T15:46:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    46,
                    53,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:46:53Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    46,
                    53,
                    0,
                    272,
                    0
                ],
                "title": "De novo peptide sequencing rescoring and FDR estimation with Winnow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "De novo peptide sequencing rescoring and FDR estimation with Winnow"
                },
                "summary": "Machine learning has markedly advanced de novo peptide sequencing (DNS) for\nmass spectrometry-based proteomics. DNS tools offer a reliable way to identify\npeptides without relying on reference databases, extending proteomic analysis\nand unlocking applications into less-charted regions of the proteome. However,\nthey still face a key limitation. DNS tools lack principled methods for\nestimating false discovery rates (FDR) and instead rely on model-specific\nconfidence scores that are often miscalibrated. This limits trust in results,\nhinders cross-model comparisons and reduces validation success. Here we present\nWinnow, a model-agnostic framework for estimating FDR from calibrated DNS\noutputs. Winnow maps raw model scores to calibrated confidences using a neural\nnetwork trained on peptide-spectrum match (PSM)-derived features. From these\ncalibrated scores, Winnow computes PSM-specific error metrics and an\nexperiment-wide FDR estimate using a novel decoy-free FDR estimator. It\nsupports both zero-shot and dataset-specific calibration, enabling flexible\napplication via direct inference, fine-tuning, or training a custom model. We\ndemonstrate that, when applied to InstaNovo predictions, Winnow's calibrator\nimproves recall at fixed FDR thresholds, and its FDR estimator tracks true\nerror rates when benchmarked against reference proteomes and database search.\nWinnow ensures accurate FDR control across datasets, helping unlock the full\npotential of DNS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has markedly advanced de novo peptide sequencing (DNS) for\nmass spectrometry-based proteomics. DNS tools offer a reliable way to identify\npeptides without relying on reference databases, extending proteomic analysis\nand unlocking applications into less-charted regions of the proteome. However,\nthey still face a key limitation. DNS tools lack principled methods for\nestimating false discovery rates (FDR) and instead rely on model-specific\nconfidence scores that are often miscalibrated. This limits trust in results,\nhinders cross-model comparisons and reduces validation success. Here we present\nWinnow, a model-agnostic framework for estimating FDR from calibrated DNS\noutputs. Winnow maps raw model scores to calibrated confidences using a neural\nnetwork trained on peptide-spectrum match (PSM)-derived features. From these\ncalibrated scores, Winnow computes PSM-specific error metrics and an\nexperiment-wide FDR estimate using a novel decoy-free FDR estimator. It\nsupports both zero-shot and dataset-specific calibration, enabling flexible\napplication via direct inference, fine-tuning, or training a custom model. We\ndemonstrate that, when applied to InstaNovo predictions, Winnow's calibrator\nimproves recall at fixed FDR thresholds, and its FDR estimator tracks true\nerror rates when benchmarked against reference proteomes and database search.\nWinnow ensures accurate FDR control across datasets, helping unlock the full\npotential of DNS."
                },
                "authors": [
                    {
                        "name": "Amandla Mabona"
                    },
                    {
                        "name": "Jemma Daniel"
                    },
                    {
                        "name": "Henrik Servais Janssen Knudsen"
                    },
                    {
                        "name": "Rachel Catzel"
                    },
                    {
                        "name": "Kevin Michael Eloff"
                    },
                    {
                        "name": "Erwin M. Schoof"
                    },
                    {
                        "name": "Nicolas Lopez Carranza"
                    },
                    {
                        "name": "Timothy P. Jenkins"
                    },
                    {
                        "name": "Jeroen Van Goey"
                    },
                    {
                        "name": "Konstantinos Kalogeropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Kalogeropoulos"
                },
                "author": "Konstantinos Kalogeropoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24949v1",
                "updated": "2025-09-29T15:45:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    45,
                    24,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:45:24Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    45,
                    24,
                    0,
                    272,
                    0
                ],
                "title": "Parallel Nested Slice Sampling for Gravitational Wave Parameter\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Nested Slice Sampling for Gravitational Wave Parameter\n  Estimation"
                },
                "summary": "Inferring parameters and testing hypotheses from gravitational wave signals\nis a computationally intensive task central to modern astrophysics. Nested\nsampling, a Bayesian inference technique, has become an established standard\nfor this in the field. However, most common implementations lack the ability to\nfully utilize modern hardware acceleration. In this work, we demonstrate that\nwhen nested sampling is reformulated in a natively vectorized form and run on\nmodern GPU hardware, we can perform inference in a fraction of the time of\nlegacy nested sampling implementations whilst preserving the accuracy and\nrobustness of the method. This scalable, GPU-accelerated approach significantly\nadvances nested sampling for future large-scale gravitational-wave analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring parameters and testing hypotheses from gravitational wave signals\nis a computationally intensive task central to modern astrophysics. Nested\nsampling, a Bayesian inference technique, has become an established standard\nfor this in the field. However, most common implementations lack the ability to\nfully utilize modern hardware acceleration. In this work, we demonstrate that\nwhen nested sampling is reformulated in a natively vectorized form and run on\nmodern GPU hardware, we can perform inference in a fraction of the time of\nlegacy nested sampling implementations whilst preserving the accuracy and\nrobustness of the method. This scalable, GPU-accelerated approach significantly\nadvances nested sampling for future large-scale gravitational-wave analyses."
                },
                "authors": [
                    {
                        "name": "David Yallup"
                    },
                    {
                        "name": "Metha Prathaban"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "To be submitted to SciPost Physics Proceedings (EuCAIFCon 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24945v1",
                "updated": "2025-09-29T15:43:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    43,
                    59,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:43:59Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    43,
                    59,
                    0,
                    272,
                    0
                ],
                "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model\n  Reasoners with Open Training Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model\n  Reasoners with Open Training Recipes"
                },
                "summary": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study."
                },
                "authors": [
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Ernie Chang"
                    },
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Chia-Jung Chang"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Chen Lai"
                    },
                    {
                        "name": "Rick Cao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Yangyang Shi"
                    },
                    {
                        "name": "Vikas Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Chandra"
                },
                "author": "Vikas Chandra",
                "arxiv_comment": "Model:\n  https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24943v1",
                "updated": "2025-09-29T15:42:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    42,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:42:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    42,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular\n  Clue Exploration with Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular\n  Clue Exploration with Interactive Agents"
                },
                "summary": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro."
                },
                "authors": [
                    {
                        "name": "Jiahua Li"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Zibo Su"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Cheng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Deng"
                },
                "author": "Cheng Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18083v2",
                "updated": "2025-09-29T15:35:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    35,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-23T16:41:08Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    41,
                    8,
                    4,
                    143,
                    0
                ],
                "title": "What Do You Need for Diverse Trajectory Composition in Diffusion\n  Planning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do You Need for Diverse Trajectory Composition in Diffusion\n  Planning?"
                },
                "summary": "In planning, stitching is an ability of algorithms to piece together\nsub-trajectories of data they are trained on to generate new and diverse\nbehaviours. While stitching is historically a strength of offline reinforcement\nlearning, recent generative behavioural cloning (BC) methods have also shown\nproficiency at stitching. However, the main factors behind this are poorly\nunderstood, hindering the development of new algorithms that can reliably\nstitch. Focusing on diffusion planners trained via BC, we find two properties\nare needed to compose: \\emph{positional equivariance} and \\emph{local\nreceptiveness}. We use these two properties to explain architecture, data, and\ninference choices in existing generative BC methods based on diffusion\nplanning, including replanning frequency, data augmentation, and data scaling.\nExperimental comparisions show that (1) while locality is more important than\npositional equivariance in creating a diffusion planner capable of composition,\nboth are crucial (2) enabling these properties through relatively simple\narchitecture choices can be competitive with more computationally expensive\nmethods such as replanning or scaling data, and (3) simple inpainting-based\nguidance can guide architecturally compositional models to enable\ngeneralization in goal-conditioned settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In planning, stitching is an ability of algorithms to piece together\nsub-trajectories of data they are trained on to generate new and diverse\nbehaviours. While stitching is historically a strength of offline reinforcement\nlearning, recent generative behavioural cloning (BC) methods have also shown\nproficiency at stitching. However, the main factors behind this are poorly\nunderstood, hindering the development of new algorithms that can reliably\nstitch. Focusing on diffusion planners trained via BC, we find two properties\nare needed to compose: \\emph{positional equivariance} and \\emph{local\nreceptiveness}. We use these two properties to explain architecture, data, and\ninference choices in existing generative BC methods based on diffusion\nplanning, including replanning frequency, data augmentation, and data scaling.\nExperimental comparisions show that (1) while locality is more important than\npositional equivariance in creating a diffusion planner capable of composition,\nboth are crucial (2) enabling these properties through relatively simple\narchitecture choices can be competitive with more computationally expensive\nmethods such as replanning or scaling data, and (3) simple inpainting-based\nguidance can guide architecturally compositional models to enable\ngeneralization in goal-conditioned settings."
                },
                "authors": [
                    {
                        "name": "Quentin Clark"
                    },
                    {
                        "name": "Florian Shkurti"
                    }
                ],
                "author_detail": {
                    "name": "Florian Shkurti"
                },
                "author": "Florian Shkurti",
                "arxiv_comment": "9 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24930v1",
                "updated": "2025-09-29T15:34:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    34,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:34:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    34,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "How Well Do LLMs Imitate Human Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Imitate Human Writing Style?"
                },
                "summary": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation."
                },
                "authors": [
                    {
                        "name": "Rebira Jemama"
                    },
                    {
                        "name": "Rajesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Kumar"
                },
                "author": "Rajesh Kumar",
                "arxiv_comment": "IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24928v1",
                "updated": "2025-09-29T15:30:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    30,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:30:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    30,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "Trajectory Prediction via Bayesian Intention Inference under Unknown\n  Goals and Kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Prediction via Bayesian Intention Inference under Unknown\n  Goals and Kinematics"
                },
                "summary": "This work introduces an adaptive Bayesian algorithm for real-time trajectory\nprediction via intention inference, where a target's intentions and motion\ncharacteristics are unknown and subject to change. The method concurrently\nestimates two critical variables: the target's current intention, modeled as a\nMarkovian latent state, and an intention parameter that describes the target's\nadherence to a shortest-path policy. By integrating this joint update\ntechnique, the algorithm maintains robustness against abrupt intention shifts\nand unknown motion dynamics. A sampling-based trajectory prediction mechanism\nthen exploits these adaptive estimates to generate probabilistic forecasts with\nquantified uncertainty. We validate the framework through numerical\nexperiments: Ablation studies of two cases, and a 500-trial Monte Carlo\nanalysis; Hardware demonstrations on quadrotor and quadrupedal platforms.\nExperimental results demonstrate that the proposed approach significantly\noutperforms non-adaptive and partially adaptive methods. The method operates in\nreal time around 270 Hz without requiring training or detailed prior knowledge\nof target behavior, showcasing its applicability in various robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces an adaptive Bayesian algorithm for real-time trajectory\nprediction via intention inference, where a target's intentions and motion\ncharacteristics are unknown and subject to change. The method concurrently\nestimates two critical variables: the target's current intention, modeled as a\nMarkovian latent state, and an intention parameter that describes the target's\nadherence to a shortest-path policy. By integrating this joint update\ntechnique, the algorithm maintains robustness against abrupt intention shifts\nand unknown motion dynamics. A sampling-based trajectory prediction mechanism\nthen exploits these adaptive estimates to generate probabilistic forecasts with\nquantified uncertainty. We validate the framework through numerical\nexperiments: Ablation studies of two cases, and a 500-trial Monte Carlo\nanalysis; Hardware demonstrations on quadrotor and quadrupedal platforms.\nExperimental results demonstrate that the proposed approach significantly\noutperforms non-adaptive and partially adaptive methods. The method operates in\nreal time around 270 Hz without requiring training or detailed prior knowledge\nof target behavior, showcasing its applicability in various robotic systems."
                },
                "authors": [
                    {
                        "name": "Shunan Yin"
                    },
                    {
                        "name": "Zehui Lu"
                    },
                    {
                        "name": "Shaoshuai Mou"
                    }
                ],
                "author_detail": {
                    "name": "Shaoshuai Mou"
                },
                "author": "Shaoshuai Mou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v7",
                "updated": "2025-09-29T15:26:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    26,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24923v1",
                "updated": "2025-09-29T15:25:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    25,
                    42,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:25:42Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    25,
                    42,
                    0,
                    272,
                    0
                ],
                "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training"
                },
                "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior."
                },
                "authors": [
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Xiaoyin Chen"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Roy Xie"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24922v1",
                "updated": "2025-09-29T15:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning"
                },
                "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Jianhui Yang"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24919v1",
                "updated": "2025-09-29T15:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    23,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    23,
                    50,
                    0,
                    272,
                    0
                ],
                "title": "Meta-Learning Theory-Informed Inductive Biases using Deep Kernel\n  Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Learning Theory-Informed Inductive Biases using Deep Kernel\n  Gaussian Processes"
                },
                "summary": "Normative and task-driven theories offer powerful top-down explanations for\nbiological systems, yet the goals of quantitatively arbitrating between\ncompeting theories, and utilizing them as inductive biases to improve\ndata-driven fits of real biological datasets are prohibitively laborious, and\noften impossible. To this end, we introduce a Bayesian meta-learning framework\ndesigned to automatically convert raw functional predictions from normative\ntheories into tractable probabilistic models. We employ adaptive deep kernel\nGaussian processes, meta-learning a kernel on synthetic data generated from a\nnormative theory. This Theory-Informed Kernel specifies a probabilistic model\nrepresenting the theory predictions -- usable for both fitting data and\nrigorously validating the theory. As a demonstration, we apply our framework to\nthe early visual system, using efficient coding as our normative theory. We\nshow improved response prediction accuracy in ex vivo recordings of mouse\nretinal ganglion cells stimulated by natural scenes compared to conventional\ndata-driven baselines, while providing well-calibrated uncertainty estimates\nand interpretable representations. Using exact Bayesian model selection, we\nalso show that our informed kernel can accurately infer the degree of\ntheory-match from data, confirming faithful encapsulation of theory structure.\nThis work provides a more general, scalable, and automated approach for\nintegrating theoretical knowledge into data-driven scientific inquiry in\nneuroscience and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative and task-driven theories offer powerful top-down explanations for\nbiological systems, yet the goals of quantitatively arbitrating between\ncompeting theories, and utilizing them as inductive biases to improve\ndata-driven fits of real biological datasets are prohibitively laborious, and\noften impossible. To this end, we introduce a Bayesian meta-learning framework\ndesigned to automatically convert raw functional predictions from normative\ntheories into tractable probabilistic models. We employ adaptive deep kernel\nGaussian processes, meta-learning a kernel on synthetic data generated from a\nnormative theory. This Theory-Informed Kernel specifies a probabilistic model\nrepresenting the theory predictions -- usable for both fitting data and\nrigorously validating the theory. As a demonstration, we apply our framework to\nthe early visual system, using efficient coding as our normative theory. We\nshow improved response prediction accuracy in ex vivo recordings of mouse\nretinal ganglion cells stimulated by natural scenes compared to conventional\ndata-driven baselines, while providing well-calibrated uncertainty estimates\nand interpretable representations. Using exact Bayesian model selection, we\nalso show that our informed kernel can accurately infer the degree of\ntheory-match from data, confirming faithful encapsulation of theory structure.\nThis work provides a more general, scalable, and automated approach for\nintegrating theoretical knowledge into data-driven scientific inquiry in\nneuroscience and beyond."
                },
                "authors": [
                    {
                        "name": "Bahti Zakirov"
                    },
                    {
                        "name": "Gaper Tkaik"
                    }
                ],
                "author_detail": {
                    "name": "Gaper Tkaik"
                },
                "author": "Gaper Tkaik",
                "arxiv_comment": "13 pages, 5 figures, 9 SI figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24908v1",
                "updated": "2025-09-29T15:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    17,
                    0,
                    272,
                    0
                ],
                "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications"
                },
                "summary": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%)."
                },
                "authors": [
                    {
                        "name": "Andrs Fernndez Garca"
                    },
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Roser Morante"
                    },
                    {
                        "name": "Enrique Amig"
                    },
                    {
                        "name": "Alejandro Benito-Santos"
                    },
                    {
                        "name": "Jorge Carrillo-de-Albornoz"
                    },
                    {
                        "name": "Vctor Fresno"
                    },
                    {
                        "name": "Adrian Ghajari"
                    },
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Laura Plaza"
                    },
                    {
                        "name": "Eva Snchez Salido"
                    }
                ],
                "author_detail": {
                    "name": "Eva Snchez Salido"
                },
                "author": "Eva Snchez Salido",
                "arxiv_comment": "Published in SEPLN 2025. 20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06608v2",
                "updated": "2025-09-29T15:10:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    10,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-08T12:26:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    26,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors"
                },
                "summary": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Vadim Kurochkin"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24893v1",
                "updated": "2025-09-29T15:03:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    3,
                    31,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:03:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    3,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth\n  Estimation and Bidirectional Warping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth\n  Estimation and Bidirectional Warping"
                },
                "summary": "Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D\nreconstruction, typically suffering from overfitting, geometric distortion, and\nincomplete scene recovery due to limited multi-view constraints. Although 3D\nGaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it\nsuffers from floating artifacts and structural inconsistencies under\nsparse-input settings. To address these issues, we propose DWGS, a novel\nunified framework that enhances 3DGS for sparse-view synthesis by integrating\nrobust structural cues, virtual view constraints, and occluded region\ncompletion. Our approach introduces three principal contributions: a\nHybrid-Loss Depth Estimation module that leverages dense matching priors with\nreprojection, point propagation, and smoothness constraints to enforce\nmulti-view consistency; a Bidirectional Warping Virtual View Synthesis method\ngenerates virtual training views to impose stronger geometric and photometric\nconstraints; and an Occlusion-Aware Reconstruction component that utilizes\ndepth-difference mask and a learning-based inpainting model to recover obscured\nregions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)\nshow that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR\nand 0.189 LPIPS, while retaining real-time inference capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D\nreconstruction, typically suffering from overfitting, geometric distortion, and\nincomplete scene recovery due to limited multi-view constraints. Although 3D\nGaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it\nsuffers from floating artifacts and structural inconsistencies under\nsparse-input settings. To address these issues, we propose DWGS, a novel\nunified framework that enhances 3DGS for sparse-view synthesis by integrating\nrobust structural cues, virtual view constraints, and occluded region\ncompletion. Our approach introduces three principal contributions: a\nHybrid-Loss Depth Estimation module that leverages dense matching priors with\nreprojection, point propagation, and smoothness constraints to enforce\nmulti-view consistency; a Bidirectional Warping Virtual View Synthesis method\ngenerates virtual training views to impose stronger geometric and photometric\nconstraints; and an Occlusion-Aware Reconstruction component that utilizes\ndepth-difference mask and a learning-based inpainting model to recover obscured\nregions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)\nshow that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR\nand 0.189 LPIPS, while retaining real-time inference capabilities."
                },
                "authors": [
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Guoliang Wei"
                    },
                    {
                        "name": "Yue Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Cheng"
                },
                "author": "Yue Cheng",
                "arxiv_comment": "14 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14399v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14399v3",
                "updated": "2025-09-29T15:03:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    3,
                    12,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-17T10:56:09Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    10,
                    56,
                    9,
                    1,
                    168,
                    0
                ],
                "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models"
                },
                "summary": "Counterfactual generation aims to simulate realistic hypothetical outcomes\nunder causal interventions. Diffusion models have emerged as a powerful tool\nfor this task, combining DDIM inversion with conditional generation and\nclassifier-free guidance (CFG). In this work, we identify a key limitation of\nCFG for counterfactual generation: it prescribes a global guidance scale for\nall attributes, leading to significant spurious changes in inferred\ncounterfactuals. To mitigate this, we propose Decoupled Classifier-Free\nGuidance (DCFG), a flexible and model-agnostic guidance technique that enables\nattribute-wise control following a causal graph. DCFG is implemented via a\nsimple attribute-split embedding strategy that disentangles semantic inputs,\nenabling selective guidance on user-defined attribute groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual generation aims to simulate realistic hypothetical outcomes\nunder causal interventions. Diffusion models have emerged as a powerful tool\nfor this task, combining DDIM inversion with conditional generation and\nclassifier-free guidance (CFG). In this work, we identify a key limitation of\nCFG for counterfactual generation: it prescribes a global guidance scale for\nall attributes, leading to significant spurious changes in inferred\ncounterfactuals. To mitigate this, we propose Decoupled Classifier-Free\nGuidance (DCFG), a flexible and model-agnostic guidance technique that enables\nattribute-wise control following a causal graph. DCFG is implemented via a\nsimple attribute-split embedding strategy that disentangles semantic inputs,\nenabling selective guidance on user-defined attribute groups."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Fabio De Sousa Ribeiro"
                    },
                    {
                        "name": "Rajat R Rasal"
                    },
                    {
                        "name": "Avinash Kori"
                    },
                    {
                        "name": "Raghav Mehta"
                    },
                    {
                        "name": "Ben Glocker"
                    }
                ],
                "author_detail": {
                    "name": "Ben Glocker"
                },
                "author": "Ben Glocker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14399v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14399v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24884v1",
                "updated": "2025-09-29T14:59:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    59,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:59:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    59,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Expanding Computation Spaces of LLMs at Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Computation Spaces of LLMs at Inference Time"
                },
                "summary": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving."
                },
                "authors": [
                    {
                        "name": "Yoonna Jang"
                    },
                    {
                        "name": "Kisu Yang"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19301v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19301v3",
                "updated": "2025-09-29T14:58:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    58,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-01-31T16:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond checkmate: exploring the creative chokepoints in AI text"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction."
                },
                "authors": [
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted at 30th Conference on Empirical Methods in Natural Language\n  Processing (EMNLP'25 Main conference). 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19301v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19301v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04362v2",
                "updated": "2025-09-29T14:57:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    57,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-04T18:21:54Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    21,
                    54,
                    2,
                    155,
                    0
                ],
                "title": "Learning Smooth State-Dependent Traversability from Dense Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Smooth State-Dependent Traversability from Dense Point Clouds"
                },
                "summary": "A key open challenge in off-road autonomy is that the traversability of\nterrain often depends on the vehicle's state. In particular, some obstacles are\nonly traversable from some orientations. However, learning this interaction by\nencoding the angle of approach as a model input demands a large and diverse\ntraining dataset and is computationally inefficient during planning due to\nrepeated model inference. To address these challenges, we present SPARTA, a\nmethod for estimating approach angle conditioned traversability from point\nclouds. Specifically, we impose geometric structure into our network by\noutputting a smooth analytical function over the 1-Sphere that predicts risk\ndistribution for any angle of approach with minimal overhead and can be reused\nfor subsequent queries. The function is composed of Fourier basis functions,\nwhich has important advantages for generalization due to their periodic nature\nand smoothness. We demonstrate SPARTA both in a high-fidelity simulation\nplatform, where our model achieves a 91\\% success rate crossing a 40m boulder\nfield (compared to 73\\% for the baseline), and on hardware, illustrating the\ngeneralization ability of the model to real-world settings. Our code will be\navailable at https://github.com/neu-autonomy/SPARTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key open challenge in off-road autonomy is that the traversability of\nterrain often depends on the vehicle's state. In particular, some obstacles are\nonly traversable from some orientations. However, learning this interaction by\nencoding the angle of approach as a model input demands a large and diverse\ntraining dataset and is computationally inefficient during planning due to\nrepeated model inference. To address these challenges, we present SPARTA, a\nmethod for estimating approach angle conditioned traversability from point\nclouds. Specifically, we impose geometric structure into our network by\noutputting a smooth analytical function over the 1-Sphere that predicts risk\ndistribution for any angle of approach with minimal overhead and can be reused\nfor subsequent queries. The function is composed of Fourier basis functions,\nwhich has important advantages for generalization due to their periodic nature\nand smoothness. We demonstrate SPARTA both in a high-fidelity simulation\nplatform, where our model achieves a 91\\% success rate crossing a 40m boulder\nfield (compared to 73\\% for the baseline), and on hardware, illustrating the\ngeneralization ability of the model to real-world settings. Our code will be\navailable at https://github.com/neu-autonomy/SPARTA."
                },
                "authors": [
                    {
                        "name": "Zihao Dong"
                    },
                    {
                        "name": "Alan Papalia"
                    },
                    {
                        "name": "Leonard Jung"
                    },
                    {
                        "name": "Alenna Spiro"
                    },
                    {
                        "name": "Philip R. Osteen"
                    },
                    {
                        "name": "Christa S. Robison"
                    },
                    {
                        "name": "Michael Everett"
                    }
                ],
                "author_detail": {
                    "name": "Michael Everett"
                },
                "author": "Michael Everett",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01752v2",
                "updated": "2025-09-29T14:57:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    57,
                    36,
                    0,
                    272,
                    0
                ],
                "published": "2024-12-02T17:47:13Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    17,
                    47,
                    13,
                    0,
                    337,
                    0
                ],
                "title": "A Neurosymbolic Fast and Slow Architecture for Graph Coloring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neurosymbolic Fast and Slow Architecture for Graph Coloring"
                },
                "summary": "Constraint Satisfaction Problems (CSPs) present significant challenges to\nartificial intelligence due to their intricate constraints and the necessity\nfor precise solutions. Existing symbolic solvers are often slow, and prior\nresearch has shown that Large Language Models (LLMs) alone struggle with CSPs\nbecause of their complexity. To bridge this gap, we build upon the existing\nSOFAI architecture (SOFAI_v1), which adapts Daniel Kahneman's ''Thinking, Fast\nand Slow'' cognitive model to AI. Our enhanced architecture, SOFAI_v2,\nintegrates refined metacognitive governance mechanisms to improve adaptability\nacross complex domains, specifically tailored here for solving the graph\ncoloring problem, a specific type of CSP. SOFAI_v2 combines a fast System 1\n(S1), leveraging LLMs, with a deliberative System 2 (S2), governed by a\nmetacognition module. S1's initial solutions, often limited by constraint\nadherence issues, are improved through targeted feedback and examples from\nmetacognition, aligning S1 more closely with CSP requirements. If S1 fails to\nresolve the problem, metacognition strategically invokes S2, ensuring accurate\nand reliable solutions. Our empirical results demonstrate that SOFAI_v2\nachieves a 10.5% higher success rate and is up to 30% faster than a traditional\nsymbolic solver in solving graph coloring problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Satisfaction Problems (CSPs) present significant challenges to\nartificial intelligence due to their intricate constraints and the necessity\nfor precise solutions. Existing symbolic solvers are often slow, and prior\nresearch has shown that Large Language Models (LLMs) alone struggle with CSPs\nbecause of their complexity. To bridge this gap, we build upon the existing\nSOFAI architecture (SOFAI_v1), which adapts Daniel Kahneman's ''Thinking, Fast\nand Slow'' cognitive model to AI. Our enhanced architecture, SOFAI_v2,\nintegrates refined metacognitive governance mechanisms to improve adaptability\nacross complex domains, specifically tailored here for solving the graph\ncoloring problem, a specific type of CSP. SOFAI_v2 combines a fast System 1\n(S1), leveraging LLMs, with a deliberative System 2 (S2), governed by a\nmetacognition module. S1's initial solutions, often limited by constraint\nadherence issues, are improved through targeted feedback and examples from\nmetacognition, aligning S1 more closely with CSP requirements. If S1 fails to\nresolve the problem, metacognition strategically invokes S2, ensuring accurate\nand reliable solutions. Our empirical results demonstrate that SOFAI_v2\nachieves a 10.5% higher success rate and is up to 30% faster than a traditional\nsymbolic solver in solving graph coloring problems."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Vishal Pallagani"
                    },
                    {
                        "name": "Biplav Srivastava"
                    },
                    {
                        "name": "Francesca Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Rossi"
                },
                "author": "Francesca Rossi",
                "arxiv_comment": "31 Pages, 18 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21184v2",
                "updated": "2025-09-29T14:57:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    57,
                    0,
                    0,
                    272,
                    0
                ],
                "published": "2025-07-27T05:45:26Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    5,
                    45,
                    26,
                    6,
                    208,
                    0
                ],
                "title": "Can Language Models Discover Scaling Laws?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Discover Scaling Laws?"
                },
                "summary": "Discovering scaling laws for predicting model performance at scale is a\nfundamental and open-ended challenge, mostly reliant on slow, case specific\nhuman experimentation. To investigate the potential for LLMs to automate this\nprocess, we collect over 5,000 experiments from existing literature and curate\nseven diverse scaling law discovery tasks. While existing agents struggle to\nproduce accurate law formulas, this paper introduces SLDAgent, an\nevolution-based agent that co-optimize the scaling law model and the\nparameters, enabling it to autonomously explore complex relationships between\nvariables. For the first time, we demonstrates that SLDAgent can automatically\ndiscover laws that exhibit consistently more accurate extrapolation than their\nestablished, human-derived counterparts across all tasks. Through comprehensive\nanalysis, we elucidate why these discovered laws are superior and verify their\npractical utility in both pretraining and finetuning applications. This work\nestablishes a new paradigm for agentic scientific discovery, showing that AI\nsystems can understand their own scaling behavior, and can contribute novel and\npractical knowledge back to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering scaling laws for predicting model performance at scale is a\nfundamental and open-ended challenge, mostly reliant on slow, case specific\nhuman experimentation. To investigate the potential for LLMs to automate this\nprocess, we collect over 5,000 experiments from existing literature and curate\nseven diverse scaling law discovery tasks. While existing agents struggle to\nproduce accurate law formulas, this paper introduces SLDAgent, an\nevolution-based agent that co-optimize the scaling law model and the\nparameters, enabling it to autonomously explore complex relationships between\nvariables. For the first time, we demonstrates that SLDAgent can automatically\ndiscover laws that exhibit consistently more accurate extrapolation than their\nestablished, human-derived counterparts across all tasks. Through comprehensive\nanalysis, we elucidate why these discovered laws are superior and verify their\npractical utility in both pretraining and finetuning applications. This work\nestablishes a new paradigm for agentic scientific discovery, showing that AI\nsystems can understand their own scaling behavior, and can contribute novel and\npractical knowledge back to the research community."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Wenzheng Feng"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Hubert Lim"
                    },
                    {
                        "name": "Zhengrui Li"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24880v1",
                "updated": "2025-09-29T14:56:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    56,
                    56,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:56:56Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    56,
                    56,
                    0,
                    272,
                    0
                ],
                "title": "Vehicle Classification under Extreme Imbalance: A Comparative Study of\n  Ensemble Learning and CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle Classification under Extreme Imbalance: A Comparative Study of\n  Ensemble Learning and CNNs"
                },
                "summary": "Accurate vehicle type recognition underpins intelligent transportation and\nlogistics, but severe class imbalance in public datasets suppresses performance\non rare categories. We curate a 16-class corpus (~47k images) by merging\nKaggle, ImageNet, and web-crawled data, and create six balanced variants via\nSMOTE oversampling and targeted undersampling. Lightweight ensembles, such as\nRandom Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2\nfeatures are benchmarked against a configurable ResNet-style CNN trained with\nstrong augmentation and label smoothing. The best ensemble (SMOTE-combined)\nattains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set\nand 81.25% on an unseen inference batch, confirming the advantage of deep\nmodels. Nonetheless, the most under-represented class (Barge) remains a failure\nmode, highlighting the limits of rebalancing alone. Results suggest\nprioritizing additional minority-class collection and cost-sensitive objectives\n(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine\ninterpretability with representational power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate vehicle type recognition underpins intelligent transportation and\nlogistics, but severe class imbalance in public datasets suppresses performance\non rare categories. We curate a 16-class corpus (~47k images) by merging\nKaggle, ImageNet, and web-crawled data, and create six balanced variants via\nSMOTE oversampling and targeted undersampling. Lightweight ensembles, such as\nRandom Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2\nfeatures are benchmarked against a configurable ResNet-style CNN trained with\nstrong augmentation and label smoothing. The best ensemble (SMOTE-combined)\nattains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set\nand 81.25% on an unseen inference batch, confirming the advantage of deep\nmodels. Nonetheless, the most under-represented class (Barge) remains a failure\nmode, highlighting the limits of rebalancing alone. Results suggest\nprioritizing additional minority-class collection and cost-sensitive objectives\n(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine\ninterpretability with representational power."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    }
                ],
                "author_detail": {
                    "name": "Abu Hanif Muhammad Syarubany"
                },
                "author": "Abu Hanif Muhammad Syarubany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24877v1",
                "updated": "2025-09-29T14:55:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    55,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:55:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    55,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "The Emergence of Social Science of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Social Science of Large Language Models"
                },
                "summary": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Xiao Jia"
                    },
                    {
                        "name": "Zhanzhan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhanzhan Zhao"
                },
                "author": "Zhanzhan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24870v1",
                "updated": "2025-09-29T14:53:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:53:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Closing the Evidence Gap: reddemcee, a Fast Adaptive Parallel Tempering\n  Sampler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Evidence Gap: reddemcee, a Fast Adaptive Parallel Tempering\n  Sampler"
                },
                "summary": "Markov Chain Monte Carlo (MCMC) excels at sampling complex posteriors but\ntraditionally lags behind nested sampling in accurate evidence estimation,\nwhich is crucial for model comparison in astrophysical problems. We introduce\nreddemcee, an Adaptive Parallel Tempering Ensemble Sampler, aiming to close\nthis gap by simultaneously presenting next-generation automated\ntemperature-ladder adaptation techniques and robust, low-bias evidence\nestimators. reddemcee couples an affine-invariant stretch move with five\ninterchangeable ladder-adaptation objectives, Uniform Swap Acceptance Rate,\nSwap Mean Distance, Gaussian-Area Overlap, Small Gaussian Gap, and Equalised\nThermodynamic Length, implemented through a common differential update rule.\nThree evidence estimators are provided: Curvature-aware Thermodynamic\nIntegration (TI+), Geometric-Bridge Stepping Stones (SS+), and a novel Hybrid\nalgorithm that blends both approaches (H+). Performance and accuracy are\nbenchmarked on n-dimensional Gaussian Shells, Gaussian Egg-box, Rosenbrock\nFunctions, and exoplanet radial-velocity time-series of HD 20794. Across Shells\nup to 15 dimensions, reddemcee presents roughly 7 times the effective sampling\nspeed of the best dynamic nested sampling configuration. The TI+, SS+ and H+\nestimators recover estimates under 3 percent error and supply realistic\nuncertainties with as few as six temperatures. In the HD 20794 case study,\nreddemcee reproduces literature model rankings and yields tighter yet\nconsistent planetary parameters compared with dynesty, with evidence errors\nthat track run-to-run dispersion. By unifying fast ladder adaptation with\nreliable evidence estimators, reddemcee delivers strong throughput and accurate\nevidence estimates, often matching, and occasionally surpassing, dynamic nested\nsampling, while preserving the rich posterior information which makes MCMC\nindispensable for modern Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain Monte Carlo (MCMC) excels at sampling complex posteriors but\ntraditionally lags behind nested sampling in accurate evidence estimation,\nwhich is crucial for model comparison in astrophysical problems. We introduce\nreddemcee, an Adaptive Parallel Tempering Ensemble Sampler, aiming to close\nthis gap by simultaneously presenting next-generation automated\ntemperature-ladder adaptation techniques and robust, low-bias evidence\nestimators. reddemcee couples an affine-invariant stretch move with five\ninterchangeable ladder-adaptation objectives, Uniform Swap Acceptance Rate,\nSwap Mean Distance, Gaussian-Area Overlap, Small Gaussian Gap, and Equalised\nThermodynamic Length, implemented through a common differential update rule.\nThree evidence estimators are provided: Curvature-aware Thermodynamic\nIntegration (TI+), Geometric-Bridge Stepping Stones (SS+), and a novel Hybrid\nalgorithm that blends both approaches (H+). Performance and accuracy are\nbenchmarked on n-dimensional Gaussian Shells, Gaussian Egg-box, Rosenbrock\nFunctions, and exoplanet radial-velocity time-series of HD 20794. Across Shells\nup to 15 dimensions, reddemcee presents roughly 7 times the effective sampling\nspeed of the best dynamic nested sampling configuration. The TI+, SS+ and H+\nestimators recover estimates under 3 percent error and supply realistic\nuncertainties with as few as six temperatures. In the HD 20794 case study,\nreddemcee reproduces literature model rankings and yields tighter yet\nconsistent planetary parameters compared with dynesty, with evidence errors\nthat track run-to-run dispersion. By unifying fast ladder adaptation with\nreliable evidence estimators, reddemcee delivers strong throughput and accurate\nevidence estimates, often matching, and occasionally surpassing, dynamic nested\nsampling, while preserving the rich posterior information which makes MCMC\nindispensable for modern Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Pablo A. Pea R."
                    },
                    {
                        "name": "James S. Jenkins"
                    }
                ],
                "author_detail": {
                    "name": "James S. Jenkins"
                },
                "author": "James S. Jenkins",
                "arxiv_comment": "v2: Revised after referee comments; resubmitted to A&A on 24 Sep 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24869v1",
                "updated": "2025-09-29T14:53:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:53:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval"
                },
                "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark."
                },
                "authors": [
                    {
                        "name": "Junwei Lan"
                    },
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24866v1",
                "updated": "2025-09-29T14:50:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:50:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning"
                },
                "summary": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them."
                },
                "authors": [
                    {
                        "name": "Matteo Fuoli"
                    },
                    {
                        "name": "Weihang Huang"
                    },
                    {
                        "name": "Jeannette Littlemore"
                    },
                    {
                        "name": "Sarah Turner"
                    },
                    {
                        "name": "Ellen Wilding"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Wilding"
                },
                "author": "Ellen Wilding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19400v2",
                "updated": "2025-09-29T14:43:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    43,
                    15,
                    0,
                    272,
                    0
                ],
                "published": "2025-01-31T18:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    57,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Vintix: Action Model via In-Context Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vintix: Action Model via In-Context Reinforcement Learning"
                },
                "summary": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code released at https://github.com/dunnolab/vintix",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code released at https://github.com/dunnolab/vintix"
                },
                "authors": [
                    {
                        "name": "Andrey Polubarov"
                    },
                    {
                        "name": "Nikita Lyubaykin"
                    },
                    {
                        "name": "Alexander Derevyagin"
                    },
                    {
                        "name": "Ilya Zisman"
                    },
                    {
                        "name": "Denis Tarasov"
                    },
                    {
                        "name": "Alexander Nikulin"
                    },
                    {
                        "name": "Vladislav Kurenkov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Kurenkov"
                },
                "author": "Vladislav Kurenkov",
                "arxiv_comment": "ICML 2025, Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24857v1",
                "updated": "2025-09-29T14:42:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    42,
                    23,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:42:23Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    42,
                    23,
                    0,
                    272,
                    0
                ],
                "title": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by\n  LLMs"
                },
                "summary": "The widespread use of chatbots powered by large language models (LLMs) such\nas ChatGPT and Llama has fundamentally reshaped how people seek information and\nadvice across domains. Increasingly, these chatbots are being used in\nhigh-stakes contexts, including emotional support and mental health concerns.\nWhile LLMs can offer scalable support, their ability to safely detect and\nrespond to acute mental health crises remains poorly understood. Progress is\nhampered by the absence of unified crisis taxonomies, robust annotated\nbenchmarks, and empirical evaluations grounded in clinical best practices. In\nthis work, we address these gaps by introducing a unified taxonomy of six\nclinically-informed mental health crisis categories, curating a diverse\nevaluation dataset, and establishing an expert-designed protocol for assessing\nresponse appropriateness. We systematically benchmark three state-of-the-art\nLLMs for their ability to classify crisis types and generate safe, appropriate\nresponses. The results reveal that while LLMs are highly consistent and\ngenerally reliable in addressing explicit crisis disclosures, significant risks\nremain. A non-negligible proportion of responses are rated as inappropriate or\nharmful, with responses generated by an open-weight model exhibiting higher\nfailure rates than those generated by the commercial ones. We also identify\nsystemic weaknesses in handling indirect or ambiguous risk signals, a reliance\non formulaic and inauthentic default replies, and frequent misalignment with\nuser context. These findings underscore the urgent need for enhanced\nsafeguards, improved crisis detection, and context-aware interventions in LLM\ndeployments. Our taxonomy, datasets, and evaluation framework lay the\ngroundwork for ongoing research and responsible innovation in AI-driven mental\nhealth support, helping to minimize harm and better protect vulnerable users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of chatbots powered by large language models (LLMs) such\nas ChatGPT and Llama has fundamentally reshaped how people seek information and\nadvice across domains. Increasingly, these chatbots are being used in\nhigh-stakes contexts, including emotional support and mental health concerns.\nWhile LLMs can offer scalable support, their ability to safely detect and\nrespond to acute mental health crises remains poorly understood. Progress is\nhampered by the absence of unified crisis taxonomies, robust annotated\nbenchmarks, and empirical evaluations grounded in clinical best practices. In\nthis work, we address these gaps by introducing a unified taxonomy of six\nclinically-informed mental health crisis categories, curating a diverse\nevaluation dataset, and establishing an expert-designed protocol for assessing\nresponse appropriateness. We systematically benchmark three state-of-the-art\nLLMs for their ability to classify crisis types and generate safe, appropriate\nresponses. The results reveal that while LLMs are highly consistent and\ngenerally reliable in addressing explicit crisis disclosures, significant risks\nremain. A non-negligible proportion of responses are rated as inappropriate or\nharmful, with responses generated by an open-weight model exhibiting higher\nfailure rates than those generated by the commercial ones. We also identify\nsystemic weaknesses in handling indirect or ambiguous risk signals, a reliance\non formulaic and inauthentic default replies, and frequent misalignment with\nuser context. These findings underscore the urgent need for enhanced\nsafeguards, improved crisis detection, and context-aware interventions in LLM\ndeployments. Our taxonomy, datasets, and evaluation framework lay the\ngroundwork for ongoing research and responsible innovation in AI-driven mental\nhealth support, helping to minimize harm and better protect vulnerable users."
                },
                "authors": [
                    {
                        "name": "Adrian Arnaiz-Rodriguez"
                    },
                    {
                        "name": "Miguel Baidal"
                    },
                    {
                        "name": "Erik Derner"
                    },
                    {
                        "name": "Jenn Layton Annable"
                    },
                    {
                        "name": "Mark Ball"
                    },
                    {
                        "name": "Mark Ince"
                    },
                    {
                        "name": "Elvira Perez Vallejos"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22390v2",
                "updated": "2025-09-29T14:27:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    27,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-27T17:00:48Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "title": "What Characteristics Make ChatGPT Effective for Software Issue\n  Resolution? An Empirical Study of Task, Project, and Conversational Signals\n  in GitHub Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Characteristics Make ChatGPT Effective for Software Issue\n  Resolution? An Empirical Study of Task, Project, and Conversational Signals\n  in GitHub Issues"
                },
                "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "arxiv_comment": "Accepted for publication in Empirical Software Engineering (EMSE),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24841v1",
                "updated": "2025-09-29T14:21:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    21,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:21:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    21,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Hierarchical Error Correction for Large Language Models: A Systematic\n  Framework for Domain-Specific AI Quality Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Error Correction for Large Language Models: A Systematic\n  Framework for Domain-Specific AI Quality Enhancement"
                },
                "summary": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment."
                },
                "authors": [
                    {
                        "name": "Zhilong Zhao"
                    },
                    {
                        "name": "Yindi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yindi Liu"
                },
                "author": "Yindi Liu",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24840v1",
                "updated": "2025-09-29T14:20:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:20:50Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    50,
                    0,
                    272,
                    0
                ],
                "title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data"
                },
                "summary": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells."
                },
                "authors": [
                    {
                        "name": "Oussama Kharouiche"
                    },
                    {
                        "name": "Aris Markogiannakis"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24837v1",
                "updated": "2025-09-29T14:20:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:20:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (VLMs) enable strong multimodal reasoning but\nincur heavy inference costs from redundant visual tokens. Token pruning\nalleviates this issue, yet existing approaches face limitations.\nAttention-based methods rely on raw attention scores, which are often unstable\nacross layers and heads and can lead to redundant selections. Diversity-based\nmethods improve robustness by selecting tokens far apart in feature space but\nrisk dropping regions needed for accurate prediction. We propose \\ours, a\ntraining-free framework built on a simple intuition: tokens with higher\nsensitivity are more likely to influence the model's output, and they should\nalso capture complementary visual cues rather than overlapping information. To\nachieve this, we estimate token sensitivity using zeroth-order perturbations at\nthe projection layer, a shallow and computationally light component of the\nmodel. This approach measures how small random perturbations affect the\nprojection outputs, allowing us to approximate each token's influence through\nlightweight forward passes without backpropagation. Extensive experiments\nacross multiple VLMs and benchmarks show that \\ours consistently outperforms\nprior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and\nsignificantly improving efficiency, achieving up to 2.30x faster end-to-end\ninference over the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) enable strong multimodal reasoning but\nincur heavy inference costs from redundant visual tokens. Token pruning\nalleviates this issue, yet existing approaches face limitations.\nAttention-based methods rely on raw attention scores, which are often unstable\nacross layers and heads and can lead to redundant selections. Diversity-based\nmethods improve robustness by selecting tokens far apart in feature space but\nrisk dropping regions needed for accurate prediction. We propose \\ours, a\ntraining-free framework built on a simple intuition: tokens with higher\nsensitivity are more likely to influence the model's output, and they should\nalso capture complementary visual cues rather than overlapping information. To\nachieve this, we estimate token sensitivity using zeroth-order perturbations at\nthe projection layer, a shallow and computationally light component of the\nmodel. This approach measures how small random perturbations affect the\nprojection outputs, allowing us to approximate each token's influence through\nlightweight forward passes without backpropagation. Extensive experiments\nacross multiple VLMs and benchmarks show that \\ours consistently outperforms\nprior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and\nsignificantly improving efficiency, achieving up to 2.30x faster end-to-end\ninference over the baseline."
                },
                "authors": [
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Huiling Liu"
                    },
                    {
                        "name": "Aecheon Jung"
                    },
                    {
                        "name": "Sunwoo Lee"
                    },
                    {
                        "name": "Sungeun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Hong"
                },
                "author": "Sungeun Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24836v2",
                "updated": "2025-09-30T06:35:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    6,
                    35,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T14:20:04Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    4,
                    0,
                    272,
                    0
                ],
                "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data\n  Reasoning Intensity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data\n  Reasoning Intensity"
                },
                "summary": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data. Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data. Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential."
                },
                "authors": [
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Zhenlin Hu"
                    },
                    {
                        "name": "Jinnan Yang"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Yida Xue"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Qing Shen"
                    },
                    {
                        "name": "Zhenfang Liu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Jungang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jungang Lou"
                },
                "author": "Jungang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24827v1",
                "updated": "2025-09-29T14:13:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:13:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants"
                },
                "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests."
                },
                "authors": [
                    {
                        "name": "Bartosz Bieganowski"
                    },
                    {
                        "name": "Daniel Strzelecki"
                    },
                    {
                        "name": "Robert Skiba"
                    },
                    {
                        "name": "Mateusz Topolewski"
                    }
                ],
                "author_detail": {
                    "name": "Mateusz Topolewski"
                },
                "author": "Mateusz Topolewski",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24826v1",
                "updated": "2025-09-29T14:12:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    12,
                    6,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:12:06Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    12,
                    6,
                    0,
                    272,
                    0
                ],
                "title": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems"
                },
                "summary": "Large language models (LLMs) are being increasingly used for planning in\norchestrated multi-agent systems. However, existing LLM-based approaches often\nfall short of human expectations and, critically, lack effective mechanisms for\nusers to inspect, understand, and control their behaviors. These limitations\ncall for enhanced transparency, controllability, and human oversight. To\naddress this, we introduce AIPOM, a system supporting human-in-the-loop\nplanning through conversational and graph-based interfaces. AIPOM enables users\nto transparently inspect, refine, and collaboratively guide LLM-generated\nplans, significantly enhancing user control and trust in multi-agent workflows.\nOur code and demo video are available at https://github.com/megagonlabs/aipom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being increasingly used for planning in\norchestrated multi-agent systems. However, existing LLM-based approaches often\nfall short of human expectations and, critically, lack effective mechanisms for\nusers to inspect, understand, and control their behaviors. These limitations\ncall for enhanced transparency, controllability, and human oversight. To\naddress this, we introduce AIPOM, a system supporting human-in-the-loop\nplanning through conversational and graph-based interfaces. AIPOM enables users\nto transparently inspect, refine, and collaboratively guide LLM-generated\nplans, significantly enhancing user control and trust in multi-agent workflows.\nOur code and demo video are available at https://github.com/megagonlabs/aipom."
                },
                "authors": [
                    {
                        "name": "Hannah Kim"
                    },
                    {
                        "name": "Kushan Mitra"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "arxiv_comment": "EMNLP 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v2",
                "updated": "2025-09-29T14:04:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    4,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24816v1",
                "updated": "2025-09-29T14:03:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    3,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:03:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    3,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical\n  Reasoning"
                },
                "summary": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage."
                },
                "authors": [
                    {
                        "name": "Xilin Dang"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Xiaorui Su"
                    },
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Iaki Arango"
                    },
                    {
                        "name": "Lucas Vittor"
                    },
                    {
                        "name": "Xinyi Long"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Pheng Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng Ann Heng"
                },
                "author": "Pheng Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06233v2",
                "updated": "2025-09-29T13:59:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    59,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-02-10T08:10:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    10,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Confidence Improves Self-Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Improves Self-Consistency in LLMs"
                },
                "summary": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic."
                },
                "authors": [
                    {
                        "name": "Amir Taubenfeld"
                    },
                    {
                        "name": "Tom Sheffer"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Ariel Goldstein"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Gal Yona"
                    }
                ],
                "author_detail": {
                    "name": "Gal Yona"
                },
                "author": "Gal Yona",
                "arxiv_doi": "10.18653/v1/2025.findings-acl.1030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.findings-acl.1030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24807v1",
                "updated": "2025-09-29T13:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    57,
                    16,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:57:16Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    57,
                    16,
                    0,
                    272,
                    0
                ],
                "title": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance\n  and Cognitive Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance\n  and Cognitive Contexts"
                },
                "summary": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models."
                },
                "authors": [
                    {
                        "name": "Dong Hyun Roh"
                    },
                    {
                        "name": "Rajesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Kumar"
                },
                "author": "Rajesh Kumar",
                "arxiv_comment": "Accepted for publication at IEEE-ICMLA 2025. Contains nine pages, six\n  figures, and two tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.25148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25148v1",
                "updated": "2025-09-29T17:53:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    53,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    53,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following"
                },
                "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
                },
                "authors": [
                    {
                        "name": "FaQiang Qian"
                    },
                    {
                        "name": "WeiKun Zhang"
                    },
                    {
                        "name": "Ziliang Wang"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Xuhui Zheng"
                    },
                    {
                        "name": "Liangjian Wen"
                    },
                    {
                        "name": "Mengya Gao"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Yichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Wu"
                },
                "author": "Yichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25144v1",
                "updated": "2025-09-29T17:51:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:51:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation"
                },
                "summary": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis."
                },
                "authors": [
                    {
                        "name": "Yen-Ju Lu"
                    },
                    {
                        "name": "Thomas Thebaud"
                    },
                    {
                        "name": "Laureano Moro-Velazquez"
                    },
                    {
                        "name": "Najim Dehak"
                    },
                    {
                        "name": "Jesus Villalba"
                    }
                ],
                "author_detail": {
                    "name": "Jesus Villalba"
                },
                "author": "Jesus Villalba",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25139v1",
                "updated": "2025-09-29T17:51:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:51:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    51,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs"
                },
                "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25138v1",
                "updated": "2025-09-29T17:50:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    32,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:50:32Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    32,
                    0,
                    272,
                    0
                ],
                "title": "Investigating Language and Retrieval Bias in Multilingual Previously\n  Fact-Checked Claim Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Language and Retrieval Bias in Multilingual Previously\n  Fact-Checked Claim Detection"
                },
                "summary": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Antonia Karamolegkou"
                    },
                    {
                        "name": "Jaroslav Kopan"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Tom Javrek"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Marin imko"
                    }
                ],
                "author_detail": {
                    "name": "Marin imko"
                },
                "author": "Marin imko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25136v1",
                "updated": "2025-09-29T17:50:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:50:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    50,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression"
                },
                "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop."
                },
                "authors": [
                    {
                        "name": "David Gonzlez Martnez"
                    }
                ],
                "author_detail": {
                    "name": "David Gonzlez Martnez"
                },
                "author": "David Gonzlez Martnez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25131v1",
                "updated": "2025-09-29T17:48:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    48,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:48:28Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    48,
                    28,
                    0,
                    272,
                    0
                ],
                "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech"
                },
                "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation."
                },
                "authors": [
                    {
                        "name": "Chengyao Wang"
                    },
                    {
                        "name": "Zhisheng Zhong"
                    },
                    {
                        "name": "Bohao Peng"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Haokun Gui"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code is available at https://github.com/dvlab-research/MGM-Omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25123v1",
                "updated": "2025-09-29T17:44:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones"
                },
                "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25120v1",
                "updated": "2025-09-29T17:43:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    43,
                    21,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:43:21Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    43,
                    21,
                    0,
                    272,
                    0
                ],
                "title": "Data-Driven Optimal Power Flow: A Behavioral Systems Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Optimal Power Flow: A Behavioral Systems Approach"
                },
                "summary": "The increasing decentralization of power systems driven by a large number of\nrenewable energy sources poses challenges in power flow optimization. Partially\nunknown power line properties can render model-based approaches unsuitable.\nWith increasing deployment of sensors, data-driven methods rise as a promising\nalternative. They offer the flexibility to adapt to varying grid structures and\nunknown line properties. In this paper, we propose a novel data-driven\nrepresentation of nonlinear power flow equations for radial grids based on\nWillems' Fundamental Lemma. The approach allows for direct integration of\ninput/output data into power flow optimisation, enabling cost minimization and\nconstraint enforcement without requiring explicit knowledge of the electrical\nproperties or the topology of the grid. Moreover, we formulate a convex\nrelaxation to ensure compatibility with state-of-the-art solvers. In a\nnumerical case study, we demonstrate that the novel approach performs similar\nto state-of-the-art methods, without the need for an explicit system\nidentification step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing decentralization of power systems driven by a large number of\nrenewable energy sources poses challenges in power flow optimization. Partially\nunknown power line properties can render model-based approaches unsuitable.\nWith increasing deployment of sensors, data-driven methods rise as a promising\nalternative. They offer the flexibility to adapt to varying grid structures and\nunknown line properties. In this paper, we propose a novel data-driven\nrepresentation of nonlinear power flow equations for radial grids based on\nWillems' Fundamental Lemma. The approach allows for direct integration of\ninput/output data into power flow optimisation, enabling cost minimization and\nconstraint enforcement without requiring explicit knowledge of the electrical\nproperties or the topology of the grid. Moreover, we formulate a convex\nrelaxation to ensure compatibility with state-of-the-art solvers. In a\nnumerical case study, we demonstrate that the novel approach performs similar\nto state-of-the-art methods, without the need for an explicit system\nidentification step."
                },
                "authors": [
                    {
                        "name": "Sebastian Otzen"
                    },
                    {
                        "name": "Hannes M. H. Wolf"
                    },
                    {
                        "name": "Christian A. Hans"
                    }
                ],
                "author_detail": {
                    "name": "Christian A. Hans"
                },
                "author": "Christian A. Hans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04633v2",
                "updated": "2025-09-29T17:40:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    40,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-04T19:51:00Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    19,
                    51,
                    0,
                    3,
                    247,
                    0
                ],
                "title": "The Physical Basis of Prediction: World Model Formation in Neural\n  Organoids via an LLM-Generated Curriculum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Physical Basis of Prediction: World Model Formation in Neural\n  Organoids via an LLM-Generated Curriculum"
                },
                "summary": "The capacity of an embodied agent to understand, predict, and interact with\nits environment is fundamentally contingent on an internal world model. This\npaper introduces a novel framework for investigating the formation and\nadaptation of such world models within a biological substrate: human neural\norganoids. We present a curriculum of three scalable, closed-loop virtual\nenvironments designed to train these biological agents and probe the underlying\nsynaptic mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments that demand progressively more sophisticated world models for\nsuccessful decision-making: (1) a conditional avoidance task for learning\nstatic state-action contingencies, (2) a one-dimensional predator-prey scenario\nfor goal-directed interaction, and (3) a replication of the classic Pong game\nfor modeling dynamic, continuous-time systems. For each environment, we\nformalize the state and action spaces, the sensory encoding and motor decoding\nmechanisms, and the feedback protocols based on predictable (reward) and\nunpredictable (punishment) stimulation, which serve to drive model refinement.\nIn a significant methodological advance, we propose a meta-learning approach\nwhere a Large Language Model automates the generative design and optimization\nof experimental protocols, thereby scaling the process of environment and\ncurriculum design. Finally, we outline a multi-modal evaluation strategy that\nmoves beyond task performance to directly measure the physical correlates of\nthe learned world model by quantifying synaptic plasticity at\nelectrophysiological, cellular, and molecular levels. This work bridges the gap\nbetween model-based reinforcement learning and computational neuroscience,\noffering a unique platform for studying embodiment, decision-making, and the\nphysical basis of intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of an embodied agent to understand, predict, and interact with\nits environment is fundamentally contingent on an internal world model. This\npaper introduces a novel framework for investigating the formation and\nadaptation of such world models within a biological substrate: human neural\norganoids. We present a curriculum of three scalable, closed-loop virtual\nenvironments designed to train these biological agents and probe the underlying\nsynaptic mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments that demand progressively more sophisticated world models for\nsuccessful decision-making: (1) a conditional avoidance task for learning\nstatic state-action contingencies, (2) a one-dimensional predator-prey scenario\nfor goal-directed interaction, and (3) a replication of the classic Pong game\nfor modeling dynamic, continuous-time systems. For each environment, we\nformalize the state and action spaces, the sensory encoding and motor decoding\nmechanisms, and the feedback protocols based on predictable (reward) and\nunpredictable (punishment) stimulation, which serve to drive model refinement.\nIn a significant methodological advance, we propose a meta-learning approach\nwhere a Large Language Model automates the generative design and optimization\nof experimental protocols, thereby scaling the process of environment and\ncurriculum design. Finally, we outline a multi-modal evaluation strategy that\nmoves beyond task performance to directly measure the physical correlates of\nthe learned world model by quantifying synaptic plasticity at\nelectrophysiological, cellular, and molecular levels. This work bridges the gap\nbetween model-based reinforcement learning and computational neuroscience,\noffering a unique platform for studying embodiment, decision-making, and the\nphysical basis of intelligence."
                },
                "authors": [
                    {
                        "name": "Brennen Hill"
                    }
                ],
                "author_detail": {
                    "name": "Brennen Hill"
                },
                "author": "Brennen Hill",
                "arxiv_comment": "Published in the proceedings of the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments\n  for Agents (SEA). Additionally accepted for presentation in NeurIPS 2025\n  Workshop: Embodied World Models for Decision Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T05, 92C20, 93E35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.3; I.6.8; D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25107v1",
                "updated": "2025-09-29T17:39:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    19,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:39:19Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    39,
                    19,
                    0,
                    272,
                    0
                ],
                "title": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant\n  for Question Answering in the Era of LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant\n  for Question Answering in the Era of LLMs?"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings."
                },
                "authors": [
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yin Huang"
                    },
                    {
                        "name": "Srishti Mehra"
                    },
                    {
                        "name": "Mohammad Kachuee"
                    },
                    {
                        "name": "Xilun Chen"
                    },
                    {
                        "name": "Renjie Tao"
                    },
                    {
                        "name": "Zhaojiang Lin"
                    },
                    {
                        "name": "Andrea Jessee"
                    },
                    {
                        "name": "Nirav Shah"
                    },
                    {
                        "name": "Alex Betty"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xin Luna Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Luna Dong"
                },
                "author": "Xin Luna Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04731v2",
                "updated": "2025-09-29T17:38:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    38,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-05T01:03:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    1,
                    3,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "Hierarchical Task Environments as the Next Frontier for Embodied World\n  Models in Robot Soccer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Task Environments as the Next Frontier for Embodied World\n  Models in Robot Soccer"
                },
                "summary": "Recent advances in agent development have focused on scaling model size and\nraw interaction data, mirroring the successes seen in large language models.\nHowever, for complex, long-horizon multi-agent tasks such as robotic soccer,\nthis end-to-end approach often fails due to intractable exploration spaces and\nsparse rewards. This position paper argues that the next frontier in developing\nembodied world models is not merely increasing the fidelity or size of\nenvironments, but scaling their structural complexity through explicit\nhierarchical scaffolding. We posit that an effective world model for\ndecision-making must model not only the world's physics but also its task\nsemantics. Drawing from a systematic review of 2024 research in low-resource\nmulti-agent soccer, we identify a clear trend towards integrating symbolic and\nhierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian\nStrategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These\nmethods decompose complex goals into manageable subgoals, creating an intrinsic\ncurriculum that shapes agent learning. We propose that such structured\nenvironments are essential for bridging the gap between simple, reactive\nbehaviors and sophisticated, strategic team play. We further extend this\nprinciple, proposing that this scaffolding can be generalized to other complex\ndomains and dynamically generated by Large Language Models (LLMs), which act as\ngenerative world models of tasks. By building environments with explicit,\ncomposable task layers, we can guide agent exploration more efficiently,\ngenerate meaningful learning signals, and ultimately train more capable and\ngeneral-purpose agents with fewer resources than purely end-to-end approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in agent development have focused on scaling model size and\nraw interaction data, mirroring the successes seen in large language models.\nHowever, for complex, long-horizon multi-agent tasks such as robotic soccer,\nthis end-to-end approach often fails due to intractable exploration spaces and\nsparse rewards. This position paper argues that the next frontier in developing\nembodied world models is not merely increasing the fidelity or size of\nenvironments, but scaling their structural complexity through explicit\nhierarchical scaffolding. We posit that an effective world model for\ndecision-making must model not only the world's physics but also its task\nsemantics. Drawing from a systematic review of 2024 research in low-resource\nmulti-agent soccer, we identify a clear trend towards integrating symbolic and\nhierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian\nStrategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These\nmethods decompose complex goals into manageable subgoals, creating an intrinsic\ncurriculum that shapes agent learning. We propose that such structured\nenvironments are essential for bridging the gap between simple, reactive\nbehaviors and sophisticated, strategic team play. We further extend this\nprinciple, proposing that this scaffolding can be generalized to other complex\ndomains and dynamically generated by Large Language Models (LLMs), which act as\ngenerative world models of tasks. By building environments with explicit,\ncomposable task layers, we can guide agent exploration more efficiently,\ngenerate meaningful learning signals, and ultimately train more capable and\ngeneral-purpose agents with fewer resources than purely end-to-end approaches."
                },
                "authors": [
                    {
                        "name": "Brennen Hill"
                    }
                ],
                "author_detail": {
                    "name": "Brennen Hill"
                },
                "author": "Brennen Hill",
                "arxiv_comment": "In the 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: Embodied World Models for Decision Making (EWM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 90C40, 91A26, 68T42, 93E35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15816v2",
                "updated": "2025-09-29T17:34:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    34,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-17T03:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    3,
                    10,
                    54,
                    6,
                    229,
                    0
                ],
                "title": "Better Together: Leveraging Multiple Digital Twins for Deployment\n  Optimization of Airborne Base Stations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Together: Leveraging Multiple Digital Twins for Deployment\n  Optimization of Airborne Base Stations"
                },
                "summary": "Airborne Base Stations (ABSs) allow for flexible geographical allocation of\nnetwork resources with dynamically changing load as well as rapid deployment of\nalternate connectivity solutions during natural disasters. Since the radio\ninfrastructure is carried by unmanned aerial vehicles (UAVs) with limited\nflight time, it is important to establish the best location for the ABS without\nexhaustive field trials. This paper proposes a digital twin (DT)-guided\napproach to achieve this through the following key contributions: (i)\nImplementation of an interactive software bridge between two open-source DTs\nsuch that the same scene is evaluated with high fidelity across NVIDIA's Sionna\nand Aerial Omniverse Digital Twin (AODT), highlighting the unique features of\neach of these platforms for this allocation problem, (ii) Design of a\nback-propagation-based algorithm in Sionna for rapidly converging on the\nphysical location of the UAVs, orientation of the antennas and transmit power\nto ensure efficient coverage across the swarm of the UAVs, and (iii) numerical\nevaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies\nthe environmental conditions in which there is agreement or divergence of\nperformance results between these twins. Finally, (iv) we propose a resilience\nmechanism to provide consistent coverage to mission-critical devices and\ndemonstrate a use case for bi-directional flow of information between the two\nDTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Airborne Base Stations (ABSs) allow for flexible geographical allocation of\nnetwork resources with dynamically changing load as well as rapid deployment of\nalternate connectivity solutions during natural disasters. Since the radio\ninfrastructure is carried by unmanned aerial vehicles (UAVs) with limited\nflight time, it is important to establish the best location for the ABS without\nexhaustive field trials. This paper proposes a digital twin (DT)-guided\napproach to achieve this through the following key contributions: (i)\nImplementation of an interactive software bridge between two open-source DTs\nsuch that the same scene is evaluated with high fidelity across NVIDIA's Sionna\nand Aerial Omniverse Digital Twin (AODT), highlighting the unique features of\neach of these platforms for this allocation problem, (ii) Design of a\nback-propagation-based algorithm in Sionna for rapidly converging on the\nphysical location of the UAVs, orientation of the antennas and transmit power\nto ensure efficient coverage across the swarm of the UAVs, and (iii) numerical\nevaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies\nthe environmental conditions in which there is agreement or divergence of\nperformance results between these twins. Finally, (iv) we propose a resilience\nmechanism to provide consistent coverage to mission-critical devices and\ndemonstrate a use case for bi-directional flow of information between the two\nDTs."
                },
                "authors": [
                    {
                        "name": "Mauro Belgiovine"
                    },
                    {
                        "name": "Chris Dick"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "author": "Kaushik Chowdhury",
                "arxiv_comment": "Submitted to IEEE Transactions on Mobile Computing (second round of\n  review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25100v1",
                "updated": "2025-09-29T17:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    34,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    34,
                    2,
                    0,
                    272,
                    0
                ],
                "title": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation"
                },
                "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines."
                },
                "authors": [
                    {
                        "name": "Aasheesh Singh"
                    },
                    {
                        "name": "Vishal Vaddina"
                    },
                    {
                        "name": "Dagnachew Birru"
                    }
                ],
                "author_detail": {
                    "name": "Dagnachew Birru"
                },
                "author": "Dagnachew Birru",
                "arxiv_comment": "Accepted at NeurIPS 2025, Efficient Reasoning Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25087v1",
                "updated": "2025-09-29T17:26:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    26,
                    11,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:26:11Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    26,
                    11,
                    0,
                    272,
                    0
                ],
                "title": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families"
                },
                "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs."
                },
                "authors": [
                    {
                        "name": "Shane Bergsma"
                    },
                    {
                        "name": "Bin Claire Zhang"
                    },
                    {
                        "name": "Nolan Dey"
                    },
                    {
                        "name": "Shaheer Muhammad"
                    },
                    {
                        "name": "Gurpreet Gosal"
                    },
                    {
                        "name": "Joel Hestness"
                    }
                ],
                "author_detail": {
                    "name": "Joel Hestness"
                },
                "author": "Joel Hestness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25086v1",
                "updated": "2025-09-29T17:25:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    25,
                    56,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:25:56Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    25,
                    56,
                    0,
                    272,
                    0
                ],
                "title": "Towards Trustworthy Lexical Simplification: Exploring Safety and\n  Efficiency with Small LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy Lexical Simplification: Exploring Safety and\n  Efficiency with Small LLMs"
                },
                "summary": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment."
                },
                "authors": [
                    {
                        "name": "Akio Hayakawa"
                    },
                    {
                        "name": "Stefan Bott"
                    },
                    {
                        "name": "Horacio Saggion"
                    }
                ],
                "author_detail": {
                    "name": "Horacio Saggion"
                },
                "author": "Horacio Saggion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25072v1",
                "updated": "2025-09-29T17:16:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    16,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:16:51Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    16,
                    51,
                    0,
                    272,
                    0
                ],
                "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications"
                },
                "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference."
                },
                "authors": [
                    {
                        "name": "Yaman Jandali"
                    },
                    {
                        "name": "Ruisi Zhang"
                    },
                    {
                        "name": "Nojan Sheybani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25063v1",
                "updated": "2025-09-29T17:12:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    12,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:12:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    12,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for\n  Survey Non-response in the German Longitudinal Election Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for\n  Survey Non-response in the German Longitudinal Election Study"
                },
                "summary": "Survey researchers face two key challenges: the rising costs of probability\nsamples and missing data (e.g., non-response or attrition), which can undermine\ninference and increase the use of convenience samples. Recent work explores\nusing large language models (LLMs) to simulate respondents via persona-based\nprompts, often without labeled data. We study a more practical setting where\npartial survey responses exist: we fine-tune LLMs on available data to impute\nself-reported vote choice under both random and systematic nonresponse, using\nthe German Longitudinal Election Study. We compare zero-shot prompting and\nsupervised fine-tuning against tabular classifiers (e.g., CatBoost) and test\nhow different convenience samples (e.g., students) used for fine-tuning affect\ngeneralization.\n  Our results show that when data are missing completely at random, fine-tuned\nLLMs match tabular classifiers but outperform zero-shot approaches. When only\nbiased convenience samples are available, fine-tuning small (3B to 8B)\nopen-source LLMs can recover both individual-level predictions and\npopulation-level distributions more accurately than zero-shot and often better\nthan tabular methods. This suggests fine-tuned LLMs offer a promising strategy\nfor researchers working with non-probability samples or systematic missingness,\nand may enable new survey designs requiring only easily accessible\nsubpopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey researchers face two key challenges: the rising costs of probability\nsamples and missing data (e.g., non-response or attrition), which can undermine\ninference and increase the use of convenience samples. Recent work explores\nusing large language models (LLMs) to simulate respondents via persona-based\nprompts, often without labeled data. We study a more practical setting where\npartial survey responses exist: we fine-tune LLMs on available data to impute\nself-reported vote choice under both random and systematic nonresponse, using\nthe German Longitudinal Election Study. We compare zero-shot prompting and\nsupervised fine-tuning against tabular classifiers (e.g., CatBoost) and test\nhow different convenience samples (e.g., students) used for fine-tuning affect\ngeneralization.\n  Our results show that when data are missing completely at random, fine-tuned\nLLMs match tabular classifiers but outperform zero-shot approaches. When only\nbiased convenience samples are available, fine-tuning small (3B to 8B)\nopen-source LLMs can recover both individual-level predictions and\npopulation-level distributions more accurately than zero-shot and often better\nthan tabular methods. This suggests fine-tuned LLMs offer a promising strategy\nfor researchers working with non-probability samples or systematic missingness,\nand may enable new survey designs requiring only easily accessible\nsubpopulations."
                },
                "authors": [
                    {
                        "name": "Tobias Holtdirk"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    },
                    {
                        "name": "Arnim Bleier"
                    },
                    {
                        "name": "Claudia Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Wagner"
                },
                "author": "Claudia Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25056v1",
                "updated": "2025-09-29T17:06:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    6,
                    25,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:06:25Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    6,
                    25,
                    0,
                    272,
                    0
                ],
                "title": "AgriCruiser: An Open Source Agriculture Robot for Over-the-row\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgriCruiser: An Open Source Agriculture Robot for Over-the-row\n  Navigation"
                },
                "summary": "We present the AgriCruiser, an open-source over-the-row agricultural robot\ndeveloped for low-cost deployment and rapid adaptation across diverse crops and\nrow layouts. The chassis provides an adjustable track width of 1.42 m to 1.57\nm, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact\npivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland\nmaneuvers. The platform is designed for the integration of the other\nsubsystems, and in this study, a precision spraying system was implemented to\nassess its effectiveness in weed management. In twelve flax plots, a single\nrobotic spray pass reduced total weed populations (pigweed and Venice mallow)\nby 24- to 42-fold compared to manual weeding in four flax plots, while also\ncausing less crop damage. Mobility experiments conducted on concrete, asphalt,\ngravel, grass, and both wet and dry soil confirmed reliable traversal\nconsistent with torque sizing. The complete chassis can be constructed from\ncommodity T-slot extrusion with minimal machining, resulting in a bill of\nmaterials costing approximately $5,000 - $6,000, which enables replication and\ncustomization. The mentioned results demonstrate that low-cost, reconfigurable\nover-the-row robots can achieve effective weed management with reduced crop\ndamage and labor requirements, while providing a versatile foundation for\nphenotyping, sensing, and other agriculture applications. Design files and\nimplementation details are released to accelerate research and adoption of\nmodular agricultural robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the AgriCruiser, an open-source over-the-row agricultural robot\ndeveloped for low-cost deployment and rapid adaptation across diverse crops and\nrow layouts. The chassis provides an adjustable track width of 1.42 m to 1.57\nm, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact\npivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland\nmaneuvers. The platform is designed for the integration of the other\nsubsystems, and in this study, a precision spraying system was implemented to\nassess its effectiveness in weed management. In twelve flax plots, a single\nrobotic spray pass reduced total weed populations (pigweed and Venice mallow)\nby 24- to 42-fold compared to manual weeding in four flax plots, while also\ncausing less crop damage. Mobility experiments conducted on concrete, asphalt,\ngravel, grass, and both wet and dry soil confirmed reliable traversal\nconsistent with torque sizing. The complete chassis can be constructed from\ncommodity T-slot extrusion with minimal machining, resulting in a bill of\nmaterials costing approximately $5,000 - $6,000, which enables replication and\ncustomization. The mentioned results demonstrate that low-cost, reconfigurable\nover-the-row robots can achieve effective weed management with reduced crop\ndamage and labor requirements, while providing a versatile foundation for\nphenotyping, sensing, and other agriculture applications. Design files and\nimplementation details are released to accelerate research and adoption of\nmodular agricultural robotics."
                },
                "authors": [
                    {
                        "name": "Kenny Truong"
                    },
                    {
                        "name": "Yongkyu Lee"
                    },
                    {
                        "name": "Jason Irie"
                    },
                    {
                        "name": "Shivam Kumar Panda"
                    },
                    {
                        "name": "Shahab Ahmad"
                    },
                    {
                        "name": "Md. Mukhlesur Rahman"
                    },
                    {
                        "name": "M. Khalid Jawed"
                    }
                ],
                "author_detail": {
                    "name": "M. Khalid Jawed"
                },
                "author": "M. Khalid Jawed",
                "arxiv_comment": "GitHub: https://github.com/structuresComp/agri-cruiser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25052v1",
                "updated": "2025-09-29T17:02:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    31,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:02:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning"
                },
                "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience."
                },
                "authors": [
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zhongwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongwen Xu"
                },
                "author": "Zhongwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25050v1",
                "updated": "2025-09-29T17:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    20,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:02:20Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    2,
                    20,
                    0,
                    272,
                    0
                ],
                "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models"
                },
                "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching."
                },
                "authors": [
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Chongjian Ge"
                    },
                    {
                        "name": "Shilong Zhang"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Zhi-Ming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Ming Ma"
                },
                "author": "Zhi-Ming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25048v1",
                "updated": "2025-09-29T17:00:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    0,
                    38,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:00:38Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    0,
                    38,
                    0,
                    272,
                    0
                ],
                "title": "Confidence-Guided Error Correction for Disordered Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Guided Error Correction for Disordered Speech Recognition"
                },
                "summary": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech."
                },
                "authors": [
                    {
                        "name": "Abner Hernandez"
                    },
                    {
                        "name": "Toms Arias Vergara"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Paula Andrea Prez-Toro"
                    }
                ],
                "author_detail": {
                    "name": "Paula Andrea Prez-Toro"
                },
                "author": "Paula Andrea Prez-Toro",
                "arxiv_comment": "Preprint submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25045v1",
                "updated": "2025-09-29T16:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    59,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    59,
                    7,
                    0,
                    272,
                    0
                ],
                "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures"
                },
                "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations."
                },
                "authors": [
                    {
                        "name": "Marco Bronzini"
                    },
                    {
                        "name": "Carlo Nicolini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Andrea Passerini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Passerini"
                },
                "author": "Andrea Passerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25043v1",
                "updated": "2025-09-29T16:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    21,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    21,
                    0,
                    272,
                    0
                ],
                "title": "Large Language Models for Software Testing: A Research Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Software Testing: A Research Roadmap"
                },
                "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field."
                },
                "authors": [
                    {
                        "name": "Cristian Augusto"
                    },
                    {
                        "name": "Antonia Bertolino"
                    },
                    {
                        "name": "Guglielmo De Angelis"
                    },
                    {
                        "name": "Francesca Lonetti"
                    },
                    {
                        "name": "Jess Morn"
                    }
                ],
                "author_detail": {
                    "name": "Jess Morn"
                },
                "author": "Jess Morn",
                "arxiv_comment": "40 pages & 10 figures Submitted on 29th September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02536v2",
                "updated": "2025-09-29T16:58:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    58,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T07:20:54Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    7,
                    20,
                    54,
                    1,
                    154,
                    0
                ],
                "title": "Answer Convergence as a Signal for Early Stopping in Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Convergence as a Signal for Early Stopping in Reasoning"
                },
                "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13316v2",
                "updated": "2025-09-29T16:57:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    58,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?"
                },
                "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey can succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets may not be ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the knowledge of the target LLM whose activations are\ndecoded. Taken together, our results indicate a need for targeted benchmarks\nand experimental controls to rigorously assess whether verbalization methods\nprovide meaningful insights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey can succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets may not be ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the knowledge of the target LLM whose activations are\ndecoded. Taken together, our results indicate a need for targeted benchmarks\nand experimental controls to rigorously assess whether verbalization methods\nprovide meaningful insights into the operations of LLMs."
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "37 pages, 6 figures. Updated content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25041v1",
                "updated": "2025-09-29T16:57:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    33,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:57:33Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    57,
                    33,
                    0,
                    272,
                    0
                ],
                "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference"
                },
                "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Lehan Pan"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Ziyang Tao"
                    },
                    {
                        "name": "Wuyang Zhang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v3",
                "updated": "2025-09-29T16:56:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    56,
                    56,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25034v1",
                "updated": "2025-09-29T16:53:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    53,
                    24,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:53:24Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    53,
                    24,
                    0,
                    272,
                    0
                ],
                "title": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence\n  and LLM Guidance for Reservoir Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence\n  and LLM Guidance for Reservoir Management"
                },
                "summary": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management."
                },
                "authors": [
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Shan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shan Lin"
                },
                "author": "Shan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25033v1",
                "updated": "2025-09-29T16:52:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    52,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:52:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    52,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"
                },
                "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qiangchang Wang"
                    },
                    {
                        "name": "Xianjing Meng"
                    },
                    {
                        "name": "Zhibin Wu"
                    },
                    {
                        "name": "Yilong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Yin"
                },
                "author": "Yilong Yin",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00031v2",
                "updated": "2025-09-29T16:45:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    45,
                    41,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-21T01:18:27Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    1,
                    18,
                    27,
                    3,
                    233,
                    0
                ],
                "title": "End-to-End On-Device Quantization-Aware Training for LLMs at Inference\n  Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End On-Device Quantization-Aware Training for LLMs at Inference\n  Cost"
                },
                "summary": "Quantization is an effective technique to reduce the deployment cost of large\nlanguage models (LLMs), and post-training quantization (PTQ) has been widely\nstudied due to its efficiency. However, existing PTQ methods are limited by\ntheir inability to fine-tune model parameters and often suffer significant\naccuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides\na more principled solution, but its reliance on backpropagation incurs\nprohibitive memory costs, limiting its practicality for LLM deployment. To\naddress these challenges, we propose ZeroQAT, a zeroth-order optimization-based\nQAT framework that supports both weight and activation quantization. ZeroQAT\nleverages forward-only gradient estimation to eliminate backpropagation,\nsubstantially reducing computational and memory overhead while retaining the\nbenefits of end-to-end optimization. We further introduce a lightweight variant\nof ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most\nparameters to further cut memory usage. Experiments show that ZeroQAT\nconsistently outperforms representative PTQ and QAT baselines while requiring\nsignificantly less memory. For example, ZeroQAT enables fine-tuning of a 13B\nmodel at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and\neven allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating\nits practicality for end-to-end QAT on resource-limited edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is an effective technique to reduce the deployment cost of large\nlanguage models (LLMs), and post-training quantization (PTQ) has been widely\nstudied due to its efficiency. However, existing PTQ methods are limited by\ntheir inability to fine-tune model parameters and often suffer significant\naccuracy loss in low-bit scenarios. Quantization-aware training (QAT) provides\na more principled solution, but its reliance on backpropagation incurs\nprohibitive memory costs, limiting its practicality for LLM deployment. To\naddress these challenges, we propose ZeroQAT, a zeroth-order optimization-based\nQAT framework that supports both weight and activation quantization. ZeroQAT\nleverages forward-only gradient estimation to eliminate backpropagation,\nsubstantially reducing computational and memory overhead while retaining the\nbenefits of end-to-end optimization. We further introduce a lightweight variant\nof ZeroQAT for quantized fine-tuning, which freezes and pre-quantizes most\nparameters to further cut memory usage. Experiments show that ZeroQAT\nconsistently outperforms representative PTQ and QAT baselines while requiring\nsignificantly less memory. For example, ZeroQAT enables fine-tuning of a 13B\nmodel at extremely low bit-widths (e.g., 2-4 bits) on a single 8GB GPU, and\neven allows fine-tuning a 6.7B model on a OnePlus 12 smartphone, demonstrating\nits practicality for end-to-end QAT on resource-limited edge devices."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Xiaoying Song"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Lingzi Hong"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25021v1",
                "updated": "2025-09-29T16:45:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    45,
                    31,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:45:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    45,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "CCAT: Mod-Cam Readout Overview and Flexible Stripline Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCAT: Mod-Cam Readout Overview and Flexible Stripline Performance"
                },
                "summary": "The CCAT Observatory's first-light and commissioning instrument, Mod-Cam, is\nnearing readiness for deployment to the Fred Young Submillimeter Telescope\n(FYST) in the Atacama Desert in northern Chile. In-lab testing of Mod-Cam and\nthe first CCAT instrument module, a 280 GHz broadband camera fielding over\n10,000 kinetic inductance detectors (KIDs), is currently underway. CCAT's\nfirst-generation science instrument, Prime-Cam, will field approximately\n100,000 KIDs across seven instrument modules. Like Mod-Cam, it employs 46 cm\nlong low-thermal-conductivity flexible circuits (\"stripline\") between 4 K and\n300 K to connect large arrays of multiplexed detectors in each instrument\nmodule to readout electronics. The 280 GHz camera currently installed in\nMod-Cam uses six striplines to read out its over 10,000 detectors across 18 RF\nchains, each individual stripline containing six traces. In-lab testing thus\nfar has allowed us to begin optimizing the attenuation in the readout chains\nfor each of the three detector arrays in the 280 GHz module and demonstrate\nsimultaneous readout of all networks using CCAT's RFSoC warm readout\nelectronics. Here we present an overview of the Mod-Cam cold readout and\ndiscuss the stripline performance by investigating how its thermal conductivity\nimpacts Mod-Cam and Prime-Cam cryogenic performance. We also report on\nstripline electrical transmission and crosstalk, identifying transition printed\ncircuit boards (PCBs) as the dominant source of crosstalk. Using this result we\noutline design improvements to these PCBs that enhance isolation between\nreadout networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CCAT Observatory's first-light and commissioning instrument, Mod-Cam, is\nnearing readiness for deployment to the Fred Young Submillimeter Telescope\n(FYST) in the Atacama Desert in northern Chile. In-lab testing of Mod-Cam and\nthe first CCAT instrument module, a 280 GHz broadband camera fielding over\n10,000 kinetic inductance detectors (KIDs), is currently underway. CCAT's\nfirst-generation science instrument, Prime-Cam, will field approximately\n100,000 KIDs across seven instrument modules. Like Mod-Cam, it employs 46 cm\nlong low-thermal-conductivity flexible circuits (\"stripline\") between 4 K and\n300 K to connect large arrays of multiplexed detectors in each instrument\nmodule to readout electronics. The 280 GHz camera currently installed in\nMod-Cam uses six striplines to read out its over 10,000 detectors across 18 RF\nchains, each individual stripline containing six traces. In-lab testing thus\nfar has allowed us to begin optimizing the attenuation in the readout chains\nfor each of the three detector arrays in the 280 GHz module and demonstrate\nsimultaneous readout of all networks using CCAT's RFSoC warm readout\nelectronics. Here we present an overview of the Mod-Cam cold readout and\ndiscuss the stripline performance by investigating how its thermal conductivity\nimpacts Mod-Cam and Prime-Cam cryogenic performance. We also report on\nstripline electrical transmission and crosstalk, identifying transition printed\ncircuit boards (PCBs) as the dominant source of crosstalk. Using this result we\noutline design improvements to these PCBs that enhance isolation between\nreadout networks."
                },
                "authors": [
                    {
                        "name": "Ben Keller"
                    },
                    {
                        "name": "Rodrigo Freundt"
                    },
                    {
                        "name": "James R. Burgoyne"
                    },
                    {
                        "name": "Scott Chapman"
                    },
                    {
                        "name": "Steve Choi"
                    },
                    {
                        "name": "Cody J. Duell"
                    },
                    {
                        "name": "Christopher Groppi"
                    },
                    {
                        "name": "Caleb Humphreys"
                    },
                    {
                        "name": "Lawrence T. Lin"
                    },
                    {
                        "name": "Alicia Middleton"
                    },
                    {
                        "name": "Michael D. Niemack"
                    },
                    {
                        "name": "Darshan Patel"
                    },
                    {
                        "name": "Eve Vavagiakis"
                    },
                    {
                        "name": "Samantha Walker"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Ruixuan"
                    },
                    {
                        "name": "Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xie"
                },
                "arxiv_affiliation": "Matt",
                "author": "Xie",
                "arxiv_comment": "6 pages, 7 figures, submitted to LTD 2025 conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25020v1",
                "updated": "2025-09-29T16:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    44,
                    22,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    44,
                    22,
                    0,
                    272,
                    0
                ],
                "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts"
                },
                "summary": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Anya Sims"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Ning Miao"
                    }
                ],
                "author_detail": {
                    "name": "Ning Miao"
                },
                "author": "Ning Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20086v2",
                "updated": "2025-09-29T16:30:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    30,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-27T17:54:15Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    54,
                    15,
                    2,
                    239,
                    0
                ],
                "title": "Smart Contract Intent Detection with Pre-trained Programming Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Contract Intent Detection with Pre-trained Programming Language\n  Model"
                },
                "summary": "Malicious developer intents in smart contracts constitute a significant\nsecurity threat in decentralized applications (DApps), leading to substantial\neconomic losses. To address this, SmartIntentNN was previously introduced as a\ndeep learning model for detecting unsafe developer intents. It integrates the\nUniversal Sentence Encoder, K-means clustering-based intent highlighting, and a\nBidirectional Long Short-Term Memory (BiLSTM) network for multi-label\nclassification, achieving an F1 score of 0.8633.\n  In this study, we present an enhanced version of this model, SmartIntentNN2\n(Smart Contract Intent Neural Network V2). The primary enhancement is the\nintegration of a BERT-based pre-trained programming language model, which we\ndomain-adaptively pre-train on a dataset of 16,000 real-world smart contracts\nusing a Masked Language Modeling (MLM) objective. SmartIntentNN2 retains the\nBiLSTM-based multi-label classification network for downstream tasks.\nExperimental results demonstrate that SmartIntentNN2 achieves superior overall\nperformance with an accuracy of 0.9789, precision of 0.9090, recall of 0.9476,\nand an F1 score of 0.9279, substantially outperforming its predecessor and\nother baseline models. Notably, SmartIntentNN2 also shows significant\nadvantages over large language models (LLMs), achieving a 65.5% relative\nimprovement in F1 score over GPT-4.1 on this specialized task. These results\nestablish SmartIntentNN2 as the new state-of-the-art model for smart contract\nintent detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious developer intents in smart contracts constitute a significant\nsecurity threat in decentralized applications (DApps), leading to substantial\neconomic losses. To address this, SmartIntentNN was previously introduced as a\ndeep learning model for detecting unsafe developer intents. It integrates the\nUniversal Sentence Encoder, K-means clustering-based intent highlighting, and a\nBidirectional Long Short-Term Memory (BiLSTM) network for multi-label\nclassification, achieving an F1 score of 0.8633.\n  In this study, we present an enhanced version of this model, SmartIntentNN2\n(Smart Contract Intent Neural Network V2). The primary enhancement is the\nintegration of a BERT-based pre-trained programming language model, which we\ndomain-adaptively pre-train on a dataset of 16,000 real-world smart contracts\nusing a Masked Language Modeling (MLM) objective. SmartIntentNN2 retains the\nBiLSTM-based multi-label classification network for downstream tasks.\nExperimental results demonstrate that SmartIntentNN2 achieves superior overall\nperformance with an accuracy of 0.9789, precision of 0.9090, recall of 0.9476,\nand an F1 score of 0.9279, substantially outperforming its predecessor and\nother baseline models. Notably, SmartIntentNN2 also shows significant\nadvantages over large language models (LLMs), achieving a 65.5% relative\nimprovement in F1 score over GPT-4.1 on this specialized task. These results\nestablish SmartIntentNN2 as the new state-of-the-art model for smart contract\nintent detection."
                },
                "authors": [
                    {
                        "name": "Youwei Huang"
                    },
                    {
                        "name": "Jianwen Li"
                    },
                    {
                        "name": "Sen Fang"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "arxiv_comment": "11 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25004v1",
                "updated": "2025-09-29T16:29:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    29,
                    4,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:29:04Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    29,
                    4,
                    0,
                    272,
                    0
                ],
                "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning"
                },
                "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models."
                },
                "authors": [
                    {
                        "name": "Shijie Zhang"
                    },
                    {
                        "name": "Guohao Sun"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Xiang Guo"
                    },
                    {
                        "name": "Rujun Guo"
                    }
                ],
                "author_detail": {
                    "name": "Rujun Guo"
                },
                "author": "Rujun Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04690v2",
                "updated": "2025-09-29T16:20:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    20,
                    58,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-05T07:13:59Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    7,
                    13,
                    59,
                    3,
                    156,
                    0
                ],
                "title": "Towards Better Generalization via Distributional Input Projection\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Generalization via Distributional Input Projection\n  Network"
                },
                "summary": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning."
                },
                "authors": [
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yanxin Lu"
                    },
                    {
                        "name": "Hanning Zhang"
                    },
                    {
                        "name": "Xinwei Shen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24988v1",
                "updated": "2025-09-29T16:19:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:19:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    19,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns"
                },
                "summary": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection."
                },
                "authors": [
                    {
                        "name": "Hanqi Xiao"
                    },
                    {
                        "name": "Vaidehi Patil"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Code:\n  https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19580v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19580v3",
                "updated": "2025-09-29T16:13:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    13,
                    52,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-23T21:09:24Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    9,
                    24,
                    1,
                    266,
                    0
                ],
                "title": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines"
                },
                "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Shifu Hou"
                    },
                    {
                        "name": "Weixiang Sun"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Yijun Ma"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Steven Corcelli"
                    },
                    {
                        "name": "Robert Goulding"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "John Lalor"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Tengfei Luo"
                    },
                    {
                        "name": "Ed Maginn"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Jason Rohr"
                    },
                    {
                        "name": "Brett Savoie"
                    },
                    {
                        "name": "Daniel Slate"
                    },
                    {
                        "name": "Tom Stapleford"
                    },
                    {
                        "name": "Matthew Webber"
                    },
                    {
                        "name": "Olaf Wiest"
                    },
                    {
                        "name": "Johnny Zhang"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19580v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19580v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24981v1",
                "updated": "2025-09-29T16:09:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    9,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:09:07Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    9,
                    7,
                    0,
                    272,
                    0
                ],
                "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards"
                },
                "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods."
                },
                "authors": [
                    {
                        "name": "Haoran He"
                    },
                    {
                        "name": "Yuxiao Ye"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Ling Pan"
                    }
                ],
                "author_detail": {
                    "name": "Ling Pan"
                },
                "author": "Ling Pan",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24975v1",
                "updated": "2025-09-29T16:04:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:04:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    4,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via\n  Repetitive Pattern"
                },
                "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open ."
                },
                "authors": [
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Yuetong Liu"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19923v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19923v3",
                "updated": "2025-09-29T16:01:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    1,
                    54,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-24T18:01:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    18,
                    1,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs"
                },
                "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas. These auxiliary\nlemmas are not limited to subgoals in the formal proof but can also include\nspecial cases or potentially useful facts derived from the assumptions, which\nhelp in discovering a viable proof strategy. It achieves an 88.1% success rate\non the MiniF2F benchmark, establishing a new state-of-the-art among methods\nusing small language models (SLMs) with a much lower sample budget than\nprevious approaches. We also present theoretical analyses and case studies that\nillustrate how these generated lemmas contribute to solving challenging\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas. These auxiliary\nlemmas are not limited to subgoals in the formal proof but can also include\nspecial cases or potentially useful facts derived from the assumptions, which\nhelp in discovering a viable proof strategy. It achieves an 88.1% success rate\non the MiniF2F benchmark, establishing a new state-of-the-art among methods\nusing small language models (SLMs) with a much lower sample budget than\nprevious approaches. We also present theoretical analyses and case studies that\nillustrate how these generated lemmas contribute to solving challenging\nproblems."
                },
                "authors": [
                    {
                        "name": "Kaito Baba"
                    },
                    {
                        "name": "Chaoran Liu"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Akiyoshi Sannai"
                    }
                ],
                "author_detail": {
                    "name": "Akiyoshi Sannai"
                },
                "author": "Akiyoshi Sannai",
                "arxiv_comment": "36 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19923v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19923v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24967v1",
                "updated": "2025-09-29T16:00:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    0,
                    41,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T16:00:41Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    0,
                    41,
                    0,
                    272,
                    0
                ],
                "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecInfer: Preventing Prompt Injection via Inference-time Scaling"
                },
                "summary": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04952v2",
                "updated": "2025-09-29T15:55:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    55,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-07-07T12:53:00Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    53,
                    0,
                    0,
                    188,
                    0
                ],
                "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation"
                },
                "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24961v1",
                "updated": "2025-09-29T15:53:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    53,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:53:47Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    53,
                    47,
                    0,
                    272,
                    0
                ],
                "title": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in\n  Recommender Systems"
                },
                "summary": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield."
                },
                "authors": [
                    {
                        "name": "Kaihong Li"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Fangjun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fangjun Huang"
                },
                "author": "Fangjun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24958v1",
                "updated": "2025-09-29T15:52:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    36,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:52:36Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    36,
                    0,
                    272,
                    0
                ],
                "title": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents'\n  Inquiry Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents'\n  Inquiry Capability"
                },
                "summary": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings."
                },
                "authors": [
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Yunghwei Lai"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24957v1",
                "updated": "2025-09-29T15:52:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    8,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:52:08Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    52,
                    8,
                    0,
                    272,
                    0
                ],
                "title": "Intra-request branch orchestration for efficient LLM reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-request branch orchestration for efficient LLM reasoning"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates."
                },
                "authors": [
                    {
                        "name": "Weifan Jiang"
                    },
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24956v1",
                "updated": "2025-09-29T15:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    50,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:50:51Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    50,
                    51,
                    0,
                    272,
                    0
                ],
                "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic\n  Manipulation"
                },
                "summary": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de."
                },
                "authors": [
                    {
                        "name": "Jan Ole von Hartz"
                    },
                    {
                        "name": "Lukas Schweizer"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24945v1",
                "updated": "2025-09-29T15:43:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    43,
                    59,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:43:59Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    43,
                    59,
                    0,
                    272,
                    0
                ],
                "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model\n  Reasoners with Open Training Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model\n  Reasoners with Open Training Recipes"
                },
                "summary": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study."
                },
                "authors": [
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Ernie Chang"
                    },
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Chia-Jung Chang"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Chen Lai"
                    },
                    {
                        "name": "Rick Cao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Yangyang Shi"
                    },
                    {
                        "name": "Vikas Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Chandra"
                },
                "author": "Vikas Chandra",
                "arxiv_comment": "Model:\n  https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24943v1",
                "updated": "2025-09-29T15:42:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    42,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:42:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    42,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular\n  Clue Exploration with Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular\n  Clue Exploration with Interactive Agents"
                },
                "summary": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro."
                },
                "authors": [
                    {
                        "name": "Jiahua Li"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Zibo Su"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Cheng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Deng"
                },
                "author": "Cheng Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24930v1",
                "updated": "2025-09-29T15:34:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    34,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:34:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    34,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "How Well Do LLMs Imitate Human Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Imitate Human Writing Style?"
                },
                "summary": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation."
                },
                "authors": [
                    {
                        "name": "Rebira Jemama"
                    },
                    {
                        "name": "Rajesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Kumar"
                },
                "author": "Rajesh Kumar",
                "arxiv_comment": "IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v7",
                "updated": "2025-09-29T15:26:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    26,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24923v1",
                "updated": "2025-09-29T15:25:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    25,
                    42,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:25:42Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    25,
                    42,
                    0,
                    272,
                    0
                ],
                "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training"
                },
                "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior."
                },
                "authors": [
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Xiaoyin Chen"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Roy Xie"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24922v1",
                "updated": "2025-09-29T15:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning"
                },
                "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Jianhui Yang"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24908v1",
                "updated": "2025-09-29T15:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    17,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    17,
                    0,
                    272,
                    0
                ],
                "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications"
                },
                "summary": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%)."
                },
                "authors": [
                    {
                        "name": "Andrs Fernndez Garca"
                    },
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Roser Morante"
                    },
                    {
                        "name": "Enrique Amig"
                    },
                    {
                        "name": "Alejandro Benito-Santos"
                    },
                    {
                        "name": "Jorge Carrillo-de-Albornoz"
                    },
                    {
                        "name": "Vctor Fresno"
                    },
                    {
                        "name": "Adrian Ghajari"
                    },
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Laura Plaza"
                    },
                    {
                        "name": "Eva Snchez Salido"
                    }
                ],
                "author_detail": {
                    "name": "Eva Snchez Salido"
                },
                "author": "Eva Snchez Salido",
                "arxiv_comment": "Published in SEPLN 2025. 20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24903v1",
                "updated": "2025-09-29T15:13:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    13,
                    3,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T15:13:03Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    13,
                    3,
                    0,
                    272,
                    0
                ],
                "title": "DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving\n  Beyond Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving\n  Beyond Limits"
                },
                "summary": "Cooperative perception enabled by Vehicle-to-Everything communication has\nshown great promise in enhancing situational awareness for autonomous vehicles\nand other mobile robotic platforms. Despite recent advances in perception\nbackbones and multi-agent fusion, real-world deployments remain challenged by\nhard detection cases, exemplified by partial detections and noise accumulation\nwhich limit downstream detection accuracy. This work presents Diffusion on\nReinforced Cooperative Perception (DRCP), a real-time deployable framework\ndesigned to address aforementioned issues in dynamic driving environments. DRCP\nintegrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,\na cross-modal cooperative perception module that leverages\ncamera-intrinsic-aware angular partitioning for attention-based fusion and\nadaptive convolution to better exploit external features; and (2)\nMask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement\nmodule that encourages robustness against feature perturbations and aligns\nbird's-eye-view features closer to the task-optimal manifold. The proposed\nsystem achieves real-time performance on mobile platforms while significantly\nimproving robustness under challenging conditions. Code will be released in\nlate 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative perception enabled by Vehicle-to-Everything communication has\nshown great promise in enhancing situational awareness for autonomous vehicles\nand other mobile robotic platforms. Despite recent advances in perception\nbackbones and multi-agent fusion, real-world deployments remain challenged by\nhard detection cases, exemplified by partial detections and noise accumulation\nwhich limit downstream detection accuracy. This work presents Diffusion on\nReinforced Cooperative Perception (DRCP), a real-time deployable framework\ndesigned to address aforementioned issues in dynamic driving environments. DRCP\nintegrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,\na cross-modal cooperative perception module that leverages\ncamera-intrinsic-aware angular partitioning for attention-based fusion and\nadaptive convolution to better exploit external features; and (2)\nMask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement\nmodule that encourages robustness against feature perturbations and aligns\nbird's-eye-view features closer to the task-optimal manifold. The proposed\nsystem achieves real-time performance on mobile platforms while significantly\nimproving robustness under challenging conditions. Code will be released in\nlate 2025."
                },
                "authors": [
                    {
                        "name": "Lantao Li"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Chen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chen Sun"
                },
                "author": "Chen Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06608v2",
                "updated": "2025-09-29T15:10:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    10,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-08T12:26:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    26,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors"
                },
                "summary": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Vadim Kurochkin"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21173v2",
                "updated": "2025-09-29T15:09:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    9,
                    0,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-25T13:54:34Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    54,
                    34,
                    3,
                    268,
                    0
                ],
                "title": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy"
                },
                "summary": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role."
                },
                "authors": [
                    {
                        "name": "Aymen Bouguerra"
                    },
                    {
                        "name": "Daniel Montoya"
                    },
                    {
                        "name": "Alexandra Gomez-Villa"
                    },
                    {
                        "name": "Fabio Arnez"
                    },
                    {
                        "name": "Chokri Mraidha"
                    }
                ],
                "author_detail": {
                    "name": "Chokri Mraidha"
                },
                "author": "Chokri Mraidha",
                "arxiv_comment": "Still unpolished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24884v1",
                "updated": "2025-09-29T14:59:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    59,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:59:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    59,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Expanding Computation Spaces of LLMs at Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Computation Spaces of LLMs at Inference Time"
                },
                "summary": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving."
                },
                "authors": [
                    {
                        "name": "Yoonna Jang"
                    },
                    {
                        "name": "Kisu Yang"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19301v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19301v3",
                "updated": "2025-09-29T14:58:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    58,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-01-31T16:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond checkmate: exploring the creative chokepoints in AI text"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has revolutionized text\ngeneration but also raised concerns about potential misuse, making detecting\nLLM-generated text (AI text) increasingly essential. While prior work has\nfocused on identifying AI text and effectively checkmating it, our study\ninvestigates a less-explored territory: portraying the nuanced distinctions\nbetween human and AI texts across text segments (introduction, body, and\nconclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity\nacross text segments, the results will critically inform their viability and\nboundaries as effective creative assistants to humans. Through an analogy with\nthe structure of chess games, comprising opening, middle, and end games, we\nanalyze segment-specific patterns to reveal where the most striking differences\nlie. Although AI texts closely resemble human writing in the body segment due\nto its length, deeper analysis shows a higher divergence in features dependent\non the continuous flow of language, making it the most informative segment for\ndetection. Additionally, human texts exhibit greater stylistic variation across\nsegments, offering a new lens for distinguishing them from AI. Overall, our\nfindings provide fresh insights into human-AI text differences and pave the way\nfor more effective and interpretable detection strategies. Codes available at\nhttps://github.com/tripto03/chess_inspired_human_ai_text_distinction."
                },
                "authors": [
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted at 30th Conference on Empirical Methods in Natural Language\n  Processing (EMNLP'25 Main conference). 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19301v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19301v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01752v2",
                "updated": "2025-09-29T14:57:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    57,
                    36,
                    0,
                    272,
                    0
                ],
                "published": "2024-12-02T17:47:13Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    17,
                    47,
                    13,
                    0,
                    337,
                    0
                ],
                "title": "A Neurosymbolic Fast and Slow Architecture for Graph Coloring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neurosymbolic Fast and Slow Architecture for Graph Coloring"
                },
                "summary": "Constraint Satisfaction Problems (CSPs) present significant challenges to\nartificial intelligence due to their intricate constraints and the necessity\nfor precise solutions. Existing symbolic solvers are often slow, and prior\nresearch has shown that Large Language Models (LLMs) alone struggle with CSPs\nbecause of their complexity. To bridge this gap, we build upon the existing\nSOFAI architecture (SOFAI_v1), which adapts Daniel Kahneman's ''Thinking, Fast\nand Slow'' cognitive model to AI. Our enhanced architecture, SOFAI_v2,\nintegrates refined metacognitive governance mechanisms to improve adaptability\nacross complex domains, specifically tailored here for solving the graph\ncoloring problem, a specific type of CSP. SOFAI_v2 combines a fast System 1\n(S1), leveraging LLMs, with a deliberative System 2 (S2), governed by a\nmetacognition module. S1's initial solutions, often limited by constraint\nadherence issues, are improved through targeted feedback and examples from\nmetacognition, aligning S1 more closely with CSP requirements. If S1 fails to\nresolve the problem, metacognition strategically invokes S2, ensuring accurate\nand reliable solutions. Our empirical results demonstrate that SOFAI_v2\nachieves a 10.5% higher success rate and is up to 30% faster than a traditional\nsymbolic solver in solving graph coloring problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Satisfaction Problems (CSPs) present significant challenges to\nartificial intelligence due to their intricate constraints and the necessity\nfor precise solutions. Existing symbolic solvers are often slow, and prior\nresearch has shown that Large Language Models (LLMs) alone struggle with CSPs\nbecause of their complexity. To bridge this gap, we build upon the existing\nSOFAI architecture (SOFAI_v1), which adapts Daniel Kahneman's ''Thinking, Fast\nand Slow'' cognitive model to AI. Our enhanced architecture, SOFAI_v2,\nintegrates refined metacognitive governance mechanisms to improve adaptability\nacross complex domains, specifically tailored here for solving the graph\ncoloring problem, a specific type of CSP. SOFAI_v2 combines a fast System 1\n(S1), leveraging LLMs, with a deliberative System 2 (S2), governed by a\nmetacognition module. S1's initial solutions, often limited by constraint\nadherence issues, are improved through targeted feedback and examples from\nmetacognition, aligning S1 more closely with CSP requirements. If S1 fails to\nresolve the problem, metacognition strategically invokes S2, ensuring accurate\nand reliable solutions. Our empirical results demonstrate that SOFAI_v2\nachieves a 10.5% higher success rate and is up to 30% faster than a traditional\nsymbolic solver in solving graph coloring problems."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Vishal Pallagani"
                    },
                    {
                        "name": "Biplav Srivastava"
                    },
                    {
                        "name": "Francesca Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Rossi"
                },
                "author": "Francesca Rossi",
                "arxiv_comment": "31 Pages, 18 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21184v2",
                "updated": "2025-09-29T14:57:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    57,
                    0,
                    0,
                    272,
                    0
                ],
                "published": "2025-07-27T05:45:26Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    5,
                    45,
                    26,
                    6,
                    208,
                    0
                ],
                "title": "Can Language Models Discover Scaling Laws?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Discover Scaling Laws?"
                },
                "summary": "Discovering scaling laws for predicting model performance at scale is a\nfundamental and open-ended challenge, mostly reliant on slow, case specific\nhuman experimentation. To investigate the potential for LLMs to automate this\nprocess, we collect over 5,000 experiments from existing literature and curate\nseven diverse scaling law discovery tasks. While existing agents struggle to\nproduce accurate law formulas, this paper introduces SLDAgent, an\nevolution-based agent that co-optimize the scaling law model and the\nparameters, enabling it to autonomously explore complex relationships between\nvariables. For the first time, we demonstrates that SLDAgent can automatically\ndiscover laws that exhibit consistently more accurate extrapolation than their\nestablished, human-derived counterparts across all tasks. Through comprehensive\nanalysis, we elucidate why these discovered laws are superior and verify their\npractical utility in both pretraining and finetuning applications. This work\nestablishes a new paradigm for agentic scientific discovery, showing that AI\nsystems can understand their own scaling behavior, and can contribute novel and\npractical knowledge back to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering scaling laws for predicting model performance at scale is a\nfundamental and open-ended challenge, mostly reliant on slow, case specific\nhuman experimentation. To investigate the potential for LLMs to automate this\nprocess, we collect over 5,000 experiments from existing literature and curate\nseven diverse scaling law discovery tasks. While existing agents struggle to\nproduce accurate law formulas, this paper introduces SLDAgent, an\nevolution-based agent that co-optimize the scaling law model and the\nparameters, enabling it to autonomously explore complex relationships between\nvariables. For the first time, we demonstrates that SLDAgent can automatically\ndiscover laws that exhibit consistently more accurate extrapolation than their\nestablished, human-derived counterparts across all tasks. Through comprehensive\nanalysis, we elucidate why these discovered laws are superior and verify their\npractical utility in both pretraining and finetuning applications. This work\nestablishes a new paradigm for agentic scientific discovery, showing that AI\nsystems can understand their own scaling behavior, and can contribute novel and\npractical knowledge back to the research community."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Wenzheng Feng"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Hubert Lim"
                    },
                    {
                        "name": "Zhengrui Li"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24877v1",
                "updated": "2025-09-29T14:55:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    55,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:55:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    55,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "The Emergence of Social Science of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Social Science of Large Language Models"
                },
                "summary": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Xiao Jia"
                    },
                    {
                        "name": "Zhanzhan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhanzhan Zhao"
                },
                "author": "Zhanzhan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24869v1",
                "updated": "2025-09-29T14:53:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:53:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    53,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval"
                },
                "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark."
                },
                "authors": [
                    {
                        "name": "Junwei Lan"
                    },
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24866v1",
                "updated": "2025-09-29T14:50:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:50:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning"
                },
                "summary": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them."
                },
                "authors": [
                    {
                        "name": "Matteo Fuoli"
                    },
                    {
                        "name": "Weihang Huang"
                    },
                    {
                        "name": "Jeannette Littlemore"
                    },
                    {
                        "name": "Sarah Turner"
                    },
                    {
                        "name": "Ellen Wilding"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Wilding"
                },
                "author": "Ellen Wilding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24857v1",
                "updated": "2025-09-29T14:42:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    42,
                    23,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:42:23Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    42,
                    23,
                    0,
                    272,
                    0
                ],
                "title": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by\n  LLMs"
                },
                "summary": "The widespread use of chatbots powered by large language models (LLMs) such\nas ChatGPT and Llama has fundamentally reshaped how people seek information and\nadvice across domains. Increasingly, these chatbots are being used in\nhigh-stakes contexts, including emotional support and mental health concerns.\nWhile LLMs can offer scalable support, their ability to safely detect and\nrespond to acute mental health crises remains poorly understood. Progress is\nhampered by the absence of unified crisis taxonomies, robust annotated\nbenchmarks, and empirical evaluations grounded in clinical best practices. In\nthis work, we address these gaps by introducing a unified taxonomy of six\nclinically-informed mental health crisis categories, curating a diverse\nevaluation dataset, and establishing an expert-designed protocol for assessing\nresponse appropriateness. We systematically benchmark three state-of-the-art\nLLMs for their ability to classify crisis types and generate safe, appropriate\nresponses. The results reveal that while LLMs are highly consistent and\ngenerally reliable in addressing explicit crisis disclosures, significant risks\nremain. A non-negligible proportion of responses are rated as inappropriate or\nharmful, with responses generated by an open-weight model exhibiting higher\nfailure rates than those generated by the commercial ones. We also identify\nsystemic weaknesses in handling indirect or ambiguous risk signals, a reliance\non formulaic and inauthentic default replies, and frequent misalignment with\nuser context. These findings underscore the urgent need for enhanced\nsafeguards, improved crisis detection, and context-aware interventions in LLM\ndeployments. Our taxonomy, datasets, and evaluation framework lay the\ngroundwork for ongoing research and responsible innovation in AI-driven mental\nhealth support, helping to minimize harm and better protect vulnerable users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of chatbots powered by large language models (LLMs) such\nas ChatGPT and Llama has fundamentally reshaped how people seek information and\nadvice across domains. Increasingly, these chatbots are being used in\nhigh-stakes contexts, including emotional support and mental health concerns.\nWhile LLMs can offer scalable support, their ability to safely detect and\nrespond to acute mental health crises remains poorly understood. Progress is\nhampered by the absence of unified crisis taxonomies, robust annotated\nbenchmarks, and empirical evaluations grounded in clinical best practices. In\nthis work, we address these gaps by introducing a unified taxonomy of six\nclinically-informed mental health crisis categories, curating a diverse\nevaluation dataset, and establishing an expert-designed protocol for assessing\nresponse appropriateness. We systematically benchmark three state-of-the-art\nLLMs for their ability to classify crisis types and generate safe, appropriate\nresponses. The results reveal that while LLMs are highly consistent and\ngenerally reliable in addressing explicit crisis disclosures, significant risks\nremain. A non-negligible proportion of responses are rated as inappropriate or\nharmful, with responses generated by an open-weight model exhibiting higher\nfailure rates than those generated by the commercial ones. We also identify\nsystemic weaknesses in handling indirect or ambiguous risk signals, a reliance\non formulaic and inauthentic default replies, and frequent misalignment with\nuser context. These findings underscore the urgent need for enhanced\nsafeguards, improved crisis detection, and context-aware interventions in LLM\ndeployments. Our taxonomy, datasets, and evaluation framework lay the\ngroundwork for ongoing research and responsible innovation in AI-driven mental\nhealth support, helping to minimize harm and better protect vulnerable users."
                },
                "authors": [
                    {
                        "name": "Adrian Arnaiz-Rodriguez"
                    },
                    {
                        "name": "Miguel Baidal"
                    },
                    {
                        "name": "Erik Derner"
                    },
                    {
                        "name": "Jenn Layton Annable"
                    },
                    {
                        "name": "Mark Ball"
                    },
                    {
                        "name": "Mark Ince"
                    },
                    {
                        "name": "Elvira Perez Vallejos"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24852v1",
                "updated": "2025-09-29T14:38:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    38,
                    57,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:38:57Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    38,
                    57,
                    0,
                    272,
                    0
                ],
                "title": "DelRec: learning delays in recurrent spiking neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelRec: learning delays in recurrent spiking neural networks"
                },
                "summary": "Spiking neural networks (SNNs) are a bio-inspired alternative to conventional\nreal-valued deep learning models, with the potential for substantially higher\nenergy efficiency. Interest in SNNs has recently exploded due to a major\nbreakthrough: surrogate gradient learning (SGL), which allows training SNNs\nwith backpropagation, strongly outperforming other approaches. In SNNs, each\nsynapse is characterized not only by a weight but also by a transmission delay.\nWhile theoretical works have long suggested that trainable delays significantly\nenhance expressivity, practical methods for learning them have only recently\nemerged. Here, we introduce ''DelRec'', the first SGL-based method to train\naxonal or synaptic delays in recurrent spiking layers, compatible with any\nspiking neuron model. DelRec leverages a differentiable interpolation technique\nto handle non-integer delays with well-defined gradients at training time. We\nshow that trainable recurrent delays outperform feedforward ones, leading to\nnew state-of-the-art (SOTA) on two challenging temporal datasets (Spiking\nSpeech Command, an audio dataset, and Permuted Sequential MNIST, a vision one),\nand match the SOTA on the now saturated Spiking Heidelberg Digit dataset using\nonly vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous)\nsynapses. Our results demonstrate that recurrent delays are critical for\ntemporal processing in SNNs and can be effectively optimized with DelRec,\npaving the way for efficient deployment on neuromorphic hardware with\nprogrammable delays. Our code is available at :\nhttps://github.com/alexmaxad/DelRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) are a bio-inspired alternative to conventional\nreal-valued deep learning models, with the potential for substantially higher\nenergy efficiency. Interest in SNNs has recently exploded due to a major\nbreakthrough: surrogate gradient learning (SGL), which allows training SNNs\nwith backpropagation, strongly outperforming other approaches. In SNNs, each\nsynapse is characterized not only by a weight but also by a transmission delay.\nWhile theoretical works have long suggested that trainable delays significantly\nenhance expressivity, practical methods for learning them have only recently\nemerged. Here, we introduce ''DelRec'', the first SGL-based method to train\naxonal or synaptic delays in recurrent spiking layers, compatible with any\nspiking neuron model. DelRec leverages a differentiable interpolation technique\nto handle non-integer delays with well-defined gradients at training time. We\nshow that trainable recurrent delays outperform feedforward ones, leading to\nnew state-of-the-art (SOTA) on two challenging temporal datasets (Spiking\nSpeech Command, an audio dataset, and Permuted Sequential MNIST, a vision one),\nand match the SOTA on the now saturated Spiking Heidelberg Digit dataset using\nonly vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous)\nsynapses. Our results demonstrate that recurrent delays are critical for\ntemporal processing in SNNs and can be effectively optimized with DelRec,\npaving the way for efficient deployment on neuromorphic hardware with\nprogrammable delays. Our code is available at :\nhttps://github.com/alexmaxad/DelRec."
                },
                "authors": [
                    {
                        "name": "Alexandre Queant"
                    },
                    {
                        "name": "Ulysse Ranon"
                    },
                    {
                        "name": "Benoit R Cottereau"
                    },
                    {
                        "name": "Timothe Masquelier"
                    }
                ],
                "author_detail": {
                    "name": "Timothe Masquelier"
                },
                "author": "Timothe Masquelier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24850v2",
                "updated": "2025-09-30T03:07:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    3,
                    7,
                    53,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T14:36:45Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    36,
                    45,
                    0,
                    272,
                    0
                ],
                "title": "PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient\n  Remote Photoplethysmography Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient\n  Remote Photoplethysmography Measurement"
                },
                "summary": "Remote photoplethysmography (rPPG) measurement enables non-contact\nphysiological monitoring but suffers from accuracy degradation under head\nmotion and illumination changes. Existing deep learning methods are mostly\nheuristic and lack theoretical grounding, which limits robustness and\ninterpretability. In this work, we propose a physics-informed rPPG paradigm\nderived from the Navier-Stokes equations of hemodynamics, showing that the\npulse signal follows a second-order dynamical system whose discrete solution\nnaturally leads to a causal convolution. This provides a theoretical\njustification for using a Temporal Convolutional Network (TCN). Based on this\nprinciple, we design PHASE-Net, a lightweight model with three key components:\n(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial\nchannels to mix distant facial regions and enhance cross-region feature\ninteraction without breaking temporal order; (2) Adaptive Spatial Filter, which\nlearns a soft spatial mask per frame to highlight signal-rich areas and\nsuppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models\nlong-range temporal dynamics for accurate pulse recovery. Extensive experiments\ndemonstrate that PHASE-Net achieves state-of-the-art performance with strong\nefficiency, offering a theoretically grounded and deployment-ready rPPG\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote photoplethysmography (rPPG) measurement enables non-contact\nphysiological monitoring but suffers from accuracy degradation under head\nmotion and illumination changes. Existing deep learning methods are mostly\nheuristic and lack theoretical grounding, which limits robustness and\ninterpretability. In this work, we propose a physics-informed rPPG paradigm\nderived from the Navier-Stokes equations of hemodynamics, showing that the\npulse signal follows a second-order dynamical system whose discrete solution\nnaturally leads to a causal convolution. This provides a theoretical\njustification for using a Temporal Convolutional Network (TCN). Based on this\nprinciple, we design PHASE-Net, a lightweight model with three key components:\n(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial\nchannels to mix distant facial regions and enhance cross-region feature\ninteraction without breaking temporal order; (2) Adaptive Spatial Filter, which\nlearns a soft spatial mask per frame to highlight signal-rich areas and\nsuppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models\nlong-range temporal dynamics for accurate pulse recovery. Extensive experiments\ndemonstrate that PHASE-Net achieves state-of-the-art performance with strong\nefficiency, offering a theoretically grounded and deployment-ready rPPG\nsolution."
                },
                "authors": [
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Junzhe Cao"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Bochao Zou"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24846v1",
                "updated": "2025-09-29T14:29:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    29,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:29:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    29,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Blockchain-Driven Federation for Distributed Edge Systems: Design and\n  Experimental Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Driven Federation for Distributed Edge Systems: Design and\n  Experimental Validation"
                },
                "summary": "Edge computing brings computation near end users, enabling the provisioning\nof novel use cases. To satisfy end-user requirements, the concept of edge\nfederation has recently emerged as a key mechanism for dynamic resources and\nservices sharing across edge systems managed by different administrative\ndomains. However, existing federation solutions often rely on pre-established\nagreements and face significant limitations, including operational complexity,\ndelays caused by manual operations, high overhead costs, and dependence on\ntrusted third parties. In this context, blockchain can create dynamic\nfederation agreements that enable service providers to securely interact and\nshare services without prior trust.\n  This article first describes the problem of edge federation, using the\nstandardized ETSI multi-access edge computing framework as a reference\narchitecture, and how it is being addressed. Then, it proposes a novel solution\nusing blockchain and smart contracts to enable distributed MEC systems to\ndynamically negotiate and execute federation in a secure, automated, and\nscalable manner. We validate our framework's feasibility through a performance\nevaluation using a private Ethereum blockchain, built on the open-source\nHyperledger Besu platform. The testbed includes a large number of MEC systems\nand compares two blockchain consensus algorithms. Experimental results\ndemonstrate that our solution automates the entire federation lifecycle-from\nnegotiation to deployment-with a quantifiable overhead, achieving federation in\napproximately 18 seconds in a baseline scenario. The framework scales\nefficiently in concurrent request scenarios, where multiple MEC systems\ninitiate federation requests simultaneously. This approach provides a promising\ndirection for addressing the complexities of dynamic, multi-domain federations\nacross the edge-to-cloud continuum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing brings computation near end users, enabling the provisioning\nof novel use cases. To satisfy end-user requirements, the concept of edge\nfederation has recently emerged as a key mechanism for dynamic resources and\nservices sharing across edge systems managed by different administrative\ndomains. However, existing federation solutions often rely on pre-established\nagreements and face significant limitations, including operational complexity,\ndelays caused by manual operations, high overhead costs, and dependence on\ntrusted third parties. In this context, blockchain can create dynamic\nfederation agreements that enable service providers to securely interact and\nshare services without prior trust.\n  This article first describes the problem of edge federation, using the\nstandardized ETSI multi-access edge computing framework as a reference\narchitecture, and how it is being addressed. Then, it proposes a novel solution\nusing blockchain and smart contracts to enable distributed MEC systems to\ndynamically negotiate and execute federation in a secure, automated, and\nscalable manner. We validate our framework's feasibility through a performance\nevaluation using a private Ethereum blockchain, built on the open-source\nHyperledger Besu platform. The testbed includes a large number of MEC systems\nand compares two blockchain consensus algorithms. Experimental results\ndemonstrate that our solution automates the entire federation lifecycle-from\nnegotiation to deployment-with a quantifiable overhead, achieving federation in\napproximately 18 seconds in a baseline scenario. The framework scales\nefficiently in concurrent request scenarios, where multiple MEC systems\ninitiate federation requests simultaneously. This approach provides a promising\ndirection for addressing the complexities of dynamic, multi-domain federations\nacross the edge-to-cloud continuum."
                },
                "authors": [
                    {
                        "name": "Adam Zahir"
                    },
                    {
                        "name": "Milan Groshev"
                    },
                    {
                        "name": "Carlos J. Bernardos"
                    },
                    {
                        "name": "Antonio de la Oliva"
                    }
                ],
                "author_detail": {
                    "name": "Antonio de la Oliva"
                },
                "author": "Antonio de la Oliva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24845v1",
                "updated": "2025-09-29T14:28:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    28,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:28:02Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    28,
                    2,
                    0,
                    272,
                    0
                ],
                "title": "Physical Layer Security over Fluid Reconfigurable Intelligent\n  Surface-assisted Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Layer Security over Fluid Reconfigurable Intelligent\n  Surface-assisted Communication Systems"
                },
                "summary": "This letter investigates the secrecy performance of wireless communication\nsystems assisted by a fluid reconfigurable intelligent surface (FRIS). Unlike\nconventional reconfigurable intelligent surfaces (RISs) with fixed geometries,\nFRISs dynamically select a subset of reflective elements based on real-time\nchannel conditions, offering enhanced spatial diversity and adaptability. Using\nthis foundation, we model a secure downlink scenario where a base station\ncommunicates with a legitimate user in the presence of an eavesdropper, and the\npropagation is assisted by a FRIS with a limited number of elements set to the\nON state. We analyze the system's secrecy performance under spatial correlation\nby deriving analytical lower and upper bounds for the secrecy outage\nprobability (SOP) and average secrecy capacity (ASC), respectively. Our results\ndemonstrate that FRIS effectively enables secure communication under spatial\ncorrelation. Even with partial activation, FRIS significantly outperforms\nconventional RISs in enhancing secrecy performance under varying deployment\ndensities and element correlations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates the secrecy performance of wireless communication\nsystems assisted by a fluid reconfigurable intelligent surface (FRIS). Unlike\nconventional reconfigurable intelligent surfaces (RISs) with fixed geometries,\nFRISs dynamically select a subset of reflective elements based on real-time\nchannel conditions, offering enhanced spatial diversity and adaptability. Using\nthis foundation, we model a secure downlink scenario where a base station\ncommunicates with a legitimate user in the presence of an eavesdropper, and the\npropagation is assisted by a FRIS with a limited number of elements set to the\nON state. We analyze the system's secrecy performance under spatial correlation\nby deriving analytical lower and upper bounds for the secrecy outage\nprobability (SOP) and average secrecy capacity (ASC), respectively. Our results\ndemonstrate that FRIS effectively enables secure communication under spatial\ncorrelation. Even with partial activation, FRIS significantly outperforms\nconventional RISs in enhancing secrecy performance under varying deployment\ndensities and element correlations."
                },
                "authors": [
                    {
                        "name": "Masoud Kaveh"
                    },
                    {
                        "name": "Farshad Rostami Ghadi"
                    },
                    {
                        "name": "Francisco Hernando-Gallego"
                    },
                    {
                        "name": "Diego Martn"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    },
                    {
                        "name": "Riku Jntti"
                    }
                ],
                "author_detail": {
                    "name": "Riku Jntti"
                },
                "author": "Riku Jntti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22390v2",
                "updated": "2025-09-29T14:27:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    27,
                    7,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-27T17:00:48Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    0,
                    48,
                    4,
                    178,
                    0
                ],
                "title": "What Characteristics Make ChatGPT Effective for Software Issue\n  Resolution? An Empirical Study of Task, Project, and Conversational Signals\n  in GitHub Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Characteristics Make ChatGPT Effective for Software Issue\n  Resolution? An Empirical Study of Task, Project, and Conversational Signals\n  in GitHub Issues"
                },
                "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "arxiv_comment": "Accepted for publication in Empirical Software Engineering (EMSE),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24841v1",
                "updated": "2025-09-29T14:21:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    21,
                    5,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:21:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    21,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "Hierarchical Error Correction for Large Language Models: A Systematic\n  Framework for Domain-Specific AI Quality Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Error Correction for Large Language Models: A Systematic\n  Framework for Domain-Specific AI Quality Enhancement"
                },
                "summary": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment."
                },
                "authors": [
                    {
                        "name": "Zhilong Zhao"
                    },
                    {
                        "name": "Yindi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yindi Liu"
                },
                "author": "Yindi Liu",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24840v1",
                "updated": "2025-09-29T14:20:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:20:50Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    50,
                    0,
                    272,
                    0
                ],
                "title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data"
                },
                "summary": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells."
                },
                "authors": [
                    {
                        "name": "Oussama Kharouiche"
                    },
                    {
                        "name": "Aris Markogiannakis"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24836v2",
                "updated": "2025-09-30T06:35:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    6,
                    35,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T14:20:04Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    20,
                    4,
                    0,
                    272,
                    0
                ],
                "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data\n  Reasoning Intensity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data\n  Reasoning Intensity"
                },
                "summary": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data. Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data. Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential."
                },
                "authors": [
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Zhenlin Hu"
                    },
                    {
                        "name": "Jinnan Yang"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Yida Xue"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Qing Shen"
                    },
                    {
                        "name": "Zhenfang Liu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Jungang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jungang Lou"
                },
                "author": "Jungang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24827v1",
                "updated": "2025-09-29T14:13:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:13:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants"
                },
                "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests."
                },
                "authors": [
                    {
                        "name": "Bartosz Bieganowski"
                    },
                    {
                        "name": "Daniel Strzelecki"
                    },
                    {
                        "name": "Robert Skiba"
                    },
                    {
                        "name": "Mateusz Topolewski"
                    }
                ],
                "author_detail": {
                    "name": "Mateusz Topolewski"
                },
                "author": "Mateusz Topolewski",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24826v1",
                "updated": "2025-09-29T14:12:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    12,
                    6,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:12:06Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    12,
                    6,
                    0,
                    272,
                    0
                ],
                "title": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems"
                },
                "summary": "Large language models (LLMs) are being increasingly used for planning in\norchestrated multi-agent systems. However, existing LLM-based approaches often\nfall short of human expectations and, critically, lack effective mechanisms for\nusers to inspect, understand, and control their behaviors. These limitations\ncall for enhanced transparency, controllability, and human oversight. To\naddress this, we introduce AIPOM, a system supporting human-in-the-loop\nplanning through conversational and graph-based interfaces. AIPOM enables users\nto transparently inspect, refine, and collaboratively guide LLM-generated\nplans, significantly enhancing user control and trust in multi-agent workflows.\nOur code and demo video are available at https://github.com/megagonlabs/aipom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being increasingly used for planning in\norchestrated multi-agent systems. However, existing LLM-based approaches often\nfall short of human expectations and, critically, lack effective mechanisms for\nusers to inspect, understand, and control their behaviors. These limitations\ncall for enhanced transparency, controllability, and human oversight. To\naddress this, we introduce AIPOM, a system supporting human-in-the-loop\nplanning through conversational and graph-based interfaces. AIPOM enables users\nto transparently inspect, refine, and collaboratively guide LLM-generated\nplans, significantly enhancing user control and trust in multi-agent workflows.\nOur code and demo video are available at https://github.com/megagonlabs/aipom."
                },
                "authors": [
                    {
                        "name": "Hannah Kim"
                    },
                    {
                        "name": "Kushan Mitra"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "arxiv_comment": "EMNLP 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24819v1",
                "updated": "2025-09-29T14:07:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    7,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:07:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    7,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Intelligent Optimization of Wireless Access Point Deployment for\n  Communication-Based Train Control Systems Using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Optimization of Wireless Access Point Deployment for\n  Communication-Based Train Control Systems Using Deep Reinforcement Learning"
                },
                "summary": "Urban railway systems increasingly rely on communication based train control\n(CBTC) systems, where optimal deployment of access points (APs) in tunnels is\ncritical for robust wireless coverage. Traditional methods, such as empirical\nmodel-based optimization algorithms, are hindered by excessive measurement\nrequirements and suboptimal solutions, while machine learning (ML) approaches\noften struggle with complex tunnel environments. This paper proposes a deep\nreinforcement learning (DRL) driven framework that integrates parabolic wave\nequation (PWE) channel modeling, conditional generative adversarial network\n(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for\nAP placement optimization. The PWE method generates high-fidelity path loss\ndistributions for a subset of AP positions, which are then expanded by the cGAN\nto create high resolution path loss maps for all candidate positions,\nsignificantly reducing simulation costs while maintaining physical accuracy. In\nthe DRL framework, the state space captures AP positions and coverage, the\naction space defines AP adjustments, and the reward function encourages signal\nimprovement while penalizing deployment costs. The dueling DQN enhances\nconvergence speed and exploration exploitation balance, increasing the\nlikelihood of reaching optimal configurations. Comparative experiments show\nthat the proposed method outperforms a conventional Hooke Jeeves optimizer and\ntraditional DQN, delivering AP configurations with higher average received\npower, better worst-case coverage, and improved computational efficiency. This\nwork integrates high-fidelity electromagnetic simulation, generative modeling,\nand AI-driven optimization, offering a scalable and data-efficient solution for\nnext-generation CBTC systems in complex tunnel environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban railway systems increasingly rely on communication based train control\n(CBTC) systems, where optimal deployment of access points (APs) in tunnels is\ncritical for robust wireless coverage. Traditional methods, such as empirical\nmodel-based optimization algorithms, are hindered by excessive measurement\nrequirements and suboptimal solutions, while machine learning (ML) approaches\noften struggle with complex tunnel environments. This paper proposes a deep\nreinforcement learning (DRL) driven framework that integrates parabolic wave\nequation (PWE) channel modeling, conditional generative adversarial network\n(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for\nAP placement optimization. The PWE method generates high-fidelity path loss\ndistributions for a subset of AP positions, which are then expanded by the cGAN\nto create high resolution path loss maps for all candidate positions,\nsignificantly reducing simulation costs while maintaining physical accuracy. In\nthe DRL framework, the state space captures AP positions and coverage, the\naction space defines AP adjustments, and the reward function encourages signal\nimprovement while penalizing deployment costs. The dueling DQN enhances\nconvergence speed and exploration exploitation balance, increasing the\nlikelihood of reaching optimal configurations. Comparative experiments show\nthat the proposed method outperforms a conventional Hooke Jeeves optimizer and\ntraditional DQN, delivering AP configurations with higher average received\npower, better worst-case coverage, and improved computational efficiency. This\nwork integrates high-fidelity electromagnetic simulation, generative modeling,\nand AI-driven optimization, offering a scalable and data-efficient solution for\nnext-generation CBTC systems in complex tunnel environments."
                },
                "authors": [
                    {
                        "name": "Kunyu Wu"
                    },
                    {
                        "name": "Qiushi Zhao"
                    },
                    {
                        "name": "Zihan Feng"
                    },
                    {
                        "name": "Yunxi Mu"
                    },
                    {
                        "name": "Hao Qin"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xingqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingqi Zhang"
                },
                "author": "Xingqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v2",
                "updated": "2025-09-29T14:04:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    4,
                    28,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24816v1",
                "updated": "2025-09-29T14:03:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    3,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:03:01Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    3,
                    1,
                    0,
                    272,
                    0
                ],
                "title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical\n  Reasoning"
                },
                "summary": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage."
                },
                "authors": [
                    {
                        "name": "Xilin Dang"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Xiaorui Su"
                    },
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Iaki Arango"
                    },
                    {
                        "name": "Lucas Vittor"
                    },
                    {
                        "name": "Xinyi Long"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Pheng Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng Ann Heng"
                },
                "author": "Pheng Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06233v2",
                "updated": "2025-09-29T13:59:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    59,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-02-10T08:10:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    10,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Confidence Improves Self-Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Improves Self-Consistency in LLMs"
                },
                "summary": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic."
                },
                "authors": [
                    {
                        "name": "Amir Taubenfeld"
                    },
                    {
                        "name": "Tom Sheffer"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Ariel Goldstein"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Gal Yona"
                    }
                ],
                "author_detail": {
                    "name": "Gal Yona"
                },
                "author": "Gal Yona",
                "arxiv_doi": "10.18653/v1/2025.findings-acl.1030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.findings-acl.1030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24807v1",
                "updated": "2025-09-29T13:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    57,
                    16,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:57:16Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    57,
                    16,
                    0,
                    272,
                    0
                ],
                "title": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance\n  and Cognitive Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance\n  and Cognitive Contexts"
                },
                "summary": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models."
                },
                "authors": [
                    {
                        "name": "Dong Hyun Roh"
                    },
                    {
                        "name": "Rajesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Kumar"
                },
                "author": "Rajesh Kumar",
                "arxiv_comment": "Accepted for publication at IEEE-ICMLA 2025. Contains nine pages, six\n  figures, and two tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22628v2",
                "updated": "2025-09-29T13:56:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    56,
                    38,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-26T17:51:46Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    51,
                    46,
                    4,
                    269,
                    0
                ],
                "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UML-CoT: Structured Reasoning and Planning with Unified Modeling\n  Language for Robotic Room Cleaning"
                },
                "summary": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting improves reasoning in large language models\n(LLMs), but its reliance on unstructured text limits interpretability and\nexecutability in embodied tasks. Prior work has explored structured CoTs using\nscene or logic graphs, yet these remain fundamentally limited: they model only\nlow-order relations, lack constructs like inheritance or behavioral\nabstraction, and provide no standardized semantics for sequential or\nconditional planning. We propose UML-CoT, a structured reasoning and planning\nframework that leverages Unified Modeling Language (UML) to generate symbolic\nCoTs and executable action plans. UML class diagrams capture compositional\nobject semantics, while activity diagrams model procedural control flow. Our\nthree-stage training pipeline combines supervised fine-tuning with Group\nRelative Policy Optimization (GRPO), including reward learning from answer-only\ndata. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered\nroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs in\ninterpretability, planning coherence, and execution success, highlighting UML\nas a more expressive and actionable structured reasoning formalism."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01992v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01992v3",
                "updated": "2025-09-29T13:51:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    51,
                    37,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-04T02:19:38Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    19,
                    38,
                    0,
                    216,
                    0
                ],
                "title": "Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic\n  Learning-Based Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic\n  Learning-Based Compensation"
                },
                "summary": "As a foundational architecture of artificial intelligence models, Transformer\nhas been recently adapted to spiking neural networks with promising performance\nacross various tasks. However, existing spiking Transformer~(ST)-based models\nrequire a substantial number of parameters and incur high computational costs,\nthus limiting their deployment in resource-constrained environments. To address\nthese challenges, we propose combining synapse pruning with a synergistic\nlearning-based compensation strategy to derive lightweight ST-based models.\nSpecifically, two types of tailored pruning strategies are introduced to reduce\nredundancy in the weight matrices of ST blocks: an unstructured\n$\\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP\nmethod to induce low-rank representations. In addition, we propose an enhanced\nspiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF)\nneuron, to effectively compensate for model pruning through synergistic\nlearning between synaptic and intrinsic plasticity mechanisms. Extensive\nexperiments on benchmark datasets demonstrate that the proposed methods\nsignificantly reduce model size and computational overhead while maintaining\ncompetitive performance. These results validate the effectiveness of the\nproposed pruning and compensation strategies in constructing efficient and\nhigh-performing ST-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a foundational architecture of artificial intelligence models, Transformer\nhas been recently adapted to spiking neural networks with promising performance\nacross various tasks. However, existing spiking Transformer~(ST)-based models\nrequire a substantial number of parameters and incur high computational costs,\nthus limiting their deployment in resource-constrained environments. To address\nthese challenges, we propose combining synapse pruning with a synergistic\nlearning-based compensation strategy to derive lightweight ST-based models.\nSpecifically, two types of tailored pruning strategies are introduced to reduce\nredundancy in the weight matrices of ST blocks: an unstructured\n$\\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP\nmethod to induce low-rank representations. In addition, we propose an enhanced\nspiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF)\nneuron, to effectively compensate for model pruning through synergistic\nlearning between synaptic and intrinsic plasticity mechanisms. Extensive\nexperiments on benchmark datasets demonstrate that the proposed methods\nsignificantly reduce model size and computational overhead while maintaining\ncompetitive performance. These results validate the effectiveness of the\nproposed pruning and compensation strategies in constructing efficient and\nhigh-performing ST-based models."
                },
                "authors": [
                    {
                        "name": "Hongze Sun"
                    },
                    {
                        "name": "Wuque Cai"
                    },
                    {
                        "name": "Duo Chen"
                    },
                    {
                        "name": "Quan Tang"
                    },
                    {
                        "name": "Shifeng Mao"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Zhenxing Wang"
                    },
                    {
                        "name": "Yan Cui"
                    },
                    {
                        "name": "Dezhong Yao"
                    },
                    {
                        "name": "Daqing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Daqing Guo"
                },
                "author": "Daqing Guo",
                "arxiv_comment": "13 pages, 11 figures, 5 tables. This manuscript has been submitted\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01992v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01992v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24792v1",
                "updated": "2025-09-29T13:46:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    46,
                    27,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:46:27Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    46,
                    27,
                    0,
                    272,
                    0
                ],
                "title": "Evaluating Spatiotemporal Consistency in Automatically Generated Sewing\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Spatiotemporal Consistency in Automatically Generated Sewing\n  Instructions"
                },
                "summary": "In this paper, we propose a novel, automatic tree-based evaluation metric for\nLLM-generated step-by-step assembly instructions, that more accurately reflects\nspatiotemporal aspects of construction than traditional metrics such as BLEU\nand BERT similarity scores. We apply our proposed metric to the domain of\nsewing instructions, and show that our metric better correlates with\nmanually-annotated error counts as well as human quality ratings, demonstrating\nour metric's superiority for evaluating the spatiotemporal soundness of sewing\ninstructions. Further experiments show that our metric is more robust than\ntraditional approaches against artificially-constructed counterfactual examples\nthat are specifically constructed to confound metrics that rely on textual\nsimilarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel, automatic tree-based evaluation metric for\nLLM-generated step-by-step assembly instructions, that more accurately reflects\nspatiotemporal aspects of construction than traditional metrics such as BLEU\nand BERT similarity scores. We apply our proposed metric to the domain of\nsewing instructions, and show that our metric better correlates with\nmanually-annotated error counts as well as human quality ratings, demonstrating\nour metric's superiority for evaluating the spatiotemporal soundness of sewing\ninstructions. Further experiments show that our metric is more robust than\ntraditional approaches against artificially-constructed counterfactual examples\nthat are specifically constructed to confound metrics that rely on textual\nsimilarity."
                },
                "authors": [
                    {
                        "name": "Luisa Geiger"
                    },
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Michael Sullivan"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "18 pages, 14 figures; to be published in EMNLP 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24789v1",
                "updated": "2025-09-29T13:44:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    44,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:44:49Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    44,
                    49,
                    0,
                    272,
                    0
                ],
                "title": "Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series\n  Forecasting"
                },
                "summary": "The evaluation of time series forecasting models is hindered by a critical\nlack of high-quality benchmarks, leading to a potential illusion of progress.\nExisting datasets suffer from issues ranging from pre-training data\ncontamination in the age of LLMs to the causal and description leakage\nprevalent in early multimodal designs. To address this, we formalize the core\nprinciples of high-fidelity benchmarking, focusing on data sourcing integrity,\nstrict causal soundness, and structural clarity. We introduce Fidel-TS, a new\nlarge-scale benchmark built from the ground up on these principles by sourcing\ndata from live APIs. Our extensive experiments validate this approach by\nexposing the critical biases and design limitations of prior benchmarks.\nFurthermore, we conclusively demonstrate that the causal relevance of textual\ninformation is the key factor in unlocking genuine performance gains in\nmultimodal forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of time series forecasting models is hindered by a critical\nlack of high-quality benchmarks, leading to a potential illusion of progress.\nExisting datasets suffer from issues ranging from pre-training data\ncontamination in the age of LLMs to the causal and description leakage\nprevalent in early multimodal designs. To address this, we formalize the core\nprinciples of high-fidelity benchmarking, focusing on data sourcing integrity,\nstrict causal soundness, and structural clarity. We introduce Fidel-TS, a new\nlarge-scale benchmark built from the ground up on these principles by sourcing\ndata from live APIs. Our extensive experiments validate this approach by\nexposing the critical biases and design limitations of prior benchmarks.\nFurthermore, we conclusively demonstrate that the causal relevance of textual\ninformation is the key factor in unlocking genuine performance gains in\nmultimodal forecasting."
                },
                "authors": [
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Wanxu Cai"
                    },
                    {
                        "name": "Xilin Dai"
                    },
                    {
                        "name": "Zhaorong Deng"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24782v1",
                "updated": "2025-09-29T13:43:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    43,
                    2,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:43:02Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    43,
                    2,
                    0,
                    272,
                    0
                ],
                "title": "Large language models for behavioral modeling: A literature survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for behavioral modeling: A literature survey"
                },
                "summary": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs."
                },
                "authors": [
                    {
                        "name": "Muhammad Laiq"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Laiq"
                },
                "author": "Muhammad Laiq",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24781v1",
                "updated": "2025-09-29T13:42:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    42,
                    41,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:42:41Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    42,
                    41,
                    0,
                    272,
                    0
                ],
                "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaPO: Strategic Error Amplification for Robust Preference Optimization\n  of Large Language Models"
                },
                "summary": "Existing alignment methods for preference optimization of large language\nmodels (LLMs) aim to enhance model performance by utilizing pairs of positive\nand negative samples. However, due to the limited capacity of models in scoring\nor generating responses, the quality of positive and negative samples may\nbecome similar during training, which complicates optimization for preference\nlearning. To address this issue, we introduce SeaPO, a Strategic Error\nAmplification method that leverages three error types commonly occurring in\nLLMs to introduce specific error patterns into the model Preference\nOptimization. This strategy ensures that negative samples are more erroneous\nthan positive samples and preference-based training is employed to mitigate the\noccurrence of these errors, thereby enhancing model performance. Evaluations\nacross five capability dimensions and different model scales (1.5B to 14B)\ndemonstrate that the generated data significantly improved overall model\nperformance, particularly in terms of truthfulness, with improvements of 5-10\npercentage points observed. Further analysis reveals that task performance\nvaries depending on the error types introduced. Injecting the most common error\ntypes improves performance in related tasks, while a mix of error types leads\nto a broader performance enhancement: most tasks show stable improvements,\nwhile a few tasks exhibit significant gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing alignment methods for preference optimization of large language\nmodels (LLMs) aim to enhance model performance by utilizing pairs of positive\nand negative samples. However, due to the limited capacity of models in scoring\nor generating responses, the quality of positive and negative samples may\nbecome similar during training, which complicates optimization for preference\nlearning. To address this issue, we introduce SeaPO, a Strategic Error\nAmplification method that leverages three error types commonly occurring in\nLLMs to introduce specific error patterns into the model Preference\nOptimization. This strategy ensures that negative samples are more erroneous\nthan positive samples and preference-based training is employed to mitigate the\noccurrence of these errors, thereby enhancing model performance. Evaluations\nacross five capability dimensions and different model scales (1.5B to 14B)\ndemonstrate that the generated data significantly improved overall model\nperformance, particularly in terms of truthfulness, with improvements of 5-10\npercentage points observed. Further analysis reveals that task performance\nvaries depending on the error types introduced. Injecting the most common error\ntypes improves performance in related tasks, while a mix of error types leads\nto a broader performance enhancement: most tasks show stable improvements,\nwhile a few tasks exhibit significant gains."
                },
                "authors": [
                    {
                        "name": "Jun Rao"
                    },
                    {
                        "name": "Yunjie Liao"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Zepeng Lin"
                    },
                    {
                        "name": "Lian Lian"
                    },
                    {
                        "name": "Dong Jin"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15610v2",
                "updated": "2025-09-29T13:42:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    42,
                    12,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-21T14:35:47Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    35,
                    47,
                    3,
                    233,
                    0
                ],
                "title": "Transduction is All You Need for Structured Data Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transduction is All You Need for Structured Data Workflows"
                },
                "summary": "This paper introduces Agentics, a functional agentic AI framework for\nbuilding LLM-based structured data workflow pipelines. Designed for both\nresearch and practical applications, Agentics offers a new data-centric\nparadigm in which agents are embedded within data types, enabling logical\ntransduction between structured states. This design shifts the focus toward\nprincipled data modeling, providing a declarative language where data types are\ndirectly exposed to large language models and composed through transductions\ntriggered by type connections. We present a range of structured data workflow\ntasks and empirical evidence demonstrating the effectiveness of this approach,\nincluding data wrangling, text-to-SQL semantic parsing, and domain-specific\nmultiple-choice question answering. The open source Agentics is available at\nhttps://github.com/IBM/Agentics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Agentics, a functional agentic AI framework for\nbuilding LLM-based structured data workflow pipelines. Designed for both\nresearch and practical applications, Agentics offers a new data-centric\nparadigm in which agents are embedded within data types, enabling logical\ntransduction between structured states. This design shifts the focus toward\nprincipled data modeling, providing a declarative language where data types are\ndirectly exposed to large language models and composed through transductions\ntriggered by type connections. We present a range of structured data workflow\ntasks and empirical evidence demonstrating the effectiveness of this approach,\nincluding data wrangling, text-to-SQL semantic parsing, and domain-specific\nmultiple-choice question answering. The open source Agentics is available at\nhttps://github.com/IBM/Agentics."
                },
                "authors": [
                    {
                        "name": "Alfio Gliozzo"
                    },
                    {
                        "name": "Naweed Khan"
                    },
                    {
                        "name": "Christodoulos Constantinides"
                    },
                    {
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "name": "Nahuel Defosse"
                    },
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Junkyu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Junkyu Lee"
                },
                "author": "Junkyu Lee",
                "arxiv_comment": "32 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24771v1",
                "updated": "2025-09-29T13:37:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    37,
                    39,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:37:39Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    37,
                    39,
                    0,
                    272,
                    0
                ],
                "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space"
                },
                "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the\nreasoning capabilities of Large Language Models (LLMs) during the inference\nphase without altering model parameters. However, existing TTS methods are\nlargely independent, implying that LLMs have not yet evolved to progressively\nlearn how to scale more effectively. With the objective of evolving LLMs to\nlearn ``how to scale test-time computation,'' we propose LatentEvolve, a\nself-evolving latent TTS framework inspired by the complementary learning\nsystem (CLS) theory. Analogous to the human brain's dual system of a\nfast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve\ncomprises two evolutionary components: \\textit{daytime scaling}, which rapidly\nretrieves historical latent representations to better guide current LLM\nreasoning; and \\textit{nighttime scaling}, which integrates past latent\noptimizations in a manner akin to the human brain's consolidation of\nexperiences during sleep. The alternation of daytime and nighttime processes\nfacilitates a fast and slow evolution of LLM TTS, mirroring human cognitive\ndynamics in a fully unsupervised manner. Extensive experiments across eight\nbenchmarks and five model backbones demonstrate that our LatentEvolve surpasses\nstate-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and\nexhibits exceptional cross-domain and cross-backbone generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the\nreasoning capabilities of Large Language Models (LLMs) during the inference\nphase without altering model parameters. However, existing TTS methods are\nlargely independent, implying that LLMs have not yet evolved to progressively\nlearn how to scale more effectively. With the objective of evolving LLMs to\nlearn ``how to scale test-time computation,'' we propose LatentEvolve, a\nself-evolving latent TTS framework inspired by the complementary learning\nsystem (CLS) theory. Analogous to the human brain's dual system of a\nfast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve\ncomprises two evolutionary components: \\textit{daytime scaling}, which rapidly\nretrieves historical latent representations to better guide current LLM\nreasoning; and \\textit{nighttime scaling}, which integrates past latent\noptimizations in a manner akin to the human brain's consolidation of\nexperiences during sleep. The alternation of daytime and nighttime processes\nfacilitates a fast and slow evolution of LLM TTS, mirroring human cognitive\ndynamics in a fully unsupervised manner. Extensive experiments across eight\nbenchmarks and five model backbones demonstrate that our LatentEvolve surpasses\nstate-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and\nexhibits exceptional cross-domain and cross-backbone generalization."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Fanci Meng"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24770v1",
                "updated": "2025-09-29T13:37:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    37,
                    12,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:37:12Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    37,
                    12,
                    0,
                    272,
                    0
                ],
                "title": "Neural Message-Passing on Attention Graphs for Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Message-Passing on Attention Graphs for Hallucination Detection"
                },
                "summary": "Large Language Models (LLMs) often generate incorrect or unsupported content,\nknown as hallucinations. Existing detection methods rely on heuristics or\nsimple models over isolated computational traces such as activations, or\nattention maps. We unify these signals by representing them as attributed\ngraphs, where tokens are nodes, edges follow attentional flows, and both carry\nfeatures from attention scores and activations. Our approach, CHARM, casts\nhallucination detection as a graph learning task and tackles it by applying\nGNNs over the above attributed graphs. We show that CHARM provably subsumes\nprior attention-based heuristics and, experimentally, it consistently\noutperforms other leading approaches across diverse benchmarks. Our results\nshed light on the relevant role played by the graph structure and on the\nbenefits of combining computational traces, whilst showing CHARM exhibits\npromising zero-shot performance on cross-dataset transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate incorrect or unsupported content,\nknown as hallucinations. Existing detection methods rely on heuristics or\nsimple models over isolated computational traces such as activations, or\nattention maps. We unify these signals by representing them as attributed\ngraphs, where tokens are nodes, edges follow attentional flows, and both carry\nfeatures from attention scores and activations. Our approach, CHARM, casts\nhallucination detection as a graph learning task and tackles it by applying\nGNNs over the above attributed graphs. We show that CHARM provably subsumes\nprior attention-based heuristics and, experimentally, it consistently\noutperforms other leading approaches across diverse benchmarks. Our results\nshed light on the relevant role played by the graph structure and on the\nbenefits of combining computational traces, whilst showing CHARM exhibits\npromising zero-shot performance on cross-dataset transfer."
                },
                "authors": [
                    {
                        "name": "Fabrizio Frasca"
                    },
                    {
                        "name": "Guy Bar-Shalom"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "arxiv_comment": "Preprint. 25 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16853v2",
                "updated": "2025-09-29T13:36:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    36,
                    1,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-20T09:02:05Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    2,
                    5,
                    4,
                    171,
                    0
                ],
                "title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models"
                },
                "summary": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, running\n4.8 times faster than naive reward-agnostic test-time search baseline on\naverage. Furthermore, with sufficient inference budget, it can achieve\ncomparable performance to learning-based baselines that require reward-specific\nfine-tuning. The code is available at https://github.com/seminkim/RATTPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, running\n4.8 times faster than naive reward-agnostic test-time search baseline on\naverage. Furthermore, with sufficient inference budget, it can achieve\ncomparable performance to learning-based baselines that require reward-specific\nfine-tuning. The code is available at https://github.com/seminkim/RATTPO."
                },
                "authors": [
                    {
                        "name": "Semin Kim"
                    },
                    {
                        "name": "Yeonwoo Cha"
                    },
                    {
                        "name": "Jaehoon Yoo"
                    },
                    {
                        "name": "Seunghoon Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghoon Hong"
                },
                "author": "Seunghoon Hong",
                "arxiv_comment": "29 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24765v2",
                "updated": "2025-09-30T03:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    3,
                    40,
                    31,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T13:31:22Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    31,
                    22,
                    0,
                    272,
                    0
                ],
                "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent\n  for LLM Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent\n  for LLM Logical Reasoning"
                },
                "summary": "Logical reasoning is a fundamental capability of large language models\n(LLMs). However, existing studies largely overlook the interplay between\nlogical complexity and semantic complexity, resulting in methods that struggle\nto address challenging scenarios involving abstract propositions, ambiguous\ncontexts, and conflicting stances, which are central to human reasoning. For\nthis gap, we propose LogicAgent, a semiotic-square-guided framework designed to\njointly address logical complexity and semantic complexity. LogicAgent\nexplicitly performs multi-perspective deduction in first-order logic (FOL),\nwhile mitigating vacuous reasoning through existential import checks that\nincorporate a three-valued decision scheme (True, False, Uncertain) to handle\nboundary cases more faithfully. Furthermore, to overcome the semantic\nsimplicity and low logical complexity of existing datasets, we introduce\nRepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)\nand exhibits substantially greater lexical and structural diversity than prior\nbenchmarks. RepublicQA is grounded in philosophical concepts, featuring\nabstract propositions and systematically organized contrary and contradictory\nrelations, making it the most semantically rich resource for evaluating logical\nreasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art\nperformance on RepublicQA, with a 6.25% average gain over strong baselines, and\ngeneralizes effectively to mainstream logical reasoning benchmarks including\nProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%\naverage gain. These results highlight the strong effectiveness of our\nsemiotic-grounded multi-perspective reasoning in boosting LLMs' logical\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a fundamental capability of large language models\n(LLMs). However, existing studies largely overlook the interplay between\nlogical complexity and semantic complexity, resulting in methods that struggle\nto address challenging scenarios involving abstract propositions, ambiguous\ncontexts, and conflicting stances, which are central to human reasoning. For\nthis gap, we propose LogicAgent, a semiotic-square-guided framework designed to\njointly address logical complexity and semantic complexity. LogicAgent\nexplicitly performs multi-perspective deduction in first-order logic (FOL),\nwhile mitigating vacuous reasoning through existential import checks that\nincorporate a three-valued decision scheme (True, False, Uncertain) to handle\nboundary cases more faithfully. Furthermore, to overcome the semantic\nsimplicity and low logical complexity of existing datasets, we introduce\nRepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)\nand exhibits substantially greater lexical and structural diversity than prior\nbenchmarks. RepublicQA is grounded in philosophical concepts, featuring\nabstract propositions and systematically organized contrary and contradictory\nrelations, making it the most semantically rich resource for evaluating logical\nreasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art\nperformance on RepublicQA, with a 6.25% average gain over strong baselines, and\ngeneralizes effectively to mainstream logical reasoning benchmarks including\nProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%\naverage gain. These results highlight the strong effectiveness of our\nsemiotic-grounded multi-perspective reasoning in boosting LLMs' logical\nperformance."
                },
                "authors": [
                    {
                        "name": "Yunyao Zhang"
                    },
                    {
                        "name": "Xinglang Zhang"
                    },
                    {
                        "name": "Junxi Sheng"
                    },
                    {
                        "name": "Wenbing Li"
                    },
                    {
                        "name": "Junqing Yu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Zikai Song"
                    }
                ],
                "author_detail": {
                    "name": "Zikai Song"
                },
                "author": "Zikai Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07697v3",
                "updated": "2025-09-29T13:29:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    29,
                    47,
                    0,
                    272,
                    0
                ],
                "published": "2025-08-11T07:19:21Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    7,
                    19,
                    21,
                    0,
                    223,
                    0
                ],
                "title": "Semantic-Enhanced Time-Series Forecasting via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Enhanced Time-Series Forecasting via Large Language Models"
                },
                "summary": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Chun Yang"
                    },
                    {
                        "name": "Zhang xiaoxing"
                    },
                    {
                        "name": "Xiaobin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobin Zhu"
                },
                "author": "Xiaobin Zhu",
                "arxiv_comment": "14 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24763v1",
                "updated": "2025-09-29T13:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    28,
                    24,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    28,
                    24,
                    0,
                    272,
                    0
                ],
                "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations\n  within a Hierarchical Exploration Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations\n  within a Hierarchical Exploration Framework"
                },
                "summary": "Zero-shot object navigation in unknown environments presents significant\nchallenges, mainly due to two key limitations: insufficient semantic guidance\nleads to inefficient exploration, while limited spatial memory resulting from\nenvironmental structure causes entrapment in local regions. To address these\nissues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object\nnavigation method based on the TARE hierarchical exploration framework,\nintegrating a viewpoint generation strategy balancing spatial coverage and\nsemantic density with an LLM-based global guidance mechanism. The performance\nimprovement of the proposed method is due to two key innovations. First, the\nviewpoint generation strategy prioritizes areas of high semantic density within\ntraversable sub-regions to maximize spatial coverage and minimize invalid\nexploration. Second, coupled with an LLM-based global guidance mechanism, it\nassesses semantic associations to direct navigation toward high-value spaces,\npreventing local entrapment and ensuring efficient exploration. Deployed on\nhybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves\nreal-time operation and superior performance. On Matterport3D and\nHabitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and\n11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,\nrespectively, over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot object navigation in unknown environments presents significant\nchallenges, mainly due to two key limitations: insufficient semantic guidance\nleads to inefficient exploration, while limited spatial memory resulting from\nenvironmental structure causes entrapment in local regions. To address these\nissues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object\nnavigation method based on the TARE hierarchical exploration framework,\nintegrating a viewpoint generation strategy balancing spatial coverage and\nsemantic density with an LLM-based global guidance mechanism. The performance\nimprovement of the proposed method is due to two key innovations. First, the\nviewpoint generation strategy prioritizes areas of high semantic density within\ntraversable sub-regions to maximize spatial coverage and minimize invalid\nexploration. Second, coupled with an LLM-based global guidance mechanism, it\nassesses semantic associations to direct navigation toward high-value spaces,\npreventing local entrapment and ensuring efficient exploration. Deployed on\nhybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves\nreal-time operation and superior performance. On Matterport3D and\nHabitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and\n11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,\nrespectively, over state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiangyi Meng"
                    },
                    {
                        "name": "Delun Li"
                    },
                    {
                        "name": "Zihao Mao"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Wenjie Song"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Song"
                },
                "author": "Wenjie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02890v2",
                "updated": "2025-09-29T13:23:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    23,
                    57,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-02T23:28:34Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    23,
                    28,
                    34,
                    1,
                    245,
                    0
                ],
                "title": "Grocery to General Merchandise: A Cross-Pollination Recommender using\n  LLMs and Real-Time Cart Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grocery to General Merchandise: A Cross-Pollination Recommender using\n  LLMs and Real-Time Cart Context"
                },
                "summary": "Modern e-commerce platforms strive to enhance customer experience by\nproviding timely and contextually relevant recommendations. However,\nrecommending general merchandise to customers focused on grocery shopping --\nsuch as pairing milk with a milk frother -- remains a critical yet\nunder-explored challenge. This paper introduces a cross-pollination (XP)\nframework, a novel approach that bridges grocery and general merchandise\ncross-category recommendations by leveraging multi-source product associations\nand real-time cart context. Our solution employs a two-stage framework: (1) A\ncandidate generation mechanism that uses co-purchase market basket analysis and\nLLM-based approach to identify novel item-item associations; and (2) a\ntransformer-based ranker that leverages the real-time sequential cart context\nand optimizes for engagement signals such as add-to-carts. Offline analysis and\nonline A/B tests show an increase of 36\\% add-to-cart rate with LLM-based\nretrieval on the item page, and 15\\% lift in add-to-cart using cart\ncontext-based ranker on the cart page. Our work contributes practical\ntechniques for cross-category recommendations and broader insights for\ne-commerce systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern e-commerce platforms strive to enhance customer experience by\nproviding timely and contextually relevant recommendations. However,\nrecommending general merchandise to customers focused on grocery shopping --\nsuch as pairing milk with a milk frother -- remains a critical yet\nunder-explored challenge. This paper introduces a cross-pollination (XP)\nframework, a novel approach that bridges grocery and general merchandise\ncross-category recommendations by leveraging multi-source product associations\nand real-time cart context. Our solution employs a two-stage framework: (1) A\ncandidate generation mechanism that uses co-purchase market basket analysis and\nLLM-based approach to identify novel item-item associations; and (2) a\ntransformer-based ranker that leverages the real-time sequential cart context\nand optimizes for engagement signals such as add-to-carts. Offline analysis and\nonline A/B tests show an increase of 36\\% add-to-cart rate with LLM-based\nretrieval on the item page, and 15\\% lift in add-to-cart using cart\ncontext-based ranker on the cart page. Our work contributes practical\ntechniques for cross-category recommendations and broader insights for\ne-commerce systems."
                },
                "authors": [
                    {
                        "name": "Akshay Kekuda"
                    },
                    {
                        "name": "Murali Mohana Krishna Dandu"
                    },
                    {
                        "name": "Rimita Lahiri"
                    },
                    {
                        "name": "Shiqin Cai"
                    },
                    {
                        "name": "Sinduja Subramaniam"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "Accepted at RecSys 2025 EARL Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24758v1",
                "updated": "2025-09-29T13:23:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    23,
                    6,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:23:06Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    23,
                    6,
                    0,
                    272,
                    0
                ],
                "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors"
                },
                "summary": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have\nenabled high-quality neural rendering; however, their large storage and\ntransmission costs hinder deployment in resource-constrained environments.\nExisting compression methods either rely on costly optimization, which is slow\nand scene-specific, or adopt training-free pruning and quantization, which\ndegrade rendering quality under high compression ratios. In contrast, recent\ndata-driven approaches provide a promising direction to overcome this\ntrade-off, enabling efficient compression while preserving high rendering\nquality. We introduce \\textbf{ExGS}, a novel feed-forward framework that\nunifies \\textbf{Universal Gaussian Compression} (UGC) with\n\\textbf{GaussPainter} for \\textbf{Ex}treme 3D\\textbf{GS} compression.\n\\textbf{UGC} performs re-optimization-free pruning to aggressively reduce\nGaussian primitives while retaining only essential information, whereas\n\\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided\nrefinement to restore high-quality renderings from heavily pruned Gaussian\nscenes. Unlike conventional inpainting, GaussPainter not only fills in missing\nregions but also enhances visible pixels, yielding substantial improvements in\ndegraded renderings. To ensure practicality, it adopts a lightweight VAE and a\none-step diffusion design, enabling real-time restoration. Our framework can\neven achieve over $100\\times$ compression (reducing a typical 354.77 MB model\nto about 3.31 MB) while preserving fidelity and significantly improving image\nquality under challenging conditions. These results highlight the central role\nof diffusion priors in bridging the gap between extreme compression and\nhigh-quality neural rendering. Our code repository will be released at\n\\href{https://github.com/chenttt2001/ExGS}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have\nenabled high-quality neural rendering; however, their large storage and\ntransmission costs hinder deployment in resource-constrained environments.\nExisting compression methods either rely on costly optimization, which is slow\nand scene-specific, or adopt training-free pruning and quantization, which\ndegrade rendering quality under high compression ratios. In contrast, recent\ndata-driven approaches provide a promising direction to overcome this\ntrade-off, enabling efficient compression while preserving high rendering\nquality. We introduce \\textbf{ExGS}, a novel feed-forward framework that\nunifies \\textbf{Universal Gaussian Compression} (UGC) with\n\\textbf{GaussPainter} for \\textbf{Ex}treme 3D\\textbf{GS} compression.\n\\textbf{UGC} performs re-optimization-free pruning to aggressively reduce\nGaussian primitives while retaining only essential information, whereas\n\\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided\nrefinement to restore high-quality renderings from heavily pruned Gaussian\nscenes. Unlike conventional inpainting, GaussPainter not only fills in missing\nregions but also enhances visible pixels, yielding substantial improvements in\ndegraded renderings. To ensure practicality, it adopts a lightweight VAE and a\none-step diffusion design, enabling real-time restoration. Our framework can\neven achieve over $100\\times$ compression (reducing a typical 354.77 MB model\nto about 3.31 MB) while preserving fidelity and significantly improving image\nquality under challenging conditions. These results highlight the central role\nof diffusion priors in bridging the gap between extreme compression and\nhigh-quality neural rendering. Our code repository will be released at\n\\href{https://github.com/chenttt2001/ExGS}{here}."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Xinhao Ji"
                    },
                    {
                        "name": "Yuanyuan Gao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuning Gong"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Zhihang Zhong"
                    },
                    {
                        "name": "Dingwen Zhang"
                    },
                    {
                        "name": "Xiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Sun"
                },
                "author": "Xiao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24748v1",
                "updated": "2025-09-29T13:15:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    15,
                    42,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    15,
                    42,
                    0,
                    272,
                    0
                ],
                "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data\n  Corruption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data\n  Corruption"
                },
                "summary": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$."
                },
                "authors": [
                    {
                        "name": "Longxiang He"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Junbo Tan"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Li Shen"
                    }
                ],
                "author_detail": {
                    "name": "Li Shen"
                },
                "author": "Li Shen",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24745v1",
                "updated": "2025-09-29T13:10:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    10,
                    39,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:10:39Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    10,
                    39,
                    0,
                    272,
                    0
                ],
                "title": "ProxyAttn: Guided Sparse Attention via Representative Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyAttn: Guided Sparse Attention via Representative Heads"
                },
                "summary": "The quadratic complexity of attention mechanisms limits the efficiency of\nLarge Language Models (LLMs) on long-text tasks. Recently, methods that\ndynamically estimate block importance have enabled efficient block sparse\nattention, leading to significant acceleration in long-text pre-filling of\nLLMs. However, their coarse-grained estimation inevitably leads to performance\ndegradation at high sparsity rates. In this work, we propose ProxyAttn, a\ntraining-free sparse attention algorithm that achieves more precise block\nestimation by compressing the dimension of attention heads. Based on our\nobservation of the similarity among multiple attention heads, we use the scores\nof pooled representative heads to approximate the scores for all heads. To\naccount for the varying sparsity among heads, we also propose a block-aware\ndynamic budget estimation method. By combining the scores from representative\nproxy heads with multi-head dynamic budgets, we achieve a more fine-grained\nblock importance evaluation at low computational cost. Experiments on a variety\nof mainstream models and extensive benchmarks confirm the underlying similarity\namong attention heads. Leveraging a fine-grained estimation, the proposed\nmethod achieves substantial gains in performance and efficiency compared to\nexisting methods. More precisely, ProxyAttn can achieve up to 10.3x attention\nacceleration and 2.4x prefilling acceleration without significant performance\nloss. Our code is available at https://github.com/wyxstriker/ProxyAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of attention mechanisms limits the efficiency of\nLarge Language Models (LLMs) on long-text tasks. Recently, methods that\ndynamically estimate block importance have enabled efficient block sparse\nattention, leading to significant acceleration in long-text pre-filling of\nLLMs. However, their coarse-grained estimation inevitably leads to performance\ndegradation at high sparsity rates. In this work, we propose ProxyAttn, a\ntraining-free sparse attention algorithm that achieves more precise block\nestimation by compressing the dimension of attention heads. Based on our\nobservation of the similarity among multiple attention heads, we use the scores\nof pooled representative heads to approximate the scores for all heads. To\naccount for the varying sparsity among heads, we also propose a block-aware\ndynamic budget estimation method. By combining the scores from representative\nproxy heads with multi-head dynamic budgets, we achieve a more fine-grained\nblock importance evaluation at low computational cost. Experiments on a variety\nof mainstream models and extensive benchmarks confirm the underlying similarity\namong attention heads. Leveraging a fine-grained estimation, the proposed\nmethod achieves substantial gains in performance and efficiency compared to\nexisting methods. More precisely, ProxyAttn can achieve up to 10.3x attention\nacceleration and 2.4x prefilling acceleration without significant performance\nloss. Our code is available at https://github.com/wyxstriker/ProxyAttn."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Huang He"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Haifeng Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "14pages, 5figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24830v2",
                "updated": "2025-09-29T12:59:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    59,
                    30,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-30T17:33:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    33,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs"
                },
                "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Annika Domres"
                    },
                    {
                        "name": "Mai Nguyen"
                    },
                    {
                        "name": "Rebecca Moser"
                    },
                    {
                        "name": "Jana Nano"
                    },
                    {
                        "name": "Felix Busch"
                    },
                    {
                        "name": "Lisa C. Adams"
                    },
                    {
                        "name": "Keno K. Bressem"
                    },
                    {
                        "name": "Denise Bernhardt"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Kai J. Borm"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Jan C. Peeken"
                    }
                ],
                "author_detail": {
                    "name": "Jan C. Peeken"
                },
                "author": "Jan C. Peeken",
                "arxiv_comment": "15 pages, 5 figures and tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]