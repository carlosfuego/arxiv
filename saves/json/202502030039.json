[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaqun Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gmez-Albarrn"
                    },
                    {
                        "name": "Jos-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "Jos-Luis Sierra"
                },
                "author": "Jos-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v1",
                "updated": "2025-01-25T01:45:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Brgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jimnez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. Garca-Len"
                    },
                    {
                        "name": "R. Garca-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqu"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elssser"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frdric Mazen"
                    },
                    {
                        "name": "Sbastien Kerdils"
                    },
                    {
                        "name": "Flix Cache"
                    },
                    {
                        "name": "Anas Drau"
                    },
                    {
                        "name": "Jean-Michel Grard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Grard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Grard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Adasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_doi": "10.1063/5.0250428",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0250428",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "arxiv_journal_ref": "J. Appl. Phys. 137, 044103 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Komrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14795v1",
                "updated": "2025-01-11T12:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T12:22:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL"
                },
                "summary": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation."
                },
                "authors": [
                    {
                        "name": "Nicolas Lee Guidotti"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Lee Guidotti"
                },
                "author": "Nicolas Lee Guidotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18596v1",
                "updated": "2025-01-30T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights"
                },
                "summary": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical."
                },
                "authors": [
                    {
                        "name": "Liana Mikaelyan"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mathew Salvaris"
                    },
                    {
                        "name": "Parth Pathak"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Fayyaz"
                },
                "author": "Mohsen Fayyaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18594v1",
                "updated": "2025-01-30T18:59:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    43,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:59:43Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    43,
                    3,
                    30,
                    0
                ],
                "title": "Foundational Models for 3D Point Clouds: A Survey and Outlook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Models for 3D Point Clouds: A Survey and Outlook"
                },
                "summary": "The 3D point cloud representation plays a crucial role in preserving the\ngeometric fidelity of the physical world, enabling more accurate complex 3D\nenvironments. While humans naturally comprehend the intricate relationships\nbetween objects and variations through a multisensory system, artificial\nintelligence (AI) systems have yet to fully replicate this capacity. To bridge\nthis gap, it becomes essential to incorporate multiple modalities. Models that\ncan seamlessly integrate and reason across these modalities are known as\nfoundation models (FMs). The development of FMs for 2D modalities, such as\nimages and text, has seen significant progress, driven by the abundant\navailability of large-scale datasets. However, the 3D domain has lagged due to\nthe scarcity of labelled data and high computational overheads. In response,\nrecent research has begun to explore the potential of applying FMs to 3D tasks,\novercoming these challenges by leveraging existing 2D knowledge. Additionally,\nlanguage, with its capacity for abstract reasoning and description of the\nenvironment, offers a promising avenue for enhancing 3D understanding through\nlarge pre-trained language models (LLMs). Despite the rapid development and\nadoption of FMs for 3D vision tasks in recent years, there remains a gap in\ncomprehensive and in-depth literature reviews. This article aims to address\nthis gap by presenting a comprehensive overview of the state-of-the-art methods\nthat utilize FMs for 3D visual understanding. We start by reviewing various\nstrategies employed in the building of various 3D FMs. Then we categorize and\nsummarize use of different FMs for tasks such as perception tasks. Finally, the\narticle offers insights into future directions for research and development in\nthis field. To help reader, we have curated list of relevant papers on the\ntopic: https://github.com/vgthengane/Awesome-FMs-in-3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 3D point cloud representation plays a crucial role in preserving the\ngeometric fidelity of the physical world, enabling more accurate complex 3D\nenvironments. While humans naturally comprehend the intricate relationships\nbetween objects and variations through a multisensory system, artificial\nintelligence (AI) systems have yet to fully replicate this capacity. To bridge\nthis gap, it becomes essential to incorporate multiple modalities. Models that\ncan seamlessly integrate and reason across these modalities are known as\nfoundation models (FMs). The development of FMs for 2D modalities, such as\nimages and text, has seen significant progress, driven by the abundant\navailability of large-scale datasets. However, the 3D domain has lagged due to\nthe scarcity of labelled data and high computational overheads. In response,\nrecent research has begun to explore the potential of applying FMs to 3D tasks,\novercoming these challenges by leveraging existing 2D knowledge. Additionally,\nlanguage, with its capacity for abstract reasoning and description of the\nenvironment, offers a promising avenue for enhancing 3D understanding through\nlarge pre-trained language models (LLMs). Despite the rapid development and\nadoption of FMs for 3D vision tasks in recent years, there remains a gap in\ncomprehensive and in-depth literature reviews. This article aims to address\nthis gap by presenting a comprehensive overview of the state-of-the-art methods\nthat utilize FMs for 3D visual understanding. We start by reviewing various\nstrategies employed in the building of various 3D FMs. Then we categorize and\nsummarize use of different FMs for tasks such as perception tasks. Finally, the\narticle offers insights into future directions for research and development in\nthis field. To help reader, we have curated list of relevant papers on the\ntopic: https://github.com/vgthengane/Awesome-FMs-in-3D."
                },
                "authors": [
                    {
                        "name": "Vishal Thengane"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Salim Bouzerdoum"
                    },
                    {
                        "name": "Son Lam Phung"
                    },
                    {
                        "name": "Yunpeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Li"
                },
                "author": "Yunpeng Li",
                "arxiv_comment": "Initial submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18585v1",
                "updated": "2025-01-30T18:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs"
                },
                "summary": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18577v1",
                "updated": "2025-01-30T18:46:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    46,
                    43,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:46:43Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    46,
                    43,
                    3,
                    30,
                    0
                ],
                "title": "Prediction-Powered Inference with Imputed Covariates and Nonuniform\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-Powered Inference with Imputed Covariates and Nonuniform\n  Sampling"
                },
                "summary": "Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions."
                },
                "authors": [
                    {
                        "name": "Dan M. Kluger"
                    },
                    {
                        "name": "Kerri Lu"
                    },
                    {
                        "name": "Tijana Zrnic"
                    },
                    {
                        "name": "Sherrie Wang"
                    },
                    {
                        "name": "Stephen Bates"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Bates"
                },
                "author": "Stephen Bates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18576v1",
                "updated": "2025-01-30T18:45:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    45,
                    51,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    45,
                    51,
                    3,
                    30,
                    0
                ],
                "title": "Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for\n  Multi-Step Reasoning Over Speed in MATH",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for\n  Multi-Step Reasoning Over Speed in MATH"
                },
                "summary": "This study investigates the performance of the DeepSeek R1 language model on\n30 challenging mathematical problems derived from the MATH dataset, problems\nthat previously proved unsolvable by other models under time constraints.\nUnlike prior work, this research removes time limitations to explore whether\nDeepSeek R1's architecture, known for its reliance on token-based reasoning,\ncan achieve accurate solutions through a multi-step process. The study compares\nDeepSeek R1 with four other models (gemini-1.5-flash-8b,\ngpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11\ntemperature settings. Results demonstrate that DeepSeek R1 achieves superior\naccuracy on these complex problems but generates significantly more tokens than\nother models, confirming its token-intensive approach. The findings highlight a\ntrade-off between accuracy and efficiency in mathematical problem-solving with\nlarge language models: while DeepSeek R1 excels in accuracy, its reliance on\nextensive token generation may not be optimal for applications requiring rapid\nresponses. The study underscores the importance of considering task-specific\nrequirements when selecting an LLM and emphasizes the role of temperature\nsettings in optimizing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the performance of the DeepSeek R1 language model on\n30 challenging mathematical problems derived from the MATH dataset, problems\nthat previously proved unsolvable by other models under time constraints.\nUnlike prior work, this research removes time limitations to explore whether\nDeepSeek R1's architecture, known for its reliance on token-based reasoning,\ncan achieve accurate solutions through a multi-step process. The study compares\nDeepSeek R1 with four other models (gemini-1.5-flash-8b,\ngpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11\ntemperature settings. Results demonstrate that DeepSeek R1 achieves superior\naccuracy on these complex problems but generates significantly more tokens than\nother models, confirming its token-intensive approach. The findings highlight a\ntrade-off between accuracy and efficiency in mathematical problem-solving with\nlarge language models: while DeepSeek R1 excels in accuracy, its reliance on\nextensive token generation may not be optimal for applications requiring rapid\nresponses. The study underscores the importance of considering task-specific\nrequirements when selecting an LLM and emphasizes the role of temperature\nsettings in optimizing performance."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "5 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18558v1",
                "updated": "2025-01-30T18:35:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    35,
                    2,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:35:02Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    35,
                    2,
                    3,
                    30,
                    0
                ],
                "title": "Log-Gaussian Cox Processes on General Metric Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Gaussian Cox Processes on General Metric Graphs"
                },
                "summary": "The modeling of spatial point processes has advanced considerably, yet\nextending these models to non-Euclidean domains, such as road networks, remains\na challenging problem. We propose a novel framework for log-Gaussian Cox\nprocesses on general compact metric graphs by leveraging the Gaussian\nWhittle-Mat\\'ern fields, which are solutions to fractional-order stochastic\ndifferential equations on metric graphs. To achieve computationally efficient\nlikelihood-based inference, we introduce a numerical approximation of the\nlikelihood that eliminates the need to approximate the Gaussian process. This\nmethod, coupled with the exact evaluation of finite-dimensional distributions\nfor Whittle-Mat\\'ern fields with integer smoothness, ensures scalability and\ntheoretical rigour, with derived convergence rates for posterior distributions.\nThe framework is implemented in the open-source MetricGraph R package, which\nintegrates seamlessly with R-INLA to support fully Bayesian inference. We\ndemonstrate the applicability and scalability of this approach through an\nanalysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over\n150,000 road segments. By identifying high-risk road segments using exceedance\nprobabilities and excursion sets, our framework provides localized insights\ninto accident hotspots and offers a powerful tool for modeling spatial point\nprocesses directly on complex networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of spatial point processes has advanced considerably, yet\nextending these models to non-Euclidean domains, such as road networks, remains\na challenging problem. We propose a novel framework for log-Gaussian Cox\nprocesses on general compact metric graphs by leveraging the Gaussian\nWhittle-Mat\\'ern fields, which are solutions to fractional-order stochastic\ndifferential equations on metric graphs. To achieve computationally efficient\nlikelihood-based inference, we introduce a numerical approximation of the\nlikelihood that eliminates the need to approximate the Gaussian process. This\nmethod, coupled with the exact evaluation of finite-dimensional distributions\nfor Whittle-Mat\\'ern fields with integer smoothness, ensures scalability and\ntheoretical rigour, with derived convergence rates for posterior distributions.\nThe framework is implemented in the open-source MetricGraph R package, which\nintegrates seamlessly with R-INLA to support fully Bayesian inference. We\ndemonstrate the applicability and scalability of this approach through an\nanalysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over\n150,000 road segments. By identifying high-risk road segments using exceedance\nprobabilities and excursion sets, our framework provides localized insights\ninto accident hotspots and offers a powerful tool for modeling spatial point\nprocesses directly on complex networks."
                },
                "authors": [
                    {
                        "name": "David Bolin"
                    },
                    {
                        "name": "Damilya Saduakhas"
                    },
                    {
                        "name": "Alexandre B. Simas"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre B. Simas"
                },
                "author": "Alexandre B. Simas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18548v1",
                "updated": "2025-01-30T18:20:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    20,
                    28,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:20:28Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    20,
                    28,
                    3,
                    30,
                    0
                ],
                "title": "The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method"
                },
                "summary": "In this work, we introduce the No-Underrun Sampler (NURS): a\nlocally-adaptive, gradient-free Markov chain Monte Carlo method that combines\nelements of Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to\nthe local geometry of the target distribution without requiring gradient\nevaluations, making it especially suitable for applications where gradients are\nunavailable or costly. We establish key theoretical properties, including\nreversibility, formal connections to Hit-and-Run and Random Walk Metropolis,\nWasserstein contraction comparable to Hit-and-Run in Gaussian targets, and\nbounds on the total variation distance between the transition kernels of\nHit-and-Run and NURS. Finally, we demonstrate - through empirical experiments\nsupported by theoretical insights - that NURS can effectively sample Neal's\nfunnel, a challenging multi-scale distribution from Bayesian hierarchical\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the No-Underrun Sampler (NURS): a\nlocally-adaptive, gradient-free Markov chain Monte Carlo method that combines\nelements of Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to\nthe local geometry of the target distribution without requiring gradient\nevaluations, making it especially suitable for applications where gradients are\nunavailable or costly. We establish key theoretical properties, including\nreversibility, formal connections to Hit-and-Run and Random Walk Metropolis,\nWasserstein contraction comparable to Hit-and-Run in Gaussian targets, and\nbounds on the total variation distance between the transition kernels of\nHit-and-Run and NURS. Finally, we demonstrate - through empirical experiments\nsupported by theoretical insights - that NURS can effectively sample Neal's\nfunnel, a challenging multi-scale distribution from Bayesian hierarchical\ninference."
                },
                "authors": [
                    {
                        "name": "Nawaf Bou-Rabee"
                    },
                    {
                        "name": "Bob Carpenter"
                    },
                    {
                        "name": "Sifan Liu"
                    },
                    {
                        "name": "Stefan Oberdrster"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Oberdrster"
                },
                "author": "Stefan Oberdrster",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J05, 65C05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18542v1",
                "updated": "2025-01-30T18:10:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    10,
                    16,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    10,
                    16,
                    3,
                    30,
                    0
                ],
                "title": "Semantic Web and Creative AI -- A Technical Report from ISWS 2023",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Web and Creative AI -- A Technical Report from ISWS 2023"
                },
                "summary": "The International Semantic Web Research School (ISWS) is a week-long\nintensive program designed to immerse participants in the field. This document\nreports a collaborative effort performed by ten teams of students, each guided\nby a senior researcher as their mentor, attending ISWS 2023. Each team provided\na different perspective to the topic of creative AI, substantiated by a set of\nresearch questions as the main subject of their investigation. The 2023 edition\nof ISWS focuses on the intersection of Semantic Web technologies and Creative\nAI. ISWS 2023 explored various intersections between Semantic Web technologies\nand creative AI. A key area of focus was the potential of LLMs as support tools\nfor knowledge engineering. Participants also delved into the multifaceted\napplications of LLMs, including legal aspects of creative content production,\nhumans in the loop, decentralised approaches to multimodal generative AI\nmodels, nanopublications and AI for personal scientific knowledge graphs,\ncommonsense knowledge in automatic story and narrative completion, generative\nAI for art critique, prompt engineering, automatic music composition,\ncommonsense prototyping and conceptual blending, and elicitation of tacit\nknowledge. As Large Language Models and semantic technologies continue to\nevolve, new exciting prospects are emerging: a future where the boundaries\nbetween creative expression and factual knowledge become increasingly permeable\nand porous, leading to a world of knowledge that is both informative and\ninspiring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Semantic Web Research School (ISWS) is a week-long\nintensive program designed to immerse participants in the field. This document\nreports a collaborative effort performed by ten teams of students, each guided\nby a senior researcher as their mentor, attending ISWS 2023. Each team provided\na different perspective to the topic of creative AI, substantiated by a set of\nresearch questions as the main subject of their investigation. The 2023 edition\nof ISWS focuses on the intersection of Semantic Web technologies and Creative\nAI. ISWS 2023 explored various intersections between Semantic Web technologies\nand creative AI. A key area of focus was the potential of LLMs as support tools\nfor knowledge engineering. Participants also delved into the multifaceted\napplications of LLMs, including legal aspects of creative content production,\nhumans in the loop, decentralised approaches to multimodal generative AI\nmodels, nanopublications and AI for personal scientific knowledge graphs,\ncommonsense knowledge in automatic story and narrative completion, generative\nAI for art critique, prompt engineering, automatic music composition,\ncommonsense prototyping and conceptual blending, and elicitation of tacit\nknowledge. As Large Language Models and semantic technologies continue to\nevolve, new exciting prospects are emerging: a future where the boundaries\nbetween creative expression and factual knowledge become increasingly permeable\nand porous, leading to a world of knowledge that is both informative and\ninspiring."
                },
                "authors": [
                    {
                        "name": "Raia Abu Ahmad"
                    },
                    {
                        "name": "Reham Alharbi"
                    },
                    {
                        "name": "Roberto Barile"
                    },
                    {
                        "name": "Martin Bckling"
                    },
                    {
                        "name": "Francisco Bolanos"
                    },
                    {
                        "name": "Sara Bonfitto"
                    },
                    {
                        "name": "Oleksandra Bruns"
                    },
                    {
                        "name": "Irene Celino"
                    },
                    {
                        "name": "Yashrajsinh Chudasama"
                    },
                    {
                        "name": "Martin Critelli"
                    },
                    {
                        "name": "Claudia d'Amato"
                    },
                    {
                        "name": "Giada D'Ippolito"
                    },
                    {
                        "name": "Ioannis Dasoulas"
                    },
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Vincenzo De Leo"
                    },
                    {
                        "name": "Chiara Di Bonaventura"
                    },
                    {
                        "name": "Marco Di Panfilo"
                    },
                    {
                        "name": "Daniil Dobriy"
                    },
                    {
                        "name": "John Domingue"
                    },
                    {
                        "name": "Xuemin Duan"
                    },
                    {
                        "name": "Michel Dumontier"
                    },
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Ruben Eschauzier"
                    },
                    {
                        "name": "Fakih Ginwa"
                    },
                    {
                        "name": "Nicolas Ferranti"
                    },
                    {
                        "name": "Arianna Graciotti"
                    },
                    {
                        "name": "Philipp Hanisch"
                    },
                    {
                        "name": "George Hannah"
                    },
                    {
                        "name": "Golsa Heidari"
                    },
                    {
                        "name": "Aidan Hogan"
                    },
                    {
                        "name": "Hassan Hussein"
                    },
                    {
                        "name": "Alexane Jouglar"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    },
                    {
                        "name": "Mano Kieffer"
                    },
                    {
                        "name": "Antonis Klironomos"
                    },
                    {
                        "name": "Ins Koch"
                    },
                    {
                        "name": "Weronika Lajewska"
                    },
                    {
                        "name": "Nicolas Lazzari"
                    },
                    {
                        "name": "Mikael Lindekrans"
                    },
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Majlinda Llugiqi"
                    },
                    {
                        "name": "Eleonora Mancini"
                    },
                    {
                        "name": "Eleonora Marzi"
                    },
                    {
                        "name": "Laura Menotti"
                    },
                    {
                        "name": "Daniela Milon Flores"
                    },
                    {
                        "name": "Soulakshmee Nagowah"
                    },
                    {
                        "name": "Kerstin Neubert"
                    },
                    {
                        "name": "Emetis Niazmand"
                    },
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Beatriz Olarte Martinez"
                    },
                    {
                        "name": "Anouk Michelle Oudshoorn"
                    },
                    {
                        "name": "Andrea Poltronieri"
                    },
                    {
                        "name": "Valentina Presutti"
                    },
                    {
                        "name": "Disha Purohit"
                    },
                    {
                        "name": "Ensiyeh Raoufi"
                    },
                    {
                        "name": "Celian Ringwald"
                    },
                    {
                        "name": "Johanna Rockstroh"
                    },
                    {
                        "name": "Sebastian Rudolph"
                    },
                    {
                        "name": "Harald Sack"
                    },
                    {
                        "name": "Zafar Saeed"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Aya Sahbi"
                    },
                    {
                        "name": "Cristian Santini"
                    },
                    {
                        "name": "Aleksandra Simic"
                    },
                    {
                        "name": "Dennis Sommer"
                    },
                    {
                        "name": "Rita Sousa"
                    },
                    {
                        "name": "Mary Ann Tan"
                    },
                    {
                        "name": "Vidyashree Tarikere"
                    },
                    {
                        "name": "Tabea Tietz"
                    },
                    {
                        "name": "Liam Tirpitz"
                    },
                    {
                        "name": "Arnaldo Tomasino"
                    },
                    {
                        "name": "Frank van Harmelen"
                    },
                    {
                        "name": "Joao Vissoci"
                    },
                    {
                        "name": "Caitlin Woods"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Heng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zheng"
                },
                "author": "Heng Zheng",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18539v1",
                "updated": "2025-01-30T18:07:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    7,
                    19,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:07:19Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    7,
                    19,
                    3,
                    30,
                    0
                ],
                "title": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method"
                },
                "summary": "Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18538v1",
                "updated": "2025-01-30T18:06:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    6,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:06:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    6,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered\n  Design"
                },
                "summary": "Facial Emotion Recognition has emerged as increasingly pivotal in the domain\nof User Experience, notably within modern usability testing, as it facilitates\na deeper comprehension of user satisfaction and engagement. This study aims to\nextend the ResEmoteNet model by employing a knowledge distillation framework to\ndevelop Mini-ResEmoteNet models - lightweight student models - tailored for\nusability testing. Experiments were conducted on the FER2013 and RAF-DB\ndatasets to assess the efficacy of three student model architectures: Student\nModel A, Student Model B, and Student Model C. Their development involves\nreducing the number of feature channels in each layer of the teacher model by\napproximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the\nFER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%,\nmarking a 0.21% absolute improvement over EmoNeXt. Moreover, the results\nexhibit absolute improvements in terms of inference speed and memory usage\nduring inference compared to the ResEmoteNet model. The findings indicate that\nthe proposed methods surpass other state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Emotion Recognition has emerged as increasingly pivotal in the domain\nof User Experience, notably within modern usability testing, as it facilitates\na deeper comprehension of user satisfaction and engagement. This study aims to\nextend the ResEmoteNet model by employing a knowledge distillation framework to\ndevelop Mini-ResEmoteNet models - lightweight student models - tailored for\nusability testing. Experiments were conducted on the FER2013 and RAF-DB\ndatasets to assess the efficacy of three student model architectures: Student\nModel A, Student Model B, and Student Model C. Their development involves\nreducing the number of feature channels in each layer of the teacher model by\napproximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the\nFER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%,\nmarking a 0.21% absolute improvement over EmoNeXt. Moreover, the results\nexhibit absolute improvements in terms of inference speed and memory usage\nduring inference compared to the ResEmoteNet model. The findings indicate that\nthe proposed methods surpass other state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Amna Murtada"
                    },
                    {
                        "name": "Omnia Abdelrhman"
                    },
                    {
                        "name": "Tahani Abdalla Attia"
                    }
                ],
                "author_detail": {
                    "name": "Tahani Abdalla Attia"
                },
                "author": "Tahani Abdalla Attia",
                "arxiv_comment": "5 pages with 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18536v1",
                "updated": "2025-01-30T18:02:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    2,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:02:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    2,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "Illusions of Relevance: Using Content Injection Attacks to Deceive\n  Retrievers, Rerankers, and LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illusions of Relevance: Using Content Injection Attacks to Deceive\n  Retrievers, Rerankers, and LLM Judges"
                },
                "summary": "Consider a scenario in which a user searches for information, only to\nencounter texts flooded with misleading or non-relevant content. This scenario\nexemplifies a simple yet potent vulnerability in neural Information Retrieval\n(IR) pipelines: content injection attacks. We find that embedding models for\nretrieval, rerankers, and large language model (LLM) relevance judges are\nvulnerable to these attacks, in which adversaries insert misleading text into\npassages to manipulate model judgements. We identify two primary threats: (1)\ninserting unrelated or harmful content within passages that still appear\ndeceptively \"relevant\", and (2) inserting entire queries or key query terms\ninto passages to boost their perceived relevance. While the second tactic has\nbeen explored in prior research, we present, to our knowledge, the first\nempirical analysis of the first threat, demonstrating how state-of-the-art\nmodels can be easily misled. Our study systematically examines the factors that\ninfluence an attack's success, such as the placement of injected content and\nthe balance between relevant and non-relevant material. Additionally, we\nexplore various defense strategies, including adversarial passage classifiers,\nretriever fine-tuning to discount manipulated content, and prompting LLM judges\nto adopt a more cautious approach. However, we find that these countermeasures\noften involve trade-offs, sacrificing effectiveness for attack robustness and\nsometimes penalizing legitimate documents in the process. Our findings\nhighlight the need for stronger defenses against these evolving adversarial\nstrategies to maintain the trustworthiness of IR systems. We release our code\nand scripts to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider a scenario in which a user searches for information, only to\nencounter texts flooded with misleading or non-relevant content. This scenario\nexemplifies a simple yet potent vulnerability in neural Information Retrieval\n(IR) pipelines: content injection attacks. We find that embedding models for\nretrieval, rerankers, and large language model (LLM) relevance judges are\nvulnerable to these attacks, in which adversaries insert misleading text into\npassages to manipulate model judgements. We identify two primary threats: (1)\ninserting unrelated or harmful content within passages that still appear\ndeceptively \"relevant\", and (2) inserting entire queries or key query terms\ninto passages to boost their perceived relevance. While the second tactic has\nbeen explored in prior research, we present, to our knowledge, the first\nempirical analysis of the first threat, demonstrating how state-of-the-art\nmodels can be easily misled. Our study systematically examines the factors that\ninfluence an attack's success, such as the placement of injected content and\nthe balance between relevant and non-relevant material. Additionally, we\nexplore various defense strategies, including adversarial passage classifiers,\nretriever fine-tuning to discount manipulated content, and prompting LLM judges\nto adopt a more cautious approach. However, we find that these countermeasures\noften involve trade-offs, sacrificing effectiveness for attack robustness and\nsometimes penalizing legitimate documents in the process. Our findings\nhighlight the need for stronger defenses against these evolving adversarial\nstrategies to maintain the trustworthiness of IR systems. We release our code\nand scripts to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Manveer Singh Tamber"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17635v2",
                "updated": "2025-01-30T17:59:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    59,
                    8,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-29T13:12:01Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "title": "In-Context Meta LoRA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Meta LoRA Generation"
                },
                "summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jingcai Guo"
                },
                "author": "Jingcai Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18532v1",
                "updated": "2025-01-30T17:58:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    58,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:58:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    58,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "Differentially Private Steering for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Steering for Large Language Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe \\textit{\\underline{P}rivate \\underline{S}teering for LLM\n\\underline{A}lignment (PSA)} algorithm to edit LLM activations with\ndifferential privacy (DP) guarantees. We conduct extensive experiments on seven\ndifferent benchmarks with open-source LLMs of different sizes (0.5B to 7B) and\nmodel families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA\nachieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our attack is tailored for activation\nediting and relies solely on the generated texts without their associated\nprobabilities. Our experiments support the theoretical guarantees by showing\nimproved guarantees for our \\textit{PSA} algorithm compared to several existing\nnon-private techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe \\textit{\\underline{P}rivate \\underline{S}teering for LLM\n\\underline{A}lignment (PSA)} algorithm to edit LLM activations with\ndifferential privacy (DP) guarantees. We conduct extensive experiments on seven\ndifferent benchmarks with open-source LLMs of different sizes (0.5B to 7B) and\nmodel families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA\nachieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our attack is tailored for activation\nediting and relies solely on the generated texts without their associated\nprobabilities. Our experiments support the theoretical guarantees by showing\nimproved guarantees for our \\textit{PSA} algorithm compared to several existing\nnon-private techniques."
                },
                "authors": [
                    {
                        "name": "Anmol Goel"
                    },
                    {
                        "name": "Yaxi Hu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Amartya Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Amartya Sanyal"
                },
                "author": "Amartya Sanyal",
                "arxiv_comment": "ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13198v2",
                "updated": "2025-01-30T17:55:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    55,
                    31,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T20:00:41Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    20,
                    0,
                    41,
                    2,
                    22,
                    0
                ],
                "title": "S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning"
                },
                "summary": "Continual Learning with foundation models has recently emerged as a promising\napproach to harnessing the power of pre-trained models for sequential tasks.\nExisting prompt-based methods generally use a gating mechanism to select\nrelevant prompts aligned with the test query for further processing. However,\nthe success of these methods largely depends on the precision of the gating\nmechanism, which becomes less scalable with additional computational overhead\nas tasks increases. To overcome these issues, we propose a Scalable Low-Rank\nAdaptation (S-LoRA) method for CL (in particular class incremental learning),\nwhich incrementally decouples the learning of the direction and magnitude of\nLoRA parameters. S-LoRA supports efficient inference by employing the\nlast-stage trained model for direct testing without a gating process. Our\ntheoretical and empirical analysis demonstrates that S-LoRA tends to follow a\nlow-loss trajectory that converges to an overlapped low-loss region, resulting\nin an excellent stability-plasticity trade-off in CL. Furthermore, based on our\nfindings, we develop variants of S-LoRA with further improved scalability.\nExtensive experiments across multiple CL benchmarks and various foundation\nmodels consistently validate the effectiveness of S-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning with foundation models has recently emerged as a promising\napproach to harnessing the power of pre-trained models for sequential tasks.\nExisting prompt-based methods generally use a gating mechanism to select\nrelevant prompts aligned with the test query for further processing. However,\nthe success of these methods largely depends on the precision of the gating\nmechanism, which becomes less scalable with additional computational overhead\nas tasks increases. To overcome these issues, we propose a Scalable Low-Rank\nAdaptation (S-LoRA) method for CL (in particular class incremental learning),\nwhich incrementally decouples the learning of the direction and magnitude of\nLoRA parameters. S-LoRA supports efficient inference by employing the\nlast-stage trained model for direct testing without a gating process. Our\ntheoretical and empirical analysis demonstrates that S-LoRA tends to follow a\nlow-loss trajectory that converges to an overlapped low-loss region, resulting\nin an excellent stability-plasticity trade-off in CL. Furthermore, based on our\nfindings, we develop variants of S-LoRA with further improved scalability.\nExtensive experiments across multiple CL benchmarks and various foundation\nmodels consistently validate the effectiveness of S-LoRA."
                },
                "authors": [
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Hongming Piao"
                    },
                    {
                        "name": "Long-Kai Huang"
                    },
                    {
                        "name": "Renzhen Wang"
                    },
                    {
                        "name": "Wanhua Li"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Kede Ma"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v3",
                "updated": "2025-01-30T17:50:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    50,
                    16,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06595v3",
                "updated": "2025-01-30T17:34:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    34,
                    51,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-10T15:39:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations."
                },
                "authors": [
                    {
                        "name": "Sacha Muller"
                    },
                    {
                        "name": "Antnio Loison"
                    },
                    {
                        "name": "Bilel Omrani"
                    },
                    {
                        "name": "Gautier Viaud"
                    }
                ],
                "author_detail": {
                    "name": "Gautier Viaud"
                },
                "author": "Gautier Viaud",
                "arxiv_comment": "Proceedings of the 31st International Conference on Computational\n  Linguistics",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics (2025), pages 4510 to 4534, Abu Dhabi, UAE. Association for\n  Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18516v1",
                "updated": "2025-01-30T17:28:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:28:11Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "title": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models"
                },
                "summary": "Object rearrangement is a significant task for collaborative robots, where\nthey are directed to manipulate objects into a specified goal state.\nDetermining the placement of objects is a major challenge that influences the\nefficiency of the rearrangement process. Most current methods heavily rely on\npre-collected datasets to train the model for predicting the goal position and\nare restricted to specific instructions, which limits their broader\napplicability and effectiveness.In this paper, we propose a framework of\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Particularly, our approach mimics human reasoning by using past\nsuccessful experiences as a reference to infer the desired goal position. Based\non LLM's strong natural language comprehension and inference ability, our\nmethod can generalise to handle various everyday objects and free-form language\ninstructions in a zero-shot manner. Experimental results demonstrate that our\nmethods can effectively execute the robotic rearrangement tasks, even those\ninvolving long sequential orders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object rearrangement is a significant task for collaborative robots, where\nthey are directed to manipulate objects into a specified goal state.\nDetermining the placement of objects is a major challenge that influences the\nefficiency of the rearrangement process. Most current methods heavily rely on\npre-collected datasets to train the model for predicting the goal position and\nare restricted to specific instructions, which limits their broader\napplicability and effectiveness.In this paper, we propose a framework of\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Particularly, our approach mimics human reasoning by using past\nsuccessful experiences as a reference to infer the desired goal position. Based\non LLM's strong natural language comprehension and inference ability, our\nmethod can generalise to handle various everyday objects and free-form language\ninstructions in a zero-shot manner. Experimental results demonstrate that our\nmethods can effectively execute the robotic rearrangement tasks, even those\ninvolving long sequential orders."
                },
                "authors": [
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Ryan Mckenna"
                    },
                    {
                        "name": "John Oyekan"
                    }
                ],
                "author_detail": {
                    "name": "John Oyekan"
                },
                "author": "John Oyekan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18512v1",
                "updated": "2025-01-30T17:23:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    23,
                    50,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:23:50Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    23,
                    50,
                    3,
                    30,
                    0
                ],
                "title": "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch"
                },
                "summary": "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Arthur Douillard"
                    },
                    {
                        "name": "Yanislav Donchev"
                    },
                    {
                        "name": "Keith Rush"
                    },
                    {
                        "name": "Satyen Kale"
                    },
                    {
                        "name": "Zachary Charles"
                    },
                    {
                        "name": "Zachary Garrett"
                    },
                    {
                        "name": "Gabriel Teston"
                    },
                    {
                        "name": "Dave Lacey"
                    },
                    {
                        "name": "Ross McIlroy"
                    },
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Arthur Szlam"
                    },
                    {
                        "name": "Marc'Aurelio Ranzato"
                    },
                    {
                        "name": "Paul Barham"
                    }
                ],
                "author_detail": {
                    "name": "Paul Barham"
                },
                "author": "Paul Barham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18511v1",
                "updated": "2025-01-30T17:21:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    21,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:21:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    21,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training"
                },
                "summary": "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Chinmay Hegde"
                    }
                ],
                "author_detail": {
                    "name": "Chinmay Hegde"
                },
                "author": "Chinmay Hegde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14265v2",
                "updated": "2025-01-30T17:19:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    19,
                    5,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-24T06:07:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    7,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement"
                },
                "summary": "In image enhancement tasks, such as low-light and underwater image\nenhancement, a degraded image can correspond to multiple plausible target\nimages due to dynamic photography conditions, such as variations in\nillumination. This naturally results in a one-to-many mapping challenge. To\naddress this, we propose a Bayesian Enhancement Model (BEM) that incorporates\nBayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse\noutputs. To achieve real-time inference, we introduce a two-stage approach:\nStage I employs a BNN to model the one-to-many mappings in the low-dimensional\nspace, while Stage II refines fine-grained image details using a Deterministic\nNeural Network (DNN). To accelerate BNN training and convergence, we introduce\na dynamic Momentum Prior. Extensive experiments on multiple low-light and\nunderwater image enhancement benchmarks demonstrate the superiority of our\nmethod over deterministic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In image enhancement tasks, such as low-light and underwater image\nenhancement, a degraded image can correspond to multiple plausible target\nimages due to dynamic photography conditions, such as variations in\nillumination. This naturally results in a one-to-many mapping challenge. To\naddress this, we propose a Bayesian Enhancement Model (BEM) that incorporates\nBayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse\noutputs. To achieve real-time inference, we introduce a two-stage approach:\nStage I employs a BNN to model the one-to-many mappings in the low-dimensional\nspace, while Stage II refines fine-grained image details using a Deterministic\nNeural Network (DNN). To accelerate BNN training and convergence, we introduce\na dynamic Momentum Prior. Extensive experiments on multiple low-light and\nunderwater image enhancement benchmarks demonstrate the superiority of our\nmethod over deterministic models."
                },
                "authors": [
                    {
                        "name": "Guoxi Huang"
                    },
                    {
                        "name": "Nantheera Anantrasirichai"
                    },
                    {
                        "name": "Fei Ye"
                    },
                    {
                        "name": "Zipeng Qi"
                    },
                    {
                        "name": "RuiRui Lin"
                    },
                    {
                        "name": "Qirui Yang"
                    },
                    {
                        "name": "David Bull"
                    }
                ],
                "author_detail": {
                    "name": "David Bull"
                },
                "author": "David Bull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18504v1",
                "updated": "2025-01-30T17:13:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:13:32Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "title": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction"
                },
                "summary": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_comment": "9 pages plus 2 pages of supplemental material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18501v1",
                "updated": "2025-01-30T17:11:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    11,
                    34,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:11:34Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    11,
                    34,
                    3,
                    30,
                    0
                ],
                "title": "Beyond Prior Limits: Addressing Distribution Misalignment in Particle\n  Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prior Limits: Addressing Distribution Misalignment in Particle\n  Filtering"
                },
                "summary": "Particle filtering is a Bayesian inference method and a fundamental tool in\nstate estimation for dynamic systems, but its effectiveness is often limited by\nthe constraints of the initial prior distribution, a phenomenon we define as\nthe Prior Boundary Phenomenon. This challenge arises when target states lie\noutside the prior's support, rendering traditional particle filtering methods\ninadequate for accurate estimation. Although techniques like unbounded priors\nand larger particle sets have been proposed, they remain computationally\nprohibitive and lack adaptability in dynamic scenarios. To systematically\novercome these limitations, we propose the Diffusion-Enhanced Particle\nFiltering Framework, which introduces three key innovations: adaptive diffusion\nthrough exploratory particles, entropy-driven regularisation to prevent weight\ncollapse, and kernel-based perturbations for dynamic support expansion. These\nmechanisms collectively enable particle filtering to explore beyond prior\nboundaries, ensuring robust state estimation for out-of-boundary targets.\nTheoretical analysis and extensive experiments validate framework's\neffectiveness, indicating significant improvements in success rates and\nestimation accuracy across high-dimensional and non-convex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle filtering is a Bayesian inference method and a fundamental tool in\nstate estimation for dynamic systems, but its effectiveness is often limited by\nthe constraints of the initial prior distribution, a phenomenon we define as\nthe Prior Boundary Phenomenon. This challenge arises when target states lie\noutside the prior's support, rendering traditional particle filtering methods\ninadequate for accurate estimation. Although techniques like unbounded priors\nand larger particle sets have been proposed, they remain computationally\nprohibitive and lack adaptability in dynamic scenarios. To systematically\novercome these limitations, we propose the Diffusion-Enhanced Particle\nFiltering Framework, which introduces three key innovations: adaptive diffusion\nthrough exploratory particles, entropy-driven regularisation to prevent weight\ncollapse, and kernel-based perturbations for dynamic support expansion. These\nmechanisms collectively enable particle filtering to explore beyond prior\nboundaries, ensuring robust state estimation for out-of-boundary targets.\nTheoretical analysis and extensive experiments validate framework's\neffectiveness, indicating significant improvements in success rates and\nestimation accuracy across high-dimensional and non-convex scenarios."
                },
                "authors": [
                    {
                        "name": "Yiwei Shi"
                    },
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Cunjia Liu"
                    },
                    {
                        "name": "Weiru Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiru Liu"
                },
                "author": "Weiru Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.03634v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.03634v5",
                "updated": "2025-01-30T17:06:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    6,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2021-11-05T17:35:45Z",
                "published_parsed": [
                    2021,
                    11,
                    5,
                    17,
                    35,
                    45,
                    4,
                    309,
                    0
                ],
                "title": "The population of merging compact binaries inferred using gravitational\n  waves through GWTC-3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population of merging compact binaries inferred using gravitational\n  waves through GWTC-3"
                },
                "summary": "We report on the population properties of compact binary mergers inferred\nfrom gravitational-wave observations of these systems during the first three\nLIGO-Virgo observing runs. The Gravitational-Wave Transient Catalog 3 contains\nsignals consistent with three classes of binary mergers: binary black hole,\nbinary neutron star, and neutron star-black hole mergers. We infer the binary\nneutron star merger rate to be between 10 and 1700 Gpc$^{-3} yr$^{-1}$ and the\nneutron star-black hole merger rate to be between 7.8 and 140 Gpc$^{-3}\nyr$^{-1}$, assuming a constant rate density in the comoving frame and taking\nthe union of 90% credible intervals for methods used in this work. We infer the\nbinary black hole merger rate, allowing for evolution with redshift, to be\nbetween 17.9 and 44 Gpc$^{-3}$ yr$^{-1}$ at a fiducial redshift (z=0.2). The\nrate of binary black hole mergers is observed to increase with redshift at a\nrate proportional to $(1+z)^\\kappa$ with $\\kappa=2.9^{+1.7}_{-1.8}$ for\n$z\\lesssim1$. Using both binary neutron star and neutron star-black hole\nbinaries, we obtain a broad, relatively flat neutron star mass distribution\nextending from $1.2^{+0.1}_{-0.2}$ to $2.0^{+0.3}_{-0.3}\\,M_\\odot$. We\nconfidently determine that the merger rate as a function of mass sharply\ndeclines after the expected maximum neutron star mass, but cannot yet confirm\nor rule out the existence of a lower mass gap between neutron stars and black\nholes. We also find the binary black hole mass distribution has localized over-\nand underdensities relative to a power-law distribution, with peaks emerging at\nchirp masses of $8.3^{+0.3}_{-0.5}$ and $27.9^{+1.9}_{-1.8}\\,M_\\odot$. While we\ncontinue to find that the mass distribution of a binary's more massive\ncomponent strongly decreases as a function of primary mass, we observe no\nevidence of a strongly suppressed merger rate above approximately $60\\,M_\\odot$\n[abridged]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the population properties of compact binary mergers inferred\nfrom gravitational-wave observations of these systems during the first three\nLIGO-Virgo observing runs. The Gravitational-Wave Transient Catalog 3 contains\nsignals consistent with three classes of binary mergers: binary black hole,\nbinary neutron star, and neutron star-black hole mergers. We infer the binary\nneutron star merger rate to be between 10 and 1700 Gpc$^{-3} yr$^{-1}$ and the\nneutron star-black hole merger rate to be between 7.8 and 140 Gpc$^{-3}\nyr$^{-1}$, assuming a constant rate density in the comoving frame and taking\nthe union of 90% credible intervals for methods used in this work. We infer the\nbinary black hole merger rate, allowing for evolution with redshift, to be\nbetween 17.9 and 44 Gpc$^{-3}$ yr$^{-1}$ at a fiducial redshift (z=0.2). The\nrate of binary black hole mergers is observed to increase with redshift at a\nrate proportional to $(1+z)^\\kappa$ with $\\kappa=2.9^{+1.7}_{-1.8}$ for\n$z\\lesssim1$. Using both binary neutron star and neutron star-black hole\nbinaries, we obtain a broad, relatively flat neutron star mass distribution\nextending from $1.2^{+0.1}_{-0.2}$ to $2.0^{+0.3}_{-0.3}\\,M_\\odot$. We\nconfidently determine that the merger rate as a function of mass sharply\ndeclines after the expected maximum neutron star mass, but cannot yet confirm\nor rule out the existence of a lower mass gap between neutron stars and black\nholes. We also find the binary black hole mass distribution has localized over-\nand underdensities relative to a power-law distribution, with peaks emerging at\nchirp masses of $8.3^{+0.3}_{-0.5}$ and $27.9^{+1.9}_{-1.8}\\,M_\\odot$. While we\ncontinue to find that the mass distribution of a binary's more massive\ncomponent strongly decreases as a function of primary mass, we observe no\nevidence of a strongly suppressed merger rate above approximately $60\\,M_\\odot$\n[abridged]"
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "R. Abbott"
                    },
                    {
                        "name": "T. D. Abbott"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. B. Adya"
                    },
                    {
                        "name": "C. Affeldt"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "K. Agatsuma"
                    },
                    {
                        "name": "N. Aggarwal"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "C. Anand"
                    },
                    {
                        "name": "S. Anand"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "T. Andrade"
                    },
                    {
                        "name": "N. Andres"
                    },
                    {
                        "name": "T. Andri"
                    },
                    {
                        "name": "S. V. Angelova"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "F. Antonini"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "Koji Arai"
                    },
                    {
                        "name": "Koya Arai"
                    },
                    {
                        "name": "Y. Arai"
                    },
                    {
                        "name": "S. Araki"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "M. Arne"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "K. G. Arun"
                    },
                    {
                        "name": "H. Asada"
                    },
                    {
                        "name": "Y. Asali"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "C. Austin"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "F. Badaracco"
                    },
                    {
                        "name": "M. K. M. Bader"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "Y. Bae"
                    },
                    {
                        "name": "A. M. Baer"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "Y. Bai"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "J. Baird"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "A. Balsamo"
                    },
                    {
                        "name": "G. Baltus"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "C. Barbieri"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "J. Bartlett"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "B. Bcsy"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "I. Belahcene"
                    },
                    {
                        "name": "V. Benedetto"
                    },
                    {
                        "name": "D. Beniwal"
                    },
                    {
                        "name": "T. F. Bennett"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. BenYaala"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "U. Bhardwaj"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "R. Birney"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscans"
                    },
                    {
                        "name": "M. Bischi"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "B. Biswas"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "R. M. Blair"
                    },
                    {
                        "name": "F. Bobba"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "M. Boer"
                    },
                    {
                        "name": "G. Bogaert"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "P. Booker"
                    },
                    {
                        "name": "B. A. Boom"
                    },
                    {
                        "name": "R. Bork"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "N. Bose"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "V. Boudart"
                    },
                    {
                        "name": "Y. Bouffanais"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Bramley"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "J. E. Brau"
                    },
                    {
                        "name": "M. Breschi"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "J. H. Briggs"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "J. Brooks"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "J. Caldern Bustillo"
                    },
                    {
                        "name": "J. D. Callaghan"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "J. Cameron"
                    },
                    {
                        "name": "J. B. Camp"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "S. Canevarolo"
                    },
                    {
                        "name": "M. Cannavacciuolo"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "Z. Cao"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "T. L. Carver"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "G. Castaldi"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavagli"
                    },
                    {
                        "name": "F. Cavalier"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "M. Ceasar"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerd-Durn"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "K. Chakravarti"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "E. Champion"
                    },
                    {
                        "name": "C. -H. Chan"
                    },
                    {
                        "name": "C. Chan"
                    },
                    {
                        "name": "C. L. Chan"
                    },
                    {
                        "name": "K. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "K. Chandra"
                    },
                    {
                        "name": "P. Chanial"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. A. Chase"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "C. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "J. Chen"
                    },
                    {
                        "name": "K. Chen"
                    },
                    {
                        "name": "X. Chen"
                    },
                    {
                        "name": "Y. -B. Chen"
                    },
                    {
                        "name": "Y. -R. Chen"
                    },
                    {
                        "name": "Z. Chen"
                    },
                    {
                        "name": "H. Cheng"
                    },
                    {
                        "name": "C. K. Cheong"
                    },
                    {
                        "name": "H. Y. Cheung"
                    },
                    {
                        "name": "H. Y. Chia"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "C-Y. Chiang"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "R. Chierici"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "G. Cho"
                    },
                    {
                        "name": "H. S. Cho"
                    },
                    {
                        "name": "R. K. Choudhary"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "H. Chu"
                    },
                    {
                        "name": "Q. Chu"
                    },
                    {
                        "name": "Y-K. Chu"
                    },
                    {
                        "name": "S. Chua"
                    },
                    {
                        "name": "K. W. Chung"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cielar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "A. A. Ciobanu"
                    },
                    {
                        "name": "R. Ciolfi"
                    },
                    {
                        "name": "F. Cipriano"
                    },
                    {
                        "name": "A. Cirone"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "E. N. Clark"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "L. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "F. Cleva"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "D. E. Cohen"
                    },
                    {
                        "name": "L. Cohen"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "M. Colpi"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "M. Constancio Jr."
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "S. J. Cooper"
                    },
                    {
                        "name": "P. Corban"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carrin"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "K. R. Corley"
                    },
                    {
                        "name": "N. Cornish"
                    },
                    {
                        "name": "D. Corre"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "S. Cortese"
                    },
                    {
                        "name": "C. A. Costa"
                    },
                    {
                        "name": "R. Cotesta"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "J. -P. Coulon"
                    },
                    {
                        "name": "S. T. Countryman"
                    },
                    {
                        "name": "B. Cousins"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "M. J. Cowart"
                    },
                    {
                        "name": "D. C. Coyne"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "A. W. Criswell"
                    },
                    {
                        "name": "M. Croquette"
                    },
                    {
                        "name": "S. G. Crowder"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "R. Cummings"
                    },
                    {
                        "name": "L. Cunningham"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Curyo"
                    },
                    {
                        "name": "P. Dabadie"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dall'Osso"
                    },
                    {
                        "name": "G. Dlya"
                    },
                    {
                        "name": "A. Dana"
                    },
                    {
                        "name": "L. M. DaneshgaranBajastani"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "C. Darsow-Fromm"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "L. E. H. Datrier"
                    },
                    {
                        "name": "S. Datta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "G. S. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "E. J. Daw"
                    },
                    {
                        "name": "R. Dean"
                    },
                    {
                        "name": "D. DeBra"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "S. Delglise"
                    },
                    {
                        "name": "V. Del Favero"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "N. De Lillo"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "L. M. DeMarchi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "V. D'Emilio"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "S. Dhurandhar"
                    },
                    {
                        "name": "M. C. Daz"
                    },
                    {
                        "name": "M. Diaz-Ortiz Jr."
                    },
                    {
                        "name": "N. A. Didio"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "C. Di Giorgio"
                    },
                    {
                        "name": "F. Di Giovanni"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "A. Di Lieto"
                    },
                    {
                        "name": "B. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "A. K. Divakarla"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "I. Dorrington"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "Y. Drori"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "P. Dupej"
                    },
                    {
                        "name": "O. Durante"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "P. -A. Duverne"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "P. J. Easter"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "B. Edelman"
                    },
                    {
                        "name": "T. B. Edo"
                    },
                    {
                        "name": "O. Edy"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "S. Eguchi"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "S. S. Eikenberry"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "R. A. Eisenstein"
                    },
                    {
                        "name": "A. Ejlli"
                    },
                    {
                        "name": "E. Engelby"
                    },
                    {
                        "name": "Y. Enomoto"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estells"
                    },
                    {
                        "name": "D. Estevez"
                    },
                    {
                        "name": "Z. Etienne"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. M. Evans"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "H. Fair"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "S. Farinon"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "N. W. Farrow"
                    },
                    {
                        "name": "E. J. Fauchon-Jones"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "D. L. Ferguson"
                    },
                    {
                        "name": "A. Fernandez-Galiana"
                    },
                    {
                        "name": "I. Ferrante"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "P. W. F. Forsyth"
                    },
                    {
                        "name": "A. Franke"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "C. Frederick"
                    },
                    {
                        "name": "J. P. Freed"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronz"
                    },
                    {
                        "name": "Y. Fujii"
                    },
                    {
                        "name": "Y. Fujikawa"
                    },
                    {
                        "name": "M. Fukunaga"
                    },
                    {
                        "name": "M. Fukushima"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "H. A. Gabbard"
                    },
                    {
                        "name": "B. U. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "J. Gais"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "D. Gao"
                    },
                    {
                        "name": "S. G. Gaonkar"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "C. Garca-Nez"
                    },
                    {
                        "name": "C. Garca-Quirs"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "S. Gaudio"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "G. -G. Ge"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "P. Gewecke"
                    },
                    {
                        "name": "S. Ghonge"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "B. Giacomazzo"
                    },
                    {
                        "name": "L. Giacoppo"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "M. Giesler"
                    },
                    {
                        "name": "P. Giri"
                    },
                    {
                        "name": "F. Gissi"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "A. E. Gleckl"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "R. Goetz"
                    },
                    {
                        "name": "N. Gohlke"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "G. Gonzlez"
                    },
                    {
                        "name": "A. Gopakumar"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "B. Grace"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "A. Grant"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "R. Green"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "E. M. Gretarsson"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "W. Griffiths"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "A. Grimaldi"
                    },
                    {
                        "name": "S. J. Grimm"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "P. Gruning"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "G. Guix"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "H. -K. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anchal Gupta"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "P. Gupta"
                    },
                    {
                        "name": "E. K. Gustafson"
                    },
                    {
                        "name": "R. Gustafson"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "S. Ha"
                    },
                    {
                        "name": "L. Haegel"
                    },
                    {
                        "name": "A. Hagiwara"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "O. Halim"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "W. -B. Han"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. Hannuksela"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "T. J. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "T. Harder"
                    },
                    {
                        "name": "T. Hardwick"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "D. Hartwig"
                    },
                    {
                        "name": "K. Hasegawa"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "R. K. Hasskew"
                    },
                    {
                        "name": "C. -J. Haster"
                    },
                    {
                        "name": "K. Hattori"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "F. J. Hayes"
                    },
                    {
                        "name": "J. Healy"
                    },
                    {
                        "name": "A. Heidmann"
                    },
                    {
                        "name": "A. Heidt"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "P. Hello"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "E. Hennes"
                    },
                    {
                        "name": "J. Hennig"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "A. G. Hernandez"
                    },
                    {
                        "name": "F. Hernandez Vivanco"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "P. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "A. S. Hines"
                    },
                    {
                        "name": "Y. Hiranuma"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "E. Hirose"
                    },
                    {
                        "name": "S. Hochheim"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "J. N. Hohmann"
                    },
                    {
                        "name": "D. G. Holcomb"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "I. J. Hollows"
                    },
                    {
                        "name": "Z. J. Holmes"
                    },
                    {
                        "name": "K. Holt"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "Z. Hong"
                    },
                    {
                        "name": "P. Hopkins"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "D. Hoyland"
                    },
                    {
                        "name": "A. Hreibi"
                    },
                    {
                        "name": "B-H. Hsieh"
                    },
                    {
                        "name": "Y. Hsu"
                    },
                    {
                        "name": "G-Z. Huang"
                    },
                    {
                        "name": "H-Y. Huang"
                    },
                    {
                        "name": "P. Huang"
                    },
                    {
                        "name": "Y-C. Huang"
                    },
                    {
                        "name": "Y. -J. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "M. T. Hbner"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "D. C. Y. Hui"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "S. H. Huttner"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "T. Huynh-Dinh"
                    },
                    {
                        "name": "S. Ide"
                    },
                    {
                        "name": "B. Idzkowski"
                    },
                    {
                        "name": "A. Iess"
                    },
                    {
                        "name": "B. Ikenoue"
                    },
                    {
                        "name": "S. Imam"
                    },
                    {
                        "name": "K. Inayoshi"
                    },
                    {
                        "name": "C. Ingram"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "K. Ioka"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "K. Isleif"
                    },
                    {
                        "name": "K. Ito"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "K. Izumi"
                    },
                    {
                        "name": "V. JaberianHamedan"
                    },
                    {
                        "name": "T. Jacqmin"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "A. Z. Jan"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "D. Jariwala"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "A. C. Jenkins"
                    },
                    {
                        "name": "K. Jenner"
                    },
                    {
                        "name": "C. Jeon"
                    },
                    {
                        "name": "M. Jeunon"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "H. -B. Jin"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "A. W. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "J. D. Jones"
                    },
                    {
                        "name": "P. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "R. J. G. Jonker"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "P. Jung"
                    },
                    {
                        "name": "k. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "K. Kaihotsu"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "M. Kakizaki"
                    },
                    {
                        "name": "C. V. Kalaghatgi"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "B. Kamai"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "Y. Kao"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "S. Karat"
                    },
                    {
                        "name": "C. Karathanasis"
                    },
                    {
                        "name": "S. Karki"
                    },
                    {
                        "name": "R. Kashyap"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "W. Kastaun"
                    },
                    {
                        "name": "S. Katsanevas"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "T. Kaur"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "K. Kawaguchi"
                    },
                    {
                        "name": "N. Kawai"
                    },
                    {
                        "name": "T. Kawasaki"
                    },
                    {
                        "name": "F. Kflian"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "S. Khan"
                    },
                    {
                        "name": "E. A. Khazanov"
                    },
                    {
                        "name": "N. Khetan"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "J. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "W. S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "N. Kimura"
                    },
                    {
                        "name": "M. Kinley-Hanlon"
                    },
                    {
                        "name": "R. Kirchhoff"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "N. Kita"
                    },
                    {
                        "name": "H. Kitazawa"
                    },
                    {
                        "name": "L. Kleybolte"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "T. D. Knowles"
                    },
                    {
                        "name": "E. Knyazev"
                    },
                    {
                        "name": "P. Koch"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "Y. Kojima"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "M. Kolstein"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "V. Kondrashov"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "N. Koper"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "K. Kotake"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "C. Kozakai"
                    },
                    {
                        "name": "R. Kozu"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "A. Krlak"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "F. Kuei"
                    },
                    {
                        "name": "P. Kuijer"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "P. Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "C. Kuo"
                    },
                    {
                        "name": "H-S. Kuo"
                    },
                    {
                        "name": "Y. Kuromiya"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "K. Kusayanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "P. Lagabbe"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "T. L. Lam"
                    },
                    {
                        "name": "A. Lamberts"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "P. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "S. Leavey"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. K. Lee"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "A. Lematre"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "C. Levesque"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "J. N. Leviton"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "B. Li"
                    },
                    {
                        "name": "J. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "C-Y. Lin"
                    },
                    {
                        "name": "F-K. Lin"
                    },
                    {
                        "name": "F-L. Lin"
                    },
                    {
                        "name": "H. L. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "F. Linde"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "J. N. Linley"
                    },
                    {
                        "name": "T. B. Littenberg"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "J. Liu"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "X. Liu"
                    },
                    {
                        "name": "F. Llamas"
                    },
                    {
                        "name": "M. Llorens-Monteagudo"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "A. Lockwood"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "M. Lorenzini"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "T. P. Lott"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "G. Lovelace"
                    },
                    {
                        "name": "J. F. Lucaccioni"
                    },
                    {
                        "name": "H. Lck"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "L. -W. Luo"
                    },
                    {
                        "name": "J. E. Lynam"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "I. Magaa Hernandez"
                    },
                    {
                        "name": "C. Magazz"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "S. Mahesh"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. Makarem"
                    },
                    {
                        "name": "I. Maksimovic"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "J. L. Mango"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "F. Marchesoni"
                    },
                    {
                        "name": "M. Marchio"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "Z. Mark"
                    },
                    {
                        "name": "S. Mrka"
                    },
                    {
                        "name": "Z. Mrka"
                    },
                    {
                        "name": "C. Markakis"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "A. Marquina"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. A. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "K. Martinovic"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "H. Masalehdan"
                    },
                    {
                        "name": "K. Mason"
                    },
                    {
                        "name": "E. Massera"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "T. J. Massinger"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "A. Matas"
                    },
                    {
                        "name": "M. Mateu-Lucena"
                    },
                    {
                        "name": "F. Matichard"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "J. J. McCann"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "P. K. McClincy"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "S. C. McGuire"
                    },
                    {
                        "name": "C. McIsaac"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "S. T. McWilliams"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "M. Mehmet"
                    },
                    {
                        "name": "A. K. Mehta"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "D. A. Melchor"
                    },
                    {
                        "name": "G. Mendell"
                    },
                    {
                        "name": "A. Menendez-Vazquez"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "S. Meshkov"
                    },
                    {
                        "name": "C. Messenger"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "P. M. Meyers"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "I. Michaloliakos"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "L. Milano"
                    },
                    {
                        "name": "A. L. Miller"
                    },
                    {
                        "name": "A. Miller"
                    },
                    {
                        "name": "B. Miller"
                    },
                    {
                        "name": "S. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "J. C. Mills"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "O. Minazzoli"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "N. Mio"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "M. Miravet-Tens"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "T. Mistry"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "G. Mitselmakher"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "A. Miyamoto"
                    },
                    {
                        "name": "Y. Miyazaki"
                    },
                    {
                        "name": "K. Miyo"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "Geoffrey Mo"
                    },
                    {
                        "name": "E. Moguel"
                    },
                    {
                        "name": "K. Mogushi"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "I. Molina"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "F. Morawski"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "C. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "Y. Mori"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "S. Mozzon"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "Arunava Mukherjee"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "J. Munch"
                    },
                    {
                        "name": "E. A. Muiz"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "R. Musenich"
                    },
                    {
                        "name": "S. Muusse"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "K. Nagano"
                    },
                    {
                        "name": "S. Nagano"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "R. Nakashima"
                    },
                    {
                        "name": "Y. Nakayama"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "B. Nayak"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "R. Negishi"
                    },
                    {
                        "name": "B. F. Neil"
                    },
                    {
                        "name": "J. Neilson"
                    },
                    {
                        "name": "G. Nelemans"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "P. Neubauer"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "K. Y. Ng"
                    },
                    {
                        "name": "S. W. S. Ng"
                    },
                    {
                        "name": "C. Nguyen"
                    },
                    {
                        "name": "P. Nguyen"
                    },
                    {
                        "name": "T. Nguyen"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "W. -T. Ni"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "E. Nitoglia"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "S. Nozaki"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "B. D. O'Brien"
                    },
                    {
                        "name": "Y. Obuchi"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "E. Oelker"
                    },
                    {
                        "name": "W. Ogaki"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "J. J. Oh"
                    },
                    {
                        "name": "K. Oh"
                    },
                    {
                        "name": "S. H. Oh"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "N. Ohishi"
                    },
                    {
                        "name": "M. Ohkawa"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "H. Ohta"
                    },
                    {
                        "name": "M. A. Okada"
                    },
                    {
                        "name": "Y. Okutani"
                    },
                    {
                        "name": "K. Okutomi"
                    },
                    {
                        "name": "C. Olivetto"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "C. Ooi"
                    },
                    {
                        "name": "R. Oram"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "R. G. Ormiston"
                    },
                    {
                        "name": "N. D. Ormsby"
                    },
                    {
                        "name": "L. F. Ortega"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "E. O'Shea"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "S. Ossokine"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "S. Otabe"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "G. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "G. Pagliaroli"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "S. A. Pai"
                    },
                    {
                        "name": "J. R. Palamos"
                    },
                    {
                        "name": "O. Palashov"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "K. Pan"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "H. Pang"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "C. Pankow"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paoli"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "H. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "M. Patel"
                    },
                    {
                        "name": "M. Pathak"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "A. S. Patron"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "M. Pegoraro"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Pea Arellano"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "A. Pereira"
                    },
                    {
                        "name": "T. Pereira"
                    },
                    {
                        "name": "C. J. Perez"
                    },
                    {
                        "name": "C. Prigois"
                    },
                    {
                        "name": "C. C. Perkins"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "S. Perris"
                    },
                    {
                        "name": "J. Petermann"
                    },
                    {
                        "name": "D. Petterson"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "G. Pillant"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "K. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Pluchar"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "D. Y. T. Pong"
                    },
                    {
                        "name": "S. Ponrathnam"
                    },
                    {
                        "name": "P. Popolizio"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "M. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "L. Prokhorov"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "L. Prudenzi"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "M. Punturo"
                    },
                    {
                        "name": "F. Puosi"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. Prrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "R. Quitzow-James"
                    },
                    {
                        "name": "F. J. Raab"
                    },
                    {
                        "name": "G. Raaijmakers"
                    },
                    {
                        "name": "H. Radkins"
                    },
                    {
                        "name": "N. Radulesco"
                    },
                    {
                        "name": "P. Raffai"
                    },
                    {
                        "name": "S. X. Rail"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "T. D. Ramirez"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "J. Rana"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "U. D. Rapol"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "N. Raza"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "L. A. Rees"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "L. Rei"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "S. W. Reid"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "P. Relton"
                    },
                    {
                        "name": "A. Renzini"
                    },
                    {
                        "name": "P. Rettegno"
                    },
                    {
                        "name": "M. Rezac"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "D. Richards"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "L. Richardson"
                    },
                    {
                        "name": "G. Riemenschneider"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "K. Rink"
                    },
                    {
                        "name": "M. Rizzo"
                    },
                    {
                        "name": "N. A. Robertson"
                    },
                    {
                        "name": "R. Robie"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "S. Rodriguez"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "M. Romanelli"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "C. L. Romel"
                    },
                    {
                        "name": "A. Romero-Rodrguez"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosiska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. J. Rowlinson"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "Santosh Roy"
                    },
                    {
                        "name": "Soumen Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "K. Ryan"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "J. Sadiq"
                    },
                    {
                        "name": "N. Sago"
                    },
                    {
                        "name": "S. Saito"
                    },
                    {
                        "name": "Y. Saito"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "Y. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "Y. Sakuno"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "J. H. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "A. Sanuy"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "H. Satari"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "S. Sato"
                    },
                    {
                        "name": "T. Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "D. Sawant"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "J. Scheuer"
                    },
                    {
                        "name": "M. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "A. Schnbeck"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "T. Sekiguchi"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "D. Sentenac"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "A. Sergeev"
                    },
                    {
                        "name": "Y. Setyawati"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "M. S. Shahriar"
                    },
                    {
                        "name": "B. Shams"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. Sharma"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "S. Shibagaki"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "R. Shimizu"
                    },
                    {
                        "name": "T. Shimoda"
                    },
                    {
                        "name": "K. Shimode"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "T. Shishido"
                    },
                    {
                        "name": "A. Shoda"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "M. Sieniawska"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "A. Singha"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "J. Soldateschi"
                    },
                    {
                        "name": "S. N. Somala"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "E. J. Son"
                    },
                    {
                        "name": "K. Soni"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "N. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "R. Soulard"
                    },
                    {
                        "name": "T. Souradeep"
                    },
                    {
                        "name": "E. Sowell"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "M. Spera"
                    },
                    {
                        "name": "R. Srinivasan"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "V. Srivastava"
                    },
                    {
                        "name": "K. Staats"
                    },
                    {
                        "name": "C. Stachie"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "D. J. Stops"
                    },
                    {
                        "name": "M. Stover"
                    },
                    {
                        "name": "K. A. Strain"
                    },
                    {
                        "name": "L. C. Strang"
                    },
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "R. Sturani"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "V. Sudhir"
                    },
                    {
                        "name": "R. Sugimoto"
                    },
                    {
                        "name": "H. G. Suh"
                    },
                    {
                        "name": "T. Z. Summerscales"
                    },
                    {
                        "name": "H. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "A. Sur"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "Takamasa Suzuki"
                    },
                    {
                        "name": "Toshikazu Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "M. J. Szczepaczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "S. C. Tait"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "S. Takano"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "M. Takeda"
                    },
                    {
                        "name": "C. J. Talbot"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "H. Tanaka"
                    },
                    {
                        "name": "Kazuyuki Tanaka"
                    },
                    {
                        "name": "Kenta Tanaka"
                    },
                    {
                        "name": "Taiki Tanaka"
                    },
                    {
                        "name": "Takahiro Tanaka"
                    },
                    {
                        "name": "A. J. Tanasijczuk"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "D. Tao"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "E. N. Tapia San Martn"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "S. Telada"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "J. E. Terhune"
                    },
                    {
                        "name": "L. Terkowski"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "Shubhanshu Tiwari"
                    },
                    {
                        "name": "Srishti Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "Y. Tomigami"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "M. Tonelli"
                    },
                    {
                        "name": "A. Torres-Forn"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "D. Tyr"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "L. Troiano"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "D. S. Tsai"
                    },
                    {
                        "name": "D. Tsai"
                    },
                    {
                        "name": "K. W. Tsang"
                    },
                    {
                        "name": "T. Tsang"
                    },
                    {
                        "name": "J-S. Tsao"
                    },
                    {
                        "name": "M. Tse"
                    },
                    {
                        "name": "R. Tso"
                    },
                    {
                        "name": "K. Tsubono"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "D. Tsuna"
                    },
                    {
                        "name": "T. Tsutsui"
                    },
                    {
                        "name": "T. Tsuzuki"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "D. Tuyenbayev"
                    },
                    {
                        "name": "A. S. Ubhi"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "A. Ueda"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "K. Ueno"
                    },
                    {
                        "name": "G. Ueshima"
                    },
                    {
                        "name": "C. S. Unnikrishnan"
                    },
                    {
                        "name": "F. Uraguchi"
                    },
                    {
                        "name": "A. L. Urban"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "A. Utina"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "G. Valdes"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "N. van Bakel"
                    },
                    {
                        "name": "M. van Beuzekom"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "D. C. Vander-Hyde"
                    },
                    {
                        "name": "L. van der Schaaf"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "M. H. P. M. van Putten"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "M. Vasth"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "J. Venneberg"
                    },
                    {
                        "name": "G. Venugopalan"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "P. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "D. Veske"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Vicer"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "A. Virtuoso"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "T. Vo"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "C. Vorvick"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "R. C. Walet"
                    },
                    {
                        "name": "M. Walker"
                    },
                    {
                        "name": "G. S. Wallace"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "S. Walsh"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "R. L. Ward"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "J. Watchi"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "C. M. Weller"
                    },
                    {
                        "name": "F. Wellmann"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "P. Weels"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "D. D. White"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "A. R. Williamson"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "D. J. Wilson"
                    },
                    {
                        "name": "W. Winkler"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "T. Wlodarczyk"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "J. K. Wofford"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "S. Wu"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "L. Xiao"
                    },
                    {
                        "name": "W-R. Xu"
                    },
                    {
                        "name": "T. Yamada"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "Kazuhiro Yamamoto"
                    },
                    {
                        "name": "Kohei Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "K. Yamashita"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "F. W. Yang"
                    },
                    {
                        "name": "L. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Z. Yang"
                    },
                    {
                        "name": "M. J. Yap"
                    },
                    {
                        "name": "D. W. Yeeles"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "M. Ying"
                    },
                    {
                        "name": "K. Yokogawa"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "T. Yoshioka"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Haocun Yu"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "A. Zadrony"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "S. Zeidler"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "M. Zhan"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "J. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "G. Zhao"
                    },
                    {
                        "name": "Y. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "Z. Zhou"
                    },
                    {
                        "name": "X. J. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "Y. Zlochower"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_doi": "10.1103/PhysRevX.13.011048",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevX.13.011048",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2111.03634v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.03634v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "v2: minor edits, most to Table 1 and caption; v3: rerun with public\n  data; Data release: https://zenodo.org/record/5655785; v4: update Fig 14; v5:\n  updated to match published version",
                "arxiv_journal_ref": "Physical Review X 13, 011048 (2023)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18492v1",
                "updated": "2025-01-30T17:06:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    6,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:06:06Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    6,
                    6,
                    3,
                    30,
                    0
                ],
                "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuardReasoner: Towards Reasoning-based LLM Safeguards"
                },
                "summary": "As LLMs increasingly impact safety-critical applications, ensuring their\nsafety using guardrails remains a key challenge. This paper proposes\nGuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to\nreason. Concretely, we first create the GuardReasonerTrain dataset, which\nconsists of 127K samples with 460K detailed reasoning steps. Then, we introduce\nreasoning SFT to unlock the reasoning capability of guard models. In addition,\nwe present hard sample DPO to further strengthen their reasoning ability. In\nthis manner, GuardReasoner achieves better performance, explainability, and\ngeneralizability. Extensive experiments and analyses on 13 benchmarks of 3\nguardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B\nsurpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on\naverage. We release the training data, code, and models with different scales\n(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs increasingly impact safety-critical applications, ensuring their\nsafety using guardrails remains a key challenge. This paper proposes\nGuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to\nreason. Concretely, we first create the GuardReasonerTrain dataset, which\nconsists of 127K samples with 460K detailed reasoning steps. Then, we introduce\nreasoning SFT to unlock the reasoning capability of guard models. In addition,\nwe present hard sample DPO to further strengthen their reasoning ability. In\nthis manner, GuardReasoner achieves better performance, explainability, and\ngeneralizability. Extensive experiments and analyses on 13 benchmarks of 3\nguardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B\nsurpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on\naverage. We release the training data, code, and models with different scales\n(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Shengfang Zhai"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiwei Xue"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "22 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18487v1",
                "updated": "2025-01-30T17:04:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    4,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:04:11Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    4,
                    11,
                    3,
                    30,
                    0
                ],
                "title": "Track-On: Transformer-based Online Point Tracking with Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Track-On: Transformer-based Online Point Tracking with Memory"
                },
                "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across multiple frames in a video,\ndespite changes in appearance, lighting, perspective, and occlusions. We target\nonline tracking on a frame-by-frame basis, making it suitable for real-world,\nstreaming scenarios. Specifically, we introduce Track-On, a simple\ntransformer-based model designed for online long-term point tracking. Unlike\nprior methods that depend on full temporal modeling, our model processes video\nframes causally without access to future frames, leveraging two memory modules\n-- spatial memory and context memory -- to capture temporal information and\nmaintain reliable point tracking over long time horizons. At inference time, it\nemploys patch classification and refinement to identify correspondences and\ntrack points with high accuracy. Through extensive experiments, we demonstrate\nthat Track-On sets a new state-of-the-art for online models and delivers\nsuperior or competitive results compared to offline approaches on seven\ndatasets, including the TAP-Vid benchmark. Our method offers a robust and\nscalable solution for real-time tracking in diverse applications. Project page:\nhttps://kuis-ai.github.io/track_on",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across multiple frames in a video,\ndespite changes in appearance, lighting, perspective, and occlusions. We target\nonline tracking on a frame-by-frame basis, making it suitable for real-world,\nstreaming scenarios. Specifically, we introduce Track-On, a simple\ntransformer-based model designed for online long-term point tracking. Unlike\nprior methods that depend on full temporal modeling, our model processes video\nframes causally without access to future frames, leveraging two memory modules\n-- spatial memory and context memory -- to capture temporal information and\nmaintain reliable point tracking over long time horizons. At inference time, it\nemploys patch classification and refinement to identify correspondences and\ntrack points with high accuracy. Through extensive experiments, we demonstrate\nthat Track-On sets a new state-of-the-art for online models and delivers\nsuperior or competitive results compared to offline approaches on seven\ndatasets, including the TAP-Vid benchmark. Our method offers a robust and\nscalable solution for real-time tracking in diverse applications. Project page:\nhttps://kuis-ai.github.io/track_on"
                },
                "authors": [
                    {
                        "name": "Grkay Aydemir"
                    },
                    {
                        "name": "Xiongyi Cai"
                    },
                    {
                        "name": "Weidi Xie"
                    },
                    {
                        "name": "Fatma Gney"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Gney"
                },
                "author": "Fatma Gney",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18482v1",
                "updated": "2025-01-30T16:56:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    56,
                    8,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:56:08Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    56,
                    8,
                    3,
                    30,
                    0
                ],
                "title": "A Tool for In-depth Analysis of Code Execution Reasoning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tool for In-depth Analysis of Code Execution Reasoning of Large\n  Language Models"
                },
                "summary": "Code Executing Reasoning is becoming a new non-functional metric that\nassesses the ability of large language models (LLMs) in programming tasks.\nState-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval)\nusually focus on LLM's prediction of a given code's input/output or\nintermediate variable states/values on limited programs. However, there is no\ntool for more in-depth analysis of the results. Without such a tool, the\nobservations about LLM's code execution reasoning cannot be generalized to more\ndatasets, preventing the research community and practitioners from devising the\nnext generation of LLMs with better code execution reasoning abilities. This\npaper introduces ExeRScope, a series of tools and heuristics to analyze the\nresult of code execution reasoning frameworks to understand better the impact\nof code properties in the studied benchmarks on the code execution reasoning.\nWith such tooling, analysis can be generalized to code with similar properties\nwithout the urgent need to design more benchmarks, which is a cumbersome\neffort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Executing Reasoning is becoming a new non-functional metric that\nassesses the ability of large language models (LLMs) in programming tasks.\nState-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval)\nusually focus on LLM's prediction of a given code's input/output or\nintermediate variable states/values on limited programs. However, there is no\ntool for more in-depth analysis of the results. Without such a tool, the\nobservations about LLM's code execution reasoning cannot be generalized to more\ndatasets, preventing the research community and practitioners from devising the\nnext generation of LLMs with better code execution reasoning abilities. This\npaper introduces ExeRScope, a series of tools and heuristics to analyze the\nresult of code execution reasoning frameworks to understand better the impact\nof code properties in the studied benchmarks on the code execution reasoning.\nWith such tooling, analysis can be generalized to code with similar properties\nwithout the urgent need to design more benchmarks, which is a cumbersome\neffort."
                },
                "authors": [
                    {
                        "name": "Changshu Liu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07291v2",
                "updated": "2025-01-30T16:53:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    53,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2024-08-14T04:49:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Personal Information Extraction and Countermeasures"
                },
                "summary": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect four datasets including a synthetic dataset generated by GPT-4 and\nthree real-world datasets with manually labeled eight categories of personal\ninformation; introduce a novel mitigation strategy based on prompt injection;\nand systematically benchmark LLM-based attacks and countermeasures using ten\nLLMs and five datasets. Our key findings include: LLM can be misused by\nattackers to accurately extract various personal information from personal\nprofiles; LLM outperforms traditional methods; and prompt injection can defend\nagainst strong LLM-based attacks, reducing the attack to less effective\ntraditional ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect four datasets including a synthetic dataset generated by GPT-4 and\nthree real-world datasets with manually labeled eight categories of personal\ninformation; introduce a novel mitigation strategy based on prompt injection;\nand systematically benchmark LLM-based attacks and countermeasures using ten\nLLMs and five datasets. Our key findings include: LLM can be misused by\nattackers to accurately extract various personal information from personal\nprofiles; LLM outperforms traditional methods; and prompt injection can defend\nagainst strong LLM-based attacks, reducing the attack to less effective\ntraditional ones."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15884v2",
                "updated": "2025-01-30T16:48:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    48,
                    49,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-24T08:56:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    56,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "Interpolation Filter Design for Sample Rate Independent Audio Effect\n  RNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpolation Filter Design for Sample Rate Independent Audio Effect\n  RNNs"
                },
                "summary": "Recurrent neural networks (RNNs) are effective at emulating the non-linear,\nstateful behavior of analog guitar amplifiers and distortion effects. Unlike\nthe case of direct circuit simulation, RNNs have a fixed sample rate encoded in\ntheir model weights, making the sample rate non-adjustable during inference.\nRecent work has proposed increasing the sample rate of RNNs at inference\n(oversampling) by increasing the feedback delay length in samples, using a\nfractional delay filter for non-integer conversions. Here, we investigate the\ntask of lowering the sample rate at inference (undersampling), and propose\nusing an extrapolation filter to approximate the required fractional signal\nadvance. We consider two filter design methods and analyse the impact of filter\norder on audio quality. Our results show that the correct choice of filter can\ngive high quality results for both oversampling and undersampling; however, in\nsome cases the sample rate adjustment leads to unwanted artefacts in the output\nsignal. We analyse these failure cases through linearised stability analysis,\nshowing that they result from instability around a fixed point. This approach\nenables an informed prediction of suitable interpolation filters for a given\nRNN model before runtime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent neural networks (RNNs) are effective at emulating the non-linear,\nstateful behavior of analog guitar amplifiers and distortion effects. Unlike\nthe case of direct circuit simulation, RNNs have a fixed sample rate encoded in\ntheir model weights, making the sample rate non-adjustable during inference.\nRecent work has proposed increasing the sample rate of RNNs at inference\n(oversampling) by increasing the feedback delay length in samples, using a\nfractional delay filter for non-integer conversions. Here, we investigate the\ntask of lowering the sample rate at inference (undersampling), and propose\nusing an extrapolation filter to approximate the required fractional signal\nadvance. We consider two filter design methods and analyse the impact of filter\norder on audio quality. Our results show that the correct choice of filter can\ngive high quality results for both oversampling and undersampling; however, in\nsome cases the sample rate adjustment leads to unwanted artefacts in the output\nsignal. We analyse these failure cases through linearised stability analysis,\nshowing that they result from instability around a fixed point. This approach\nenables an informed prediction of suitable interpolation filters for a given\nRNN model before runtime."
                },
                "authors": [
                    {
                        "name": "Alistair Carson"
                    },
                    {
                        "name": "Alec Wright"
                    },
                    {
                        "name": "Stefan Bilbao"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bilbao"
                },
                "author": "Stefan Bilbao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18475v1",
                "updated": "2025-01-30T16:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    48,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:48:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    48,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA\n  Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA\n  Initialization"
                },
                "summary": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has\nbecome a highly efficient approach for downstream tasks, particularly in\nscenarios with limited computational resources. However, applying LoRA\ntechniques to quantized LLMs poses unique challenges due to the reduced\nrepresentational precision of quantized weights. In this paper, we introduce\nCLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic\ninitialization strategy designed to overcome these challenges. Our approach\nfocuses on minimizing the layer-wise discrepancy between the original LLM and\nits quantized counterpart with LoRA components during initialization. By\nleveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and\ndetermines the optimal LoRA components for each layer, ensuring a strong\nfoundation for subsequent fine-tuning. A key contribution of this work is a\nnovel theoretical result that enables the accurate and closed-form construction\nof these optimal LoRA components. We validate the efficacy of CLoQ across\nmultiple tasks such as language generation, arithmetic reasoning, and\ncommonsense reasoning, demonstrating that it consistently outperforms existing\nLoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit\nwidths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has\nbecome a highly efficient approach for downstream tasks, particularly in\nscenarios with limited computational resources. However, applying LoRA\ntechniques to quantized LLMs poses unique challenges due to the reduced\nrepresentational precision of quantized weights. In this paper, we introduce\nCLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic\ninitialization strategy designed to overcome these challenges. Our approach\nfocuses on minimizing the layer-wise discrepancy between the original LLM and\nits quantized counterpart with LoRA components during initialization. By\nleveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and\ndetermines the optimal LoRA components for each layer, ensuring a strong\nfoundation for subsequent fine-tuning. A key contribution of this work is a\nnovel theoretical result that enables the accurate and closed-form construction\nof these optimal LoRA components. We validate the efficacy of CLoQ across\nmultiple tasks such as language generation, arithmetic reasoning, and\ncommonsense reasoning, demonstrating that it consistently outperforms existing\nLoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit\nwidths."
                },
                "authors": [
                    {
                        "name": "Yanxia Deng"
                    },
                    {
                        "name": "Aozhong Zhang"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Selcuk Gurses"
                    },
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Penghang Yin"
                    }
                ],
                "author_detail": {
                    "name": "Penghang Yin"
                },
                "author": "Penghang Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18470v1",
                "updated": "2025-01-30T16:44:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    44,
                    49,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:44:49Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    44,
                    49,
                    3,
                    30,
                    0
                ],
                "title": "Resampling Filter Design for Multirate Neural Audio Effect Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resampling Filter Design for Multirate Neural Audio Effect Processing"
                },
                "summary": "Neural networks have become ubiquitous in audio effects modelling, especially\nfor guitar amplifiers and distortion pedals. One limitation of such models is\nthat the sample rate of the training data is implicitly encoded in the model\nweights and therefore not readily adjustable at inference. Recent work explored\nmodifications to recurrent neural network architecture to approximate a sample\nrate independent system, enabling audio processing at a rate that differs from\nthe original training rate. This method works well for integer oversampling and\ncan reduce aliasing caused by nonlinear activation functions. For small\nfractional changes in sample rate, fractional delay filters can be used to\napproximate sample rate independence, but in some cases this method fails\nentirely. Here, we explore the use of signal resampling at the input and output\nof the neural network as an alternative solution. We investigate several\nresampling filter designs and show that a two-stage design consisting of a\nhalf-band IIR filter cascaded with a Kaiser window FIR filter can give similar\nor better results to the previously proposed model adjustment method with many\nfewer operations per sample and less than one millisecond of latency at typical\naudio rates. Furthermore, we investigate interpolation and decimation filters\nfor the task of integer oversampling and show that cascaded half-band IIR and\nFIR designs can be used in conjunction with the model adjustment method to\nreduce aliasing in a range of distortion effect models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have become ubiquitous in audio effects modelling, especially\nfor guitar amplifiers and distortion pedals. One limitation of such models is\nthat the sample rate of the training data is implicitly encoded in the model\nweights and therefore not readily adjustable at inference. Recent work explored\nmodifications to recurrent neural network architecture to approximate a sample\nrate independent system, enabling audio processing at a rate that differs from\nthe original training rate. This method works well for integer oversampling and\ncan reduce aliasing caused by nonlinear activation functions. For small\nfractional changes in sample rate, fractional delay filters can be used to\napproximate sample rate independence, but in some cases this method fails\nentirely. Here, we explore the use of signal resampling at the input and output\nof the neural network as an alternative solution. We investigate several\nresampling filter designs and show that a two-stage design consisting of a\nhalf-band IIR filter cascaded with a Kaiser window FIR filter can give similar\nor better results to the previously proposed model adjustment method with many\nfewer operations per sample and less than one millisecond of latency at typical\naudio rates. Furthermore, we investigate interpolation and decimation filters\nfor the task of integer oversampling and show that cascaded half-band IIR and\nFIR designs can be used in conjunction with the model adjustment method to\nreduce aliasing in a range of distortion effect models."
                },
                "authors": [
                    {
                        "name": "Alistair Carson"
                    },
                    {
                        "name": "Vesa Vlimki"
                    },
                    {
                        "name": "Alec Wright"
                    },
                    {
                        "name": "Stefan Bilbao"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bilbao"
                },
                "author": "Stefan Bilbao",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16273v2",
                "updated": "2025-01-30T16:44:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    44,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T18:06:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs"
                },
                "summary": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount."
                },
                "authors": [
                    {
                        "name": "Mohamed Elfeki"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Chad Voegele"
                    }
                ],
                "author_detail": {
                    "name": "Chad Voegele"
                },
                "author": "Chad Voegele",
                "arxiv_comment": "13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13514v2",
                "updated": "2025-01-30T16:43:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    43,
                    39,
                    3,
                    30,
                    0
                ],
                "published": "2024-08-24T08:34:51Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    8,
                    34,
                    51,
                    5,
                    237,
                    0
                ],
                "title": "Cross Sectional Regression with Cluster Dependence: Inference based on\n  Averaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross Sectional Regression with Cluster Dependence: Inference based on\n  Averaging"
                },
                "summary": "We re-investigate the asymptotic properties of the traditional OLS (pooled)\nestimator, $\\hat{\\beta} _P$, in the context of cluster dependence. The present\nstudy considers various scenarios under various restrictions on the cluster\nsizes and number of clusters. It is shown that $\\hat{\\beta}_P$ could be\ninconsistent in many realistic situations. We propose a simple estimator,\n$\\hat{\\beta}_A$ based on data averaging. The asymptotic properties of\n$\\hat{\\beta}_A$ are studied. It is shown that $\\hat{\\beta}_A$ is consistent\neven when $\\hat{\\beta}_P$ is inconsistent. It is further shown that the\nproposed estimator $\\hat{\\beta}_A$ is more efficient than $\\hat{\\beta}_P$ in\nmany practical scenarios. As a consequence of averaging, we show that\n$\\hat{\\beta}_A$ retains consistency, asymptotic normality under classical\nmeasurement error problem circumventing the use of Instrumental Variables (IV).\nA detailed simulation study shows the efficacy of $\\hat{\\beta}_A$. It is also\nseen that $\\hat{\\beta}_A$ yields better goodness of fit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We re-investigate the asymptotic properties of the traditional OLS (pooled)\nestimator, $\\hat{\\beta} _P$, in the context of cluster dependence. The present\nstudy considers various scenarios under various restrictions on the cluster\nsizes and number of clusters. It is shown that $\\hat{\\beta}_P$ could be\ninconsistent in many realistic situations. We propose a simple estimator,\n$\\hat{\\beta}_A$ based on data averaging. The asymptotic properties of\n$\\hat{\\beta}_A$ are studied. It is shown that $\\hat{\\beta}_A$ is consistent\neven when $\\hat{\\beta}_P$ is inconsistent. It is further shown that the\nproposed estimator $\\hat{\\beta}_A$ is more efficient than $\\hat{\\beta}_P$ in\nmany practical scenarios. As a consequence of averaging, we show that\n$\\hat{\\beta}_A$ retains consistency, asymptotic normality under classical\nmeasurement error problem circumventing the use of Instrumental Variables (IV).\nA detailed simulation study shows the efficacy of $\\hat{\\beta}_A$. It is also\nseen that $\\hat{\\beta}_A$ yields better goodness of fit."
                },
                "authors": [
                    {
                        "name": "Subhodeep Dey"
                    },
                    {
                        "name": "Gopal K. Basak"
                    },
                    {
                        "name": "Samarjit Das"
                    }
                ],
                "author_detail": {
                    "name": "Samarjit Das"
                },
                "author": "Samarjit Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18469v1",
                "updated": "2025-01-30T16:42:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    42,
                    21,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:42:21Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    42,
                    21,
                    3,
                    30,
                    0
                ],
                "title": "Dissipation bounds the coherence of stochastic limit cycles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissipation bounds the coherence of stochastic limit cycles"
                },
                "summary": "Overdamped stochastic systems maintained far from equilibrium can display\nsustained oscillations with fluctuations that decrease with the system size.\nThe correlation time of such noisy limit cycles expressed in units of the cycle\nperiod is upper-bounded by the entropy produced per oscillation. We prove this\nconstraint for first-order nonlinear systems in arbitrary dimensions perturbed\nby weak, uncorrelated Gaussian noise. We then extend the result to important\nexamples of more general stochastic dynamics, including electronic and chemical\nclocks, illustrating the practical relevance of the dissipation-coherence bound\nfor electronic computing and thermodynamic inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overdamped stochastic systems maintained far from equilibrium can display\nsustained oscillations with fluctuations that decrease with the system size.\nThe correlation time of such noisy limit cycles expressed in units of the cycle\nperiod is upper-bounded by the entropy produced per oscillation. We prove this\nconstraint for first-order nonlinear systems in arbitrary dimensions perturbed\nby weak, uncorrelated Gaussian noise. We then extend the result to important\nexamples of more general stochastic dynamics, including electronic and chemical\nclocks, illustrating the practical relevance of the dissipation-coherence bound\nfor electronic computing and thermodynamic inference."
                },
                "authors": [
                    {
                        "name": "Davide Santolin"
                    },
                    {
                        "name": "Gianmaria Falasco"
                    }
                ],
                "author_detail": {
                    "name": "Gianmaria Falasco"
                },
                "author": "Gianmaria Falasco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16673v2",
                "updated": "2025-01-30T16:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    40,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T03:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "title": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow"
                },
                "summary": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research."
                },
                "authors": [
                    {
                        "name": "Li Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "arxiv_affiliation": "Atlas",
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18467v1",
                "updated": "2025-01-30T16:36:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    36,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:36:14Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    36,
                    14,
                    3,
                    30,
                    0
                ],
                "title": "IV Estimation of Heterogeneous Spatial Dynamic Panel Models with\n  Interactive Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IV Estimation of Heterogeneous Spatial Dynamic Panel Models with\n  Interactive Effects"
                },
                "summary": "This paper develops a Mean Group Instrumental Variables (MGIV) estimator for\nspatial dynamic panel data models with interactive effects, under large N and T\nasymptotics. Unlike existing approaches that typically impose slope-parameter\nhomogeneity, MGIV accommodates cross-sectional heterogeneity in slope\ncoefficients. The proposed estimator is linear, making it computationally\nefficient and robust. Furthermore, it avoids the incidental parameters problem,\nenabling asymptotically valid inferences without requiring bias correction. The\nMonte Carlo experiments indicate strong finite-sample performance of the MGIV\nestimator across various sample sizes and parameter configurations. The\npractical utility of the estimator is illustrated through an application to\nregional economic growth in Europe. By explicitly incorporating heterogeneity,\nour approach provides fresh insights into the determinants of regional growth,\nunderscoring the critical roles of spatial and temporal dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a Mean Group Instrumental Variables (MGIV) estimator for\nspatial dynamic panel data models with interactive effects, under large N and T\nasymptotics. Unlike existing approaches that typically impose slope-parameter\nhomogeneity, MGIV accommodates cross-sectional heterogeneity in slope\ncoefficients. The proposed estimator is linear, making it computationally\nefficient and robust. Furthermore, it avoids the incidental parameters problem,\nenabling asymptotically valid inferences without requiring bias correction. The\nMonte Carlo experiments indicate strong finite-sample performance of the MGIV\nestimator across various sample sizes and parameter configurations. The\npractical utility of the estimator is illustrated through an application to\nregional economic growth in Europe. By explicitly incorporating heterogeneity,\nour approach provides fresh insights into the determinants of regional growth,\nunderscoring the critical roles of spatial and temporal dependencies."
                },
                "authors": [
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Guowei Cui"
                    },
                    {
                        "name": "Vasilis Sarafidis"
                    },
                    {
                        "name": "Takashi Yamagata"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Yamagata"
                },
                "author": "Takashi Yamagata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03249v3",
                "updated": "2025-01-30T16:31:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    31,
                    31,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-04T09:14:11Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    14,
                    11,
                    4,
                    278,
                    0
                ],
                "title": "How Much Can We Forget about Data Contamination?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Can We Forget about Data Contamination?"
                },
                "summary": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Suraj Srinivas"
                    },
                    {
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "name": "Ulrike von Luxburg"
                    }
                ],
                "author_detail": {
                    "name": "Ulrike von Luxburg"
                },
                "author": "Ulrike von Luxburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18463v1",
                "updated": "2025-01-30T16:30:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    30,
                    20,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    30,
                    20,
                    3,
                    30,
                    0
                ],
                "title": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models"
                },
                "summary": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Banchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Banchmarks."
                },
                "authors": [
                    {
                        "name": "Shiho Noda"
                    },
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18460v1",
                "updated": "2025-01-30T16:18:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    18,
                    52,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:18:52Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    18,
                    52,
                    3,
                    30,
                    0
                ],
                "title": "ExeCoder: Empowering Large Language Models with Executability\n  Representation for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExeCoder: Empowering Large Language Models with Executability\n  Representation for Code Translation"
                },
                "summary": "Code translation is a crucial activity in the software development and\nmaintenance process, and researchers have recently begun to focus on using\npre-trained large language models (LLMs) for code translation. However,\nexisting LLMs only learn the contextual semantics of code during pre-training,\nneglecting executability information closely related to the execution state of\nthe code, which results in unguaranteed code executability and unreliable\nautomated code translation. To address this issue, we propose ExeCoder, an LLM\nspecifically designed for code translation, aimed at utilizing executability\nrepresentations such as functional semantics, syntax structures, and variable\ndependencies to enhance the capabilities of LLMs in code translation. To\nevaluate the effectiveness of ExeCoder, we manually enhanced the widely used\nbenchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X\nthat serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder\nachieves state-of-the-art performance in code translation, surpassing existing\nopen-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two\nmetrics, and even outperforms the renowned closed-source LLM GPT-4o. Website:\nhttps://execoder4trans.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation is a crucial activity in the software development and\nmaintenance process, and researchers have recently begun to focus on using\npre-trained large language models (LLMs) for code translation. However,\nexisting LLMs only learn the contextual semantics of code during pre-training,\nneglecting executability information closely related to the execution state of\nthe code, which results in unguaranteed code executability and unreliable\nautomated code translation. To address this issue, we propose ExeCoder, an LLM\nspecifically designed for code translation, aimed at utilizing executability\nrepresentations such as functional semantics, syntax structures, and variable\ndependencies to enhance the capabilities of LLMs in code translation. To\nevaluate the effectiveness of ExeCoder, we manually enhanced the widely used\nbenchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X\nthat serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder\nachieves state-of-the-art performance in code translation, surpassing existing\nopen-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two\nmetrics, and even outperforms the renowned closed-source LLM GPT-4o. Website:\nhttps://execoder4trans.github.io/"
                },
                "authors": [
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Wenjie Yin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16727v2",
                "updated": "2025-01-30T16:17:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    17,
                    56,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T06:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking"
                },
                "summary": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Shuaimin Li"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18457v1",
                "updated": "2025-01-30T16:15:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    15,
                    38,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:15:38Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    15,
                    38,
                    3,
                    30,
                    0
                ],
                "title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering"
                },
                "summary": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages, and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and retrieval\naugmented settings. We also found that increasing the number of languages\ninvolved in CALM training leads to even higher accuracy and consistency. We\noffer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability. The source code\nand data of this paper are available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages, and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and retrieval\naugmented settings. We also found that increasing the number of languages\ninvolved in CALM training leads to even higher accuracy and consistency. We\noffer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability. The source code\nand data of this paper are available on GitHub."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.15881v2",
                "updated": "2025-01-30T15:55:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    55,
                    29,
                    3,
                    30,
                    0
                ],
                "published": "2023-05-25T09:23:33Z",
                "published_parsed": [
                    2023,
                    5,
                    25,
                    9,
                    23,
                    33,
                    3,
                    145,
                    0
                ],
                "title": "Generative Adversarial Reduced Order Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Adversarial Reduced Order Modelling"
                },
                "summary": "In this work, we present GAROM, a new approach for reduced order modelling\n(ROM) based on generative adversarial networks (GANs). GANs have the potential\nto learn data distribution and generate more realistic data. While widely\napplied in many areas of deep learning, little research is done on their\napplication for ROM, i.e. approximating a high-fidelity model with a simpler\none. In this work, we combine the GAN and ROM framework, by introducing a\ndata-driven generative adversarial model able to learn solutions to parametric\ndifferential equations. The latter is achieved by modelling the discriminator\nnetwork as an autoencoder, extracting relevant features of the input, and\napplying a conditioning mechanism to the generator and discriminator networks\nspecifying the differential equation parameters. We show how to apply our\nmethodology for inference, provide experimental evidence of the model\ngeneralisation, and perform a convergence study of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present GAROM, a new approach for reduced order modelling\n(ROM) based on generative adversarial networks (GANs). GANs have the potential\nto learn data distribution and generate more realistic data. While widely\napplied in many areas of deep learning, little research is done on their\napplication for ROM, i.e. approximating a high-fidelity model with a simpler\none. In this work, we combine the GAN and ROM framework, by introducing a\ndata-driven generative adversarial model able to learn solutions to parametric\ndifferential equations. The latter is achieved by modelling the discriminator\nnetwork as an autoencoder, extracting relevant features of the input, and\napplying a conditioning mechanism to the generator and discriminator networks\nspecifying the differential equation parameters. We show how to apply our\nmethodology for inference, provide experimental evidence of the model\ngeneralisation, and perform a convergence study of the method."
                },
                "authors": [
                    {
                        "name": "Dario Coscia"
                    },
                    {
                        "name": "Nicola Demo"
                    },
                    {
                        "name": "Gianluigi Rozza"
                    }
                ],
                "author_detail": {
                    "name": "Gianluigi Rozza"
                },
                "author": "Gianluigi Rozza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10898v2",
                "updated": "2025-01-30T15:47:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    47,
                    33,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-17T05:26:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    26,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction\n  with LIME Explainable AI and a RAG-based Chatbot for Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction\n  with LIME Explainable AI and a RAG-based Chatbot for Insights"
                },
                "summary": "Ensuring safe water supplies requires effective water quality monitoring,\nespecially in developing countries like Nepal, where contamination risks are\nhigh. This paper introduces a hybrid deep learning model to predict Nepal's\nseasonal water quality using a small dataset with multiple water quality\nparameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along\nwith a neural network combining CNN and RNN layers, are used to capture\ntemporal and spatial patterns in the data. The model demonstrated notable\naccuracy improvements, aiding proactive water quality control. CatBoost,\nXGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values\nwith an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers\nachieved 99 percent accuracy, cross-validated across models. LIME analysis\nhighlighted the importance of indicators like EC and DO levels in XGBoost\nclassification decisions. The neural network model achieved 92 percent\nclassification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in\nregression analysis. Furthermore, a multifunctional application was developed\nto predict WQI values using both regression and classification methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safe water supplies requires effective water quality monitoring,\nespecially in developing countries like Nepal, where contamination risks are\nhigh. This paper introduces a hybrid deep learning model to predict Nepal's\nseasonal water quality using a small dataset with multiple water quality\nparameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along\nwith a neural network combining CNN and RNN layers, are used to capture\ntemporal and spatial patterns in the data. The model demonstrated notable\naccuracy improvements, aiding proactive water quality control. CatBoost,\nXGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values\nwith an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers\nachieved 99 percent accuracy, cross-validated across models. LIME analysis\nhighlighted the importance of indicators like EC and DO levels in XGBoost\nclassification decisions. The neural network model achieved 92 percent\nclassification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in\nregression analysis. Furthermore, a multifunctional application was developed\nto predict WQI values using both regression and classification methods."
                },
                "authors": [
                    {
                        "name": "Biplov Paneru"
                    },
                    {
                        "name": "Bishwash Paneru"
                    }
                ],
                "author_detail": {
                    "name": "Bishwash Paneru"
                },
                "author": "Bishwash Paneru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18438v1",
                "updated": "2025-01-30T15:45:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:45:56Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "o3-mini vs DeepSeek-R1: Which One is Safer?"
                },
                "summary": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "Jos Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2501.17749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18417v2",
                "updated": "2025-01-30T15:45:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-24T04:02:30Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    4,
                    2,
                    30,
                    3,
                    298,
                    0
                ],
                "title": "Large Language Models Reflect the Ideology of their Creators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Reflect the Ideology of their Creators"
                },
                "summary": "Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'."
                },
                "authors": [
                    {
                        "name": "Maarten Buyl"
                    },
                    {
                        "name": "Alexander Rogiers"
                    },
                    {
                        "name": "Sander Noels"
                    },
                    {
                        "name": "Guillaume Bied"
                    },
                    {
                        "name": "Iris Dominguez-Catena"
                    },
                    {
                        "name": "Edith Heiter"
                    },
                    {
                        "name": "Iman Johary"
                    },
                    {
                        "name": "Alexandru-Cristian Mara"
                    },
                    {
                        "name": "Raphal Romero"
                    },
                    {
                        "name": "Jefrey Lijffijt"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18435v1",
                "updated": "2025-01-30T15:42:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    42,
                    24,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:42:24Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    42,
                    24,
                    3,
                    30,
                    0
                ],
                "title": "GENIE: Generative Note Information Extraction model for structuring EHR\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENIE: Generative Note Information Extraction model for structuring EHR\n  data"
                },
                "summary": "Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Jinsen Lu"
                    },
                    {
                        "name": "Zitian Qu"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Zhengyun Zhao"
                    },
                    {
                        "name": "Isaac Kohane"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Sheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Yu"
                },
                "author": "Sheng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18427v1",
                "updated": "2025-01-30T15:31:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    31,
                    48,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:31:48Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    31,
                    48,
                    3,
                    30,
                    0
                ],
                "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute\n  in Linear Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute\n  in Linear Diffusion Transformer"
                },
                "summary": "This paper presents SANA-1.5, a linear Diffusion Transformer for efficient\nscaling in text-to-image generation. Building upon SANA-1.0, we introduce three\nkey innovations: (1) Efficient Training Scaling: A depth-growth paradigm that\nenables scaling from 1.6B to 4.8B parameters with significantly reduced\ncomputational resources, combined with a memory-efficient 8-bit optimizer. (2)\nModel Depth Pruning: A block importance analysis technique for efficient model\ncompression to arbitrary sizes with minimal quality loss. (3) Inference-time\nScaling: A repeated sampling strategy that trades computation for model\ncapacity, enabling smaller models to match larger model quality at inference\ntime. Through these strategies, SANA-1.5 achieves a text-image alignment score\nof 0.72 on GenEval, which can be further improved to 0.80 through inference\nscaling, establishing a new SoTA on GenEval benchmark. These innovations enable\nefficient model scaling across different compute budgets while maintaining high\nquality, making high-quality image generation more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents SANA-1.5, a linear Diffusion Transformer for efficient\nscaling in text-to-image generation. Building upon SANA-1.0, we introduce three\nkey innovations: (1) Efficient Training Scaling: A depth-growth paradigm that\nenables scaling from 1.6B to 4.8B parameters with significantly reduced\ncomputational resources, combined with a memory-efficient 8-bit optimizer. (2)\nModel Depth Pruning: A block importance analysis technique for efficient model\ncompression to arbitrary sizes with minimal quality loss. (3) Inference-time\nScaling: A repeated sampling strategy that trades computation for model\ncapacity, enabling smaller models to match larger model quality at inference\ntime. Through these strategies, SANA-1.5 achieves a text-image alignment score\nof 0.72 on GenEval, which can be further improved to 0.80 through inference\nscaling, establishing a new SoTA on GenEval benchmark. These innovations enable\nefficient model scaling across different compute budgets while maintaining high\nquality, making high-quality image generation more accessible."
                },
                "authors": [
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00965v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00965v5",
                "updated": "2025-01-30T15:30:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    30,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-02T08:12:51Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    8,
                    12,
                    51,
                    0,
                    275,
                    0
                ],
                "title": "Effective Learning with Node Perturbation in Multi-Layer Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Learning with Node Perturbation in Multi-Layer Neural Networks"
                },
                "summary": "Backpropagation (BP) remains the dominant and most successful method for\ntraining parameters of deep neural network models. However, BP relies on two\ncomputationally distinct phases, does not provide a satisfactory explanation of\nbiological learning, and can be challenging to apply for training of networks\nwith discontinuities or noisy node dynamics. By comparison, node perturbation\n(NP) proposes learning by the injection of noise into network activations, and\nsubsequent measurement of the induced loss change. NP relies on two forward\n(inference) passes, does not make use of network derivatives, and has been\nproposed as a model for learning in biological systems. However, standard NP is\nhighly data inefficient and unstable due to its unguided noise-based search\nprocess. In this work, we investigate different formulations of NP and relate\nit to the concept of directional derivatives as well as combining it with a\ndecorrelating mechanism for layer-wise inputs. We find that a closer alignment\nwith directional derivatives together with input decorrelation at every layer\nstrongly enhances performance of NP learning with large improvements in\nparameter convergence and much higher performance on the test data, approaching\nthat of BP. Furthermore, our novel formulation allows for application to noisy\nsystems in which the noise process itself is inaccessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation (BP) remains the dominant and most successful method for\ntraining parameters of deep neural network models. However, BP relies on two\ncomputationally distinct phases, does not provide a satisfactory explanation of\nbiological learning, and can be challenging to apply for training of networks\nwith discontinuities or noisy node dynamics. By comparison, node perturbation\n(NP) proposes learning by the injection of noise into network activations, and\nsubsequent measurement of the induced loss change. NP relies on two forward\n(inference) passes, does not make use of network derivatives, and has been\nproposed as a model for learning in biological systems. However, standard NP is\nhighly data inefficient and unstable due to its unguided noise-based search\nprocess. In this work, we investigate different formulations of NP and relate\nit to the concept of directional derivatives as well as combining it with a\ndecorrelating mechanism for layer-wise inputs. We find that a closer alignment\nwith directional derivatives together with input decorrelation at every layer\nstrongly enhances performance of NP learning with large improvements in\nparameter convergence and much higher performance on the test data, approaching\nthat of BP. Furthermore, our novel formulation allows for application to noisy\nsystems in which the noise process itself is inaccessible."
                },
                "authors": [
                    {
                        "name": "Sander Dalm"
                    },
                    {
                        "name": "Marcel van Gerven"
                    },
                    {
                        "name": "Nasir Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Ahmad"
                },
                "author": "Nasir Ahmad",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00965v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00965v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18423v1",
                "updated": "2025-01-30T15:25:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    25,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    25,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "DeepExtractor: Time-domain reconstruction of signals and glitches in\n  gravitational wave data with deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepExtractor: Time-domain reconstruction of signals and glitches in\n  gravitational wave data with deep learning"
                },
                "summary": "Gravitational wave (GW) interferometers, detect faint signals from distant\nastrophysical events, such as binary black hole mergers. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called \"glitches\"\nthat can mimic astrophysical signals or mask their characteristics. Fast and\naccurate reconstruction of both signals and glitches is crucial for reliable\nscientific inference. In this study, we present DeepExtractor, a deep learning\nframework designed to reconstruct signals and glitches with power exceeding\ninterferometer noise, regardless of their source. We design DeepExtractor to\nmodel the inherent noise distribution of GW interferometers, following\nconventional assumptions that the noise is Gaussian and stationary over short\ntime scales. It operates by predicting and subtracting the noise component of\nthe data, retaining only the clean reconstruction. Our approach achieves\nsuperior generalization capabilities for arbitrary signals and glitches\ncompared to methods that directly map inputs to the clean training waveforms.\nWe validate DeepExtractor's effectiveness through three experiments: (1)\nreconstructing simulated glitches injected into simulated detector noise, (2)\ncomparing performance with the state-of-the-art BayesWave algorithm, and (3)\nanalyzing real data from the Gravity Spy dataset to demonstrate effective\nglitch subtraction from LIGO strain data. DeepExtractor achieves a median\nmismatch of only 0.9% for simulated glitches, outperforming several deep\nlearning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch\nrecovery, offering a dramatic computational speedup by reconstructing one\nglitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's\nprocessing time of approx. one hour per glitch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave (GW) interferometers, detect faint signals from distant\nastrophysical events, such as binary black hole mergers. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called \"glitches\"\nthat can mimic astrophysical signals or mask their characteristics. Fast and\naccurate reconstruction of both signals and glitches is crucial for reliable\nscientific inference. In this study, we present DeepExtractor, a deep learning\nframework designed to reconstruct signals and glitches with power exceeding\ninterferometer noise, regardless of their source. We design DeepExtractor to\nmodel the inherent noise distribution of GW interferometers, following\nconventional assumptions that the noise is Gaussian and stationary over short\ntime scales. It operates by predicting and subtracting the noise component of\nthe data, retaining only the clean reconstruction. Our approach achieves\nsuperior generalization capabilities for arbitrary signals and glitches\ncompared to methods that directly map inputs to the clean training waveforms.\nWe validate DeepExtractor's effectiveness through three experiments: (1)\nreconstructing simulated glitches injected into simulated detector noise, (2)\ncomparing performance with the state-of-the-art BayesWave algorithm, and (3)\nanalyzing real data from the Gravity Spy dataset to demonstrate effective\nglitch subtraction from LIGO strain data. DeepExtractor achieves a median\nmismatch of only 0.9% for simulated glitches, outperforming several deep\nlearning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch\nrecovery, offering a dramatic computational speedup by reconstructing one\nglitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's\nprocessing time of approx. one hour per glitch."
                },
                "authors": [
                    {
                        "name": "Tom Dooney"
                    },
                    {
                        "name": "Harsh Narola"
                    },
                    {
                        "name": "Stefano Bromuri"
                    },
                    {
                        "name": "R. Lyana Curier"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    },
                    {
                        "name": "Daniel Stanley Tan"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Stanley Tan"
                },
                "author": "Daniel Stanley Tan",
                "arxiv_comment": "22 pages, 16 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v3",
                "updated": "2025-01-30T15:24:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    24,
                    28,
                    3,
                    30,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18419v1",
                "updated": "2025-01-30T15:16:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    16,
                    9,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:16:09Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    16,
                    9,
                    3,
                    30,
                    0
                ],
                "title": "Optimizers for Stabilizing Likelihood-free Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizers for Stabilizing Likelihood-free Inference"
                },
                "summary": "A growing number of applications in particle physics and beyond use neural\nnetworks as unbinned likelihood ratio estimators applied to real or simulated\ndata. Precision requirements on the inference tasks demand a high-level of\nstability from these networks, which are affected by the stochastic nature of\ntraining. We show how physics concepts can be used to stabilize network\ntraining through a physics-inspired optimizer. In particular, the Energy\nConserving Descent (ECD) optimization framework uses classical Hamiltonian\ndynamics on the space of network parameters to reduce the dependence on the\ninitial conditions while also stabilizing the result near the minimum of the\nloss function. We develop a version of this optimizer known as $ECD_{q=1}$,\nwhich has few free hyperparameters with limited ranges guided by physical\nreasoning. We apply $ECD_{q=1}$ to representative likelihood-ratio estimation\ntasks in particle physics and find that it out-performs the widely-used Adam\noptimizer. We expect that ECD will be a useful tool for wide array of\ndata-limited problems, where it is computationally expensive to exhaustively\noptimize hyperparameters and mitigate fluctuations with ensembling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing number of applications in particle physics and beyond use neural\nnetworks as unbinned likelihood ratio estimators applied to real or simulated\ndata. Precision requirements on the inference tasks demand a high-level of\nstability from these networks, which are affected by the stochastic nature of\ntraining. We show how physics concepts can be used to stabilize network\ntraining through a physics-inspired optimizer. In particular, the Energy\nConserving Descent (ECD) optimization framework uses classical Hamiltonian\ndynamics on the space of network parameters to reduce the dependence on the\ninitial conditions while also stabilizing the result near the minimum of the\nloss function. We develop a version of this optimizer known as $ECD_{q=1}$,\nwhich has few free hyperparameters with limited ranges guided by physical\nreasoning. We apply $ECD_{q=1}$ to representative likelihood-ratio estimation\ntasks in particle physics and find that it out-performs the widely-used Adam\noptimizer. We expect that ECD will be a useful tool for wide array of\ndata-limited problems, where it is computationally expensive to exhaustively\noptimize hyperparameters and mitigate fluctuations with ensembling."
                },
                "authors": [
                    {
                        "name": "G. Bruno De Luca"
                    },
                    {
                        "name": "Benjamin Nachman"
                    },
                    {
                        "name": "Eva Silverstein"
                    },
                    {
                        "name": "Henry Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Henry Zheng"
                },
                "author": "Henry Zheng",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18417v1",
                "updated": "2025-01-30T15:15:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    15,
                    17,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:15:17Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    15,
                    17,
                    3,
                    30,
                    0
                ],
                "title": "Causal Inference Real-Time Anomaly Detection with Synthetic Anomaly\n  Monitoring (SAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference Real-Time Anomaly Detection with Synthetic Anomaly\n  Monitoring (SAM)"
                },
                "summary": "Anomaly detection is essential for identifying rare and significant events\nacross diverse domains such as finance, cybersecurity, and network monitoring.\nThis paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach\nthat applies synthetic control methods from causal inference to improve both\nthe accuracy and interpretability of anomaly detection processes. By modeling\nnormal behavior through the treatment of each feature as a control unit, SAM\nidentifies anomalies as deviations within this causal framework. We conducted\nextensive experiments comparing SAM with established benchmark models,\nincluding Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors\n(kNN), and One-Class Support Vector Machine (SVM), across five diverse\ndatasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup\n1999, among others. Our results demonstrate that SAM consistently delivers\nrobust performance, highlighting its potential as a powerful tool for real-time\nanomaly detection in dynamic and complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection is essential for identifying rare and significant events\nacross diverse domains such as finance, cybersecurity, and network monitoring.\nThis paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach\nthat applies synthetic control methods from causal inference to improve both\nthe accuracy and interpretability of anomaly detection processes. By modeling\nnormal behavior through the treatment of each feature as a control unit, SAM\nidentifies anomalies as deviations within this causal framework. We conducted\nextensive experiments comparing SAM with established benchmark models,\nincluding Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors\n(kNN), and One-Class Support Vector Machine (SVM), across five diverse\ndatasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup\n1999, among others. Our results demonstrate that SAM consistently delivers\nrobust performance, highlighting its potential as a powerful tool for real-time\nanomaly detection in dynamic and complex environments."
                },
                "authors": [
                    {
                        "name": "Emanuele Luzio"
                    },
                    {
                        "name": "Moacir Antonelli Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Moacir Antonelli Ponti"
                },
                "author": "Moacir Antonelli Ponti",
                "arxiv_comment": "19 pages, 3 figures, submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H30, 68T05, 62G99, 91G80, 68M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; K.6.5; I.2.6; H.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18416v1",
                "updated": "2025-01-30T15:14:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:14:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs\n  and Their Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs\n  and Their Mitigation"
                },
                "summary": "Federated Learning (FL) is increasingly being adopted in military\ncollaborations to develop Large Language Models (LLMs) while preserving data\nsovereignty. However, prompt injection attacks-malicious manipulations of input\nprompts-pose new threats that may undermine operational security, disrupt\ndecision-making, and erode trust among allies. This perspective paper\nhighlights four potential vulnerabilities in federated military LLMs: secret\ndata leakage, free-rider exploitation, system disruption, and misinformation\nspread. To address these potential risks, we propose a human-AI collaborative\nframework that introduces both technical and policy countermeasures. On the\ntechnical side, our framework uses red/blue team wargaming and quality\nassurance to detect and mitigate adversarial behaviors of shared LLM weights.\nOn the policy side, it promotes joint AI-human policy development and\nverification of security protocols. Our findings will guide future research and\nemphasize proactive strategies for emerging military contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is increasingly being adopted in military\ncollaborations to develop Large Language Models (LLMs) while preserving data\nsovereignty. However, prompt injection attacks-malicious manipulations of input\nprompts-pose new threats that may undermine operational security, disrupt\ndecision-making, and erode trust among allies. This perspective paper\nhighlights four potential vulnerabilities in federated military LLMs: secret\ndata leakage, free-rider exploitation, system disruption, and misinformation\nspread. To address these potential risks, we propose a human-AI collaborative\nframework that introduces both technical and policy countermeasures. On the\ntechnical side, our framework uses red/blue team wargaming and quality\nassurance to detect and mitigate adversarial behaviors of shared LLM weights.\nOn the policy side, it promotes joint AI-human policy development and\nverification of security protocols. Our findings will guide future research and\nemphasize proactive strategies for emerging military contexts."
                },
                "authors": [
                    {
                        "name": "Youngjoon Lee"
                    },
                    {
                        "name": "Taehyun Park"
                    },
                    {
                        "name": "Yunho Lee"
                    },
                    {
                        "name": "Jinu Gong"
                    },
                    {
                        "name": "Joonhyuk Kang"
                    }
                ],
                "author_detail": {
                    "name": "Joonhyuk Kang"
                },
                "author": "Joonhyuk Kang",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08533v2",
                "updated": "2025-01-30T15:12:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    12,
                    17,
                    3,
                    30,
                    0
                ],
                "published": "2024-04-12T15:24:19Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    15,
                    24,
                    19,
                    4,
                    103,
                    0
                ],
                "title": "A Data Fusion Model for Meteorological Data using the INLA-SPDE method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data Fusion Model for Meteorological Data using the INLA-SPDE method"
                },
                "summary": "This work aims to combine two primary meteorological data sources in the\nPhilippines: data from a sparse network of weather stations and outcomes of a\nnumerical weather prediction model. To this end, we propose a data fusion model\nwhich is primarily motivated by the problem of sparsity in the observational\ndata and the use of a numerical prediction model as an additional data source\nin order to obtain better predictions for the variables of interest. The\nproposed data fusion model assumes that the different data sources are\nerror-prone realizations of a common latent process. The outcomes from the\nweather stations follow the classical error model while the outcomes of the\nnumerical weather prediction model involves a constant multiplicative bias\nparameter and an additive bias which is spatially-structured and time-varying.\nWe use a Bayesian model averaging approach with the integrated nested Laplace\napproximation (INLA) for doing inference. The proposed data fusion model\noutperforms the stations-only model and the regression calibration approach,\nwhen assessed using leave-group-out cross-validation (LGOCV). We assess the\nbenefits of data fusion and evaluate the accuracy of predictions and parameter\nestimation through a simulation study. The results show that the proposed data\nfusion model generally gives better predictions compared to the stations-only\napproach especially with sparse observational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to combine two primary meteorological data sources in the\nPhilippines: data from a sparse network of weather stations and outcomes of a\nnumerical weather prediction model. To this end, we propose a data fusion model\nwhich is primarily motivated by the problem of sparsity in the observational\ndata and the use of a numerical prediction model as an additional data source\nin order to obtain better predictions for the variables of interest. The\nproposed data fusion model assumes that the different data sources are\nerror-prone realizations of a common latent process. The outcomes from the\nweather stations follow the classical error model while the outcomes of the\nnumerical weather prediction model involves a constant multiplicative bias\nparameter and an additive bias which is spatially-structured and time-varying.\nWe use a Bayesian model averaging approach with the integrated nested Laplace\napproximation (INLA) for doing inference. The proposed data fusion model\noutperforms the stations-only model and the regression calibration approach,\nwhen assessed using leave-group-out cross-validation (LGOCV). We assess the\nbenefits of data fusion and evaluate the accuracy of predictions and parameter\nestimation through a simulation study. The results show that the proposed data\nfusion model generally gives better predictions compared to the stations-only\napproach especially with sparse observational data."
                },
                "authors": [
                    {
                        "name": "Stephen Jun Villejo"
                    },
                    {
                        "name": "Sara Martino"
                    },
                    {
                        "name": "Finn Lindgren"
                    },
                    {
                        "name": "Janine Illian"
                    }
                ],
                "author_detail": {
                    "name": "Janine Illian"
                },
                "author": "Janine Illian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04078v3",
                "updated": "2025-01-30T15:11:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    11,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-05T08:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    15,
                    45,
                    5,
                    279,
                    0
                ],
                "title": "TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles\n  with Simulated Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles\n  with Simulated Students"
                },
                "summary": "Large language models (LLMs) can empower teachers to build pedagogical\nconversational agents (PCAs) customized for their students. As students have\ndifferent prior knowledge and motivation levels, teachers must review the\nadaptivity of their PCAs to diverse students. Existing chatbot reviewing\nmethods (e.g., direct chat and benchmarks) are either manually intensive for\nmultiple iterations or limited to testing only single-turn interactions. We\npresent TeachTune, where teachers can create simulated students and review PCAs\nby observing automated chats between PCAs and simulated students. Our technical\npipeline instructs an LLM-based student to simulate prescribed knowledge levels\nand traits, helping teachers explore diverse conversation patterns. Our\npipeline could produce simulated students whose behaviors correlate highly to\ntheir input knowledge and motivation levels within 5% and 10% accuracy gaps.\nThirty science teachers designed PCAs in a between-subjects study, and using\nTeachTune resulted in a lower task load and higher student profile coverage\nover a baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can empower teachers to build pedagogical\nconversational agents (PCAs) customized for their students. As students have\ndifferent prior knowledge and motivation levels, teachers must review the\nadaptivity of their PCAs to diverse students. Existing chatbot reviewing\nmethods (e.g., direct chat and benchmarks) are either manually intensive for\nmultiple iterations or limited to testing only single-turn interactions. We\npresent TeachTune, where teachers can create simulated students and review PCAs\nby observing automated chats between PCAs and simulated students. Our technical\npipeline instructs an LLM-based student to simulate prescribed knowledge levels\nand traits, helping teachers explore diverse conversation patterns. Our\npipeline could produce simulated students whose behaviors correlate highly to\ntheir input knowledge and motivation levels within 5% and 10% accuracy gaps.\nThirty science teachers designed PCAs in a between-subjects study, and using\nTeachTune resulted in a lower task load and higher student profile coverage\nover a baseline."
                },
                "authors": [
                    {
                        "name": "Hyoungwook Jin"
                    },
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Jeongeon Park"
                    },
                    {
                        "name": "Yokyung Lee"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11562v2",
                "updated": "2025-01-30T14:40:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    40,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-20T16:06:24Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    16,
                    6,
                    24,
                    0,
                    20,
                    0
                ],
                "title": "Investigating the complex absorbers of Mrk 766 with XMM-Newton",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the complex absorbers of Mrk 766 with XMM-Newton"
                },
                "summary": "Aims. We examine the high energy resolution X-ray spectrum of the narrow-line\nSeyfert 1 galaxy Mrk 766 using 4 observations taken with XMM-Newton in 2005, to\ninvestigate the properties of the complex ionised absorber / emitter along the\nline of sight, as well as absorption by dust intrinsic to the source.\n  Methods. We make use of the high-energy resolution RGS spectrum to infer the\nproperties of the intervening matter. We also use the spectrum obtained by\nEPIC-pn and the photometric measurements of OM to obtain the spectral energy\ndistribution of the source, necessary for the photoionisation modelling of the\nionised outflow.\n  Results. The warm absorber in Mrk 766 consists of two phases of\nphotoionisation. In addition to these two warm absorber components with\n$\\log\\xi\\sim 2.15$ and $\\log\\xi\\sim -0.58$, we find evidence of absorption by a\ncollisionally ionised component ($T\\sim51$ eV). We discuss the implication of\nthis additional component in light of theoretical predictions. Moreover, we\ndetect signs of absorption by a dusty medium with $N_\\text{dust}\\sim 7.29\\times\n10^{16}$ cm$^{-2}$. Finally the relatively weak emission features in the\nspectrum seem to be unrelated to the absorbers and probably originated by an\nout-of sight-line ionised plasma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. We examine the high energy resolution X-ray spectrum of the narrow-line\nSeyfert 1 galaxy Mrk 766 using 4 observations taken with XMM-Newton in 2005, to\ninvestigate the properties of the complex ionised absorber / emitter along the\nline of sight, as well as absorption by dust intrinsic to the source.\n  Methods. We make use of the high-energy resolution RGS spectrum to infer the\nproperties of the intervening matter. We also use the spectrum obtained by\nEPIC-pn and the photometric measurements of OM to obtain the spectral energy\ndistribution of the source, necessary for the photoionisation modelling of the\nionised outflow.\n  Results. The warm absorber in Mrk 766 consists of two phases of\nphotoionisation. In addition to these two warm absorber components with\n$\\log\\xi\\sim 2.15$ and $\\log\\xi\\sim -0.58$, we find evidence of absorption by a\ncollisionally ionised component ($T\\sim51$ eV). We discuss the implication of\nthis additional component in light of theoretical predictions. Moreover, we\ndetect signs of absorption by a dusty medium with $N_\\text{dust}\\sim 7.29\\times\n10^{16}$ cm$^{-2}$. Finally the relatively weak emission features in the\nspectrum seem to be unrelated to the absorbers and probably originated by an\nout-of sight-line ionised plasma."
                },
                "authors": [
                    {
                        "name": "T. Matamoro Zatarain"
                    },
                    {
                        "name": "E. Costantini"
                    },
                    {
                        "name": "A. Jurov"
                    },
                    {
                        "name": "D. Rogantini"
                    }
                ],
                "author_detail": {
                    "name": "D. Rogantini"
                },
                "author": "D. Rogantini",
                "arxiv_comment": "10 pages, 9 figures. Accepted for publication in Astronomy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06967v2",
                "updated": "2025-01-30T14:37:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    37,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2024-06-11T05:50:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    5,
                    50,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?"
                },
                "summary": "The dual thinking framework considers fast, intuitive processing and slower,\nlogical processing. The perception of dual thinking in vision requires images\nwhere inferences from intuitive and logical processing differ. We introduce an\nadversarial dataset to provide evidence for the dual thinking framework in\nhuman vision, which also aids in studying the qualitative behavior of deep\nlearning models. The evidence underscores the importance of shape in\nidentifying instances in human vision. Our psychophysical studies show the\npresence of multiple inferences in rapid succession, and analysis of errors\nshows the early stopping of visual processing can result in missing relevant\ninformation. Our study shows that segmentation models lack an understanding of\nsub-structures, as indicated by errors related to the position and number of\nsub-components. Additionally, the similarity in errors made by models and\nintuitive human processing indicates that models only address intuitive\nthinking in human vision. In contrast, multi-modal LLMs, including open-source\nmodels, demonstrate tremendous progress on errors made in intuitive processing.\nThe models have improved performance on images that require logical reasoning\nand show recognition of sub-components. However, they have not matched the\nperformance improvements made on errors in intuitive processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dual thinking framework considers fast, intuitive processing and slower,\nlogical processing. The perception of dual thinking in vision requires images\nwhere inferences from intuitive and logical processing differ. We introduce an\nadversarial dataset to provide evidence for the dual thinking framework in\nhuman vision, which also aids in studying the qualitative behavior of deep\nlearning models. The evidence underscores the importance of shape in\nidentifying instances in human vision. Our psychophysical studies show the\npresence of multiple inferences in rapid succession, and analysis of errors\nshows the early stopping of visual processing can result in missing relevant\ninformation. Our study shows that segmentation models lack an understanding of\nsub-structures, as indicated by errors related to the position and number of\nsub-components. Additionally, the similarity in errors made by models and\nintuitive human processing indicates that models only address intuitive\nthinking in human vision. In contrast, multi-modal LLMs, including open-source\nmodels, demonstrate tremendous progress on errors made in intuitive processing.\nThe models have improved performance on images that require logical reasoning\nand show recognition of sub-components. However, they have not matched the\nperformance improvements made on errors in intuitive processing."
                },
                "authors": [
                    {
                        "name": "Kailas Dayanandan"
                    },
                    {
                        "name": "Nikhil Kumar"
                    },
                    {
                        "name": "Anand Sinha"
                    },
                    {
                        "name": "Brejesh Lall"
                    }
                ],
                "author_detail": {
                    "name": "Brejesh Lall"
                },
                "author": "Brejesh Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v2",
                "updated": "2025-01-30T14:36:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    36,
                    52,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Learning?"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18365v1",
                "updated": "2025-01-30T14:15:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    15,
                    9,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:15:09Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    15,
                    9,
                    3,
                    30,
                    0
                ],
                "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques."
                },
                "authors": [
                    {
                        "name": "Yiteng Tu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16512v2",
                "updated": "2025-01-30T14:14:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    14,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2023-12-27T10:41:58Z",
                "published_parsed": [
                    2023,
                    12,
                    27,
                    10,
                    41,
                    58,
                    2,
                    361,
                    0
                ],
                "title": "Degrees-of-freedom penalized piecewise regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees-of-freedom penalized piecewise regression"
                },
                "summary": "Many popular piecewise regression models rely on minimizing a cost function\non the model fit with a linear penalty on the number of segments. However, this\npenalty does not take into account varying complexities of the model functions\non the segments potentially leading to overfitting when models with varying\ncomplexities, such as polynomials of different degrees, are used. In this work,\nwe enhance on this approach by instead using a penalty on the sum of the\ndegrees of freedom over all segments, called degrees-of-freedom penalized\npiecewise regression (DofPPR). We show that the solutions of the resulting\nminimization problem are unique for almost all input data in a least squares\nsetting. We develop a fast algorithm which does not only compute a minimizer\nbut also determines an optimal hyperparameter -- in the sense of rolling cross\nvalidation with the one standard error rule -- exactly. This eliminates manual\nhyperparameter selection. Our method supports optional user parameters for\nincorporating domain knowledge. We provide an open-source Python/Rust code for\nthe piecewise polynomial least squares case which can be extended to further\nmodels. We demonstrate the practical utility through a simulation study and by\napplications to real data. A constrained variant of the proposed method gives\nstate-of-the-art results in the Turing benchmark for unsupervised changepoint\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many popular piecewise regression models rely on minimizing a cost function\non the model fit with a linear penalty on the number of segments. However, this\npenalty does not take into account varying complexities of the model functions\non the segments potentially leading to overfitting when models with varying\ncomplexities, such as polynomials of different degrees, are used. In this work,\nwe enhance on this approach by instead using a penalty on the sum of the\ndegrees of freedom over all segments, called degrees-of-freedom penalized\npiecewise regression (DofPPR). We show that the solutions of the resulting\nminimization problem are unique for almost all input data in a least squares\nsetting. We develop a fast algorithm which does not only compute a minimizer\nbut also determines an optimal hyperparameter -- in the sense of rolling cross\nvalidation with the one standard error rule -- exactly. This eliminates manual\nhyperparameter selection. Our method supports optional user parameters for\nincorporating domain knowledge. We provide an open-source Python/Rust code for\nthe piecewise polynomial least squares case which can be extended to further\nmodels. We demonstrate the practical utility through a simulation study and by\napplications to real data. A constrained variant of the proposed method gives\nstate-of-the-art results in the Turing benchmark for unsupervised changepoint\ndetection."
                },
                "authors": [
                    {
                        "name": "Stefan Volz"
                    },
                    {
                        "name": "Martin Storath"
                    },
                    {
                        "name": "Andreas Weinmann"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Weinmann"
                },
                "author": "Andreas Weinmann",
                "arxiv_comment": "29 pages (40 including supplementary), 19 figures, accepted for\n  publication in \"Information and Inference: a Journal of the IMA\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65K05 (Primary) 90C26 (Secondary) 62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.2; G.1.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.06061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.06061v3",
                "updated": "2025-01-30T14:00:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    0,
                    22,
                    3,
                    30,
                    0
                ],
                "published": "2023-11-10T13:50:45Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    13,
                    50,
                    45,
                    4,
                    314,
                    0
                ],
                "title": "New binary black hole mergers in the LIGO-Virgo O3b data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New binary black hole mergers in the LIGO-Virgo O3b data"
                },
                "summary": "We report the detection of 6 new candidate binary black hole (BBH) merger\nsignals in the publicly released data from the second half of the third\nobserving run (O3b) of advanced LIGO and advanced Virgo. The LIGO-Virgo-KAGRA\n(LVK) collaboration reported 35 compact binary coalescences (CBCs) in their\nanalysis of the O3b data [1], with 30 BBH mergers having coincidence in the\nHanford and Livingston detectors. We confirm 17 of these for a total of 23\ndetections in our analysis of the Hanford-Livingston coincident O3b data. We\nidentify candidates using a search pipeline employing aligned-spin\nquadrupole-only waveforms. Our pipeline is similar to the one used in our O3a\ncoincident analysis [2], except for a few improvements in the veto procedure\nand the ranking statistic, and we continue to use an astrophysical probability\nof one half as our detection threshold, following the approach of the LVK\ncatalogs. Most of the new candidates reported in this work are placed in the\nupper and lower-mass gap of the black hole (BH) mass distribution. We also\nidentify a possible neutron star-black hole (NSBH) merger. We expect these\nevents to help inform the black hole mass and spin distributions inferred in a\nfull population analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the detection of 6 new candidate binary black hole (BBH) merger\nsignals in the publicly released data from the second half of the third\nobserving run (O3b) of advanced LIGO and advanced Virgo. The LIGO-Virgo-KAGRA\n(LVK) collaboration reported 35 compact binary coalescences (CBCs) in their\nanalysis of the O3b data [1], with 30 BBH mergers having coincidence in the\nHanford and Livingston detectors. We confirm 17 of these for a total of 23\ndetections in our analysis of the Hanford-Livingston coincident O3b data. We\nidentify candidates using a search pipeline employing aligned-spin\nquadrupole-only waveforms. Our pipeline is similar to the one used in our O3a\ncoincident analysis [2], except for a few improvements in the veto procedure\nand the ranking statistic, and we continue to use an astrophysical probability\nof one half as our detection threshold, following the approach of the LVK\ncatalogs. Most of the new candidates reported in this work are placed in the\nupper and lower-mass gap of the black hole (BH) mass distribution. We also\nidentify a possible neutron star-black hole (NSBH) merger. We expect these\nevents to help inform the black hole mass and spin distributions inferred in a\nfull population analysis."
                },
                "authors": [
                    {
                        "name": "Ajit Kumar Mehta"
                    },
                    {
                        "name": "Seth Olsen"
                    },
                    {
                        "name": "Digvijay Wadekar"
                    },
                    {
                        "name": "Javier Roulet"
                    },
                    {
                        "name": "Tejaswi Venumadhav"
                    },
                    {
                        "name": "Jonathan Mushkin"
                    },
                    {
                        "name": "Barak Zackay"
                    },
                    {
                        "name": "Matias Zaldarriaga"
                    }
                ],
                "author_detail": {
                    "name": "Matias Zaldarriaga"
                },
                "author": "Matias Zaldarriaga",
                "arxiv_doi": "10.1103/PhysRevD.111.024049",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.024049",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.06061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.06061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 12 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 024049 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18337v1",
                "updated": "2025-01-30T13:34:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    34,
                    48,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T13:34:48Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    34,
                    48,
                    3,
                    30,
                    0
                ],
                "title": "Unfaithful Probability Distributions in Binary Triple of Causality\n  Directed Acyclic Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unfaithful Probability Distributions in Binary Triple of Causality\n  Directed Acyclic Graph"
                },
                "summary": "Faithfulness is the foundation of probability distribution and graph in\ncausal discovery and causal inference. In this paper, several unfaithful\nprobability distribution examples are constructed in three--vertices binary\ncausality directed acyclic graph (DAG) structure, which are not faithful to\ncausal DAGs described in J.M.,Robins,et al. Uniform consistency in causal\ninference. Biometrika (2003),90(3): 491--515. And the general unfaithful\nprobability distribution with multiple independence and conditional\nindependence in binary triple causal DAG is given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness is the foundation of probability distribution and graph in\ncausal discovery and causal inference. In this paper, several unfaithful\nprobability distribution examples are constructed in three--vertices binary\ncausality directed acyclic graph (DAG) structure, which are not faithful to\ncausal DAGs described in J.M.,Robins,et al. Uniform consistency in causal\ninference. Biometrika (2003),90(3): 491--515. And the general unfaithful\nprobability distribution with multiple independence and conditional\nindependence in binary triple causal DAG is given."
                },
                "authors": [
                    {
                        "name": "Jingwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Liu"
                },
                "author": "Jingwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18320v1",
                "updated": "2025-01-30T13:00:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    0,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T13:00:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    0,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "Leveraging LLM Agents for Automated Optimization Modeling for SASP\n  Problems: A Graph-RAG based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Agents for Automated Optimization Modeling for SASP\n  Problems: A Graph-RAG based Approach"
                },
                "summary": "Automated optimization modeling (AOM) has evoked considerable interest with\nthe rapid evolution of large language models (LLMs). Existing approaches\npredominantly rely on prompt engineering, utilizing meticulously designed\nexpert response chains or structured guidance. However, prompt-based techniques\nhave failed to perform well in the sensor array signal processing (SASP) area\ndue the lack of specific domain knowledge. To address this issue, we propose an\nautomated modeling approach based on retrieval-augmented generation (RAG)\ntechnique, which consists of two principal components: a multi-agent (MA)\nstructure and a graph-based RAG (Graph-RAG) process. The MA structure is\ntailored for the architectural AOM process, with each agent being designed\nbased on principles of human modeling procedure. The Graph-RAG process serves\nto match user query with specific SASP modeling knowledge, thereby enhancing\nthe modeling result. Results on ten classical signal processing problems\ndemonstrate that the proposed approach (termed as MAG-RAG) outperforms several\nAOM benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated optimization modeling (AOM) has evoked considerable interest with\nthe rapid evolution of large language models (LLMs). Existing approaches\npredominantly rely on prompt engineering, utilizing meticulously designed\nexpert response chains or structured guidance. However, prompt-based techniques\nhave failed to perform well in the sensor array signal processing (SASP) area\ndue the lack of specific domain knowledge. To address this issue, we propose an\nautomated modeling approach based on retrieval-augmented generation (RAG)\ntechnique, which consists of two principal components: a multi-agent (MA)\nstructure and a graph-based RAG (Graph-RAG) process. The MA structure is\ntailored for the architectural AOM process, with each agent being designed\nbased on principles of human modeling procedure. The Graph-RAG process serves\nto match user query with specific SASP modeling knowledge, thereby enhancing\nthe modeling result. Results on ten classical signal processing problems\ndemonstrate that the proposed approach (termed as MAG-RAG) outperforms several\nAOM benchmarks."
                },
                "authors": [
                    {
                        "name": "Tianpeng Pan"
                    },
                    {
                        "name": "Wenqiang Pu"
                    },
                    {
                        "name": "Licheng Zhao"
                    },
                    {
                        "name": "Rui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhou"
                },
                "author": "Rui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v7",
                "updated": "2025-01-30T12:39:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    39,
                    52,
                    3,
                    30,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I error rate and, consequently, an unknown reduction in its\ncapability to license inferences severely. I conclude that preregistration does\nnot improve the transparent evaluation of severity in Popper's philosophy of\nscience or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I error rate and, consequently, an unknown reduction in its\ncapability to license inferences severely. I conclude that preregistration does\nnot improve the transparent evaluation of severity in Popper's philosophy of\nscience or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18310v1",
                "updated": "2025-01-30T12:37:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    37,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T12:37:06Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    37,
                    6,
                    3,
                    30,
                    0
                ],
                "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Neural Theorem Proving via Fine-grained Proof Structure\n  Analysis"
                },
                "summary": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug."
                },
                "authors": [
                    {
                        "name": "Haoxiong Liu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03420v2",
                "updated": "2025-01-30T12:35:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    35,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2024-12-04T16:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    0,
                    14,
                    2,
                    339,
                    0
                ],
                "title": "Automated Test-Case Generation for REST APIs Using Model Inference\n  Search Heuristic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Test-Case Generation for REST APIs Using Model Inference\n  Search Heuristic"
                },
                "summary": "The rising popularity of the microservice architectural style has led to a\ngrowing demand for automated testing approaches tailored to these systems.\nEvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to\nautomatically generate test cases for microservices' REST APIs. One limitation\nof these EAs is the use of unit-level search heuristics, such as branch\ndistances, which focus on fine-grained code coverage and may not effectively\ncapture the complex, interconnected behaviors characteristic of system-level\ntesting. To address this limitation, we propose a new search heuristic (MISH)\nthat uses real-time automaton learning to guide the test case generation\nprocess. We capture the sequential call patterns exhibited by a test case by\nlearning an automaton from the stream of log events outputted by different\nmicroservices within the same system. Therefore, MISH learns a representation\nof the systemwide behavior, allowing us to define the fitness of a test case\nbased on the path it traverses within the inferred automaton. We empirically\nevaluate MISH's effectiveness on six real-world benchmark microservice\napplications and compare it against a state-of-the-art technique, MOSA, for\ntesting REST APIs. Our evaluation shows promising results for using MISH to\nguide the automated test case generation within EvoMaster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising popularity of the microservice architectural style has led to a\ngrowing demand for automated testing approaches tailored to these systems.\nEvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to\nautomatically generate test cases for microservices' REST APIs. One limitation\nof these EAs is the use of unit-level search heuristics, such as branch\ndistances, which focus on fine-grained code coverage and may not effectively\ncapture the complex, interconnected behaviors characteristic of system-level\ntesting. To address this limitation, we propose a new search heuristic (MISH)\nthat uses real-time automaton learning to guide the test case generation\nprocess. We capture the sequential call patterns exhibited by a test case by\nlearning an automaton from the stream of log events outputted by different\nmicroservices within the same system. Therefore, MISH learns a representation\nof the systemwide behavior, allowing us to define the fitness of a test case\nbased on the path it traverses within the inferred automaton. We empirically\nevaluate MISH's effectiveness on six real-world benchmark microservice\napplications and compare it against a state-of-the-art technique, MOSA, for\ntesting REST APIs. Our evaluation shows promising results for using MISH to\nguide the automated test case generation within EvoMaster."
                },
                "authors": [
                    {
                        "name": "Clinton Cao"
                    },
                    {
                        "name": "Annibale Panichella"
                    },
                    {
                        "name": "Sicco Verwer"
                    }
                ],
                "author_detail": {
                    "name": "Sicco Verwer"
                },
                "author": "Sicco Verwer",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15345v2",
                "updated": "2025-01-30T12:20:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    20,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-10T10:59:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    59,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual\n  perception beyond human capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual\n  perception beyond human capabilities"
                },
                "summary": "Optical flow, inspired by the mechanisms of biological visual systems,\ncalculates spatial motion vectors within visual scenes that are necessary for\nenabling robotics to excel in complex and dynamic working environments.\nHowever, current optical flow algorithms, despite human-competitive task\nperformance on benchmark datasets, remain constrained by unacceptable time\ndelays (~0.6 seconds per inference, 4X human processing speed) in practical\ndeployment. Here, we introduce a neuromorphic optical flow approach that\naddresses delay bottlenecks by encoding temporal information directly in a\nsynaptic transistor array to assist spatial motion analysis. Compared to\nconventional spatial-only optical flow methods, our spatiotemporal neuromorphic\noptical flow offers the spatial-temporal consistency of motion information,\nrapidly identifying regions of interest in as little as 1-2 ms using the\ntemporal motion cues derived from the embedded temporal information in the\ntwo-dimensional floating gate synaptic transistors. Thus, the visual input can\nbe selectively filtered to achieve faster velocity calculations and various\ntask execution. At the hardware level, due to the atomically sharp interfaces\nbetween distinct functional layers in two-dimensional van der Waals\nheterostructures, the synaptic transistor offers high-frequency response (~100\n{\\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000\ncycles), enabling robust visual processing. In software benchmarks, our system\noutperforms state-of-the-art algorithms with a 400% speedup, frequently\nsurpassing human-level performance while maintaining or enhancing accuracy by\nutilizing the temporal priors provided by the embedded temporal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical flow, inspired by the mechanisms of biological visual systems,\ncalculates spatial motion vectors within visual scenes that are necessary for\nenabling robotics to excel in complex and dynamic working environments.\nHowever, current optical flow algorithms, despite human-competitive task\nperformance on benchmark datasets, remain constrained by unacceptable time\ndelays (~0.6 seconds per inference, 4X human processing speed) in practical\ndeployment. Here, we introduce a neuromorphic optical flow approach that\naddresses delay bottlenecks by encoding temporal information directly in a\nsynaptic transistor array to assist spatial motion analysis. Compared to\nconventional spatial-only optical flow methods, our spatiotemporal neuromorphic\noptical flow offers the spatial-temporal consistency of motion information,\nrapidly identifying regions of interest in as little as 1-2 ms using the\ntemporal motion cues derived from the embedded temporal information in the\ntwo-dimensional floating gate synaptic transistors. Thus, the visual input can\nbe selectively filtered to achieve faster velocity calculations and various\ntask execution. At the hardware level, due to the atomically sharp interfaces\nbetween distinct functional layers in two-dimensional van der Waals\nheterostructures, the synaptic transistor offers high-frequency response (~100\n{\\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000\ncycles), enabling robust visual processing. In software benchmarks, our system\noutperforms state-of-the-art algorithms with a 400% speedup, frequently\nsurpassing human-level performance while maintaining or enhancing accuracy by\nutilizing the temporal priors provided by the embedded temporal information."
                },
                "authors": [
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Jingwen Zhao"
                    },
                    {
                        "name": "Tongming Pu"
                    },
                    {
                        "name": "Liangbing Zhao"
                    },
                    {
                        "name": "Xiaoyu Guo"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Weihao Ma"
                    },
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Zhenyu Xu"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Luigi Occhipinti"
                    },
                    {
                        "name": "Arokia Nathan"
                    },
                    {
                        "name": "Ravinder Dahiya"
                    },
                    {
                        "name": "Huaqiang Wu"
                    },
                    {
                        "name": "Li Tao"
                    },
                    {
                        "name": "Shuo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Gao"
                },
                "author": "Shuo Gao",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19243v2",
                "updated": "2025-01-30T12:17:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    17,
                    43,
                    3,
                    30,
                    0
                ],
                "published": "2024-03-28T08:58:20Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    8,
                    58,
                    20,
                    3,
                    88,
                    0
                ],
                "title": "Efficient Learning With Sine-Activated Low-rank Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Learning With Sine-Activated Low-rank Matrices"
                },
                "summary": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling."
                },
                "authors": [
                    {
                        "name": "Yiping Ji"
                    },
                    {
                        "name": "Hemanth Saratchandran"
                    },
                    {
                        "name": "Cameron Gordon"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Simon Lucey"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lucey"
                },
                "author": "Simon Lucey",
                "arxiv_comment": "The first two authors contributed equally. Paper accepted at ICLR\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18287v1",
                "updated": "2025-01-30T11:55:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    55,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:55:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    55,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models"
                },
                "summary": "This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Zachary Laubach"
                    },
                    {
                        "name": "Tarek Al Mustafa"
                    },
                    {
                        "name": "Sina Zarrie"
                    },
                    {
                        "name": "Robert Frhstckl"
                    },
                    {
                        "name": "Phyllis Illari"
                    }
                ],
                "author_detail": {
                    "name": "Phyllis Illari"
                },
                "author": "Phyllis Illari",
                "arxiv_comment": "8 pages, 2 figures, accepted to the NLP4Ecology Workshop 2025\n  (https://nlp4ecology2025.di.unito.it/) co-located with the Joint 25th Nordic\n  Conference on Computational Linguistics and 11th Baltic Conference on Human\n  Language Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18282v1",
                "updated": "2025-01-30T11:41:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    41,
                    13,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:41:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    41,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Leveraging Sparsity for Sample-Efficient Preference Learning: A\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Sparsity for Sample-Efficient Preference Learning: A\n  Theoretical Perspective"
                },
                "summary": "This paper considers the sample-efficiency of preference learning, which\nmodels and predicts human choices based on comparative judgments. The minimax\noptimal estimation rate $\\Theta(d/n)$ in traditional estimation theory requires\nthat the number of samples $n$ scales linearly with the dimensionality of the\nfeature space $d$. However, the high dimensionality of the feature space and\nthe high cost of collecting human-annotated data challenge the efficiency of\ntraditional estimation methods. To remedy this, we leverage sparsity in the\npreference model and establish sharp estimation rates. We show that under the\nsparse random utility model, where the parameter of the reward function is\n$k$-sparse, the minimax optimal rate can be reduced to $\\Theta(k/n \\log(d/k))$.\nFurthermore, we analyze the $\\ell_{1}$-regularized estimator and show that it\nachieves near-optimal rate under mild assumptions on the Gram matrix.\nExperiments on synthetic data and LLM alignment data validate our theoretical\nfindings, showing that sparsity-aware methods significantly reduce sample\ncomplexity and improve prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the sample-efficiency of preference learning, which\nmodels and predicts human choices based on comparative judgments. The minimax\noptimal estimation rate $\\Theta(d/n)$ in traditional estimation theory requires\nthat the number of samples $n$ scales linearly with the dimensionality of the\nfeature space $d$. However, the high dimensionality of the feature space and\nthe high cost of collecting human-annotated data challenge the efficiency of\ntraditional estimation methods. To remedy this, we leverage sparsity in the\npreference model and establish sharp estimation rates. We show that under the\nsparse random utility model, where the parameter of the reward function is\n$k$-sparse, the minimax optimal rate can be reduced to $\\Theta(k/n \\log(d/k))$.\nFurthermore, we analyze the $\\ell_{1}$-regularized estimator and show that it\nachieves near-optimal rate under mild assumptions on the Gram matrix.\nExperiments on synthetic data and LLM alignment data validate our theoretical\nfindings, showing that sparsity-aware methods significantly reduce sample\ncomplexity and improve prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Yunzhen Yao"
                    },
                    {
                        "name": "Lie He"
                    },
                    {
                        "name": "Michael Gastpar"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gastpar"
                },
                "author": "Michael Gastpar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18280v1",
                "updated": "2025-01-30T11:37:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:37:40Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models"
                },
                "summary": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner."
                },
                "authors": [
                    {
                        "name": "Haoyu Liang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yunfeng Cai"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02817v2",
                "updated": "2025-01-30T11:11:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    11,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-03-05T09:37:13Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    9,
                    37,
                    13,
                    1,
                    65,
                    0
                ],
                "title": "Here Comes The AI Worm: Unleashing Zero-click Worms that Target\n  GenAI-Powered Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here Comes The AI Worm: Unleashing Zero-click Worms that Target\n  GenAI-Powered Applications"
                },
                "summary": "In this paper, we show that when the communication between GenAI-powered\napplications relies on RAG-based inference, an attacker can initiate a computer\nworm-like chain reaction that we call Morris-II. This is done by crafting an\nadversarial self-replicating prompt that triggers a cascade of indirect prompt\ninjections within the ecosystem and forces each affected application to perform\nmalicious actions and compromise the RAG of additional applications. We\nevaluate the performance of the worm in creating a chain of confidential user\ndata extraction within a GenAI ecosystem of GenAI-powered email assistants and\nanalyze how the performance of the worm is affected by the size of the context,\nthe adversarial self-replicating prompt used, the type and size of the\nembedding algorithm employed, and the number of hops in the propagation.\nFinally, we introduce the Virtual Donkey, a guardrail intended to detect and\nprevent the propagation of Morris-II with minimal latency, high accuracy, and a\nlow false-positive rate. We evaluate the guardrail's performance and show that\nit yields a perfect true-positive rate of 1.0 with a false-positive rate of\n0.015, and is robust against out-of-distribution worms, consisting of unseen\njailbreaking commands, a different email dataset, and various worm usecases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we show that when the communication between GenAI-powered\napplications relies on RAG-based inference, an attacker can initiate a computer\nworm-like chain reaction that we call Morris-II. This is done by crafting an\nadversarial self-replicating prompt that triggers a cascade of indirect prompt\ninjections within the ecosystem and forces each affected application to perform\nmalicious actions and compromise the RAG of additional applications. We\nevaluate the performance of the worm in creating a chain of confidential user\ndata extraction within a GenAI ecosystem of GenAI-powered email assistants and\nanalyze how the performance of the worm is affected by the size of the context,\nthe adversarial self-replicating prompt used, the type and size of the\nembedding algorithm employed, and the number of hops in the propagation.\nFinally, we introduce the Virtual Donkey, a guardrail intended to detect and\nprevent the propagation of Morris-II with minimal latency, high accuracy, and a\nlow false-positive rate. We evaluate the guardrail's performance and show that\nit yields a perfect true-positive rate of 1.0 with a false-positive rate of\n0.015, and is robust against out-of-distribution worms, consisting of unseen\njailbreaking commands, a different email dataset, and various worm usecases."
                },
                "authors": [
                    {
                        "name": "Stav Cohen"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Ben Nassi"
                    }
                ],
                "author_detail": {
                    "name": "Ben Nassi"
                },
                "author": "Ben Nassi",
                "arxiv_comment": "Website: https://sites.google.com/view/compromptmized",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18265v1",
                "updated": "2025-01-30T11:04:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    4,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:04:14Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    4,
                    14,
                    3,
                    30,
                    0
                ],
                "title": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence"
                },
                "summary": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments."
                },
                "authors": [
                    {
                        "name": "Kevin Roitero"
                    },
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Michael Soprano"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Stefano Mizzaro"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Mizzaro"
                },
                "author": "Stefano Mizzaro",
                "arxiv_comment": "18 pages; 7 figures; 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18243v1",
                "updated": "2025-01-30T10:21:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    21,
                    10,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T10:21:10Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    21,
                    10,
                    3,
                    30,
                    0
                ],
                "title": "Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance"
                },
                "summary": "The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Assaf Toledo"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Toledo"
                },
                "author": "Assaf Toledo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17387v2",
                "updated": "2025-01-30T09:56:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    56,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-29T02:39:57Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    39,
                    57,
                    2,
                    29,
                    0
                ],
                "title": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection"
                },
                "summary": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 % and 72.36 %), as well\nas mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset\n2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with\nreaching 82.44 \\% on dataset 1 and 81.46 % in dataset 2, making them\nparticularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 % and 72.36 %), as well\nas mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset\n2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with\nreaching 82.44 \\% on dataset 1 and 81.46 % in dataset 2, making them\nparticularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups."
                },
                "authors": [
                    {
                        "name": "Alicia Allmendinger"
                    },
                    {
                        "name": "Ahmet Ouz Saltk"
                    },
                    {
                        "name": "Gerassimos G. Peteinatos"
                    },
                    {
                        "name": "Anthony Stein"
                    },
                    {
                        "name": "Roland Gerhards"
                    }
                ],
                "author_detail": {
                    "name": "Roland Gerhards"
                },
                "author": "Roland Gerhards",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18229v1",
                "updated": "2025-01-30T09:35:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    35,
                    17,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T09:35:17Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    35,
                    17,
                    3,
                    30,
                    0
                ],
                "title": "GPD: Guided Polynomial Diffusion for Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPD: Guided Polynomial Diffusion for Motion Planning"
                },
                "summary": "Diffusion-based motion planners are becoming popular due to their\nwell-established performance improvements, stemming from sample diversity and\nthe ease of incorporating new constraints directly during inference. However, a\nprimary limitation of the diffusion process is the requirement for a\nsubstantial number of denoising steps, especially when the denoising process is\ncoupled with gradient-based guidance. In this paper, we introduce, diffusion in\nthe parametric space of trajectories, where the parameters are represented as\nBernstein coefficients. We show that this representation greatly improves the\neffectiveness of the cost function guidance and the inference speed. We also\nintroduce a novel stitching algorithm that leverages the diversity in\ndiffusion-generated trajectories to produce collision-free trajectories with\njust a single cost function-guided model. We demonstrate that our approaches\noutperform current SOTA diffusion-based motion planners for manipulators and\nprovide an ablation study on key components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based motion planners are becoming popular due to their\nwell-established performance improvements, stemming from sample diversity and\nthe ease of incorporating new constraints directly during inference. However, a\nprimary limitation of the diffusion process is the requirement for a\nsubstantial number of denoising steps, especially when the denoising process is\ncoupled with gradient-based guidance. In this paper, we introduce, diffusion in\nthe parametric space of trajectories, where the parameters are represented as\nBernstein coefficients. We show that this representation greatly improves the\neffectiveness of the cost function guidance and the inference speed. We also\nintroduce a novel stitching algorithm that leverages the diversity in\ndiffusion-generated trajectories to produce collision-free trajectories with\njust a single cost function-guided model. We demonstrate that our approaches\noutperform current SOTA diffusion-based motion planners for manipulators and\nprovide an ablation study on key components."
                },
                "authors": [
                    {
                        "name": "Ajit Srikanth"
                    },
                    {
                        "name": "Parth Mahanjan"
                    },
                    {
                        "name": "Kallol Saha"
                    },
                    {
                        "name": "Vishal Mandadi"
                    },
                    {
                        "name": "Pranjal Paul"
                    },
                    {
                        "name": "Pawan Wadhwani"
                    },
                    {
                        "name": "Brojeshwar Bhowmick"
                    },
                    {
                        "name": "Arun Singh"
                    },
                    {
                        "name": "Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Madhava Krishna"
                },
                "author": "Madhava Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10979v2",
                "updated": "2025-01-30T09:17:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    17,
                    22,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-19T08:06:06Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    8,
                    6,
                    6,
                    6,
                    19,
                    0
                ],
                "title": "Control LLM: Controlled Evolution for Intelligence Retention in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control LLM: Controlled Evolution for Intelligence Retention in LLM"
                },
                "summary": "Large Language Models (LLMs) demand significant computational resources,\nmaking it essential to enhance their capabilities without retraining from\nscratch. A key challenge in this domain is \\textit{catastrophic forgetting}\n(CF), which hampers performance during Continuous Pre-training (CPT) and\nContinuous Supervised Fine-Tuning (CSFT). We propose \\textbf{Control LLM}, a\nnovel approach that leverages parallel pre-trained and expanded transformer\nblocks, aligning their hidden-states through interpolation strategies This\nmethod effectively preserves performance on existing tasks while seamlessly\nintegrating new knowledge.\n  Extensive experiments demonstrate the effectiveness of Control LLM in both\nCPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in\nmathematical reasoning ($+14.4\\%$ on Math-Hard) and coding performance ($+10\\%$\non MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\\%$\non C-Eval, $+6.8\\%$ on CMMLU, and $+30.2\\%$ on CMMLU-0shot-CoT). It surpasses\nexisting methods and achieves SOTA among open-source models tuned from the same\nbase model, using substantially less data and compute. Crucially, these gains\nare realized while preserving strong original capabilities, with minimal\ndegradation ($<4.3\\% \\text{on MMLU}$) compared to $>35\\%$ in open-source Math\nand Coding models. This approach has been successfully deployed in LinkedIn's\nGenAI-powered job seeker and Ads unit products.\n  To support further research, we release the training and evaluation code\n(https://github.com/linkedin/ControlLLM) along with models trained on public\ndatasets (https://huggingface.co/ControlLLM) to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demand significant computational resources,\nmaking it essential to enhance their capabilities without retraining from\nscratch. A key challenge in this domain is \\textit{catastrophic forgetting}\n(CF), which hampers performance during Continuous Pre-training (CPT) and\nContinuous Supervised Fine-Tuning (CSFT). We propose \\textbf{Control LLM}, a\nnovel approach that leverages parallel pre-trained and expanded transformer\nblocks, aligning their hidden-states through interpolation strategies This\nmethod effectively preserves performance on existing tasks while seamlessly\nintegrating new knowledge.\n  Extensive experiments demonstrate the effectiveness of Control LLM in both\nCPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in\nmathematical reasoning ($+14.4\\%$ on Math-Hard) and coding performance ($+10\\%$\non MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\\%$\non C-Eval, $+6.8\\%$ on CMMLU, and $+30.2\\%$ on CMMLU-0shot-CoT). It surpasses\nexisting methods and achieves SOTA among open-source models tuned from the same\nbase model, using substantially less data and compute. Crucially, these gains\nare realized while preserving strong original capabilities, with minimal\ndegradation ($<4.3\\% \\text{on MMLU}$) compared to $>35\\%$ in open-source Math\nand Coding models. This approach has been successfully deployed in LinkedIn's\nGenAI-powered job seeker and Ads unit products.\n  To support further research, we release the training and evaluation code\n(https://github.com/linkedin/ControlLLM) along with models trained on public\ndatasets (https://huggingface.co/ControlLLM) to the community."
                },
                "authors": [
                    {
                        "name": "Haichao Wei"
                    },
                    {
                        "name": "Yunxiang Ren"
                    },
                    {
                        "name": "Zhoutong Fu"
                    },
                    {
                        "name": "Aman Lunia"
                    },
                    {
                        "name": "Yi-Lin Chen"
                    },
                    {
                        "name": "Alice Leung"
                    },
                    {
                        "name": "Ya Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ya Xu"
                },
                "author": "Ya Xu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13117v2",
                "updated": "2025-01-30T09:15:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    15,
                    26,
                    3,
                    30,
                    0
                ],
                "published": "2024-11-20T08:21:53Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    21,
                    53,
                    2,
                    325,
                    0
                ],
                "title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders"
                },
                "summary": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. Using compressed sensing theory, we prove\nthat an SAE encoder is inherently insufficient for accurate sparse inference,\neven in solvable cases. We then decouple encoding and decoding processes to\nempirically explore conditions where more sophisticated sparse inference\nmethods outperform traditional SAE encoders. Our results reveal substantial\nperformance gains with minimal compute increases in correct inference of sparse\ncodes. We demonstrate this generalises to SAEs applied to large language\nmodels, where more expressive encoders achieve greater interpretability. This\nwork opens new avenues for understanding neural network representations and\nanalysing large language model activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. Using compressed sensing theory, we prove\nthat an SAE encoder is inherently insufficient for accurate sparse inference,\neven in solvable cases. We then decouple encoding and decoding processes to\nempirically explore conditions where more sophisticated sparse inference\nmethods outperform traditional SAE encoders. Our results reveal substantial\nperformance gains with minimal compute increases in correct inference of sparse\ncodes. We demonstrate this generalises to SAEs applied to large language\nmodels, where more expressive encoders achieve greater interpretability. This\nwork opens new avenues for understanding neural network representations and\nanalysing large language model activations."
                },
                "authors": [
                    {
                        "name": "Charles O'Neill"
                    },
                    {
                        "name": "Alim Gumran"
                    },
                    {
                        "name": "David Klindt"
                    }
                ],
                "author_detail": {
                    "name": "David Klindt"
                },
                "author": "David Klindt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18968v2",
                "updated": "2025-01-30T08:55:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    55,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-11T13:47:47Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    47,
                    47,
                    2,
                    255,
                    0
                ],
                "title": "Safety challenges of AI in medicine in the era of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety challenges of AI in medicine in the era of large language models"
                },
                "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs), have unlocked significant potential to enhance the\nquality and efficiency of medical care. By introducing a novel way to interact\nwith AI and data through natural language, LLMs offer new opportunities for\nmedical practitioners, patients, and researchers. However, as AI and LLMs\nbecome more powerful and especially achieve superhuman performance in some\nmedical tasks, public concerns over their safety have intensified. These\nconcerns about AI safety have emerged as the most significant obstacles to the\nadoption of AI in medicine. In response, this review examines emerging risks in\nAI utilization during the LLM era. First, we explore LLM-specific safety\nchallenges from functional and communication perspectives, addressing issues\nacross data collection, model training, and real-world application. We then\nconsider inherent safety problems shared by all AI systems, along with\nadditional complications introduced by LLMs. Last, we discussed how safety\nissues of using AI in clinical practice and healthcare system operation would\nundermine trust among patient, clinicians and the public, and how to build\nconfidence in these systems. By emphasizing the development of safe AI, we\nbelieve these technologies can be more rapidly and reliably integrated into\neveryday medical practice to benefit both patients and clinicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs), have unlocked significant potential to enhance the\nquality and efficiency of medical care. By introducing a novel way to interact\nwith AI and data through natural language, LLMs offer new opportunities for\nmedical practitioners, patients, and researchers. However, as AI and LLMs\nbecome more powerful and especially achieve superhuman performance in some\nmedical tasks, public concerns over their safety have intensified. These\nconcerns about AI safety have emerged as the most significant obstacles to the\nadoption of AI in medicine. In response, this review examines emerging risks in\nAI utilization during the LLM era. First, we explore LLM-specific safety\nchallenges from functional and communication perspectives, addressing issues\nacross data collection, model training, and real-world application. We then\nconsider inherent safety problems shared by all AI systems, along with\nadditional complications introduced by LLMs. Last, we discussed how safety\nissues of using AI in clinical practice and healthcare system operation would\nundermine trust among patient, clinicians and the public, and how to build\nconfidence in these systems. By emphasizing the development of safe AI, we\nbelieve these technologies can be more rapidly and reliably integrated into\neveryday medical practice to benefit both patients and clinicians."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Nicole Xi Zhang"
                    },
                    {
                        "name": "Hongyu He"
                    },
                    {
                        "name": "Trang Nguyen"
                    },
                    {
                        "name": "Kun-Hsing Yu"
                    },
                    {
                        "name": "Hao Deng"
                    },
                    {
                        "name": "Cynthia Brandt"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Ching-Yu Cheng"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dianbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Liu"
                },
                "author": "Dianbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20132v4",
                "updated": "2025-01-30T08:54:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    54,
                    54,
                    3,
                    30,
                    0
                ],
                "published": "2024-05-30T15:10:59Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    10,
                    59,
                    3,
                    151,
                    0
                ],
                "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs."
                },
                "authors": [
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bck"
                },
                "author": "Thomas Bck",
                "arxiv_comment": "Accepted at IEEE TEVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18205v1",
                "updated": "2025-01-30T08:51:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    51,
                    48,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T08:51:48Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    51,
                    48,
                    3,
                    30,
                    0
                ],
                "title": "Contextually Structured Token Dependency Encoding for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Structured Token Dependency Encoding for Large Language\n  Models"
                },
                "summary": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation."
                },
                "authors": [
                    {
                        "name": "James Blades"
                    },
                    {
                        "name": "Frederick Somerfield"
                    },
                    {
                        "name": "William Langley"
                    },
                    {
                        "name": "Susan Everingham"
                    },
                    {
                        "name": "Maurice Witherington"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Witherington"
                },
                "author": "Maurice Witherington",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18202v1",
                "updated": "2025-01-30T08:49:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    49,
                    25,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T08:49:25Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    49,
                    25,
                    3,
                    30,
                    0
                ],
                "title": "On Scaling Neurosymbolic Programming through Guided Logical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Scaling Neurosymbolic Programming through Guided Logical Inference"
                },
                "summary": "Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process."
                },
                "authors": [
                    {
                        "name": "Thomas Jean-Michel Valentin"
                    },
                    {
                        "name": "Luisa Sophie Werner"
                    },
                    {
                        "name": "Pierre Genevs"
                    },
                    {
                        "name": "Nabil Layada"
                    }
                ],
                "author_detail": {
                    "name": "Nabil Layada"
                },
                "arxiv_affiliation": "LIG, TYREX",
                "author": "Nabil Layada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08436v2",
                "updated": "2025-01-30T08:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    6,
                    33,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-11T00:45:50Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    0,
                    45,
                    50,
                    4,
                    285,
                    0
                ],
                "title": "Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models"
                },
                "summary": "When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults."
                },
                "authors": [
                    {
                        "name": "Zi'ou Zheng"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "arxiv_comment": "Accepted by EMNLP2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16513v2",
                "updated": "2025-01-30T08:00:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    0,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T21:26:37Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    21,
                    26,
                    37,
                    0,
                    27,
                    0
                ],
                "title": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation."
                },
                "authors": [
                    {
                        "name": "Sudarshan Kamath Barkur"
                    },
                    {
                        "name": "Sigurd Schacht"
                    },
                    {
                        "name": "Johannes Scholl"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Scholl"
                },
                "author": "Johannes Scholl",
                "arxiv_comment": "Corrected Version - Solved Some Issues with reference compilation by\n  latex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16493v3",
                "updated": "2025-01-30T07:23:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    23,
                    32,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-24T22:36:44Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    22,
                    36,
                    44,
                    1,
                    268,
                    0
                ],
                "title": "NoTeeline: Supporting Real-Time, Personalized Notetaking with\n  LLM-Enhanced Micronotes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoTeeline: Supporting Real-Time, Personalized Notetaking with\n  LLM-Enhanced Micronotes"
                },
                "summary": "Taking notes quickly while effectively capturing key information can be\nchallenging, especially when watching videos that present simultaneous visual\nand auditory streams. Manually taken notes often miss crucial details due to\nthe fast-paced nature of the content, while automatically generated notes fail\nto incorporate user preferences and discourage active engagement with the\ncontent. To address this, we propose an interactive system, NoTeeline, for\nsupporting real-time, personalized notetaking. Given micronotes, NoTeeline\nautomatically expands them into full-fledged notes using a Large Language Model\n(LLM). The generated notes build on the content of micronotes by adding\nrelevant details while maintaining consistency with the user's writing style.\nIn a within-subjects study (n=12), we found that NoTeeline creates high-quality\nnotes that capture the essence of participant micronotes with 93.2% factual\ncorrectness and accurately align with participant writing style (8.33%\nimprovement). Using NoTeeline, participants could capture their desired notes\nwith significantly reduced mental effort, writing 47.0% less text and\ncompleting their notes in 43.9% less time compared to a manual notetaking\nbaseline. Our results suggest that NoTeeline enables users to integrate LLM\nassistance in a familiar notetaking workflow while ensuring consistency with\ntheir preferences - providing an example of how to address broader challenges\nin designing AI-assisted tools to augment human capabilities without\ncompromising user autonomy and personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taking notes quickly while effectively capturing key information can be\nchallenging, especially when watching videos that present simultaneous visual\nand auditory streams. Manually taken notes often miss crucial details due to\nthe fast-paced nature of the content, while automatically generated notes fail\nto incorporate user preferences and discourage active engagement with the\ncontent. To address this, we propose an interactive system, NoTeeline, for\nsupporting real-time, personalized notetaking. Given micronotes, NoTeeline\nautomatically expands them into full-fledged notes using a Large Language Model\n(LLM). The generated notes build on the content of micronotes by adding\nrelevant details while maintaining consistency with the user's writing style.\nIn a within-subjects study (n=12), we found that NoTeeline creates high-quality\nnotes that capture the essence of participant micronotes with 93.2% factual\ncorrectness and accurately align with participant writing style (8.33%\nimprovement). Using NoTeeline, participants could capture their desired notes\nwith significantly reduced mental effort, writing 47.0% less text and\ncompleting their notes in 43.9% less time compared to a manual notetaking\nbaseline. Our results suggest that NoTeeline enables users to integrate LLM\nassistance in a familiar notetaking workflow while ensuring consistency with\ntheir preferences - providing an example of how to address broader challenges\nin designing AI-assisted tools to augment human capabilities without\ncompromising user autonomy and personalization."
                },
                "authors": [
                    {
                        "name": "Faria Huq"
                    },
                    {
                        "name": "Abdus Samee"
                    },
                    {
                        "name": "David Chuan-en Lin"
                    },
                    {
                        "name": "Xiaodi Alice Tang"
                    },
                    {
                        "name": "Jeffrey P. Bigham"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey P. Bigham"
                },
                "author": "Jeffrey P. Bigham",
                "arxiv_doi": "10.1145/3708359.3712086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conditionally accepted to IUI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17310v2",
                "updated": "2025-01-30T07:15:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    15,
                    4,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T21:43:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding"
                },
                "summary": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model. As\nLLMs' world model is a fundamental prerequisite for many real-world tasks,\ne.g., human-AI teaming, our findings have broad implications for the AI\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model. As\nLLMs' world model is a fundamental prerequisite for many real-world tasks,\ne.g., human-AI teaming, our findings have broad implications for the AI\ncommunity."
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Nikunj Harlalka"
                    },
                    {
                        "name": "Sameer Narendran"
                    },
                    {
                        "name": "Alexander Cheung"
                    },
                    {
                        "name": "Sizhe Gao"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy T. Rogers"
                },
                "author": "Timothy T. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.14185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.14185v4",
                "updated": "2025-01-30T07:12:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    12,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2023-04-27T13:41:12Z",
                "published_parsed": [
                    2023,
                    4,
                    27,
                    13,
                    41,
                    12,
                    3,
                    117,
                    0
                ],
                "title": "HPSCAN: Human Perception-Based Scattered Data Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPSCAN: Human Perception-Based Scattered Data Clustering"
                },
                "summary": "Cluster separation is a task typically tackled by widely used clustering\ntechniques, such as k-means or DBSCAN. However, these algorithms are based on\nnon-perceptual metrics, and our experiments demonstrate that their output does\nnot reflect human cluster perception. To bridge the gap between human cluster\nperception and machine-computed clusters, we propose HPSCAN, a learning\nstrategy that operates directly on scattered data. To learn perceptual cluster\nseparation on such data, we crowdsourced the labeling of 7,320 bivariate\n(scatterplot) datasets to 384 human participants. We train our HPSCAN model on\nthese human-annotated data. Instead of rendering these data as scatterplot\nimages, we used their x and y point coordinates as input to a modified\nPointNet++ architecture, enabling direct inference on point clouds. In this\nwork, we provide details on how we collected our dataset, report statistics of\nthe resulting annotations, and investigate the perceptual agreement of cluster\nseparation for real-world data. We also report the training and evaluation\nprotocol for HPSCAN and introduce a novel metric, that measures the accuracy\nbetween a clustering technique and a group of human annotators. We explore\npredicting point-wise human agreement to detect ambiguities. Finally, we\ncompare our approach to ten established clustering techniques and demonstrate\nthat HPSCAN is capable of generalizing to unseen and out-of-scope data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster separation is a task typically tackled by widely used clustering\ntechniques, such as k-means or DBSCAN. However, these algorithms are based on\nnon-perceptual metrics, and our experiments demonstrate that their output does\nnot reflect human cluster perception. To bridge the gap between human cluster\nperception and machine-computed clusters, we propose HPSCAN, a learning\nstrategy that operates directly on scattered data. To learn perceptual cluster\nseparation on such data, we crowdsourced the labeling of 7,320 bivariate\n(scatterplot) datasets to 384 human participants. We train our HPSCAN model on\nthese human-annotated data. Instead of rendering these data as scatterplot\nimages, we used their x and y point coordinates as input to a modified\nPointNet++ architecture, enabling direct inference on point clouds. In this\nwork, we provide details on how we collected our dataset, report statistics of\nthe resulting annotations, and investigate the perceptual agreement of cluster\nseparation for real-world data. We also report the training and evaluation\nprotocol for HPSCAN and introduce a novel metric, that measures the accuracy\nbetween a clustering technique and a group of human annotators. We explore\npredicting point-wise human agreement to detect ambiguities. Finally, we\ncompare our approach to ten established clustering techniques and demonstrate\nthat HPSCAN is capable of generalizing to unseen and out-of-scope data."
                },
                "authors": [
                    {
                        "name": "Sebastian Hartwig"
                    },
                    {
                        "name": "Christian van Onzenoodt"
                    },
                    {
                        "name": "Dominik Engel"
                    },
                    {
                        "name": "Pedro Hermosilla"
                    },
                    {
                        "name": "Timo Ropinski"
                    }
                ],
                "author_detail": {
                    "name": "Timo Ropinski"
                },
                "author": "Timo Ropinski",
                "arxiv_comment": "Currently, this manuscript is under revision at CGF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.14185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.14185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12746v3",
                "updated": "2025-01-30T07:11:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    11,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T09:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering"
                },
                "summary": "When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted evidence analysis to provide high-quality answers. Current\nLLM-based answer generation methods lack a detailed definition and learning\nprocess for evidence analysis, leading to the risk of error propagation and\nhallucinations while using evidence. Although increasing the parameter size of\nLLMs can alleviate these issues, it also presents challenges in model training\nand deployment with limited resources. In this study, we propose EvidenceMap,\nwhich aims to enable a tiny pre-trained language model to explicitly learn\nmultiple aspects of biomedical evidence, including supportive evaluation,\nlogical correlation and content summarization, thereby latently guiding a small\ngenerative model (around 3B parameters) to provide textual responses.\nExperimental results demonstrate that our method, fine-tuning a language model\nwith 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in\nreference-based quality and accuracy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted evidence analysis to provide high-quality answers. Current\nLLM-based answer generation methods lack a detailed definition and learning\nprocess for evidence analysis, leading to the risk of error propagation and\nhallucinations while using evidence. Although increasing the parameter size of\nLLMs can alleviate these issues, it also presents challenges in model training\nand deployment with limited resources. In this study, we propose EvidenceMap,\nwhich aims to enable a tiny pre-trained language model to explicitly learn\nmultiple aspects of biomedical evidence, including supportive evaluation,\nlogical correlation and content summarization, thereby latently guiding a small\ngenerative model (around 3B parameters) to provide textual responses.\nExperimental results demonstrate that our method, fine-tuning a language model\nwith 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in\nreference-based quality and accuracy, respectively."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Jian Wan"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v3",
                "updated": "2025-01-30T07:08:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    8,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation"
                },
                "summary": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "arxiv_comment": "Name change: LlamaFusion to LMFusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v3",
                "updated": "2025-01-30T06:46:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    46,
                    59,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15797v2",
                "updated": "2025-01-30T06:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T05:46:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models"
                },
                "summary": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language."
                },
                "authors": [
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Hongyi Zhao"
                    },
                    {
                        "name": "Tianshuo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianshuo Yang"
                },
                "author": "Tianshuo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18160v1",
                "updated": "2025-01-30T05:56:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    56,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:56:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    56,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing"
                },
                "summary": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios.\n  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable\nprecise and efficient repository-level code auditing. Equipped with the agent\nmemory, RepoAudit explores the code repository on demand, analyzing data-flow\nfacts along different feasible program paths in individual functions. It also\nintroduces the validator to check the data-flow facts for hallucination\nmitigation and examine the satisfiability of path conditions of potential buggy\npaths, which enables RepoAudit to discard false positives in the code auditing.\nOur experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully\nfinds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per\nproject on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios.\n  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable\nprecise and efficient repository-level code auditing. Equipped with the agent\nmemory, RepoAudit explores the code repository on demand, analyzing data-flow\nfacts along different feasible program paths in individual functions. It also\nintroduces the validator to check the data-flow facts for hallucination\nmitigation and examine the satisfiability of path conditions of potential buggy\npaths, which enables RepoAudit to discard false positives in the code auditing.\nOur experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully\nfinds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per\nproject on average."
                },
                "authors": [
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 8 tables, 5 figures, 3 listings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18158v1",
                "updated": "2025-01-30T05:48:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:48:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study"
                },
                "summary": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data."
                },
                "authors": [
                    {
                        "name": "Yuchen Lei"
                    },
                    {
                        "name": "Yuexin Xiang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Rafael Dowsley"
                    },
                    {
                        "name": "Tsz Hon Yuen"
                    },
                    {
                        "name": "Jiangshan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangshan Yu"
                },
                "author": "Jiangshan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18157v1",
                "updated": "2025-01-30T05:46:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    46,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:46:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    46,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment"
                },
                "summary": "Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%."
                },
                "authors": [
                    {
                        "name": "Joanna Hong"
                    },
                    {
                        "name": "Sanjeel Parekh"
                    },
                    {
                        "name": "Honglie Chen"
                    },
                    {
                        "name": "Jacob Donley"
                    },
                    {
                        "name": "Ke Tan"
                    },
                    {
                        "name": "Buye Xu"
                    },
                    {
                        "name": "Anurag Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Kumar"
                },
                "author": "Anurag Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18154v1",
                "updated": "2025-01-30T05:39:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    39,
                    1,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:39:01Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    39,
                    1,
                    3,
                    30,
                    0
                ],
                "title": "Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models"
                },
                "summary": "Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Yichen Xiao"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Hongyang Zhao"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Malu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Malu Zhang"
                },
                "author": "Malu Zhang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18145v1",
                "updated": "2025-01-30T05:26:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    26,
                    32,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:26:32Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    26,
                    32,
                    3,
                    30,
                    0
                ],
                "title": "Utilizing API Response for Test Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing API Response for Test Refinement"
                },
                "summary": "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools."
                },
                "authors": [
                    {
                        "name": "Devika Sondhi"
                    },
                    {
                        "name": "Ananya Sharma"
                    },
                    {
                        "name": "Diptikalyan Saha"
                    }
                ],
                "author_detail": {
                    "name": "Diptikalyan Saha"
                },
                "author": "Diptikalyan Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14489v2",
                "updated": "2025-01-30T05:09:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    9,
                    17,
                    3,
                    30,
                    0
                ],
                "published": "2024-12-19T03:26:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    26,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "QADM-Net: Multi-Level Quality-Adaptive Dynamic Network for Reliable\n  Multimodal Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QADM-Net: Multi-Level Quality-Adaptive Dynamic Network for Reliable\n  Multimodal Classification"
                },
                "summary": "Multimodal machine learning has achieved remarkable progress in many\nscenarios, but its reliability is undermined by varying sample quality. In this\npaper, we find that current multimodal classification methods lack dynamic\nnetworks for sample-specific depth and parameters to achieve reliable\ninference. To this end, a novel framework for multimodal reliable\nclassification termed Multi-Level Quality-Adaptive Dynamic Multimodal Network\n(QADM-Net) is proposed. QADM-Net first adopts a novel approach based on\nnoise-free prototypes and a classifier-free design to reliably estimate the\nquality of each sample at both modality and feature levels. It then achieves\nsample-specific network depth via the \\textbf{\\textit{Global Confidence\nNormalized Depth (GCND)}} mechanism. By normalizing depth across modalities and\nsamples, \\textit{\\textbf{GCND}} effectively mitigates the impact of challenging\nmodality inputs on dynamic depth reliability. Furthermore, QADM-Net provides\nsample-adaptive network parameters via the \\textbf{\\textit{Layer-wise Greedy\nParameter (LGP)}} mechanism driven by feature-level quality. The cross-modality\nlayer-wise greedy strategy in \\textbf{\\textit{LGP}} designs a reliable\nparameter prediction paradigm for multimodal networks with variable depths for\nthe first time. Experiments conducted on four datasets demonstrate that\nQADM-Net significantly outperforms state-of-the-art methods in classification\nperformance and reliability, exhibiting strong adaptability to data with\ndiverse quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal machine learning has achieved remarkable progress in many\nscenarios, but its reliability is undermined by varying sample quality. In this\npaper, we find that current multimodal classification methods lack dynamic\nnetworks for sample-specific depth and parameters to achieve reliable\ninference. To this end, a novel framework for multimodal reliable\nclassification termed Multi-Level Quality-Adaptive Dynamic Multimodal Network\n(QADM-Net) is proposed. QADM-Net first adopts a novel approach based on\nnoise-free prototypes and a classifier-free design to reliably estimate the\nquality of each sample at both modality and feature levels. It then achieves\nsample-specific network depth via the \\textbf{\\textit{Global Confidence\nNormalized Depth (GCND)}} mechanism. By normalizing depth across modalities and\nsamples, \\textit{\\textbf{GCND}} effectively mitigates the impact of challenging\nmodality inputs on dynamic depth reliability. Furthermore, QADM-Net provides\nsample-adaptive network parameters via the \\textbf{\\textit{Layer-wise Greedy\nParameter (LGP)}} mechanism driven by feature-level quality. The cross-modality\nlayer-wise greedy strategy in \\textbf{\\textit{LGP}} designs a reliable\nparameter prediction paradigm for multimodal networks with variable depths for\nthe first time. Experiments conducted on four datasets demonstrate that\nQADM-Net significantly outperforms state-of-the-art methods in classification\nperformance and reliability, exhibiting strong adaptability to data with\ndiverse quality."
                },
                "authors": [
                    {
                        "name": "Shu Shen"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18134v1",
                "updated": "2025-01-30T04:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    54,
                    59,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T04:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    54,
                    59,
                    3,
                    30,
                    0
                ],
                "title": "Nonlocal prior mixture-based Bayesian wavelet regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal prior mixture-based Bayesian wavelet regression"
                },
                "summary": "We propose a novel Bayesian wavelet regression approach using a\nthree-component spike-and-slab prior for wavelet coefficients, combining a\npoint mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior.\nThis flexible prior supports small and large coefficients differently, offering\nadvantages for highly dispersed data where wavelet coefficients span multiple\nscales. The IMOM prior's heavy tails capture large coefficients, while the MOM\nprior is better suited for smaller non-zero coefficients. Further, our method\nintroduces innovative hyperparameter specifications for mixture probabilities\nand scaling parameters, including generalized logit, hyperbolic secant, and\ngeneralized normal decay for probabilities, and double exponential decay for\nscaling. Hyperparameters are estimated via an empirical Bayes approach,\nenabling posterior inference tailored to the data. Extensive simulations\ndemonstrate significant performance gains over two-component wavelet methods.\nApplications to electroencephalography and noisy audio data illustrate the\nmethod's utility in capturing complex signal characteristics. We implement our\nmethod in an R package NLPwavelet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel Bayesian wavelet regression approach using a\nthree-component spike-and-slab prior for wavelet coefficients, combining a\npoint mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior.\nThis flexible prior supports small and large coefficients differently, offering\nadvantages for highly dispersed data where wavelet coefficients span multiple\nscales. The IMOM prior's heavy tails capture large coefficients, while the MOM\nprior is better suited for smaller non-zero coefficients. Further, our method\nintroduces innovative hyperparameter specifications for mixture probabilities\nand scaling parameters, including generalized logit, hyperbolic secant, and\ngeneralized normal decay for probabilities, and double exponential decay for\nscaling. Hyperparameters are estimated via an empirical Bayes approach,\nenabling posterior inference tailored to the data. Extensive simulations\ndemonstrate significant performance gains over two-component wavelet methods.\nApplications to electroencephalography and noisy audio data illustrate the\nmethod's utility in capturing complex signal characteristics. We implement our\nmethod in an R package NLPwavelet."
                },
                "authors": [
                    {
                        "name": "Nilotpal Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Nilotpal Sanyal"
                },
                "author": "Nilotpal Sanyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13101v2",
                "updated": "2025-01-30T04:51:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    51,
                    41,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-18T02:19:00Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    2,
                    19,
                    0,
                    3,
                    200,
                    0
                ],
                "title": "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach"
                },
                "summary": "Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length."
                },
                "authors": [
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Zhang"
                },
                "author": "Zhiqiang Zhang",
                "arxiv_comment": "Accepted by WWW2025 Agent4IR Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18596v1",
                "updated": "2025-01-30T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights"
                },
                "summary": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical."
                },
                "authors": [
                    {
                        "name": "Liana Mikaelyan"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mathew Salvaris"
                    },
                    {
                        "name": "Parth Pathak"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Fayyaz"
                },
                "author": "Mohsen Fayyaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18594v1",
                "updated": "2025-01-30T18:59:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    43,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:59:43Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    43,
                    3,
                    30,
                    0
                ],
                "title": "Foundational Models for 3D Point Clouds: A Survey and Outlook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Models for 3D Point Clouds: A Survey and Outlook"
                },
                "summary": "The 3D point cloud representation plays a crucial role in preserving the\ngeometric fidelity of the physical world, enabling more accurate complex 3D\nenvironments. While humans naturally comprehend the intricate relationships\nbetween objects and variations through a multisensory system, artificial\nintelligence (AI) systems have yet to fully replicate this capacity. To bridge\nthis gap, it becomes essential to incorporate multiple modalities. Models that\ncan seamlessly integrate and reason across these modalities are known as\nfoundation models (FMs). The development of FMs for 2D modalities, such as\nimages and text, has seen significant progress, driven by the abundant\navailability of large-scale datasets. However, the 3D domain has lagged due to\nthe scarcity of labelled data and high computational overheads. In response,\nrecent research has begun to explore the potential of applying FMs to 3D tasks,\novercoming these challenges by leveraging existing 2D knowledge. Additionally,\nlanguage, with its capacity for abstract reasoning and description of the\nenvironment, offers a promising avenue for enhancing 3D understanding through\nlarge pre-trained language models (LLMs). Despite the rapid development and\nadoption of FMs for 3D vision tasks in recent years, there remains a gap in\ncomprehensive and in-depth literature reviews. This article aims to address\nthis gap by presenting a comprehensive overview of the state-of-the-art methods\nthat utilize FMs for 3D visual understanding. We start by reviewing various\nstrategies employed in the building of various 3D FMs. Then we categorize and\nsummarize use of different FMs for tasks such as perception tasks. Finally, the\narticle offers insights into future directions for research and development in\nthis field. To help reader, we have curated list of relevant papers on the\ntopic: https://github.com/vgthengane/Awesome-FMs-in-3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 3D point cloud representation plays a crucial role in preserving the\ngeometric fidelity of the physical world, enabling more accurate complex 3D\nenvironments. While humans naturally comprehend the intricate relationships\nbetween objects and variations through a multisensory system, artificial\nintelligence (AI) systems have yet to fully replicate this capacity. To bridge\nthis gap, it becomes essential to incorporate multiple modalities. Models that\ncan seamlessly integrate and reason across these modalities are known as\nfoundation models (FMs). The development of FMs for 2D modalities, such as\nimages and text, has seen significant progress, driven by the abundant\navailability of large-scale datasets. However, the 3D domain has lagged due to\nthe scarcity of labelled data and high computational overheads. In response,\nrecent research has begun to explore the potential of applying FMs to 3D tasks,\novercoming these challenges by leveraging existing 2D knowledge. Additionally,\nlanguage, with its capacity for abstract reasoning and description of the\nenvironment, offers a promising avenue for enhancing 3D understanding through\nlarge pre-trained language models (LLMs). Despite the rapid development and\nadoption of FMs for 3D vision tasks in recent years, there remains a gap in\ncomprehensive and in-depth literature reviews. This article aims to address\nthis gap by presenting a comprehensive overview of the state-of-the-art methods\nthat utilize FMs for 3D visual understanding. We start by reviewing various\nstrategies employed in the building of various 3D FMs. Then we categorize and\nsummarize use of different FMs for tasks such as perception tasks. Finally, the\narticle offers insights into future directions for research and development in\nthis field. To help reader, we have curated list of relevant papers on the\ntopic: https://github.com/vgthengane/Awesome-FMs-in-3D."
                },
                "authors": [
                    {
                        "name": "Vishal Thengane"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Salim Bouzerdoum"
                    },
                    {
                        "name": "Son Lam Phung"
                    },
                    {
                        "name": "Yunpeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Li"
                },
                "author": "Yunpeng Li",
                "arxiv_comment": "Initial submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18585v1",
                "updated": "2025-01-30T18:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs"
                },
                "summary": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18576v1",
                "updated": "2025-01-30T18:45:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    45,
                    51,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    45,
                    51,
                    3,
                    30,
                    0
                ],
                "title": "Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for\n  Multi-Step Reasoning Over Speed in MATH",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for\n  Multi-Step Reasoning Over Speed in MATH"
                },
                "summary": "This study investigates the performance of the DeepSeek R1 language model on\n30 challenging mathematical problems derived from the MATH dataset, problems\nthat previously proved unsolvable by other models under time constraints.\nUnlike prior work, this research removes time limitations to explore whether\nDeepSeek R1's architecture, known for its reliance on token-based reasoning,\ncan achieve accurate solutions through a multi-step process. The study compares\nDeepSeek R1 with four other models (gemini-1.5-flash-8b,\ngpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11\ntemperature settings. Results demonstrate that DeepSeek R1 achieves superior\naccuracy on these complex problems but generates significantly more tokens than\nother models, confirming its token-intensive approach. The findings highlight a\ntrade-off between accuracy and efficiency in mathematical problem-solving with\nlarge language models: while DeepSeek R1 excels in accuracy, its reliance on\nextensive token generation may not be optimal for applications requiring rapid\nresponses. The study underscores the importance of considering task-specific\nrequirements when selecting an LLM and emphasizes the role of temperature\nsettings in optimizing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the performance of the DeepSeek R1 language model on\n30 challenging mathematical problems derived from the MATH dataset, problems\nthat previously proved unsolvable by other models under time constraints.\nUnlike prior work, this research removes time limitations to explore whether\nDeepSeek R1's architecture, known for its reliance on token-based reasoning,\ncan achieve accurate solutions through a multi-step process. The study compares\nDeepSeek R1 with four other models (gemini-1.5-flash-8b,\ngpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11\ntemperature settings. Results demonstrate that DeepSeek R1 achieves superior\naccuracy on these complex problems but generates significantly more tokens than\nother models, confirming its token-intensive approach. The findings highlight a\ntrade-off between accuracy and efficiency in mathematical problem-solving with\nlarge language models: while DeepSeek R1 excels in accuracy, its reliance on\nextensive token generation may not be optimal for applications requiring rapid\nresponses. The study underscores the importance of considering task-specific\nrequirements when selecting an LLM and emphasizes the role of temperature\nsettings in optimizing performance."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "5 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18555v1",
                "updated": "2025-01-30T18:32:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    32,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:32:46Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    32,
                    46,
                    3,
                    30,
                    0
                ],
                "title": "An Empirical Study of Dotfiles Repositories Containing User-Specific\n  Configuration Files",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Dotfiles Repositories Containing User-Specific\n  Configuration Files"
                },
                "summary": "Storing user-specific configuration files in a \"dotfiles\" repository is a\ncommon practice among software developers, with hundreds of thousands choosing\nto publicly host their repositories on GitHub. This practice not only provides\ndevelopers with a simple backup mechanism for their essential configuration\nfiles, but also facilitates sharing ideas and learning from others on how best\nto configure applications that are key to their daily workflows. However, our\ncurrent understanding of these repository sharing practices is limited and\nmostly anecdotal. To address this gap, we conducted a study to delve deeper\ninto this phenomenon. Beginning with collecting and analyzing publicly-hosted\ndotfiles repositories on GitHub, we discovered that maintaining dotfiles is\nwidespread among developers. Notably, we found that 25.8% of the top 500\nmost-starred GitHub users maintain some form of publicly accessible dotfiles\nrepository. Among these, configurations for text editors like Vim and shells\nsuch as bash and zsh are the most commonly tracked. Our analysis reveals that\nupdating dotfiles is primarily driven by the need to adjust configurations\n(63.3%) and project meta-management (25.4%). Surprisingly, we found no\nsignificant difference in the types of dotfiles observed across code churn\nhistory patterns, suggesting that the frequency of dotfile modifications\ndepends more on the developer than the properties of the specific dotfile and\nits associated application. Finally, we discuss the challenges associated with\nmanaging dotfiles, including the necessity for a reliable and effective\ndeployment mechanism, and how the insights gleaned from dotfiles can inform\ntool designers by offering real-world usage information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storing user-specific configuration files in a \"dotfiles\" repository is a\ncommon practice among software developers, with hundreds of thousands choosing\nto publicly host their repositories on GitHub. This practice not only provides\ndevelopers with a simple backup mechanism for their essential configuration\nfiles, but also facilitates sharing ideas and learning from others on how best\nto configure applications that are key to their daily workflows. However, our\ncurrent understanding of these repository sharing practices is limited and\nmostly anecdotal. To address this gap, we conducted a study to delve deeper\ninto this phenomenon. Beginning with collecting and analyzing publicly-hosted\ndotfiles repositories on GitHub, we discovered that maintaining dotfiles is\nwidespread among developers. Notably, we found that 25.8% of the top 500\nmost-starred GitHub users maintain some form of publicly accessible dotfiles\nrepository. Among these, configurations for text editors like Vim and shells\nsuch as bash and zsh are the most commonly tracked. Our analysis reveals that\nupdating dotfiles is primarily driven by the need to adjust configurations\n(63.3%) and project meta-management (25.4%). Surprisingly, we found no\nsignificant difference in the types of dotfiles observed across code churn\nhistory patterns, suggesting that the frequency of dotfile modifications\ndepends more on the developer than the properties of the specific dotfile and\nits associated application. Finally, we discuss the challenges associated with\nmanaging dotfiles, including the necessity for a reliable and effective\ndeployment mechanism, and how the insights gleaned from dotfiles can inform\ntool designers by offering real-world usage information."
                },
                "authors": [
                    {
                        "name": "Wenhan Zhu"
                    },
                    {
                        "name": "Michael W. Godfrey"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Godfrey"
                },
                "author": "Michael W. Godfrey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18542v1",
                "updated": "2025-01-30T18:10:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    10,
                    16,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    10,
                    16,
                    3,
                    30,
                    0
                ],
                "title": "Semantic Web and Creative AI -- A Technical Report from ISWS 2023",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Web and Creative AI -- A Technical Report from ISWS 2023"
                },
                "summary": "The International Semantic Web Research School (ISWS) is a week-long\nintensive program designed to immerse participants in the field. This document\nreports a collaborative effort performed by ten teams of students, each guided\nby a senior researcher as their mentor, attending ISWS 2023. Each team provided\na different perspective to the topic of creative AI, substantiated by a set of\nresearch questions as the main subject of their investigation. The 2023 edition\nof ISWS focuses on the intersection of Semantic Web technologies and Creative\nAI. ISWS 2023 explored various intersections between Semantic Web technologies\nand creative AI. A key area of focus was the potential of LLMs as support tools\nfor knowledge engineering. Participants also delved into the multifaceted\napplications of LLMs, including legal aspects of creative content production,\nhumans in the loop, decentralised approaches to multimodal generative AI\nmodels, nanopublications and AI for personal scientific knowledge graphs,\ncommonsense knowledge in automatic story and narrative completion, generative\nAI for art critique, prompt engineering, automatic music composition,\ncommonsense prototyping and conceptual blending, and elicitation of tacit\nknowledge. As Large Language Models and semantic technologies continue to\nevolve, new exciting prospects are emerging: a future where the boundaries\nbetween creative expression and factual knowledge become increasingly permeable\nand porous, leading to a world of knowledge that is both informative and\ninspiring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Semantic Web Research School (ISWS) is a week-long\nintensive program designed to immerse participants in the field. This document\nreports a collaborative effort performed by ten teams of students, each guided\nby a senior researcher as their mentor, attending ISWS 2023. Each team provided\na different perspective to the topic of creative AI, substantiated by a set of\nresearch questions as the main subject of their investigation. The 2023 edition\nof ISWS focuses on the intersection of Semantic Web technologies and Creative\nAI. ISWS 2023 explored various intersections between Semantic Web technologies\nand creative AI. A key area of focus was the potential of LLMs as support tools\nfor knowledge engineering. Participants also delved into the multifaceted\napplications of LLMs, including legal aspects of creative content production,\nhumans in the loop, decentralised approaches to multimodal generative AI\nmodels, nanopublications and AI for personal scientific knowledge graphs,\ncommonsense knowledge in automatic story and narrative completion, generative\nAI for art critique, prompt engineering, automatic music composition,\ncommonsense prototyping and conceptual blending, and elicitation of tacit\nknowledge. As Large Language Models and semantic technologies continue to\nevolve, new exciting prospects are emerging: a future where the boundaries\nbetween creative expression and factual knowledge become increasingly permeable\nand porous, leading to a world of knowledge that is both informative and\ninspiring."
                },
                "authors": [
                    {
                        "name": "Raia Abu Ahmad"
                    },
                    {
                        "name": "Reham Alharbi"
                    },
                    {
                        "name": "Roberto Barile"
                    },
                    {
                        "name": "Martin Bckling"
                    },
                    {
                        "name": "Francisco Bolanos"
                    },
                    {
                        "name": "Sara Bonfitto"
                    },
                    {
                        "name": "Oleksandra Bruns"
                    },
                    {
                        "name": "Irene Celino"
                    },
                    {
                        "name": "Yashrajsinh Chudasama"
                    },
                    {
                        "name": "Martin Critelli"
                    },
                    {
                        "name": "Claudia d'Amato"
                    },
                    {
                        "name": "Giada D'Ippolito"
                    },
                    {
                        "name": "Ioannis Dasoulas"
                    },
                    {
                        "name": "Stefano De Giorgis"
                    },
                    {
                        "name": "Vincenzo De Leo"
                    },
                    {
                        "name": "Chiara Di Bonaventura"
                    },
                    {
                        "name": "Marco Di Panfilo"
                    },
                    {
                        "name": "Daniil Dobriy"
                    },
                    {
                        "name": "John Domingue"
                    },
                    {
                        "name": "Xuemin Duan"
                    },
                    {
                        "name": "Michel Dumontier"
                    },
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Ruben Eschauzier"
                    },
                    {
                        "name": "Fakih Ginwa"
                    },
                    {
                        "name": "Nicolas Ferranti"
                    },
                    {
                        "name": "Arianna Graciotti"
                    },
                    {
                        "name": "Philipp Hanisch"
                    },
                    {
                        "name": "George Hannah"
                    },
                    {
                        "name": "Golsa Heidari"
                    },
                    {
                        "name": "Aidan Hogan"
                    },
                    {
                        "name": "Hassan Hussein"
                    },
                    {
                        "name": "Alexane Jouglar"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    },
                    {
                        "name": "Mano Kieffer"
                    },
                    {
                        "name": "Antonis Klironomos"
                    },
                    {
                        "name": "Ins Koch"
                    },
                    {
                        "name": "Weronika Lajewska"
                    },
                    {
                        "name": "Nicolas Lazzari"
                    },
                    {
                        "name": "Mikael Lindekrans"
                    },
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Majlinda Llugiqi"
                    },
                    {
                        "name": "Eleonora Mancini"
                    },
                    {
                        "name": "Eleonora Marzi"
                    },
                    {
                        "name": "Laura Menotti"
                    },
                    {
                        "name": "Daniela Milon Flores"
                    },
                    {
                        "name": "Soulakshmee Nagowah"
                    },
                    {
                        "name": "Kerstin Neubert"
                    },
                    {
                        "name": "Emetis Niazmand"
                    },
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Beatriz Olarte Martinez"
                    },
                    {
                        "name": "Anouk Michelle Oudshoorn"
                    },
                    {
                        "name": "Andrea Poltronieri"
                    },
                    {
                        "name": "Valentina Presutti"
                    },
                    {
                        "name": "Disha Purohit"
                    },
                    {
                        "name": "Ensiyeh Raoufi"
                    },
                    {
                        "name": "Celian Ringwald"
                    },
                    {
                        "name": "Johanna Rockstroh"
                    },
                    {
                        "name": "Sebastian Rudolph"
                    },
                    {
                        "name": "Harald Sack"
                    },
                    {
                        "name": "Zafar Saeed"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Aya Sahbi"
                    },
                    {
                        "name": "Cristian Santini"
                    },
                    {
                        "name": "Aleksandra Simic"
                    },
                    {
                        "name": "Dennis Sommer"
                    },
                    {
                        "name": "Rita Sousa"
                    },
                    {
                        "name": "Mary Ann Tan"
                    },
                    {
                        "name": "Vidyashree Tarikere"
                    },
                    {
                        "name": "Tabea Tietz"
                    },
                    {
                        "name": "Liam Tirpitz"
                    },
                    {
                        "name": "Arnaldo Tomasino"
                    },
                    {
                        "name": "Frank van Harmelen"
                    },
                    {
                        "name": "Joao Vissoci"
                    },
                    {
                        "name": "Caitlin Woods"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Heng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zheng"
                },
                "author": "Heng Zheng",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18539v1",
                "updated": "2025-01-30T18:07:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    7,
                    19,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:07:19Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    7,
                    19,
                    3,
                    30,
                    0
                ],
                "title": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method"
                },
                "summary": "Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18536v1",
                "updated": "2025-01-30T18:02:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    2,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T18:02:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    2,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "Illusions of Relevance: Using Content Injection Attacks to Deceive\n  Retrievers, Rerankers, and LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illusions of Relevance: Using Content Injection Attacks to Deceive\n  Retrievers, Rerankers, and LLM Judges"
                },
                "summary": "Consider a scenario in which a user searches for information, only to\nencounter texts flooded with misleading or non-relevant content. This scenario\nexemplifies a simple yet potent vulnerability in neural Information Retrieval\n(IR) pipelines: content injection attacks. We find that embedding models for\nretrieval, rerankers, and large language model (LLM) relevance judges are\nvulnerable to these attacks, in which adversaries insert misleading text into\npassages to manipulate model judgements. We identify two primary threats: (1)\ninserting unrelated or harmful content within passages that still appear\ndeceptively \"relevant\", and (2) inserting entire queries or key query terms\ninto passages to boost their perceived relevance. While the second tactic has\nbeen explored in prior research, we present, to our knowledge, the first\nempirical analysis of the first threat, demonstrating how state-of-the-art\nmodels can be easily misled. Our study systematically examines the factors that\ninfluence an attack's success, such as the placement of injected content and\nthe balance between relevant and non-relevant material. Additionally, we\nexplore various defense strategies, including adversarial passage classifiers,\nretriever fine-tuning to discount manipulated content, and prompting LLM judges\nto adopt a more cautious approach. However, we find that these countermeasures\noften involve trade-offs, sacrificing effectiveness for attack robustness and\nsometimes penalizing legitimate documents in the process. Our findings\nhighlight the need for stronger defenses against these evolving adversarial\nstrategies to maintain the trustworthiness of IR systems. We release our code\nand scripts to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider a scenario in which a user searches for information, only to\nencounter texts flooded with misleading or non-relevant content. This scenario\nexemplifies a simple yet potent vulnerability in neural Information Retrieval\n(IR) pipelines: content injection attacks. We find that embedding models for\nretrieval, rerankers, and large language model (LLM) relevance judges are\nvulnerable to these attacks, in which adversaries insert misleading text into\npassages to manipulate model judgements. We identify two primary threats: (1)\ninserting unrelated or harmful content within passages that still appear\ndeceptively \"relevant\", and (2) inserting entire queries or key query terms\ninto passages to boost their perceived relevance. While the second tactic has\nbeen explored in prior research, we present, to our knowledge, the first\nempirical analysis of the first threat, demonstrating how state-of-the-art\nmodels can be easily misled. Our study systematically examines the factors that\ninfluence an attack's success, such as the placement of injected content and\nthe balance between relevant and non-relevant material. Additionally, we\nexplore various defense strategies, including adversarial passage classifiers,\nretriever fine-tuning to discount manipulated content, and prompting LLM judges\nto adopt a more cautious approach. However, we find that these countermeasures\noften involve trade-offs, sacrificing effectiveness for attack robustness and\nsometimes penalizing legitimate documents in the process. Our findings\nhighlight the need for stronger defenses against these evolving adversarial\nstrategies to maintain the trustworthiness of IR systems. We release our code\nand scripts to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Manveer Singh Tamber"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18533v1",
                "updated": "2025-01-30T17:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    59,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    59,
                    45,
                    3,
                    30,
                    0
                ],
                "title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models"
                },
                "summary": "Large Vision-Language Models (VLMs) have achieved remarkable performance\nacross a wide range of tasks. However, their deployment in safety-critical\ndomains poses significant challenges. Existing safety fine-tuning methods,\nwhich focus on textual or multimodal content, fall short in addressing\nchallenging cases or disrupt the balance between helpfulness and harmlessness.\nOur evaluation highlights a safety reasoning gap: these methods lack safety\nvisual reasoning ability, leading to such bottlenecks. To address this\nlimitation and enhance both visual perception and reasoning in safety-critical\ncontexts, we propose a novel dataset that integrates multi-image inputs with\nsafety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve\nmodel performance. Specifically, we introduce the Multi-Image Safety (MIS)\ndataset, an instruction-following dataset tailored for multi-image safety\nscenarios, consisting of training and test splits. Our experiments demonstrate\nthat fine-tuning InternVL2.5-8B with MIS significantly outperforms both\npowerful open-source models and API-based models in challenging multi-image\ntasks requiring safety-related visual reasoning. This approach not only\ndelivers exceptional safety performance but also preserves general capabilities\nwithout any trade-offs. Specifically, fine-tuning with MIS increases average\naccuracy by 0.83% across five general benchmarks and reduces the Attack Success\nRate (ASR) on multiple safety benchmarks by a large margin. Data and Models are\nreleased under:\n\\href{https://dripnowhy.github.io/MIS/}{\\texttt{https://dripnowhy.github.io/MIS/}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) have achieved remarkable performance\nacross a wide range of tasks. However, their deployment in safety-critical\ndomains poses significant challenges. Existing safety fine-tuning methods,\nwhich focus on textual or multimodal content, fall short in addressing\nchallenging cases or disrupt the balance between helpfulness and harmlessness.\nOur evaluation highlights a safety reasoning gap: these methods lack safety\nvisual reasoning ability, leading to such bottlenecks. To address this\nlimitation and enhance both visual perception and reasoning in safety-critical\ncontexts, we propose a novel dataset that integrates multi-image inputs with\nsafety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve\nmodel performance. Specifically, we introduce the Multi-Image Safety (MIS)\ndataset, an instruction-following dataset tailored for multi-image safety\nscenarios, consisting of training and test splits. Our experiments demonstrate\nthat fine-tuning InternVL2.5-8B with MIS significantly outperforms both\npowerful open-source models and API-based models in challenging multi-image\ntasks requiring safety-related visual reasoning. This approach not only\ndelivers exceptional safety performance but also preserves general capabilities\nwithout any trade-offs. Specifically, fine-tuning with MIS increases average\naccuracy by 0.83% across five general benchmarks and reduces the Attack Success\nRate (ASR) on multiple safety benchmarks by a large margin. Data and Models are\nreleased under:\n\\href{https://dripnowhy.github.io/MIS/}{\\texttt{https://dripnowhy.github.io/MIS/}}"
                },
                "authors": [
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Bing Cao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17635v2",
                "updated": "2025-01-30T17:59:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    59,
                    8,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-29T13:12:01Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "title": "In-Context Meta LoRA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Meta LoRA Generation"
                },
                "summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jingcai Guo"
                },
                "author": "Jingcai Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18532v1",
                "updated": "2025-01-30T17:58:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    58,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:58:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    58,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "Differentially Private Steering for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Steering for Large Language Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe \\textit{\\underline{P}rivate \\underline{S}teering for LLM\n\\underline{A}lignment (PSA)} algorithm to edit LLM activations with\ndifferential privacy (DP) guarantees. We conduct extensive experiments on seven\ndifferent benchmarks with open-source LLMs of different sizes (0.5B to 7B) and\nmodel families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA\nachieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our attack is tailored for activation\nediting and relies solely on the generated texts without their associated\nprobabilities. Our experiments support the theoretical guarantees by showing\nimproved guarantees for our \\textit{PSA} algorithm compared to several existing\nnon-private techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe \\textit{\\underline{P}rivate \\underline{S}teering for LLM\n\\underline{A}lignment (PSA)} algorithm to edit LLM activations with\ndifferential privacy (DP) guarantees. We conduct extensive experiments on seven\ndifferent benchmarks with open-source LLMs of different sizes (0.5B to 7B) and\nmodel families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA\nachieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our attack is tailored for activation\nediting and relies solely on the generated texts without their associated\nprobabilities. Our experiments support the theoretical guarantees by showing\nimproved guarantees for our \\textit{PSA} algorithm compared to several existing\nnon-private techniques."
                },
                "authors": [
                    {
                        "name": "Anmol Goel"
                    },
                    {
                        "name": "Yaxi Hu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Amartya Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Amartya Sanyal"
                },
                "author": "Amartya Sanyal",
                "arxiv_comment": "ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v3",
                "updated": "2025-01-30T17:50:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    50,
                    16,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06595v3",
                "updated": "2025-01-30T17:34:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    34,
                    51,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-10T15:39:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations."
                },
                "authors": [
                    {
                        "name": "Sacha Muller"
                    },
                    {
                        "name": "Antnio Loison"
                    },
                    {
                        "name": "Bilel Omrani"
                    },
                    {
                        "name": "Gautier Viaud"
                    }
                ],
                "author_detail": {
                    "name": "Gautier Viaud"
                },
                "author": "Gautier Viaud",
                "arxiv_comment": "Proceedings of the 31st International Conference on Computational\n  Linguistics",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics (2025), pages 4510 to 4534, Abu Dhabi, UAE. Association for\n  Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18516v1",
                "updated": "2025-01-30T17:28:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:28:11Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "title": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models"
                },
                "summary": "Object rearrangement is a significant task for collaborative robots, where\nthey are directed to manipulate objects into a specified goal state.\nDetermining the placement of objects is a major challenge that influences the\nefficiency of the rearrangement process. Most current methods heavily rely on\npre-collected datasets to train the model for predicting the goal position and\nare restricted to specific instructions, which limits their broader\napplicability and effectiveness.In this paper, we propose a framework of\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Particularly, our approach mimics human reasoning by using past\nsuccessful experiences as a reference to infer the desired goal position. Based\non LLM's strong natural language comprehension and inference ability, our\nmethod can generalise to handle various everyday objects and free-form language\ninstructions in a zero-shot manner. Experimental results demonstrate that our\nmethods can effectively execute the robotic rearrangement tasks, even those\ninvolving long sequential orders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object rearrangement is a significant task for collaborative robots, where\nthey are directed to manipulate objects into a specified goal state.\nDetermining the placement of objects is a major challenge that influences the\nefficiency of the rearrangement process. Most current methods heavily rely on\npre-collected datasets to train the model for predicting the goal position and\nare restricted to specific instructions, which limits their broader\napplicability and effectiveness.In this paper, we propose a framework of\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Particularly, our approach mimics human reasoning by using past\nsuccessful experiences as a reference to infer the desired goal position. Based\non LLM's strong natural language comprehension and inference ability, our\nmethod can generalise to handle various everyday objects and free-form language\ninstructions in a zero-shot manner. Experimental results demonstrate that our\nmethods can effectively execute the robotic rearrangement tasks, even those\ninvolving long sequential orders."
                },
                "authors": [
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Ryan Mckenna"
                    },
                    {
                        "name": "John Oyekan"
                    }
                ],
                "author_detail": {
                    "name": "John Oyekan"
                },
                "author": "John Oyekan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18512v1",
                "updated": "2025-01-30T17:23:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    23,
                    50,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:23:50Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    23,
                    50,
                    3,
                    30,
                    0
                ],
                "title": "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch"
                },
                "summary": "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Arthur Douillard"
                    },
                    {
                        "name": "Yanislav Donchev"
                    },
                    {
                        "name": "Keith Rush"
                    },
                    {
                        "name": "Satyen Kale"
                    },
                    {
                        "name": "Zachary Charles"
                    },
                    {
                        "name": "Zachary Garrett"
                    },
                    {
                        "name": "Gabriel Teston"
                    },
                    {
                        "name": "Dave Lacey"
                    },
                    {
                        "name": "Ross McIlroy"
                    },
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Arthur Szlam"
                    },
                    {
                        "name": "Marc'Aurelio Ranzato"
                    },
                    {
                        "name": "Paul Barham"
                    }
                ],
                "author_detail": {
                    "name": "Paul Barham"
                },
                "author": "Paul Barham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18511v1",
                "updated": "2025-01-30T17:21:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    21,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:21:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    21,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training"
                },
                "summary": "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Chinmay Hegde"
                    }
                ],
                "author_detail": {
                    "name": "Chinmay Hegde"
                },
                "author": "Chinmay Hegde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18504v1",
                "updated": "2025-01-30T17:13:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:13:32Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "title": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction"
                },
                "summary": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_comment": "9 pages plus 2 pages of supplemental material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18492v1",
                "updated": "2025-01-30T17:06:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    6,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T17:06:06Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    6,
                    6,
                    3,
                    30,
                    0
                ],
                "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuardReasoner: Towards Reasoning-based LLM Safeguards"
                },
                "summary": "As LLMs increasingly impact safety-critical applications, ensuring their\nsafety using guardrails remains a key challenge. This paper proposes\nGuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to\nreason. Concretely, we first create the GuardReasonerTrain dataset, which\nconsists of 127K samples with 460K detailed reasoning steps. Then, we introduce\nreasoning SFT to unlock the reasoning capability of guard models. In addition,\nwe present hard sample DPO to further strengthen their reasoning ability. In\nthis manner, GuardReasoner achieves better performance, explainability, and\ngeneralizability. Extensive experiments and analyses on 13 benchmarks of 3\nguardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B\nsurpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on\naverage. We release the training data, code, and models with different scales\n(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs increasingly impact safety-critical applications, ensuring their\nsafety using guardrails remains a key challenge. This paper proposes\nGuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to\nreason. Concretely, we first create the GuardReasonerTrain dataset, which\nconsists of 127K samples with 460K detailed reasoning steps. Then, we introduce\nreasoning SFT to unlock the reasoning capability of guard models. In addition,\nwe present hard sample DPO to further strengthen their reasoning ability. In\nthis manner, GuardReasoner achieves better performance, explainability, and\ngeneralizability. Extensive experiments and analyses on 13 benchmarks of 3\nguardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B\nsurpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on\naverage. We release the training data, code, and models with different scales\n(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Shengfang Zhai"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiwei Xue"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "22 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18482v1",
                "updated": "2025-01-30T16:56:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    56,
                    8,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:56:08Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    56,
                    8,
                    3,
                    30,
                    0
                ],
                "title": "A Tool for In-depth Analysis of Code Execution Reasoning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tool for In-depth Analysis of Code Execution Reasoning of Large\n  Language Models"
                },
                "summary": "Code Executing Reasoning is becoming a new non-functional metric that\nassesses the ability of large language models (LLMs) in programming tasks.\nState-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval)\nusually focus on LLM's prediction of a given code's input/output or\nintermediate variable states/values on limited programs. However, there is no\ntool for more in-depth analysis of the results. Without such a tool, the\nobservations about LLM's code execution reasoning cannot be generalized to more\ndatasets, preventing the research community and practitioners from devising the\nnext generation of LLMs with better code execution reasoning abilities. This\npaper introduces ExeRScope, a series of tools and heuristics to analyze the\nresult of code execution reasoning frameworks to understand better the impact\nof code properties in the studied benchmarks on the code execution reasoning.\nWith such tooling, analysis can be generalized to code with similar properties\nwithout the urgent need to design more benchmarks, which is a cumbersome\neffort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Executing Reasoning is becoming a new non-functional metric that\nassesses the ability of large language models (LLMs) in programming tasks.\nState-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval)\nusually focus on LLM's prediction of a given code's input/output or\nintermediate variable states/values on limited programs. However, there is no\ntool for more in-depth analysis of the results. Without such a tool, the\nobservations about LLM's code execution reasoning cannot be generalized to more\ndatasets, preventing the research community and practitioners from devising the\nnext generation of LLMs with better code execution reasoning abilities. This\npaper introduces ExeRScope, a series of tools and heuristics to analyze the\nresult of code execution reasoning frameworks to understand better the impact\nof code properties in the studied benchmarks on the code execution reasoning.\nWith such tooling, analysis can be generalized to code with similar properties\nwithout the urgent need to design more benchmarks, which is a cumbersome\neffort."
                },
                "authors": [
                    {
                        "name": "Changshu Liu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07291v2",
                "updated": "2025-01-30T16:53:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    53,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2024-08-14T04:49:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Personal Information Extraction and Countermeasures"
                },
                "summary": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect four datasets including a synthetic dataset generated by GPT-4 and\nthree real-world datasets with manually labeled eight categories of personal\ninformation; introduce a novel mitigation strategy based on prompt injection;\nand systematically benchmark LLM-based attacks and countermeasures using ten\nLLMs and five datasets. Our key findings include: LLM can be misused by\nattackers to accurately extract various personal information from personal\nprofiles; LLM outperforms traditional methods; and prompt injection can defend\nagainst strong LLM-based attacks, reducing the attack to less effective\ntraditional ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect four datasets including a synthetic dataset generated by GPT-4 and\nthree real-world datasets with manually labeled eight categories of personal\ninformation; introduce a novel mitigation strategy based on prompt injection;\nand systematically benchmark LLM-based attacks and countermeasures using ten\nLLMs and five datasets. Our key findings include: LLM can be misused by\nattackers to accurately extract various personal information from personal\nprofiles; LLM outperforms traditional methods; and prompt injection can defend\nagainst strong LLM-based attacks, reducing the attack to less effective\ntraditional ones."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18475v1",
                "updated": "2025-01-30T16:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    48,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:48:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    48,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA\n  Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA\n  Initialization"
                },
                "summary": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has\nbecome a highly efficient approach for downstream tasks, particularly in\nscenarios with limited computational resources. However, applying LoRA\ntechniques to quantized LLMs poses unique challenges due to the reduced\nrepresentational precision of quantized weights. In this paper, we introduce\nCLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic\ninitialization strategy designed to overcome these challenges. Our approach\nfocuses on minimizing the layer-wise discrepancy between the original LLM and\nits quantized counterpart with LoRA components during initialization. By\nleveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and\ndetermines the optimal LoRA components for each layer, ensuring a strong\nfoundation for subsequent fine-tuning. A key contribution of this work is a\nnovel theoretical result that enables the accurate and closed-form construction\nof these optimal LoRA components. We validate the efficacy of CLoQ across\nmultiple tasks such as language generation, arithmetic reasoning, and\ncommonsense reasoning, demonstrating that it consistently outperforms existing\nLoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit\nwidths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has\nbecome a highly efficient approach for downstream tasks, particularly in\nscenarios with limited computational resources. However, applying LoRA\ntechniques to quantized LLMs poses unique challenges due to the reduced\nrepresentational precision of quantized weights. In this paper, we introduce\nCLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic\ninitialization strategy designed to overcome these challenges. Our approach\nfocuses on minimizing the layer-wise discrepancy between the original LLM and\nits quantized counterpart with LoRA components during initialization. By\nleveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and\ndetermines the optimal LoRA components for each layer, ensuring a strong\nfoundation for subsequent fine-tuning. A key contribution of this work is a\nnovel theoretical result that enables the accurate and closed-form construction\nof these optimal LoRA components. We validate the efficacy of CLoQ across\nmultiple tasks such as language generation, arithmetic reasoning, and\ncommonsense reasoning, demonstrating that it consistently outperforms existing\nLoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit\nwidths."
                },
                "authors": [
                    {
                        "name": "Yanxia Deng"
                    },
                    {
                        "name": "Aozhong Zhang"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Selcuk Gurses"
                    },
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Penghang Yin"
                    }
                ],
                "author_detail": {
                    "name": "Penghang Yin"
                },
                "author": "Penghang Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16273v2",
                "updated": "2025-01-30T16:44:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    44,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T18:06:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs"
                },
                "summary": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount."
                },
                "authors": [
                    {
                        "name": "Mohamed Elfeki"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Chad Voegele"
                    }
                ],
                "author_detail": {
                    "name": "Chad Voegele"
                },
                "author": "Chad Voegele",
                "arxiv_comment": "13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16673v2",
                "updated": "2025-01-30T16:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    40,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T03:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    18,
                    48,
                    1,
                    28,
                    0
                ],
                "title": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow"
                },
                "summary": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research."
                },
                "authors": [
                    {
                        "name": "Li Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "arxiv_affiliation": "Atlas",
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03249v3",
                "updated": "2025-01-30T16:31:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    31,
                    31,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-04T09:14:11Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    14,
                    11,
                    4,
                    278,
                    0
                ],
                "title": "How Much Can We Forget about Data Contamination?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Can We Forget about Data Contamination?"
                },
                "summary": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Suraj Srinivas"
                    },
                    {
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "name": "Ulrike von Luxburg"
                    }
                ],
                "author_detail": {
                    "name": "Ulrike von Luxburg"
                },
                "author": "Ulrike von Luxburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18460v1",
                "updated": "2025-01-30T16:18:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    18,
                    52,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:18:52Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    18,
                    52,
                    3,
                    30,
                    0
                ],
                "title": "ExeCoder: Empowering Large Language Models with Executability\n  Representation for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExeCoder: Empowering Large Language Models with Executability\n  Representation for Code Translation"
                },
                "summary": "Code translation is a crucial activity in the software development and\nmaintenance process, and researchers have recently begun to focus on using\npre-trained large language models (LLMs) for code translation. However,\nexisting LLMs only learn the contextual semantics of code during pre-training,\nneglecting executability information closely related to the execution state of\nthe code, which results in unguaranteed code executability and unreliable\nautomated code translation. To address this issue, we propose ExeCoder, an LLM\nspecifically designed for code translation, aimed at utilizing executability\nrepresentations such as functional semantics, syntax structures, and variable\ndependencies to enhance the capabilities of LLMs in code translation. To\nevaluate the effectiveness of ExeCoder, we manually enhanced the widely used\nbenchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X\nthat serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder\nachieves state-of-the-art performance in code translation, surpassing existing\nopen-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two\nmetrics, and even outperforms the renowned closed-source LLM GPT-4o. Website:\nhttps://execoder4trans.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation is a crucial activity in the software development and\nmaintenance process, and researchers have recently begun to focus on using\npre-trained large language models (LLMs) for code translation. However,\nexisting LLMs only learn the contextual semantics of code during pre-training,\nneglecting executability information closely related to the execution state of\nthe code, which results in unguaranteed code executability and unreliable\nautomated code translation. To address this issue, we propose ExeCoder, an LLM\nspecifically designed for code translation, aimed at utilizing executability\nrepresentations such as functional semantics, syntax structures, and variable\ndependencies to enhance the capabilities of LLMs in code translation. To\nevaluate the effectiveness of ExeCoder, we manually enhanced the widely used\nbenchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X\nthat serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder\nachieves state-of-the-art performance in code translation, surpassing existing\nopen-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two\nmetrics, and even outperforms the renowned closed-source LLM GPT-4o. Website:\nhttps://execoder4trans.github.io/"
                },
                "authors": [
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Wenjie Yin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16727v2",
                "updated": "2025-01-30T16:17:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    17,
                    56,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T06:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    7,
                    58,
                    1,
                    28,
                    0
                ],
                "title": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking"
                },
                "summary": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Shuaimin Li"
                    },
                    {
                        "name": "Liyang Fan"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18457v1",
                "updated": "2025-01-30T16:15:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    15,
                    38,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T16:15:38Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    15,
                    38,
                    3,
                    30,
                    0
                ],
                "title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering"
                },
                "summary": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages, and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and retrieval\naugmented settings. We also found that increasing the number of languages\ninvolved in CALM training leads to even higher accuracy and consistency. We\noffer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability. The source code\nand data of this paper are available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages, and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and retrieval\naugmented settings. We also found that increasing the number of languages\ninvolved in CALM training leads to even higher accuracy and consistency. We\noffer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability. The source code\nand data of this paper are available on GitHub."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10898v2",
                "updated": "2025-01-30T15:47:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    47,
                    33,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-17T05:26:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    26,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction\n  with LIME Explainable AI and a RAG-based Chatbot for Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction\n  with LIME Explainable AI and a RAG-based Chatbot for Insights"
                },
                "summary": "Ensuring safe water supplies requires effective water quality monitoring,\nespecially in developing countries like Nepal, where contamination risks are\nhigh. This paper introduces a hybrid deep learning model to predict Nepal's\nseasonal water quality using a small dataset with multiple water quality\nparameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along\nwith a neural network combining CNN and RNN layers, are used to capture\ntemporal and spatial patterns in the data. The model demonstrated notable\naccuracy improvements, aiding proactive water quality control. CatBoost,\nXGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values\nwith an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers\nachieved 99 percent accuracy, cross-validated across models. LIME analysis\nhighlighted the importance of indicators like EC and DO levels in XGBoost\nclassification decisions. The neural network model achieved 92 percent\nclassification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in\nregression analysis. Furthermore, a multifunctional application was developed\nto predict WQI values using both regression and classification methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safe water supplies requires effective water quality monitoring,\nespecially in developing countries like Nepal, where contamination risks are\nhigh. This paper introduces a hybrid deep learning model to predict Nepal's\nseasonal water quality using a small dataset with multiple water quality\nparameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along\nwith a neural network combining CNN and RNN layers, are used to capture\ntemporal and spatial patterns in the data. The model demonstrated notable\naccuracy improvements, aiding proactive water quality control. CatBoost,\nXGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values\nwith an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers\nachieved 99 percent accuracy, cross-validated across models. LIME analysis\nhighlighted the importance of indicators like EC and DO levels in XGBoost\nclassification decisions. The neural network model achieved 92 percent\nclassification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in\nregression analysis. Furthermore, a multifunctional application was developed\nto predict WQI values using both regression and classification methods."
                },
                "authors": [
                    {
                        "name": "Biplov Paneru"
                    },
                    {
                        "name": "Bishwash Paneru"
                    }
                ],
                "author_detail": {
                    "name": "Bishwash Paneru"
                },
                "author": "Bishwash Paneru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18438v1",
                "updated": "2025-01-30T15:45:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:45:56Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "o3-mini vs DeepSeek-R1: Which One is Safer?"
                },
                "summary": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "Jos Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2501.17749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18417v2",
                "updated": "2025-01-30T15:45:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-24T04:02:30Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    4,
                    2,
                    30,
                    3,
                    298,
                    0
                ],
                "title": "Large Language Models Reflect the Ideology of their Creators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Reflect the Ideology of their Creators"
                },
                "summary": "Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'."
                },
                "authors": [
                    {
                        "name": "Maarten Buyl"
                    },
                    {
                        "name": "Alexander Rogiers"
                    },
                    {
                        "name": "Sander Noels"
                    },
                    {
                        "name": "Guillaume Bied"
                    },
                    {
                        "name": "Iris Dominguez-Catena"
                    },
                    {
                        "name": "Edith Heiter"
                    },
                    {
                        "name": "Iman Johary"
                    },
                    {
                        "name": "Alexandru-Cristian Mara"
                    },
                    {
                        "name": "Raphal Romero"
                    },
                    {
                        "name": "Jefrey Lijffijt"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18435v1",
                "updated": "2025-01-30T15:42:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    42,
                    24,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:42:24Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    42,
                    24,
                    3,
                    30,
                    0
                ],
                "title": "GENIE: Generative Note Information Extraction model for structuring EHR\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENIE: Generative Note Information Extraction model for structuring EHR\n  data"
                },
                "summary": "Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Jinsen Lu"
                    },
                    {
                        "name": "Zitian Qu"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Zhengyun Zhao"
                    },
                    {
                        "name": "Isaac Kohane"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Sheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Yu"
                },
                "author": "Sheng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v3",
                "updated": "2025-01-30T15:24:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    24,
                    28,
                    3,
                    30,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18416v1",
                "updated": "2025-01-30T15:14:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:14:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs\n  and Their Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs\n  and Their Mitigation"
                },
                "summary": "Federated Learning (FL) is increasingly being adopted in military\ncollaborations to develop Large Language Models (LLMs) while preserving data\nsovereignty. However, prompt injection attacks-malicious manipulations of input\nprompts-pose new threats that may undermine operational security, disrupt\ndecision-making, and erode trust among allies. This perspective paper\nhighlights four potential vulnerabilities in federated military LLMs: secret\ndata leakage, free-rider exploitation, system disruption, and misinformation\nspread. To address these potential risks, we propose a human-AI collaborative\nframework that introduces both technical and policy countermeasures. On the\ntechnical side, our framework uses red/blue team wargaming and quality\nassurance to detect and mitigate adversarial behaviors of shared LLM weights.\nOn the policy side, it promotes joint AI-human policy development and\nverification of security protocols. Our findings will guide future research and\nemphasize proactive strategies for emerging military contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is increasingly being adopted in military\ncollaborations to develop Large Language Models (LLMs) while preserving data\nsovereignty. However, prompt injection attacks-malicious manipulations of input\nprompts-pose new threats that may undermine operational security, disrupt\ndecision-making, and erode trust among allies. This perspective paper\nhighlights four potential vulnerabilities in federated military LLMs: secret\ndata leakage, free-rider exploitation, system disruption, and misinformation\nspread. To address these potential risks, we propose a human-AI collaborative\nframework that introduces both technical and policy countermeasures. On the\ntechnical side, our framework uses red/blue team wargaming and quality\nassurance to detect and mitigate adversarial behaviors of shared LLM weights.\nOn the policy side, it promotes joint AI-human policy development and\nverification of security protocols. Our findings will guide future research and\nemphasize proactive strategies for emerging military contexts."
                },
                "authors": [
                    {
                        "name": "Youngjoon Lee"
                    },
                    {
                        "name": "Taehyun Park"
                    },
                    {
                        "name": "Yunho Lee"
                    },
                    {
                        "name": "Jinu Gong"
                    },
                    {
                        "name": "Joonhyuk Kang"
                    }
                ],
                "author_detail": {
                    "name": "Joonhyuk Kang"
                },
                "author": "Joonhyuk Kang",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18415v1",
                "updated": "2025-01-30T15:14:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T15:14:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    14,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "Consensus statement on the credibility assessment of ML predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus statement on the credibility assessment of ML predictors"
                },
                "summary": "The rapid integration of machine learning (ML) predictors into in silico\nmedicine has revolutionized the estimation of quantities of interest (QIs) that\nare otherwise challenging to measure directly. However, the credibility of\nthese predictors is critical, especially when they inform high-stakes\nhealthcare decisions. This position paper presents a consensus statement\ndeveloped by experts within the In Silico World Community of Practice. We\noutline twelve key statements forming the theoretical foundation for evaluating\nthe credibility of ML predictors, emphasizing the necessity of causal\nknowledge, rigorous error quantification, and robustness to biases. By\ncomparing ML predictors with biophysical models, we highlight unique challenges\nassociated with implicit causal knowledge and propose strategies to ensure\nreliability and applicability. Our recommendations aim to guide researchers,\ndevelopers, and regulators in the rigorous assessment and deployment of ML\npredictors in clinical and biomedical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of machine learning (ML) predictors into in silico\nmedicine has revolutionized the estimation of quantities of interest (QIs) that\nare otherwise challenging to measure directly. However, the credibility of\nthese predictors is critical, especially when they inform high-stakes\nhealthcare decisions. This position paper presents a consensus statement\ndeveloped by experts within the In Silico World Community of Practice. We\noutline twelve key statements forming the theoretical foundation for evaluating\nthe credibility of ML predictors, emphasizing the necessity of causal\nknowledge, rigorous error quantification, and robustness to biases. By\ncomparing ML predictors with biophysical models, we highlight unique challenges\nassociated with implicit causal knowledge and propose strategies to ensure\nreliability and applicability. Our recommendations aim to guide researchers,\ndevelopers, and regulators in the rigorous assessment and deployment of ML\npredictors in clinical and biomedical contexts."
                },
                "authors": [
                    {
                        "name": "Alessandra Aldieri"
                    },
                    {
                        "name": "Thiranja Prasad Babarenda Gamage"
                    },
                    {
                        "name": "Antonino Amedeo La Mattina"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Axel Loewe"
                    },
                    {
                        "name": "Francesco Pappalardo"
                    },
                    {
                        "name": "Marco Viceconti Italy"
                    }
                ],
                "author_detail": {
                    "name": "Marco Viceconti Italy"
                },
                "author": "Marco Viceconti Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04078v3",
                "updated": "2025-01-30T15:11:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    11,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-05T08:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    15,
                    45,
                    5,
                    279,
                    0
                ],
                "title": "TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles\n  with Simulated Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles\n  with Simulated Students"
                },
                "summary": "Large language models (LLMs) can empower teachers to build pedagogical\nconversational agents (PCAs) customized for their students. As students have\ndifferent prior knowledge and motivation levels, teachers must review the\nadaptivity of their PCAs to diverse students. Existing chatbot reviewing\nmethods (e.g., direct chat and benchmarks) are either manually intensive for\nmultiple iterations or limited to testing only single-turn interactions. We\npresent TeachTune, where teachers can create simulated students and review PCAs\nby observing automated chats between PCAs and simulated students. Our technical\npipeline instructs an LLM-based student to simulate prescribed knowledge levels\nand traits, helping teachers explore diverse conversation patterns. Our\npipeline could produce simulated students whose behaviors correlate highly to\ntheir input knowledge and motivation levels within 5% and 10% accuracy gaps.\nThirty science teachers designed PCAs in a between-subjects study, and using\nTeachTune resulted in a lower task load and higher student profile coverage\nover a baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can empower teachers to build pedagogical\nconversational agents (PCAs) customized for their students. As students have\ndifferent prior knowledge and motivation levels, teachers must review the\nadaptivity of their PCAs to diverse students. Existing chatbot reviewing\nmethods (e.g., direct chat and benchmarks) are either manually intensive for\nmultiple iterations or limited to testing only single-turn interactions. We\npresent TeachTune, where teachers can create simulated students and review PCAs\nby observing automated chats between PCAs and simulated students. Our technical\npipeline instructs an LLM-based student to simulate prescribed knowledge levels\nand traits, helping teachers explore diverse conversation patterns. Our\npipeline could produce simulated students whose behaviors correlate highly to\ntheir input knowledge and motivation levels within 5% and 10% accuracy gaps.\nThirty science teachers designed PCAs in a between-subjects study, and using\nTeachTune resulted in a lower task load and higher student profile coverage\nover a baseline."
                },
                "authors": [
                    {
                        "name": "Hyoungwook Jin"
                    },
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Jeongeon Park"
                    },
                    {
                        "name": "Yokyung Lee"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06967v2",
                "updated": "2025-01-30T14:37:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    37,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2024-06-11T05:50:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    5,
                    50,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?"
                },
                "summary": "The dual thinking framework considers fast, intuitive processing and slower,\nlogical processing. The perception of dual thinking in vision requires images\nwhere inferences from intuitive and logical processing differ. We introduce an\nadversarial dataset to provide evidence for the dual thinking framework in\nhuman vision, which also aids in studying the qualitative behavior of deep\nlearning models. The evidence underscores the importance of shape in\nidentifying instances in human vision. Our psychophysical studies show the\npresence of multiple inferences in rapid succession, and analysis of errors\nshows the early stopping of visual processing can result in missing relevant\ninformation. Our study shows that segmentation models lack an understanding of\nsub-structures, as indicated by errors related to the position and number of\nsub-components. Additionally, the similarity in errors made by models and\nintuitive human processing indicates that models only address intuitive\nthinking in human vision. In contrast, multi-modal LLMs, including open-source\nmodels, demonstrate tremendous progress on errors made in intuitive processing.\nThe models have improved performance on images that require logical reasoning\nand show recognition of sub-components. However, they have not matched the\nperformance improvements made on errors in intuitive processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dual thinking framework considers fast, intuitive processing and slower,\nlogical processing. The perception of dual thinking in vision requires images\nwhere inferences from intuitive and logical processing differ. We introduce an\nadversarial dataset to provide evidence for the dual thinking framework in\nhuman vision, which also aids in studying the qualitative behavior of deep\nlearning models. The evidence underscores the importance of shape in\nidentifying instances in human vision. Our psychophysical studies show the\npresence of multiple inferences in rapid succession, and analysis of errors\nshows the early stopping of visual processing can result in missing relevant\ninformation. Our study shows that segmentation models lack an understanding of\nsub-structures, as indicated by errors related to the position and number of\nsub-components. Additionally, the similarity in errors made by models and\nintuitive human processing indicates that models only address intuitive\nthinking in human vision. In contrast, multi-modal LLMs, including open-source\nmodels, demonstrate tremendous progress on errors made in intuitive processing.\nThe models have improved performance on images that require logical reasoning\nand show recognition of sub-components. However, they have not matched the\nperformance improvements made on errors in intuitive processing."
                },
                "authors": [
                    {
                        "name": "Kailas Dayanandan"
                    },
                    {
                        "name": "Nikhil Kumar"
                    },
                    {
                        "name": "Anand Sinha"
                    },
                    {
                        "name": "Brejesh Lall"
                    }
                ],
                "author_detail": {
                    "name": "Brejesh Lall"
                },
                "author": "Brejesh Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v2",
                "updated": "2025-01-30T14:36:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    36,
                    52,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Learning?"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18365v1",
                "updated": "2025-01-30T14:15:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    15,
                    9,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:15:09Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    15,
                    9,
                    3,
                    30,
                    0
                ],
                "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques."
                },
                "authors": [
                    {
                        "name": "Yiteng Tu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18351v1",
                "updated": "2025-01-30T13:55:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    55,
                    8,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T13:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    55,
                    8,
                    3,
                    30,
                    0
                ],
                "title": "Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic\n  Navigation in Unstructured Outdoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic\n  Navigation in Unstructured Outdoor Environments"
                },
                "summary": "Path planning with strong environmental adaptability plays a crucial role in\nrobotic navigation in unstructured outdoor environments, especially in the case\nof low-quality location and map information. The path planning ability of a\nrobot depends on the identification of the traversability of global and local\nground areas. In real-world scenarios, the complexity of outdoor open\nenvironments makes it difficult for robots to identify the traversability of\nground areas that lack a clearly defined structure. Moreover, most existing\nmethods have rarely analyzed the integration of local and global traversability\nidentifications in unstructured outdoor scenarios. To address this problem, we\npropose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV)\nrepresentations into local planning to generate high-quality traversable paths.\nThen, these paths are projected onto the global traversability map generated by\nthe global BEV planning model to obtain the optimal waypoints. By integrating\nthe traversability from both local and global BEV, we establish a dual-layer\nBEV heuristic planning paradigm, enabling long-distance navigation in\nunstructured outdoor environments. We test our approach through both public\ndataset evaluations and real-world robot deployments, yielding promising\nresults. Compared to baselines, the Dual-BEV Nav improved temporal distance\nprediction accuracy by up to $18.7\\%$. In the real-world deployment, under\nconditions significantly different from the training set and with notable\nocclusions in the global BEV, the Dual-BEV Nav successfully achieved a\n65-meter-long outdoor navigation. Further analysis demonstrates that the local\nBEV representation significantly enhances the rationality of the planning,\nwhile the global BEV probability map ensures the robustness of the overall\nplanning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planning with strong environmental adaptability plays a crucial role in\nrobotic navigation in unstructured outdoor environments, especially in the case\nof low-quality location and map information. The path planning ability of a\nrobot depends on the identification of the traversability of global and local\nground areas. In real-world scenarios, the complexity of outdoor open\nenvironments makes it difficult for robots to identify the traversability of\nground areas that lack a clearly defined structure. Moreover, most existing\nmethods have rarely analyzed the integration of local and global traversability\nidentifications in unstructured outdoor scenarios. To address this problem, we\npropose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV)\nrepresentations into local planning to generate high-quality traversable paths.\nThen, these paths are projected onto the global traversability map generated by\nthe global BEV planning model to obtain the optimal waypoints. By integrating\nthe traversability from both local and global BEV, we establish a dual-layer\nBEV heuristic planning paradigm, enabling long-distance navigation in\nunstructured outdoor environments. We test our approach through both public\ndataset evaluations and real-world robot deployments, yielding promising\nresults. Compared to baselines, the Dual-BEV Nav improved temporal distance\nprediction accuracy by up to $18.7\\%$. In the real-world deployment, under\nconditions significantly different from the training set and with notable\nocclusions in the global BEV, the Dual-BEV Nav successfully achieved a\n65-meter-long outdoor navigation. Further analysis demonstrates that the local\nBEV representation significantly enhances the rationality of the planning,\nwhile the global BEV probability map ensures the robustness of the overall\nplanning."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "name": "Hanlin Dong"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiahui Liu"
                    },
                    {
                        "name": "Shibo Huang"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xuan Tang"
                    },
                    {
                        "name": "Xian Wei"
                    },
                    {
                        "name": "Xiong You"
                    }
                ],
                "author_detail": {
                    "name": "Xiong You"
                },
                "author": "Xiong You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18320v1",
                "updated": "2025-01-30T13:00:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    0,
                    15,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T13:00:15Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    0,
                    15,
                    3,
                    30,
                    0
                ],
                "title": "Leveraging LLM Agents for Automated Optimization Modeling for SASP\n  Problems: A Graph-RAG based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Agents for Automated Optimization Modeling for SASP\n  Problems: A Graph-RAG based Approach"
                },
                "summary": "Automated optimization modeling (AOM) has evoked considerable interest with\nthe rapid evolution of large language models (LLMs). Existing approaches\npredominantly rely on prompt engineering, utilizing meticulously designed\nexpert response chains or structured guidance. However, prompt-based techniques\nhave failed to perform well in the sensor array signal processing (SASP) area\ndue the lack of specific domain knowledge. To address this issue, we propose an\nautomated modeling approach based on retrieval-augmented generation (RAG)\ntechnique, which consists of two principal components: a multi-agent (MA)\nstructure and a graph-based RAG (Graph-RAG) process. The MA structure is\ntailored for the architectural AOM process, with each agent being designed\nbased on principles of human modeling procedure. The Graph-RAG process serves\nto match user query with specific SASP modeling knowledge, thereby enhancing\nthe modeling result. Results on ten classical signal processing problems\ndemonstrate that the proposed approach (termed as MAG-RAG) outperforms several\nAOM benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated optimization modeling (AOM) has evoked considerable interest with\nthe rapid evolution of large language models (LLMs). Existing approaches\npredominantly rely on prompt engineering, utilizing meticulously designed\nexpert response chains or structured guidance. However, prompt-based techniques\nhave failed to perform well in the sensor array signal processing (SASP) area\ndue the lack of specific domain knowledge. To address this issue, we propose an\nautomated modeling approach based on retrieval-augmented generation (RAG)\ntechnique, which consists of two principal components: a multi-agent (MA)\nstructure and a graph-based RAG (Graph-RAG) process. The MA structure is\ntailored for the architectural AOM process, with each agent being designed\nbased on principles of human modeling procedure. The Graph-RAG process serves\nto match user query with specific SASP modeling knowledge, thereby enhancing\nthe modeling result. Results on ten classical signal processing problems\ndemonstrate that the proposed approach (termed as MAG-RAG) outperforms several\nAOM benchmarks."
                },
                "authors": [
                    {
                        "name": "Tianpeng Pan"
                    },
                    {
                        "name": "Wenqiang Pu"
                    },
                    {
                        "name": "Licheng Zhao"
                    },
                    {
                        "name": "Rui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhou"
                },
                "author": "Rui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03577v2",
                "updated": "2025-01-30T12:44:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    44,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-05T14:31:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    31,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning (RL) agents are costly to train and fragile to\nenvironmental changes. They often perform poorly when there are many changing\ntasks, prohibiting their widespread deployment in the real world. Many Lifelong\nRL agent designs have been proposed to mitigate issues such as catastrophic\nforgetting or demonstrate positive characteristics like forward transfer when\nchange occurs. However, no prior work has established whether the impact on\nagent performance can be predicted from the change itself. Understanding this\nrelationship will help agents proactively mitigate a change's impact for\nimproved learning performance. We propose Change-Induced Regret Proxy (CHIRP)\nmetrics to link change to agent performance drops and use two environments to\ndemonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent\nachieved $48\\%$ higher performance than the next best method in one benchmark\nand attained the best success rates in 8 of 10 tasks in a second benchmark\nwhich proved difficult for existing lifelong RL agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) agents are costly to train and fragile to\nenvironmental changes. They often perform poorly when there are many changing\ntasks, prohibiting their widespread deployment in the real world. Many Lifelong\nRL agent designs have been proposed to mitigate issues such as catastrophic\nforgetting or demonstrate positive characteristics like forward transfer when\nchange occurs. However, no prior work has established whether the impact on\nagent performance can be predicted from the change itself. Understanding this\nrelationship will help agents proactively mitigate a change's impact for\nimproved learning performance. We propose Change-Induced Regret Proxy (CHIRP)\nmetrics to link change to agent performance drops and use two environments to\ndemonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent\nachieved $48\\%$ higher performance than the next best method in one benchmark\nand attained the best success rates in 8 of 10 tasks in a second benchmark\nwhich proved difficult for existing lifelong RL agents."
                },
                "authors": [
                    {
                        "name": "John Birkbeck"
                    },
                    {
                        "name": "Adam Sobey"
                    },
                    {
                        "name": "Federico Cerutti"
                    },
                    {
                        "name": "Katherine Heseltine Hurley Flynn"
                    },
                    {
                        "name": "Timothy J. Norman"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Norman"
                },
                "author": "Timothy J. Norman",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18310v1",
                "updated": "2025-01-30T12:37:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    37,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T12:37:06Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    37,
                    6,
                    3,
                    30,
                    0
                ],
                "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Neural Theorem Proving via Fine-grained Proof Structure\n  Analysis"
                },
                "summary": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug."
                },
                "authors": [
                    {
                        "name": "Haoxiong Liu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18299v1",
                "updated": "2025-01-30T12:21:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    21,
                    50,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T12:21:50Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    21,
                    50,
                    3,
                    30,
                    0
                ],
                "title": "Model-Free RL Agents Demonstrate System 1-Like Intentionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Free RL Agents Demonstrate System 1-Like Intentionality"
                },
                "summary": "This paper argues that model-free reinforcement learning (RL) agents, while\nlacking explicit planning mechanisms, exhibit behaviours that can be analogised\nto System 1 (\"thinking fast\") processes in human cognition. Unlike model-based\nRL agents, which operate akin to System 2 (\"thinking slow\") reasoning by\nleveraging internal representations for planning, model-free agents react to\nenvironmental stimuli without anticipatory modelling. We propose a novel\nframework linking the dichotomy of System 1 and System 2 to the distinction\nbetween model-free and model-based RL. This framing challenges the prevailing\nassumption that intentionality and purposeful behaviour require planning,\nsuggesting instead that intentionality can manifest in the structured, reactive\nbehaviours of model-free agents. By drawing on interdisciplinary insights from\ncognitive psychology, legal theory, and experimental jurisprudence, we explore\nthe implications of this perspective for attributing responsibility and\nensuring AI safety. These insights advocate for a broader, contextually\ninformed interpretation of intentionality in RL systems, with implications for\ntheir ethical deployment and regulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper argues that model-free reinforcement learning (RL) agents, while\nlacking explicit planning mechanisms, exhibit behaviours that can be analogised\nto System 1 (\"thinking fast\") processes in human cognition. Unlike model-based\nRL agents, which operate akin to System 2 (\"thinking slow\") reasoning by\nleveraging internal representations for planning, model-free agents react to\nenvironmental stimuli without anticipatory modelling. We propose a novel\nframework linking the dichotomy of System 1 and System 2 to the distinction\nbetween model-free and model-based RL. This framing challenges the prevailing\nassumption that intentionality and purposeful behaviour require planning,\nsuggesting instead that intentionality can manifest in the structured, reactive\nbehaviours of model-free agents. By drawing on interdisciplinary insights from\ncognitive psychology, legal theory, and experimental jurisprudence, we explore\nthe implications of this perspective for attributing responsibility and\nensuring AI safety. These insights advocate for a broader, contextually\ninformed interpretation of intentionality in RL systems, with implications for\ntheir ethical deployment and regulation."
                },
                "authors": [
                    {
                        "name": "Hal Ashton"
                    },
                    {
                        "name": "Matija Franklin"
                    }
                ],
                "author_detail": {
                    "name": "Matija Franklin"
                },
                "author": "Matija Franklin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15345v2",
                "updated": "2025-01-30T12:20:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    20,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-10T10:59:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    10,
                    59,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual\n  perception beyond human capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual\n  perception beyond human capabilities"
                },
                "summary": "Optical flow, inspired by the mechanisms of biological visual systems,\ncalculates spatial motion vectors within visual scenes that are necessary for\nenabling robotics to excel in complex and dynamic working environments.\nHowever, current optical flow algorithms, despite human-competitive task\nperformance on benchmark datasets, remain constrained by unacceptable time\ndelays (~0.6 seconds per inference, 4X human processing speed) in practical\ndeployment. Here, we introduce a neuromorphic optical flow approach that\naddresses delay bottlenecks by encoding temporal information directly in a\nsynaptic transistor array to assist spatial motion analysis. Compared to\nconventional spatial-only optical flow methods, our spatiotemporal neuromorphic\noptical flow offers the spatial-temporal consistency of motion information,\nrapidly identifying regions of interest in as little as 1-2 ms using the\ntemporal motion cues derived from the embedded temporal information in the\ntwo-dimensional floating gate synaptic transistors. Thus, the visual input can\nbe selectively filtered to achieve faster velocity calculations and various\ntask execution. At the hardware level, due to the atomically sharp interfaces\nbetween distinct functional layers in two-dimensional van der Waals\nheterostructures, the synaptic transistor offers high-frequency response (~100\n{\\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000\ncycles), enabling robust visual processing. In software benchmarks, our system\noutperforms state-of-the-art algorithms with a 400% speedup, frequently\nsurpassing human-level performance while maintaining or enhancing accuracy by\nutilizing the temporal priors provided by the embedded temporal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical flow, inspired by the mechanisms of biological visual systems,\ncalculates spatial motion vectors within visual scenes that are necessary for\nenabling robotics to excel in complex and dynamic working environments.\nHowever, current optical flow algorithms, despite human-competitive task\nperformance on benchmark datasets, remain constrained by unacceptable time\ndelays (~0.6 seconds per inference, 4X human processing speed) in practical\ndeployment. Here, we introduce a neuromorphic optical flow approach that\naddresses delay bottlenecks by encoding temporal information directly in a\nsynaptic transistor array to assist spatial motion analysis. Compared to\nconventional spatial-only optical flow methods, our spatiotemporal neuromorphic\noptical flow offers the spatial-temporal consistency of motion information,\nrapidly identifying regions of interest in as little as 1-2 ms using the\ntemporal motion cues derived from the embedded temporal information in the\ntwo-dimensional floating gate synaptic transistors. Thus, the visual input can\nbe selectively filtered to achieve faster velocity calculations and various\ntask execution. At the hardware level, due to the atomically sharp interfaces\nbetween distinct functional layers in two-dimensional van der Waals\nheterostructures, the synaptic transistor offers high-frequency response (~100\n{\\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000\ncycles), enabling robust visual processing. In software benchmarks, our system\noutperforms state-of-the-art algorithms with a 400% speedup, frequently\nsurpassing human-level performance while maintaining or enhancing accuracy by\nutilizing the temporal priors provided by the embedded temporal information."
                },
                "authors": [
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Jingwen Zhao"
                    },
                    {
                        "name": "Tongming Pu"
                    },
                    {
                        "name": "Liangbing Zhao"
                    },
                    {
                        "name": "Xiaoyu Guo"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Weihao Ma"
                    },
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Zhenyu Xu"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Luigi Occhipinti"
                    },
                    {
                        "name": "Arokia Nathan"
                    },
                    {
                        "name": "Ravinder Dahiya"
                    },
                    {
                        "name": "Huaqiang Wu"
                    },
                    {
                        "name": "Li Tao"
                    },
                    {
                        "name": "Shuo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Gao"
                },
                "author": "Shuo Gao",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19243v2",
                "updated": "2025-01-30T12:17:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    12,
                    17,
                    43,
                    3,
                    30,
                    0
                ],
                "published": "2024-03-28T08:58:20Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    8,
                    58,
                    20,
                    3,
                    88,
                    0
                ],
                "title": "Efficient Learning With Sine-Activated Low-rank Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Learning With Sine-Activated Low-rank Matrices"
                },
                "summary": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling."
                },
                "authors": [
                    {
                        "name": "Yiping Ji"
                    },
                    {
                        "name": "Hemanth Saratchandran"
                    },
                    {
                        "name": "Cameron Gordon"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Simon Lucey"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lucey"
                },
                "author": "Simon Lucey",
                "arxiv_comment": "The first two authors contributed equally. Paper accepted at ICLR\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18287v1",
                "updated": "2025-01-30T11:55:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    55,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:55:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    55,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models"
                },
                "summary": "This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Zachary Laubach"
                    },
                    {
                        "name": "Tarek Al Mustafa"
                    },
                    {
                        "name": "Sina Zarrie"
                    },
                    {
                        "name": "Robert Frhstckl"
                    },
                    {
                        "name": "Phyllis Illari"
                    }
                ],
                "author_detail": {
                    "name": "Phyllis Illari"
                },
                "author": "Phyllis Illari",
                "arxiv_comment": "8 pages, 2 figures, accepted to the NLP4Ecology Workshop 2025\n  (https://nlp4ecology2025.di.unito.it/) co-located with the Joint 25th Nordic\n  Conference on Computational Linguistics and 11th Baltic Conference on Human\n  Language Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18282v1",
                "updated": "2025-01-30T11:41:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    41,
                    13,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:41:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    41,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Leveraging Sparsity for Sample-Efficient Preference Learning: A\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Sparsity for Sample-Efficient Preference Learning: A\n  Theoretical Perspective"
                },
                "summary": "This paper considers the sample-efficiency of preference learning, which\nmodels and predicts human choices based on comparative judgments. The minimax\noptimal estimation rate $\\Theta(d/n)$ in traditional estimation theory requires\nthat the number of samples $n$ scales linearly with the dimensionality of the\nfeature space $d$. However, the high dimensionality of the feature space and\nthe high cost of collecting human-annotated data challenge the efficiency of\ntraditional estimation methods. To remedy this, we leverage sparsity in the\npreference model and establish sharp estimation rates. We show that under the\nsparse random utility model, where the parameter of the reward function is\n$k$-sparse, the minimax optimal rate can be reduced to $\\Theta(k/n \\log(d/k))$.\nFurthermore, we analyze the $\\ell_{1}$-regularized estimator and show that it\nachieves near-optimal rate under mild assumptions on the Gram matrix.\nExperiments on synthetic data and LLM alignment data validate our theoretical\nfindings, showing that sparsity-aware methods significantly reduce sample\ncomplexity and improve prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the sample-efficiency of preference learning, which\nmodels and predicts human choices based on comparative judgments. The minimax\noptimal estimation rate $\\Theta(d/n)$ in traditional estimation theory requires\nthat the number of samples $n$ scales linearly with the dimensionality of the\nfeature space $d$. However, the high dimensionality of the feature space and\nthe high cost of collecting human-annotated data challenge the efficiency of\ntraditional estimation methods. To remedy this, we leverage sparsity in the\npreference model and establish sharp estimation rates. We show that under the\nsparse random utility model, where the parameter of the reward function is\n$k$-sparse, the minimax optimal rate can be reduced to $\\Theta(k/n \\log(d/k))$.\nFurthermore, we analyze the $\\ell_{1}$-regularized estimator and show that it\nachieves near-optimal rate under mild assumptions on the Gram matrix.\nExperiments on synthetic data and LLM alignment data validate our theoretical\nfindings, showing that sparsity-aware methods significantly reduce sample\ncomplexity and improve prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Yunzhen Yao"
                    },
                    {
                        "name": "Lie He"
                    },
                    {
                        "name": "Michael Gastpar"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gastpar"
                },
                "author": "Michael Gastpar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18280v1",
                "updated": "2025-01-30T11:37:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:37:40Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models"
                },
                "summary": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner."
                },
                "authors": [
                    {
                        "name": "Haoyu Liang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yunfeng Cai"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13503v2",
                "updated": "2025-01-30T11:15:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    15,
                    59,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-23T09:28:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    28,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop\n  and Fault-Ride-Through capabilities of PV inverters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop\n  and Fault-Ride-Through capabilities of PV inverters"
                },
                "summary": "The deployment of PV inverters is rapidly expanding across Europe, where\nthese devices must increasingly comply with stringent grid requirements.This\nstudy presents a benchmark analysis of four PV inverter manufacturers, focusing\non their Fault Ride Through capabilities under varying grid strengths, voltage\ndips, and fault durations, parameters critical for grid operators during fault\nconditions.The findings highlight the influence of different inverter controls\non key metrics such as total harmonic distortion of current and voltage\nsignals, as well as system stability following grid faults.Additionally, the\nstudy evaluates transient stability using two distinct testing approaches.The\nfirst approach employs the current standard method, which is testing with an\nideal voltage source. The second utilizes a Power Hardware in the Loop\nmethodology with a benchmark CIGRE grid model.The results reveal that while\ntesting with an ideal voltage source is cost-effective and convenient in the\nshort term, it lacks the ability to capture the dynamic interactions and\nfeedback loops of physical grid components.This limitation can obscure critical\nreal world factors, potentially leading to unexpected inverter behavior and\noperational challenges in grids with high PV penetration.This study underscores\nthe importance of re-evaluating conventional testing methods and incorporating\nPower Hardware in the Loop structures to achieve test results that more closely\nalign with real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of PV inverters is rapidly expanding across Europe, where\nthese devices must increasingly comply with stringent grid requirements.This\nstudy presents a benchmark analysis of four PV inverter manufacturers, focusing\non their Fault Ride Through capabilities under varying grid strengths, voltage\ndips, and fault durations, parameters critical for grid operators during fault\nconditions.The findings highlight the influence of different inverter controls\non key metrics such as total harmonic distortion of current and voltage\nsignals, as well as system stability following grid faults.Additionally, the\nstudy evaluates transient stability using two distinct testing approaches.The\nfirst approach employs the current standard method, which is testing with an\nideal voltage source. The second utilizes a Power Hardware in the Loop\nmethodology with a benchmark CIGRE grid model.The results reveal that while\ntesting with an ideal voltage source is cost-effective and convenient in the\nshort term, it lacks the ability to capture the dynamic interactions and\nfeedback loops of physical grid components.This limitation can obscure critical\nreal world factors, potentially leading to unexpected inverter behavior and\noperational challenges in grids with high PV penetration.This study underscores\nthe importance of re-evaluating conventional testing methods and incorporating\nPower Hardware in the Loop structures to achieve test results that more closely\nalign with real-world conditions."
                },
                "authors": [
                    {
                        "name": "Carina Lehmal"
                    },
                    {
                        "name": "Ziqian Zhang"
                    },
                    {
                        "name": "Robert Schrhuber"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schrhuber"
                },
                "author": "Robert Schrhuber",
                "arxiv_comment": "7 pages, 9 figures, study of behaviour of different inverters during\n  different grid strength",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18270v1",
                "updated": "2025-01-30T11:10:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    10,
                    44,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:10:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    10,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "The iToBoS dataset: skin region images extracted from 3D total body\n  photographs for lesion detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The iToBoS dataset: skin region images extracted from 3D total body\n  photographs for lesion detection"
                },
                "summary": "Artificial intelligence has significantly advanced skin cancer diagnosis by\nenabling rapid and accurate detection of malignant lesions. In this domain,\nmost publicly available image datasets consist of single, isolated skin lesions\npositioned at the center of the image. While these lesion-centric datasets have\nbeen fundamental for developing diagnostic algorithms, they lack the context of\nthe surrounding skin, which is critical for improving lesion detection. The\niToBoS dataset was created to address this challenge. It includes 16,954 images\nof skin regions from 100 participants, captured using 3D total body\nphotography. Each image roughly corresponds to a $7 \\times 9$ cm section of\nskin with all suspicious lesions annotated using bounding boxes. Additionally,\nthe dataset provides metadata such as anatomical location, age group, and sun\ndamage score for each image. This dataset aims to facilitate training and\nbenchmarking of algorithms, with the goal of enabling early detection of skin\ncancer and deployment of this technology in non-clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has significantly advanced skin cancer diagnosis by\nenabling rapid and accurate detection of malignant lesions. In this domain,\nmost publicly available image datasets consist of single, isolated skin lesions\npositioned at the center of the image. While these lesion-centric datasets have\nbeen fundamental for developing diagnostic algorithms, they lack the context of\nthe surrounding skin, which is critical for improving lesion detection. The\niToBoS dataset was created to address this challenge. It includes 16,954 images\nof skin regions from 100 participants, captured using 3D total body\nphotography. Each image roughly corresponds to a $7 \\times 9$ cm section of\nskin with all suspicious lesions annotated using bounding boxes. Additionally,\nthe dataset provides metadata such as anatomical location, age group, and sun\ndamage score for each image. This dataset aims to facilitate training and\nbenchmarking of algorithms, with the goal of enabling early detection of skin\ncancer and deployment of this technology in non-clinical environments."
                },
                "authors": [
                    {
                        "name": "Anup Saha"
                    },
                    {
                        "name": "Joseph Adeola"
                    },
                    {
                        "name": "Nuria Ferrera"
                    },
                    {
                        "name": "Adam Mothershaw"
                    },
                    {
                        "name": "Gisele Rezze"
                    },
                    {
                        "name": "Sraphin Gaborit"
                    },
                    {
                        "name": "Brian D'Alessandro"
                    },
                    {
                        "name": "James Hudson"
                    },
                    {
                        "name": "Gyula Szab"
                    },
                    {
                        "name": "Balazs Pataki"
                    },
                    {
                        "name": "Hayat Rajani"
                    },
                    {
                        "name": "Sana Nazari"
                    },
                    {
                        "name": "Hassan Hayat"
                    },
                    {
                        "name": "Clare Primiero"
                    },
                    {
                        "name": "H. Peter Soyer"
                    },
                    {
                        "name": "Josep Malvehy"
                    },
                    {
                        "name": "Rafael Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Garcia"
                },
                "author": "Rafael Garcia",
                "arxiv_comment": "Article Submitted to Scientific Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.2.6; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18265v1",
                "updated": "2025-01-30T11:04:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    4,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T11:04:14Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    4,
                    14,
                    3,
                    30,
                    0
                ],
                "title": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence"
                },
                "summary": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments."
                },
                "authors": [
                    {
                        "name": "Kevin Roitero"
                    },
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Michael Soprano"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Stefano Mizzaro"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Mizzaro"
                },
                "author": "Stefano Mizzaro",
                "arxiv_comment": "18 pages; 7 figures; 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18250v1",
                "updated": "2025-01-30T10:31:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    31,
                    34,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T10:31:34Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    31,
                    34,
                    3,
                    30,
                    0
                ],
                "title": "Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression"
                },
                "summary": "Efficient channel state information (CSI) compression is crucial in frequency\ndivision duplexing (FDD) massive multiple-input multiple-output (MIMO) systems\ndue to excessive feedback overhead. Recently, deep learning-based compression\ntechniques have demonstrated superior performance across various data types,\nincluding CSI. However, these approaches often experience performance\ndegradation when the data distribution changes due to their limited\ngeneralization capabilities. To address this challenge, we propose a model\nfine-tuning approach for CSI feedback in massive MIMO systems. The idea is to\nfine-tune the encoder/decoder network models in a dynamic fashion using the\nrecent CSI samples. First, we explore encoder-only fine-tuning, where only the\nencoder parameters are updated, leaving the decoder and latent parameters\nunchanged. Next, we consider full-model fine-tuning, where the encoder and\ndecoder models are jointly updated. Unlike encoder-only fine-tuning, full-model\nfine-tuning requires the updated decoder and latent parameters to be\ntransmitted to the decoder side. To efficiently handle this, we propose\ndifferent prior distributions for model updates, such as uniform and truncated\nGaussian to entropy code them together with the compressed CSI and account for\nadditional feedback overhead imposed by conveying the model updates. Moreover,\nwe incorporate quantized model updates during fine-tuning to reflect the impact\nof quantization in the deployment phase. Our results demonstrate that\nfull-model fine-tuning significantly enhances the rate-distortion (RD)\nperformance of neural CSI compression. Furthermore, we analyze how often the\nfull-model fine-tuning should be applied in a new wireless environment and\nidentify an optimal period interval for achieving the best RD trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient channel state information (CSI) compression is crucial in frequency\ndivision duplexing (FDD) massive multiple-input multiple-output (MIMO) systems\ndue to excessive feedback overhead. Recently, deep learning-based compression\ntechniques have demonstrated superior performance across various data types,\nincluding CSI. However, these approaches often experience performance\ndegradation when the data distribution changes due to their limited\ngeneralization capabilities. To address this challenge, we propose a model\nfine-tuning approach for CSI feedback in massive MIMO systems. The idea is to\nfine-tune the encoder/decoder network models in a dynamic fashion using the\nrecent CSI samples. First, we explore encoder-only fine-tuning, where only the\nencoder parameters are updated, leaving the decoder and latent parameters\nunchanged. Next, we consider full-model fine-tuning, where the encoder and\ndecoder models are jointly updated. Unlike encoder-only fine-tuning, full-model\nfine-tuning requires the updated decoder and latent parameters to be\ntransmitted to the decoder side. To efficiently handle this, we propose\ndifferent prior distributions for model updates, such as uniform and truncated\nGaussian to entropy code them together with the compressed CSI and account for\nadditional feedback overhead imposed by conveying the model updates. Moreover,\nwe incorporate quantized model updates during fine-tuning to reflect the impact\nof quantization in the deployment phase. Our results demonstrate that\nfull-model fine-tuning significantly enhances the rate-distortion (RD)\nperformance of neural CSI compression. Furthermore, we analyze how often the\nfull-model fine-tuning should be applied in a new wireless environment and\nidentify an optimal period interval for achieving the best RD trade-off."
                },
                "authors": [
                    {
                        "name": "Mehdi Sattari"
                    },
                    {
                        "name": "Deniz Gndz"
                    },
                    {
                        "name": "Tommmy Svensson"
                    }
                ],
                "author_detail": {
                    "name": "Tommmy Svensson"
                },
                "author": "Tommmy Svensson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18243v1",
                "updated": "2025-01-30T10:21:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    21,
                    10,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T10:21:10Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    10,
                    21,
                    10,
                    3,
                    30,
                    0
                ],
                "title": "Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance"
                },
                "summary": "The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Assaf Toledo"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Toledo"
                },
                "author": "Assaf Toledo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17387v2",
                "updated": "2025-01-30T09:56:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    56,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-29T02:39:57Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    39,
                    57,
                    2,
                    29,
                    0
                ],
                "title": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection"
                },
                "summary": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 % and 72.36 %), as well\nas mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset\n2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with\nreaching 82.44 \\% on dataset 1 and 81.46 % in dataset 2, making them\nparticularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 % and 72.36 %), as well\nas mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset\n2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with\nreaching 82.44 \\% on dataset 1 and 81.46 % in dataset 2, making them\nparticularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups."
                },
                "authors": [
                    {
                        "name": "Alicia Allmendinger"
                    },
                    {
                        "name": "Ahmet Ouz Saltk"
                    },
                    {
                        "name": "Gerassimos G. Peteinatos"
                    },
                    {
                        "name": "Anthony Stein"
                    },
                    {
                        "name": "Roland Gerhards"
                    }
                ],
                "author_detail": {
                    "name": "Roland Gerhards"
                },
                "author": "Roland Gerhards",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10979v2",
                "updated": "2025-01-30T09:17:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    17,
                    22,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-19T08:06:06Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    8,
                    6,
                    6,
                    6,
                    19,
                    0
                ],
                "title": "Control LLM: Controlled Evolution for Intelligence Retention in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control LLM: Controlled Evolution for Intelligence Retention in LLM"
                },
                "summary": "Large Language Models (LLMs) demand significant computational resources,\nmaking it essential to enhance their capabilities without retraining from\nscratch. A key challenge in this domain is \\textit{catastrophic forgetting}\n(CF), which hampers performance during Continuous Pre-training (CPT) and\nContinuous Supervised Fine-Tuning (CSFT). We propose \\textbf{Control LLM}, a\nnovel approach that leverages parallel pre-trained and expanded transformer\nblocks, aligning their hidden-states through interpolation strategies This\nmethod effectively preserves performance on existing tasks while seamlessly\nintegrating new knowledge.\n  Extensive experiments demonstrate the effectiveness of Control LLM in both\nCPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in\nmathematical reasoning ($+14.4\\%$ on Math-Hard) and coding performance ($+10\\%$\non MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\\%$\non C-Eval, $+6.8\\%$ on CMMLU, and $+30.2\\%$ on CMMLU-0shot-CoT). It surpasses\nexisting methods and achieves SOTA among open-source models tuned from the same\nbase model, using substantially less data and compute. Crucially, these gains\nare realized while preserving strong original capabilities, with minimal\ndegradation ($<4.3\\% \\text{on MMLU}$) compared to $>35\\%$ in open-source Math\nand Coding models. This approach has been successfully deployed in LinkedIn's\nGenAI-powered job seeker and Ads unit products.\n  To support further research, we release the training and evaluation code\n(https://github.com/linkedin/ControlLLM) along with models trained on public\ndatasets (https://huggingface.co/ControlLLM) to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demand significant computational resources,\nmaking it essential to enhance their capabilities without retraining from\nscratch. A key challenge in this domain is \\textit{catastrophic forgetting}\n(CF), which hampers performance during Continuous Pre-training (CPT) and\nContinuous Supervised Fine-Tuning (CSFT). We propose \\textbf{Control LLM}, a\nnovel approach that leverages parallel pre-trained and expanded transformer\nblocks, aligning their hidden-states through interpolation strategies This\nmethod effectively preserves performance on existing tasks while seamlessly\nintegrating new knowledge.\n  Extensive experiments demonstrate the effectiveness of Control LLM in both\nCPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in\nmathematical reasoning ($+14.4\\%$ on Math-Hard) and coding performance ($+10\\%$\non MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\\%$\non C-Eval, $+6.8\\%$ on CMMLU, and $+30.2\\%$ on CMMLU-0shot-CoT). It surpasses\nexisting methods and achieves SOTA among open-source models tuned from the same\nbase model, using substantially less data and compute. Crucially, these gains\nare realized while preserving strong original capabilities, with minimal\ndegradation ($<4.3\\% \\text{on MMLU}$) compared to $>35\\%$ in open-source Math\nand Coding models. This approach has been successfully deployed in LinkedIn's\nGenAI-powered job seeker and Ads unit products.\n  To support further research, we release the training and evaluation code\n(https://github.com/linkedin/ControlLLM) along with models trained on public\ndatasets (https://huggingface.co/ControlLLM) to the community."
                },
                "authors": [
                    {
                        "name": "Haichao Wei"
                    },
                    {
                        "name": "Yunxiang Ren"
                    },
                    {
                        "name": "Zhoutong Fu"
                    },
                    {
                        "name": "Aman Lunia"
                    },
                    {
                        "name": "Yi-Lin Chen"
                    },
                    {
                        "name": "Alice Leung"
                    },
                    {
                        "name": "Ya Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ya Xu"
                },
                "author": "Ya Xu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18214v1",
                "updated": "2025-01-30T09:15:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    15,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T09:15:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    9,
                    15,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "RIO EPICS device support application case study on an ion source control\n  system (ISHP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIO EPICS device support application case study on an ion source control\n  system (ISHP)"
                },
                "summary": "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture."
                },
                "authors": [
                    {
                        "name": "Diego Sanz"
                    },
                    {
                        "name": "Mariano Ruiz"
                    },
                    {
                        "name": "Mikel Eguiraun"
                    },
                    {
                        "name": "Iigo Arredondo"
                    },
                    {
                        "name": "Inari Badillo"
                    },
                    {
                        "name": "Josu Jugo"
                    },
                    {
                        "name": "Jess Vega"
                    },
                    {
                        "name": "Rodrigo Castro"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Castro"
                },
                "author": "Rodrigo Castro",
                "arxiv_doi": "10.1016/j.fusengdes.2015.02.040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2015.02.040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.18214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18968v2",
                "updated": "2025-01-30T08:55:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    55,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-11T13:47:47Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    47,
                    47,
                    2,
                    255,
                    0
                ],
                "title": "Safety challenges of AI in medicine in the era of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety challenges of AI in medicine in the era of large language models"
                },
                "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs), have unlocked significant potential to enhance the\nquality and efficiency of medical care. By introducing a novel way to interact\nwith AI and data through natural language, LLMs offer new opportunities for\nmedical practitioners, patients, and researchers. However, as AI and LLMs\nbecome more powerful and especially achieve superhuman performance in some\nmedical tasks, public concerns over their safety have intensified. These\nconcerns about AI safety have emerged as the most significant obstacles to the\nadoption of AI in medicine. In response, this review examines emerging risks in\nAI utilization during the LLM era. First, we explore LLM-specific safety\nchallenges from functional and communication perspectives, addressing issues\nacross data collection, model training, and real-world application. We then\nconsider inherent safety problems shared by all AI systems, along with\nadditional complications introduced by LLMs. Last, we discussed how safety\nissues of using AI in clinical practice and healthcare system operation would\nundermine trust among patient, clinicians and the public, and how to build\nconfidence in these systems. By emphasizing the development of safe AI, we\nbelieve these technologies can be more rapidly and reliably integrated into\neveryday medical practice to benefit both patients and clinicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs), have unlocked significant potential to enhance the\nquality and efficiency of medical care. By introducing a novel way to interact\nwith AI and data through natural language, LLMs offer new opportunities for\nmedical practitioners, patients, and researchers. However, as AI and LLMs\nbecome more powerful and especially achieve superhuman performance in some\nmedical tasks, public concerns over their safety have intensified. These\nconcerns about AI safety have emerged as the most significant obstacles to the\nadoption of AI in medicine. In response, this review examines emerging risks in\nAI utilization during the LLM era. First, we explore LLM-specific safety\nchallenges from functional and communication perspectives, addressing issues\nacross data collection, model training, and real-world application. We then\nconsider inherent safety problems shared by all AI systems, along with\nadditional complications introduced by LLMs. Last, we discussed how safety\nissues of using AI in clinical practice and healthcare system operation would\nundermine trust among patient, clinicians and the public, and how to build\nconfidence in these systems. By emphasizing the development of safe AI, we\nbelieve these technologies can be more rapidly and reliably integrated into\neveryday medical practice to benefit both patients and clinicians."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Nicole Xi Zhang"
                    },
                    {
                        "name": "Hongyu He"
                    },
                    {
                        "name": "Trang Nguyen"
                    },
                    {
                        "name": "Kun-Hsing Yu"
                    },
                    {
                        "name": "Hao Deng"
                    },
                    {
                        "name": "Cynthia Brandt"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Ching-Yu Cheng"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dianbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Liu"
                },
                "author": "Dianbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20132v4",
                "updated": "2025-01-30T08:54:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    54,
                    54,
                    3,
                    30,
                    0
                ],
                "published": "2024-05-30T15:10:59Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    10,
                    59,
                    3,
                    151,
                    0
                ],
                "title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs."
                },
                "authors": [
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bck"
                },
                "author": "Thomas Bck",
                "arxiv_comment": "Accepted at IEEE TEVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08436v2",
                "updated": "2025-01-30T08:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    6,
                    33,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-11T00:45:50Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    0,
                    45,
                    50,
                    4,
                    285,
                    0
                ],
                "title": "Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models"
                },
                "summary": "When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults."
                },
                "authors": [
                    {
                        "name": "Zi'ou Zheng"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Martin Renqiang Min"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "arxiv_comment": "Accepted by EMNLP2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16513v2",
                "updated": "2025-01-30T08:00:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    0,
                    14,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T21:26:37Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    21,
                    26,
                    37,
                    0,
                    27,
                    0
                ],
                "title": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation."
                },
                "authors": [
                    {
                        "name": "Sudarshan Kamath Barkur"
                    },
                    {
                        "name": "Sigurd Schacht"
                    },
                    {
                        "name": "Johannes Scholl"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Scholl"
                },
                "author": "Johannes Scholl",
                "arxiv_comment": "Corrected Version - Solved Some Issues with reference compilation by\n  latex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16493v3",
                "updated": "2025-01-30T07:23:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    23,
                    32,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-24T22:36:44Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    22,
                    36,
                    44,
                    1,
                    268,
                    0
                ],
                "title": "NoTeeline: Supporting Real-Time, Personalized Notetaking with\n  LLM-Enhanced Micronotes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoTeeline: Supporting Real-Time, Personalized Notetaking with\n  LLM-Enhanced Micronotes"
                },
                "summary": "Taking notes quickly while effectively capturing key information can be\nchallenging, especially when watching videos that present simultaneous visual\nand auditory streams. Manually taken notes often miss crucial details due to\nthe fast-paced nature of the content, while automatically generated notes fail\nto incorporate user preferences and discourage active engagement with the\ncontent. To address this, we propose an interactive system, NoTeeline, for\nsupporting real-time, personalized notetaking. Given micronotes, NoTeeline\nautomatically expands them into full-fledged notes using a Large Language Model\n(LLM). The generated notes build on the content of micronotes by adding\nrelevant details while maintaining consistency with the user's writing style.\nIn a within-subjects study (n=12), we found that NoTeeline creates high-quality\nnotes that capture the essence of participant micronotes with 93.2% factual\ncorrectness and accurately align with participant writing style (8.33%\nimprovement). Using NoTeeline, participants could capture their desired notes\nwith significantly reduced mental effort, writing 47.0% less text and\ncompleting their notes in 43.9% less time compared to a manual notetaking\nbaseline. Our results suggest that NoTeeline enables users to integrate LLM\nassistance in a familiar notetaking workflow while ensuring consistency with\ntheir preferences - providing an example of how to address broader challenges\nin designing AI-assisted tools to augment human capabilities without\ncompromising user autonomy and personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taking notes quickly while effectively capturing key information can be\nchallenging, especially when watching videos that present simultaneous visual\nand auditory streams. Manually taken notes often miss crucial details due to\nthe fast-paced nature of the content, while automatically generated notes fail\nto incorporate user preferences and discourage active engagement with the\ncontent. To address this, we propose an interactive system, NoTeeline, for\nsupporting real-time, personalized notetaking. Given micronotes, NoTeeline\nautomatically expands them into full-fledged notes using a Large Language Model\n(LLM). The generated notes build on the content of micronotes by adding\nrelevant details while maintaining consistency with the user's writing style.\nIn a within-subjects study (n=12), we found that NoTeeline creates high-quality\nnotes that capture the essence of participant micronotes with 93.2% factual\ncorrectness and accurately align with participant writing style (8.33%\nimprovement). Using NoTeeline, participants could capture their desired notes\nwith significantly reduced mental effort, writing 47.0% less text and\ncompleting their notes in 43.9% less time compared to a manual notetaking\nbaseline. Our results suggest that NoTeeline enables users to integrate LLM\nassistance in a familiar notetaking workflow while ensuring consistency with\ntheir preferences - providing an example of how to address broader challenges\nin designing AI-assisted tools to augment human capabilities without\ncompromising user autonomy and personalization."
                },
                "authors": [
                    {
                        "name": "Faria Huq"
                    },
                    {
                        "name": "Abdus Samee"
                    },
                    {
                        "name": "David Chuan-en Lin"
                    },
                    {
                        "name": "Xiaodi Alice Tang"
                    },
                    {
                        "name": "Jeffrey P. Bigham"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey P. Bigham"
                },
                "author": "Jeffrey P. Bigham",
                "arxiv_doi": "10.1145/3708359.3712086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conditionally accepted to IUI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17310v2",
                "updated": "2025-01-30T07:15:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    15,
                    4,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T21:43:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding"
                },
                "summary": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model. As\nLLMs' world model is a fundamental prerequisite for many real-world tasks,\ne.g., human-AI teaming, our findings have broad implications for the AI\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model. As\nLLMs' world model is a fundamental prerequisite for many real-world tasks,\ne.g., human-AI teaming, our findings have broad implications for the AI\ncommunity."
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Nikunj Harlalka"
                    },
                    {
                        "name": "Sameer Narendran"
                    },
                    {
                        "name": "Alexander Cheung"
                    },
                    {
                        "name": "Sizhe Gao"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy T. Rogers"
                },
                "author": "Timothy T. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12746v3",
                "updated": "2025-01-30T07:11:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    11,
                    6,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T09:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering"
                },
                "summary": "When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted evidence analysis to provide high-quality answers. Current\nLLM-based answer generation methods lack a detailed definition and learning\nprocess for evidence analysis, leading to the risk of error propagation and\nhallucinations while using evidence. Although increasing the parameter size of\nLLMs can alleviate these issues, it also presents challenges in model training\nand deployment with limited resources. In this study, we propose EvidenceMap,\nwhich aims to enable a tiny pre-trained language model to explicitly learn\nmultiple aspects of biomedical evidence, including supportive evaluation,\nlogical correlation and content summarization, thereby latently guiding a small\ngenerative model (around 3B parameters) to provide textual responses.\nExperimental results demonstrate that our method, fine-tuning a language model\nwith 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in\nreference-based quality and accuracy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted evidence analysis to provide high-quality answers. Current\nLLM-based answer generation methods lack a detailed definition and learning\nprocess for evidence analysis, leading to the risk of error propagation and\nhallucinations while using evidence. Although increasing the parameter size of\nLLMs can alleviate these issues, it also presents challenges in model training\nand deployment with limited resources. In this study, we propose EvidenceMap,\nwhich aims to enable a tiny pre-trained language model to explicitly learn\nmultiple aspects of biomedical evidence, including supportive evaluation,\nlogical correlation and content summarization, thereby latently guiding a small\ngenerative model (around 3B parameters) to provide textual responses.\nExperimental results demonstrate that our method, fine-tuning a language model\nwith 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in\nreference-based quality and accuracy, respectively."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Jian Wan"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v3",
                "updated": "2025-01-30T07:08:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    7,
                    8,
                    45,
                    3,
                    30,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation"
                },
                "summary": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "arxiv_comment": "Name change: LlamaFusion to LMFusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v3",
                "updated": "2025-01-30T06:46:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    46,
                    59,
                    3,
                    30,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15797v2",
                "updated": "2025-01-30T06:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-27T05:46:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models"
                },
                "summary": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language."
                },
                "authors": [
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Hongyi Zhao"
                    },
                    {
                        "name": "Tianshuo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianshuo Yang"
                },
                "author": "Tianshuo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18160v1",
                "updated": "2025-01-30T05:56:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    56,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:56:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    56,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing"
                },
                "summary": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios.\n  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable\nprecise and efficient repository-level code auditing. Equipped with the agent\nmemory, RepoAudit explores the code repository on demand, analyzing data-flow\nfacts along different feasible program paths in individual functions. It also\nintroduces the validator to check the data-flow facts for hallucination\nmitigation and examine the satisfiability of path conditions of potential buggy\npaths, which enables RepoAudit to discard false positives in the code auditing.\nOur experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully\nfinds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per\nproject on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios.\n  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable\nprecise and efficient repository-level code auditing. Equipped with the agent\nmemory, RepoAudit explores the code repository on demand, analyzing data-flow\nfacts along different feasible program paths in individual functions. It also\nintroduces the validator to check the data-flow facts for hallucination\nmitigation and examine the satisfiability of path conditions of potential buggy\npaths, which enables RepoAudit to discard false positives in the code auditing.\nOur experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully\nfinds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per\nproject on average."
                },
                "authors": [
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 8 tables, 5 figures, 3 listings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18158v1",
                "updated": "2025-01-30T05:48:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:48:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study"
                },
                "summary": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data."
                },
                "authors": [
                    {
                        "name": "Yuchen Lei"
                    },
                    {
                        "name": "Yuexin Xiang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Rafael Dowsley"
                    },
                    {
                        "name": "Tsz Hon Yuen"
                    },
                    {
                        "name": "Jiangshan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangshan Yu"
                },
                "author": "Jiangshan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18157v1",
                "updated": "2025-01-30T05:46:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    46,
                    30,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:46:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    46,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment"
                },
                "summary": "Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%."
                },
                "authors": [
                    {
                        "name": "Joanna Hong"
                    },
                    {
                        "name": "Sanjeel Parekh"
                    },
                    {
                        "name": "Honglie Chen"
                    },
                    {
                        "name": "Jacob Donley"
                    },
                    {
                        "name": "Ke Tan"
                    },
                    {
                        "name": "Buye Xu"
                    },
                    {
                        "name": "Anurag Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Kumar"
                },
                "author": "Anurag Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18154v1",
                "updated": "2025-01-30T05:39:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    39,
                    1,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T05:39:01Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    39,
                    1,
                    3,
                    30,
                    0
                ],
                "title": "Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models"
                },
                "summary": "Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Yichen Xiao"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Hongyang Zhao"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Malu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Malu Zhang"
                },
                "author": "Malu Zhang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16288v2",
                "updated": "2025-01-30T05:31:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    31,
                    9,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-23T08:43:28Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    43,
                    28,
                    1,
                    205,
                    0
                ],
                "title": "On the Use of Immersive Digital Technologies for Designing and Operating\n  UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Use of Immersive Digital Technologies for Designing and Operating\n  UAVs"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) offer agile, secure and efficient solutions\nfor communication relay networks. However, their modeling and control are\nchallenging, and the mismatch between simulations and actual conditions limits\nreal-world deployment. Moreover, improving situational awareness is essential.\nSeveral studies proposed integrating the operation of UAVs with immersive\ndigital technologies such as Digital Twin (DT) and Extended Reality (XR) to\novercome these challenges. This paper provides a comprehensive overview of the\nlatest research and developments involving immersive digital technologies for\nUAVs. We explore the use of Machine Learning (ML) techniques, particularly Deep\nReinforcement Learning (DRL), to improve the capabilities of DT for UAV\nsystems. We provide discussion, identify key research gaps, and propose\ncountermeasures based on Generative AI (GAI), emphasizing the significant role\nof AI in advancing DT technology for UAVs. Furthermore, we review the\nliterature, provide discussion, and examine how the XR technology can transform\nUAV operations with the support of GAI, and explore its practical challenges.\nFinally, we propose future research directions to further develop the\napplication of immersive digital technologies for UAV operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) offer agile, secure and efficient solutions\nfor communication relay networks. However, their modeling and control are\nchallenging, and the mismatch between simulations and actual conditions limits\nreal-world deployment. Moreover, improving situational awareness is essential.\nSeveral studies proposed integrating the operation of UAVs with immersive\ndigital technologies such as Digital Twin (DT) and Extended Reality (XR) to\novercome these challenges. This paper provides a comprehensive overview of the\nlatest research and developments involving immersive digital technologies for\nUAVs. We explore the use of Machine Learning (ML) techniques, particularly Deep\nReinforcement Learning (DRL), to improve the capabilities of DT for UAV\nsystems. We provide discussion, identify key research gaps, and propose\ncountermeasures based on Generative AI (GAI), emphasizing the significant role\nof AI in advancing DT technology for UAVs. Furthermore, we review the\nliterature, provide discussion, and examine how the XR technology can transform\nUAV operations with the support of GAI, and explore its practical challenges.\nFinally, we propose future research directions to further develop the\napplication of immersive digital technologies for UAV operation."
                },
                "authors": [
                    {
                        "name": "Yousef Emami"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Luis Almeida"
                    },
                    {
                        "name": "Sai Zou"
                    },
                    {
                        "name": "Wei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ni"
                },
                "author": "Wei Ni",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53-02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; I.6; C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13101v2",
                "updated": "2025-01-30T04:51:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    51,
                    41,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-18T02:19:00Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    2,
                    19,
                    0,
                    3,
                    200,
                    0
                ],
                "title": "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach"
                },
                "summary": "Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length."
                },
                "authors": [
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Zhang"
                },
                "author": "Zhiqiang Zhang",
                "arxiv_comment": "Accepted by WWW2025 Agent4IR Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12619v2",
                "updated": "2025-01-30T04:25:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    25,
                    0,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-22T03:57:52Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    57,
                    52,
                    2,
                    22,
                    0
                ],
                "title": "Distillation Quantification for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Quantification for Large Language Models"
                },
                "summary": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Chang Ao"
                    },
                    {
                        "name": "Kaige Li"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Sirui He"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18128v1",
                "updated": "2025-01-30T04:20:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    20,
                    16,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T04:20:16Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    20,
                    16,
                    3,
                    30,
                    0
                ],
                "title": "Unraveling the Capabilities of Language Models in News Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the Capabilities of Language Models in News Summarization"
                },
                "summary": "Given the recent introduction of multiple language models and the ongoing\ndemand for improved Natural Language Processing tasks, particularly\nsummarization, this work provides a comprehensive benchmarking of 20 recent\nlanguage models, focusing on smaller ones for the news summarization task. In\nthis work, we systematically test the capabilities and effectiveness of these\nmodels in summarizing news article texts which are written in different styles\nand presented in three distinct datasets. Specifically, we focus in this study\non zero-shot and few-shot learning settings and we apply a robust evaluation\nmethodology that combines different evaluation concepts including automatic\nmetrics, human evaluation, and LLM-as-a-judge. Interestingly, including\ndemonstration examples in the few-shot learning setting did not enhance models'\nperformance and, in some cases, even led to worse quality of the generated\nsummaries. This issue arises mainly due to the poor quality of the gold\nsummaries that have been used as reference summaries, which negatively impacts\nthe models' performance. Furthermore, our study's results highlight the\nexceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate\ndue to their advanced capabilities. However, among the public models evaluated,\ncertain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B\nand Zephyr-7B-Beta demonstrated promising results. These models showed\nsignificant potential, positioning them as competitive alternatives to large\nmodels for the task of news summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the recent introduction of multiple language models and the ongoing\ndemand for improved Natural Language Processing tasks, particularly\nsummarization, this work provides a comprehensive benchmarking of 20 recent\nlanguage models, focusing on smaller ones for the news summarization task. In\nthis work, we systematically test the capabilities and effectiveness of these\nmodels in summarizing news article texts which are written in different styles\nand presented in three distinct datasets. Specifically, we focus in this study\non zero-shot and few-shot learning settings and we apply a robust evaluation\nmethodology that combines different evaluation concepts including automatic\nmetrics, human evaluation, and LLM-as-a-judge. Interestingly, including\ndemonstration examples in the few-shot learning setting did not enhance models'\nperformance and, in some cases, even led to worse quality of the generated\nsummaries. This issue arises mainly due to the poor quality of the gold\nsummaries that have been used as reference summaries, which negatively impacts\nthe models' performance. Furthermore, our study's results highlight the\nexceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate\ndue to their advanced capabilities. However, among the public models evaluated,\ncertain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B\nand Zephyr-7B-Beta demonstrated promising results. These models showed\nsignificant potential, positioning them as competitive alternatives to large\nmodels for the task of news summarization."
                },
                "authors": [
                    {
                        "name": "Abdurrahman Odaba"
                    },
                    {
                        "name": "Gksel Biricik"
                    }
                ],
                "author_detail": {
                    "name": "Gksel Biricik"
                },
                "author": "Gksel Biricik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14232v2",
                "updated": "2025-01-30T04:02:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    2,
                    49,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-24T04:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    4,
                    40,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "Learning-Augmented Online Control for Decarbonizing Water\n  Infrastructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Control for Decarbonizing Water\n  Infrastructures"
                },
                "summary": "Water infrastructures are essential for drinking water supply, irrigation,\nfire protection, and other critical applications. However, water pumping\nsystems, which are key to transporting water to the point of use, consume\nsignificant amounts of energy and emit millions of tons of greenhouse gases\nannually. With the wide deployment of digital water meters and sensors in these\ninfrastructures, Machine Learning (ML) has the potential to optimize water\nsupply control and reduce greenhouse gas emissions. Nevertheless, the inherent\nvulnerability of ML methods in terms of worst-case performance raises safety\nconcerns when deployed in critical water infrastructures. To address this\nchallenge, we propose a learning-augmented online control algorithm, termed\nLAOC, designed to dynamically schedule the activation and/or speed of water\npumps. To ensure safety, we introduce a novel design of safe action sets for\nonline control problems. By leveraging these safe action sets, LAOC can\nprovably guarantee safety constraints while utilizing ML predictions to reduce\nenergy and environmental costs. Our analysis reveals the tradeoff between\nsafety requirements and average energy/environmental cost performance.\nAdditionally, we conduct an experimental study on a building water supply\nsystem to demonstrate the empirical performance of LAOC. The results indicate\nthat LAOC can effectively reduce environmental and energy costs while\nguaranteeing safety constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water infrastructures are essential for drinking water supply, irrigation,\nfire protection, and other critical applications. However, water pumping\nsystems, which are key to transporting water to the point of use, consume\nsignificant amounts of energy and emit millions of tons of greenhouse gases\nannually. With the wide deployment of digital water meters and sensors in these\ninfrastructures, Machine Learning (ML) has the potential to optimize water\nsupply control and reduce greenhouse gas emissions. Nevertheless, the inherent\nvulnerability of ML methods in terms of worst-case performance raises safety\nconcerns when deployed in critical water infrastructures. To address this\nchallenge, we propose a learning-augmented online control algorithm, termed\nLAOC, designed to dynamically schedule the activation and/or speed of water\npumps. To ensure safety, we introduce a novel design of safe action sets for\nonline control problems. By leveraging these safe action sets, LAOC can\nprovably guarantee safety constraints while utilizing ML predictions to reduce\nenergy and environmental costs. Our analysis reveals the tradeoff between\nsafety requirements and average energy/environmental cost performance.\nAdditionally, we conduct an experimental study on a building water supply\nsystem to demonstrate the empirical performance of LAOC. The results indicate\nthat LAOC can effectively reduce environmental and energy costs while\nguaranteeing safety constraints."
                },
                "authors": [
                    {
                        "name": "Jianyi Yang"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Tongxin Li"
                    },
                    {
                        "name": "Adam Wierman"
                    },
                    {
                        "name": "Shaolei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shaolei Ren"
                },
                "author": "Shaolei Ren",
                "arxiv_comment": "Accepted by e-Energy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17186v2",
                "updated": "2025-01-30T04:02:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    4,
                    2,
                    48,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-26T09:43:39Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    9,
                    43,
                    39,
                    6,
                    26,
                    0
                ],
                "title": "Complete Chess Games Enable LLM Become A Chess Master",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complete Chess Games Enable LLM Become A Chess Master"
                },
                "summary": "Large language models (LLM) have shown remarkable abilities in text\ngeneration, question answering, language translation, reasoning and many other\ntasks. It continues to advance rapidly and is becoming increasingly influential\nin various fields, from technology and business to education and entertainment.\nDespite LLM's success in multiple areas, its ability to play abstract games,\nsuch as chess, is underexplored. Chess-playing requires the language models to\noutput legal and reasonable moves from textual inputs. Here, we propose the\nLarge language model ChessLLM to play full chess games. We transform the game\ninto a textual format with the best move represented in the Forsyth-Edwards\nNotation. We show that by simply supervised fine-tuning, our model has achieved\na professional-level Elo rating of 1788 in matches against the standard\nElo-rated Stockfish when permitted to sample 10 times. We further show that\ndata quality is important. Long-round data supervision enjoys a 350 Elo rating\nimprovement over short-round data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have shown remarkable abilities in text\ngeneration, question answering, language translation, reasoning and many other\ntasks. It continues to advance rapidly and is becoming increasingly influential\nin various fields, from technology and business to education and entertainment.\nDespite LLM's success in multiple areas, its ability to play abstract games,\nsuch as chess, is underexplored. Chess-playing requires the language models to\noutput legal and reasonable moves from textual inputs. Here, we propose the\nLarge language model ChessLLM to play full chess games. We transform the game\ninto a textual format with the best move represented in the Forsyth-Edwards\nNotation. We show that by simply supervised fine-tuning, our model has achieved\na professional-level Elo rating of 1788 in matches against the standard\nElo-rated Stockfish when permitted to sample 10 times. We further show that\ndata quality is important. Long-round data supervision enjoys a 350 Elo rating\nimprovement over short-round data."
                },
                "authors": [
                    {
                        "name": "Yinqi Zhang"
                    },
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Haolong Li"
                    },
                    {
                        "name": "Kedi Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18123v1",
                "updated": "2025-01-30T03:55:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    55,
                    56,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T03:55:56Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    55,
                    56,
                    3,
                    30,
                    0
                ],
                "title": "Battery State of Health Estimation Using LLM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Battery State of Health Estimation Using LLM Framework"
                },
                "summary": "Battery health monitoring is critical for the efficient and reliable\noperation of electric vehicles (EVs). This study introduces a transformer-based\nframework for estimating the State of Health (SoH) and predicting the Remaining\nUseful Life (RUL) of lithium titanate (LTO) battery cells by utilizing both\ncycle-based and instantaneous discharge data. Testing on eight LTO cells under\nvarious cycling conditions over 500 cycles, we demonstrate the impact of charge\ndurations on energy storage trends and apply Differential Voltage Analysis\n(DVA) to monitor capacity changes (dQ/dV) across voltage ranges. Our LLM model\nachieves superior performance, with a Mean Absolute Error (MAE) as low as\n0.87\\% and varied latency metrics that support efficient processing,\ndemonstrating its strong potential for real-time integration into EVs. The\nframework effectively identifies early signs of degradation through anomaly\ndetection in high-resolution data, facilitating predictive maintenance to\nprevent sudden battery failures and enhance energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Battery health monitoring is critical for the efficient and reliable\noperation of electric vehicles (EVs). This study introduces a transformer-based\nframework for estimating the State of Health (SoH) and predicting the Remaining\nUseful Life (RUL) of lithium titanate (LTO) battery cells by utilizing both\ncycle-based and instantaneous discharge data. Testing on eight LTO cells under\nvarious cycling conditions over 500 cycles, we demonstrate the impact of charge\ndurations on energy storage trends and apply Differential Voltage Analysis\n(DVA) to monitor capacity changes (dQ/dV) across voltage ranges. Our LLM model\nachieves superior performance, with a Mean Absolute Error (MAE) as low as\n0.87\\% and varied latency metrics that support efficient processing,\ndemonstrating its strong potential for real-time integration into EVs. The\nframework effectively identifies early signs of degradation through anomaly\ndetection in high-resolution data, facilitating predictive maintenance to\nprevent sudden battery failures and enhance energy efficiency."
                },
                "authors": [
                    {
                        "name": "Aybars Yunusoglu"
                    },
                    {
                        "name": "Dexter Le"
                    },
                    {
                        "name": "Karn Tiwari"
                    },
                    {
                        "name": "Murat Isik"
                    },
                    {
                        "name": "I. Can Dikmen"
                    }
                ],
                "author_detail": {
                    "name": "I. Can Dikmen"
                },
                "author": "I. Can Dikmen",
                "arxiv_comment": "Accepted at The 26th International Symposium on Quality Electronic\n  Design (ISQED'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18119v1",
                "updated": "2025-01-30T03:40:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    40,
                    20,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T03:40:20Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    40,
                    20,
                    3,
                    30,
                    0
                ],
                "title": "Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models"
                },
                "summary": "Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods."
                },
                "authors": [
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Tianzhe Zhao"
                    },
                    {
                        "name": "Kai He"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Ling Huang"
                    },
                    {
                        "name": "Jingying Ma"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18103v1",
                "updated": "2025-01-30T03:01:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    1,
                    1,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T03:01:01Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    1,
                    1,
                    3,
                    30,
                    0
                ],
                "title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM\n  Interactions"
                },
                "summary": "Traditional text-based human-AI interactions often adhere to a strict\nturn-taking approach. In this research, we propose a novel approach that\nincorporates overlapping messages, mirroring natural human conversations.\nThrough a formative study, we observed that even in text-based contexts, users\ninstinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B:\nyeah.\" To capitalize on these insights, we developed OverlapBot, a prototype\nchatbot where both AI and users can initiate overlapping. Our user study\nrevealed that OverlapBot was perceived as more communicative and immersive than\ntraditional turn-taking chatbot, fostering faster and more natural\ninteractions. Our findings contribute to the understanding of design space for\noverlapping interactions. We also provide recommendations for implementing\noverlap-capable AI interactions to enhance the fluidity and engagement of\ntext-based conversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional text-based human-AI interactions often adhere to a strict\nturn-taking approach. In this research, we propose a novel approach that\nincorporates overlapping messages, mirroring natural human conversations.\nThrough a formative study, we observed that even in text-based contexts, users\ninstinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B:\nyeah.\" To capitalize on these insights, we developed OverlapBot, a prototype\nchatbot where both AI and users can initiate overlapping. Our user study\nrevealed that OverlapBot was perceived as more communicative and immersive than\ntraditional turn-taking chatbot, fostering faster and more natural\ninteractions. Our findings contribute to the understanding of design space for\noverlapping interactions. We also provide recommendations for implementing\noverlap-capable AI interactions to enhance the fluidity and engagement of\ntext-based conversations."
                },
                "authors": [
                    {
                        "name": "JiWoo Kim"
                    },
                    {
                        "name": "Minsuk Chang"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18100v1",
                "updated": "2025-01-30T02:47:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    47,
                    9,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T02:47:09Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    47,
                    9,
                    3,
                    30,
                    0
                ],
                "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via\n  Post-fine-tuning Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via\n  Post-fine-tuning Perturbation"
                },
                "summary": "Harmful fine-tuning attack introduces significant security risks to the\nfine-tuning services. Mainstream defenses aim to vaccinate the model such that\nthe later harmful fine-tuning attack is less effective. However, our evaluation\nresults show that such defenses are fragile -- with a few fine-tuning steps,\nthe model still can learn the harmful knowledge. To this end, we do further\nexperiment and find that an embarrassingly simple solution -- adding purely\nrandom perturbations to the fine-tuned model, can recover the model from\nharmful behavior, though it leads to a degradation in the model's fine-tuning\nperformance. To address the degradation of fine-tuning performance, we further\npropose Panacea, which optimizes an adaptive perturbation that will be applied\nto the model after fine-tuning. Panacea maintains model's safety alignment\nperformance without compromising downstream fine-tuning performance.\nComprehensive experiments are conducted on different harmful ratios,\nfine-tuning tasks and mainstream LLMs, where the average harmful scores are\nreduced by up-to 21.5%, while maintaining fine-tuning performance. As a\nby-product, we analyze the optimized perturbation and show that different\nlayers in various LLMs have distinct safety coefficients. Source code available\nat https://github.com/w-yibo/Panacea",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmful fine-tuning attack introduces significant security risks to the\nfine-tuning services. Mainstream defenses aim to vaccinate the model such that\nthe later harmful fine-tuning attack is less effective. However, our evaluation\nresults show that such defenses are fragile -- with a few fine-tuning steps,\nthe model still can learn the harmful knowledge. To this end, we do further\nexperiment and find that an embarrassingly simple solution -- adding purely\nrandom perturbations to the fine-tuned model, can recover the model from\nharmful behavior, though it leads to a degradation in the model's fine-tuning\nperformance. To address the degradation of fine-tuning performance, we further\npropose Panacea, which optimizes an adaptive perturbation that will be applied\nto the model after fine-tuning. Panacea maintains model's safety alignment\nperformance without compromising downstream fine-tuning performance.\nComprehensive experiments are conducted on different harmful ratios,\nfine-tuning tasks and mainstream LLMs, where the average harmful scores are\nreduced by up-to 21.5%, while maintaining fine-tuning performance. As a\nby-product, we analyze the optimized perturbation and show that different\nlayers in various LLMs have distinct safety coefficients. Source code available\nat https://github.com/w-yibo/Panacea"
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Huanjin Yao"
                    },
                    {
                        "name": "Haotian Luo"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17799v2",
                "updated": "2025-01-30T02:23:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    23,
                    25,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-29T17:38:39Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    38,
                    39,
                    2,
                    29,
                    0
                ],
                "title": "Leveraging Multimodal LLM for Inspirational User Interface Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Multimodal LLM for Inspirational User Interface Search"
                },
                "summary": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies."
                },
                "authors": [
                    {
                        "name": "Seokhyeon Park"
                    },
                    {
                        "name": "Yumin Song"
                    },
                    {
                        "name": "Soohyun Lee"
                    },
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "arxiv_doi": "10.1145/3706598.3714213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems (CHI '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18099v1",
                "updated": "2025-01-30T02:21:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    21,
                    59,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T02:21:59Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    21,
                    59,
                    3,
                    30,
                    0
                ],
                "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models."
                },
                "authors": [
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Tianlu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Wang"
                },
                "author": "Tianlu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18096v1",
                "updated": "2025-01-30T02:16:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    16,
                    35,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T02:16:35Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    16,
                    35,
                    3,
                    30,
                    0
                ],
                "title": "LLMs can see and hear without any training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can see and hear without any training"
                },
                "summary": "We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple,\ntraining-free approach, to imbue multimodal capabilities into your favorite\nLLM. Leveraging their innate ability to perform multi-step reasoning, MILS\nprompts the LLM to generate candidate outputs, each of which are scored and fed\nback iteratively, eventually generating a solution to the task. This enables\nvarious applications that typically require training specialized models on\ntask-specific data. In particular, we establish a new state-of-the-art on\nemergent zero-shot image, video and audio captioning. MILS seamlessly applies\nto media generation as well, discovering prompt rewrites to improve\ntext-to-image generation, and even edit prompts for style transfer! Finally,\nbeing a gradient-free optimization approach, MILS can invert multimodal\nembeddings into text, enabling applications like cross-modal arithmetic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple,\ntraining-free approach, to imbue multimodal capabilities into your favorite\nLLM. Leveraging their innate ability to perform multi-step reasoning, MILS\nprompts the LLM to generate candidate outputs, each of which are scored and fed\nback iteratively, eventually generating a solution to the task. This enables\nvarious applications that typically require training specialized models on\ntask-specific data. In particular, we establish a new state-of-the-art on\nemergent zero-shot image, video and audio captioning. MILS seamlessly applies\nto media generation as well, discovering prompt rewrites to improve\ntext-to-image generation, and even edit prompts for style transfer! Finally,\nbeing a gradient-free optimization approach, MILS can invert multimodal\nembeddings into text, enabling applications like cross-modal arithmetic."
                },
                "authors": [
                    {
                        "name": "Kumar Ashutosh"
                    },
                    {
                        "name": "Yossi Gandelsman"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Ishan Misra"
                    },
                    {
                        "name": "Rohit Girdhar"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Girdhar"
                },
                "author": "Rohit Girdhar",
                "arxiv_comment": "Code: https://github.com/facebookresearch/MILS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18094v1",
                "updated": "2025-01-30T02:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T02:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "title": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates"
                },
                "summary": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this\n\\href{https://github.com/MaeChd/AlphaAdam}{link}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this\n\\href{https://github.com/MaeChd/AlphaAdam}{link}"
                },
                "authors": [
                    {
                        "name": "Da Chang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Ganzhao Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Ganzhao Yuan"
                },
                "author": "Ganzhao Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06711v2",
                "updated": "2025-01-30T01:47:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    47,
                    47,
                    3,
                    30,
                    0
                ],
                "published": "2024-04-10T03:35:51Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    3,
                    35,
                    51,
                    2,
                    101,
                    0
                ],
                "title": "MathVC: An LLM-Simulated Multi-Character Virtual Classroom for\n  Mathematics Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathVC: An LLM-Simulated Multi-Character Virtual Classroom for\n  Mathematics Education"
                },
                "summary": "Mathematical modeling (MM) is considered a fundamental skill for students in\nSTEM disciplines. Practicing the MM skill is often the most effective when\nstudents can engage in group discussion and collaborative problem-solving.\nHowever, due to unevenly distributed teachers and educational resources needed\nto monitor such group activities, students do not always receive equal\nopportunities for this practice. Excitingly, large language models (LLMs) have\nrecently demonstrated strong capability in both modeling mathematical problems\nand simulating characters with different traits and properties. Drawing\ninspiration from the advancement of LLMs, in this work, we present MATHVC, the\nvery first LLM-powered virtual classroom containing multiple LLM-simulated\nstudent characters, with whom a human student can practice their MM skill. To\nencourage each LLM character's behaviors to be aligned with their specified\nmath-relevant properties (termed \"characteristics alignment\") and the overall\nconversational procedure to be close to an authentic student MM discussion\n(termed \"conversational procedural alignment\"), we proposed three innovations:\nintegrating MM domain knowledge into the simulation, defining a symbolic schema\nas the ground for character simulation, and designing a meta planner at the\nplatform level to drive the conversational procedure. Through experiments and\nablation studies, we confirmed the effectiveness of our simulation approach and\nshowed the promise for MATHVC to benefit real-life students in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical modeling (MM) is considered a fundamental skill for students in\nSTEM disciplines. Practicing the MM skill is often the most effective when\nstudents can engage in group discussion and collaborative problem-solving.\nHowever, due to unevenly distributed teachers and educational resources needed\nto monitor such group activities, students do not always receive equal\nopportunities for this practice. Excitingly, large language models (LLMs) have\nrecently demonstrated strong capability in both modeling mathematical problems\nand simulating characters with different traits and properties. Drawing\ninspiration from the advancement of LLMs, in this work, we present MATHVC, the\nvery first LLM-powered virtual classroom containing multiple LLM-simulated\nstudent characters, with whom a human student can practice their MM skill. To\nencourage each LLM character's behaviors to be aligned with their specified\nmath-relevant properties (termed \"characteristics alignment\") and the overall\nconversational procedure to be close to an authentic student MM discussion\n(termed \"conversational procedural alignment\"), we proposed three innovations:\nintegrating MM domain knowledge into the simulation, defining a symbolic schema\nas the ground for character simulation, and designing a meta planner at the\nplatform level to drive the conversational procedure. Through experiments and\nablation studies, we confirmed the effectiveness of our simulation approach and\nshowed the promise for MATHVC to benefit real-life students in the future."
                },
                "authors": [
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Wenhan Lyu"
                    },
                    {
                        "name": "Wijdane Mifdal"
                    },
                    {
                        "name": "Jennifer Suh"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "arxiv_comment": "Accepted by AAAI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18081v1",
                "updated": "2025-01-30T01:29:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    29,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T01:29:46Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    29,
                    46,
                    3,
                    30,
                    0
                ],
                "title": "Normative Evaluation of Large Language Models with Everyday Moral\n  Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative Evaluation of Large Language Models with Everyday Moral\n  Dilemmas"
                },
                "summary": "The rapid adoption of large language models (LLMs) has spurred extensive\nresearch into their encoded moral norms and decision-making processes. Much of\nthis research relies on prompting LLMs with survey-style questions to assess\nhow well models are aligned with certain demographic groups, moral beliefs, or\npolitical ideologies. While informative, the adherence of these approaches to\nrelatively superficial constructs tends to oversimplify the complexity and\nnuance underlying everyday moral dilemmas. We argue that auditing LLMs along\nmore detailed axes of human interaction is of paramount importance to better\nassess the degree to which they may impact human beliefs and actions. To this\nend, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am\nI the Asshole\" (AITA) community on Reddit, where users seek moral judgments on\neveryday conflicts from other community members. We prompted seven LLMs to\nassign blame and provide explanations for over 10,000 AITA moral dilemmas. We\nthen compared the LLMs' judgments and explanations to those of Redditors and to\neach other, aiming to uncover patterns in their moral reasoning. Our results\ndemonstrate that large language models exhibit distinct patterns of moral\njudgment, varying substantially from human evaluations on the AITA subreddit.\nLLMs demonstrate moderate to high self-consistency but low inter-model\nagreement. Further analysis of model explanations reveals distinct patterns in\nhow models invoke various moral principles. These findings highlight the\ncomplexity of implementing consistent moral reasoning in artificial systems and\nthe need for careful evaluation of how different models approach ethical\njudgment. As LLMs continue to be used in roles requiring ethical\ndecision-making such as therapists and companions, careful evaluation is\ncrucial to mitigate potential biases and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) has spurred extensive\nresearch into their encoded moral norms and decision-making processes. Much of\nthis research relies on prompting LLMs with survey-style questions to assess\nhow well models are aligned with certain demographic groups, moral beliefs, or\npolitical ideologies. While informative, the adherence of these approaches to\nrelatively superficial constructs tends to oversimplify the complexity and\nnuance underlying everyday moral dilemmas. We argue that auditing LLMs along\nmore detailed axes of human interaction is of paramount importance to better\nassess the degree to which they may impact human beliefs and actions. To this\nend, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am\nI the Asshole\" (AITA) community on Reddit, where users seek moral judgments on\neveryday conflicts from other community members. We prompted seven LLMs to\nassign blame and provide explanations for over 10,000 AITA moral dilemmas. We\nthen compared the LLMs' judgments and explanations to those of Redditors and to\neach other, aiming to uncover patterns in their moral reasoning. Our results\ndemonstrate that large language models exhibit distinct patterns of moral\njudgment, varying substantially from human evaluations on the AITA subreddit.\nLLMs demonstrate moderate to high self-consistency but low inter-model\nagreement. Further analysis of model explanations reveals distinct patterns in\nhow models invoke various moral principles. These findings highlight the\ncomplexity of implementing consistent moral reasoning in artificial systems and\nthe need for careful evaluation of how different models approach ethical\njudgment. As LLMs continue to be used in roles requiring ethical\ndecision-making such as therapists and companions, careful evaluation is\ncrucial to mitigate potential biases and limitations."
                },
                "authors": [
                    {
                        "name": "Pratik S. Sachdeva"
                    },
                    {
                        "name": "Tom van Nuenen"
                    }
                ],
                "author_detail": {
                    "name": "Tom van Nuenen"
                },
                "author": "Tom van Nuenen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21159v2",
                "updated": "2025-01-30T01:29:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    29,
                    3,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-28T15:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    59,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants"
                },
                "summary": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (with 337 use cases each) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising desires above\nsafety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (with 337 use cases each) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising desires above\nsafety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants."
                },
                "authors": [
                    {
                        "name": "Lize Alberts"
                    },
                    {
                        "name": "Benjamin Ellis"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; K.4.2; H.5.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17282v2",
                "updated": "2025-01-30T01:25:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    25,
                    12,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-28T20:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    30,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "From Natural Language to Extensive-Form Game Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Extensive-Form Game Representations"
                },
                "summary": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success."
                },
                "authors": [
                    {
                        "name": "Shilong Deng"
                    },
                    {
                        "name": "Yongzhao Wang"
                    },
                    {
                        "name": "Rahul Savani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Savani"
                },
                "author": "Rahul Savani",
                "arxiv_comment": "This work has been accepted as a full paper for AAMAS 2025. This is a\n  full version of the AAMAS 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18080v1",
                "updated": "2025-01-30T01:18:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    18,
                    24,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T01:18:24Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    1,
                    18,
                    24,
                    3,
                    30,
                    0
                ],
                "title": "PAC Codes Meet CRC-Polar Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAC Codes Meet CRC-Polar Codes"
                },
                "summary": "CRC-Polar codes under SC list decoding are well-regarded for their\ncompetitive error performance. This paper examines these codes by focusing on\nminimum weight codewords and breaking them down into the rows of the polar\ntransform. Inspired by the significant impact of parity check bits and their\npositions, we apply a shifted rate-profile for polarization-adjusted\nconvolutional (PS-PAC) codes, thereby achieving similar improvements in the\nweight distribution of polar codes through precoding. The results demonstrate a\nsignificant improvement in error performance, achieving up to a 0.5 dB power\ngain with short PS-PAC codes. Additionally, leveraging convolutional precoding\nin PAC codes, we adopt a continuous deployment (masking) of parity check bits\nderived from the remainder of continuous division of the partial message\npolynomial and the CRC polynomial over frozen positions in the rate-profile.\nThis approach enhances performance for long codes, with an overall improvement\nof 0.12 dB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRC-Polar codes under SC list decoding are well-regarded for their\ncompetitive error performance. This paper examines these codes by focusing on\nminimum weight codewords and breaking them down into the rows of the polar\ntransform. Inspired by the significant impact of parity check bits and their\npositions, we apply a shifted rate-profile for polarization-adjusted\nconvolutional (PS-PAC) codes, thereby achieving similar improvements in the\nweight distribution of polar codes through precoding. The results demonstrate a\nsignificant improvement in error performance, achieving up to a 0.5 dB power\ngain with short PS-PAC codes. Additionally, leveraging convolutional precoding\nin PAC codes, we adopt a continuous deployment (masking) of parity check bits\nderived from the remainder of continuous division of the partial message\npolynomial and the CRC polynomial over frozen positions in the rate-profile.\nThis approach enhances performance for long codes, with an overall improvement\nof 0.12 dB."
                },
                "authors": [
                    {
                        "name": "Xinyi Gu"
                    },
                    {
                        "name": "Mohammad Rowshan"
                    },
                    {
                        "name": "Jinhong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jinhong Yuan"
                },
                "author": "Jinhong Yuan",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.01903",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08137v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08137v3",
                "updated": "2025-01-30T00:52:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    0,
                    52,
                    34,
                    3,
                    30,
                    0
                ],
                "published": "2024-04-11T21:48:54Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    21,
                    48,
                    54,
                    3,
                    102,
                    0
                ],
                "title": "Generative Information Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Information Retrieval Evaluation"
                },
                "summary": "In this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change."
                },
                "authors": [
                    {
                        "name": "Marwah Alaofi"
                    },
                    {
                        "name": "Negar Arabzadeh"
                    },
                    {
                        "name": "Charles L. A. Clarke"
                    },
                    {
                        "name": "Mark Sanderson"
                    }
                ],
                "author_detail": {
                    "name": "Mark Sanderson"
                },
                "author": "Mark Sanderson",
                "arxiv_doi": "10.1007/978-3-031-73147-1_6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-73147-1_6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.08137v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08137v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This chapter is part of the book Information Access in the Era of\n  Generative AI, co-edited by Chirag Shah and Ryen White",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18062v1",
                "updated": "2025-01-30T00:06:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    0,
                    6,
                    55,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T00:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    0,
                    6,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of\n  Large Language Models"
                },
                "summary": "FinanceQA is a testing suite that evaluates LLMs' performance on complex\nnumerical financial analysis tasks that mirror real-world investment work.\nDespite recent advances, current LLMs fail to meet the strict accuracy\nrequirements of financial institutions, with models failing approximately 60%\nof realistic tasks that mimic on-the-job analyses at hedge funds, private\nequity firms, investment banks, and other financial institutions. The primary\nchallenges include hand-spreading metrics, adhering to standard accounting and\ncorporate valuation conventions, and performing analysis under incomplete\ninformation - particularly in multi-step tasks requiring assumption generation.\nThis performance gap highlights the disconnect between existing LLM\ncapabilities and the demands of professional financial analysis that are\ninadequately tested by current testing architectures. Results show that\nhigher-quality training data is needed to support such tasks, which we\nexperiment with using OpenAI's fine-tuning API. FinanceQA is publicly released\nat [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinanceQA is a testing suite that evaluates LLMs' performance on complex\nnumerical financial analysis tasks that mirror real-world investment work.\nDespite recent advances, current LLMs fail to meet the strict accuracy\nrequirements of financial institutions, with models failing approximately 60%\nof realistic tasks that mimic on-the-job analyses at hedge funds, private\nequity firms, investment banks, and other financial institutions. The primary\nchallenges include hand-spreading metrics, adhering to standard accounting and\ncorporate valuation conventions, and performing analysis under incomplete\ninformation - particularly in multi-step tasks requiring assumption generation.\nThis performance gap highlights the disconnect between existing LLM\ncapabilities and the demands of professional financial analysis that are\ninadequately tested by current testing architectures. Results show that\nhigher-quality training data is needed to support such tasks, which we\nexperiment with using OpenAI's fine-tuning API. FinanceQA is publicly released\nat [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA)."
                },
                "authors": [
                    {
                        "name": "Spencer Mateega"
                    },
                    {
                        "name": "Carlos Georgescu"
                    },
                    {
                        "name": "Danny Tang"
                    }
                ],
                "author_detail": {
                    "name": "Danny Tang"
                },
                "author": "Danny Tang",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16250v2",
                "updated": "2025-01-29T23:56:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    56,
                    8,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-25T14:15:01Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    14,
                    15,
                    1,
                    5,
                    146,
                    0
                ],
                "title": "Conformal Robust Control of Linear Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Robust Control of Linear Systems"
                },
                "summary": "End-to-end engineering design pipelines, in which designs are evaluated using\nconcurrently defined optimal controllers, are becoming increasingly common in\npractice. To discover designs that perform well even under the misspecification\nof system dynamics, such end-to-end pipelines have now begun evaluating designs\nwith a robust control objective in place of the nominal optimal control setup.\nCurrent approaches of specifying such robust control subproblems, however, rely\non hand specification of perturbations anticipated to be present upon\ndeployment or margin methods that ignore problem structure, resulting in a lack\nof theoretical guarantees and overly conservative empirical performance. We,\ninstead, propose a novel methodology for LQR systems that leverages conformal\nprediction to specify such uncertainty regions in a data-driven fashion. Such\nregions have distribution-free coverage guarantees on the true system dynamics,\nin turn allowing for a probabilistic characterization of the regret of the\nresulting robust controller. We then demonstrate that such a controller can be\nefficiently produced via a novel policy gradient method that has convergence\nguarantees. We finally demonstrate the superior empirical performance of our\nmethod over alternate robust control specifications, such as $H_{\\infty}$ and\nLQR with multiplicative noise, across a collection of engineering control\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end engineering design pipelines, in which designs are evaluated using\nconcurrently defined optimal controllers, are becoming increasingly common in\npractice. To discover designs that perform well even under the misspecification\nof system dynamics, such end-to-end pipelines have now begun evaluating designs\nwith a robust control objective in place of the nominal optimal control setup.\nCurrent approaches of specifying such robust control subproblems, however, rely\non hand specification of perturbations anticipated to be present upon\ndeployment or margin methods that ignore problem structure, resulting in a lack\nof theoretical guarantees and overly conservative empirical performance. We,\ninstead, propose a novel methodology for LQR systems that leverages conformal\nprediction to specify such uncertainty regions in a data-driven fashion. Such\nregions have distribution-free coverage guarantees on the true system dynamics,\nin turn allowing for a probabilistic characterization of the regret of the\nresulting robust controller. We then demonstrate that such a controller can be\nefficiently produced via a novel policy gradient method that has convergence\nguarantees. We finally demonstrate the superior empirical performance of our\nmethod over alternate robust control specifications, such as $H_{\\infty}$ and\nLQR with multiplicative noise, across a collection of engineering control\nsystems."
                },
                "authors": [
                    {
                        "name": "Yash Patel"
                    },
                    {
                        "name": "Sahana Rayan"
                    },
                    {
                        "name": "Ambuj Tewari"
                    }
                ],
                "author_detail": {
                    "name": "Ambuj Tewari"
                },
                "author": "Ambuj Tewari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18059v1",
                "updated": "2025-01-29T23:54:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    54,
                    46,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T23:54:46Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    54,
                    46,
                    2,
                    29,
                    0
                ],
                "title": "Learning the Optimal Stopping for Early Classification within Finite\n  Horizons via Sequential Probability Ratio Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Optimal Stopping for Early Classification within Finite\n  Horizons via Sequential Probability Ratio Test"
                },
                "summary": "Time-sensitive machine learning benefits from Sequential Probability Ratio\nTest (SPRT), which provides an optimal stopping time for early classification\nof time series. However, in finite horizon scenarios, where input lengths are\nfinite, determining the optimal stopping rule becomes computationally intensive\ndue to the need for backward induction, limiting practical applicability. We\nthus introduce FIRMBOUND, an SPRT-based framework that efficiently estimates\nthe solution to backward induction from training data, bridging the gap between\noptimal stopping theory and real-world deployment. It employs density ratio\nestimation and convex function learning to provide statistically consistent\nestimators for sufficient statistic and conditional expectation, both essential\nfor solving backward induction; consequently, FIRMBOUND minimizes Bayes risk to\nreach optimality. Additionally, we present a faster alternative using Gaussian\nprocess regression, which significantly reduces training time while retaining\nlow deployment overhead, albeit with potential compromise in statistical\nconsistency. Experiments across independent and identically distributed\n(i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets\nshow that FIRMBOUND achieves optimalities in the sense of Bayes risk and\nspeed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward\noptimality when possible and reduces decision-time variance, ensuring reliable\ndecision-making. Code is publicly available at\nhttps://github.com/Akinori-F-Ebihara/FIRMBOUND",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-sensitive machine learning benefits from Sequential Probability Ratio\nTest (SPRT), which provides an optimal stopping time for early classification\nof time series. However, in finite horizon scenarios, where input lengths are\nfinite, determining the optimal stopping rule becomes computationally intensive\ndue to the need for backward induction, limiting practical applicability. We\nthus introduce FIRMBOUND, an SPRT-based framework that efficiently estimates\nthe solution to backward induction from training data, bridging the gap between\noptimal stopping theory and real-world deployment. It employs density ratio\nestimation and convex function learning to provide statistically consistent\nestimators for sufficient statistic and conditional expectation, both essential\nfor solving backward induction; consequently, FIRMBOUND minimizes Bayes risk to\nreach optimality. Additionally, we present a faster alternative using Gaussian\nprocess regression, which significantly reduces training time while retaining\nlow deployment overhead, albeit with potential compromise in statistical\nconsistency. Experiments across independent and identically distributed\n(i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets\nshow that FIRMBOUND achieves optimalities in the sense of Bayes risk and\nspeed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward\noptimality when possible and reduces decision-time variance, ensuring reliable\ndecision-making. Code is publicly available at\nhttps://github.com/Akinori-F-Ebihara/FIRMBOUND"
                },
                "authors": [
                    {
                        "name": "Akinori F. Ebihara"
                    },
                    {
                        "name": "Taiki Miyagawa"
                    },
                    {
                        "name": "Kazuyuki Sakurai"
                    },
                    {
                        "name": "Hitoshi Imaoka"
                    }
                ],
                "author_detail": {
                    "name": "Hitoshi Imaoka"
                },
                "author": "Hitoshi Imaoka",
                "arxiv_comment": "Accepted to International Conference on Learning Representations\n  (ICLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16246v2",
                "updated": "2025-01-29T23:46:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    46,
                    43,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-25T14:11:01Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    14,
                    11,
                    1,
                    5,
                    146,
                    0
                ],
                "title": "Conformal Prediction for Ensembles: Improving Efficiency via Score-Based\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction for Ensembles: Improving Efficiency via Score-Based\n  Aggregation"
                },
                "summary": "Distribution-free uncertainty estimation for ensemble methods is increasingly\ndesirable due to the widening deployment of multi-modal black-box predictive\nmodels. Conformal prediction is one approach that avoids such distributional\nassumptions. Methods for conformal aggregation have in turn been proposed for\nensembled prediction, where the prediction regions of individual models are\nmerged as to retain coverage guarantees while minimizing conservatism. Merging\nthe prediction regions directly, however, sacrifices structures present in the\nconformal scores that can further reduce conservatism. We, therefore, propose a\nnovel framework that extends the standard scalar formulation of a score\nfunction to a multivariate score that produces more efficient prediction\nregions. We then demonstrate that such a framework can be efficiently leveraged\nin both classification and predict-then-optimize regression settings downstream\nand empirically show the advantage over alternate conformal aggregation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-free uncertainty estimation for ensemble methods is increasingly\ndesirable due to the widening deployment of multi-modal black-box predictive\nmodels. Conformal prediction is one approach that avoids such distributional\nassumptions. Methods for conformal aggregation have in turn been proposed for\nensembled prediction, where the prediction regions of individual models are\nmerged as to retain coverage guarantees while minimizing conservatism. Merging\nthe prediction regions directly, however, sacrifices structures present in the\nconformal scores that can further reduce conservatism. We, therefore, propose a\nnovel framework that extends the standard scalar formulation of a score\nfunction to a multivariate score that produces more efficient prediction\nregions. We then demonstrate that such a framework can be efficiently leveraged\nin both classification and predict-then-optimize regression settings downstream\nand empirically show the advantage over alternate conformal aggregation\nmethods."
                },
                "authors": [
                    {
                        "name": "Eduardo Ochoa Rivera"
                    },
                    {
                        "name": "Yash Patel"
                    },
                    {
                        "name": "Ambuj Tewari"
                    }
                ],
                "author_detail": {
                    "name": "Ambuj Tewari"
                },
                "author": "Ambuj Tewari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18056v1",
                "updated": "2025-01-29T23:41:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    41,
                    12,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T23:41:12Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    23,
                    41,
                    12,
                    2,
                    29,
                    0
                ],
                "title": "RL-based Query Rewriting with Distilled LLM for online E-Commerce\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-based Query Rewriting with Distilled LLM for online E-Commerce\n  Systems"
                },
                "summary": "Query rewriting (QR) is a critical technique in e-commerce search, addressing\nthe lexical gap between user queries and product descriptions to enhance search\nperformance. Existing QR approaches typically fall into two categories:\ndiscriminative models and generative methods leveraging large language models\n(LLMs). Discriminative models often struggle with natural language\nunderstanding and offer limited flexibility in rewriting, while generative\nLLMs, despite producing high-quality rewrites, face high inference latency and\ncost in online settings. These limitations force offline deployment, making\nthem vulnerable to issues like information staleness and semantic drift. To\novercome these challenges, we propose a novel hybrid pipeline for QR that\nbalances efficiency and effectiveness. Our approach combines offline knowledge\ndistillation to create a lightweight but efficient student model with online\nreinforcement learning (RL) to refine query rewriting dynamically using\nreal-time feedback. A key innovation is the use of LLMs as simulated human\nfeedback, enabling scalable reward signals and cost-effective evaluation\nwithout manual annotations. Experimental results on Amazon ESCI dataset\ndemonstrate significant improvements in query relevance, diversity, and\nadaptability, as well as positive feedback from the LLM simulation. This work\ncontributes to advancing LLM capabilities for domain-specific applications,\noffering a robust solution for dynamic and complex e-commerce search\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting (QR) is a critical technique in e-commerce search, addressing\nthe lexical gap between user queries and product descriptions to enhance search\nperformance. Existing QR approaches typically fall into two categories:\ndiscriminative models and generative methods leveraging large language models\n(LLMs). Discriminative models often struggle with natural language\nunderstanding and offer limited flexibility in rewriting, while generative\nLLMs, despite producing high-quality rewrites, face high inference latency and\ncost in online settings. These limitations force offline deployment, making\nthem vulnerable to issues like information staleness and semantic drift. To\novercome these challenges, we propose a novel hybrid pipeline for QR that\nbalances efficiency and effectiveness. Our approach combines offline knowledge\ndistillation to create a lightweight but efficient student model with online\nreinforcement learning (RL) to refine query rewriting dynamically using\nreal-time feedback. A key innovation is the use of LLMs as simulated human\nfeedback, enabling scalable reward signals and cost-effective evaluation\nwithout manual annotations. Experimental results on Amazon ESCI dataset\ndemonstrate significant improvements in query relevance, diversity, and\nadaptability, as well as positive feedback from the LLM simulation. This work\ncontributes to advancing LLM capabilities for domain-specific applications,\noffering a robust solution for dynamic and complex e-commerce search\nenvironments."
                },
                "authors": [
                    {
                        "name": "Duy A. Nguyen"
                    },
                    {
                        "name": "Rishi Kesav Mohan"
                    },
                    {
                        "name": "Van Yang"
                    },
                    {
                        "name": "Pritom Saha Akash"
                    },
                    {
                        "name": "Kevin Chen-Chuan Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Chen-Chuan Chang"
                },
                "author": "Kevin Chen-Chuan Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09013v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09013v3",
                "updated": "2025-01-29T22:51:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    22,
                    51,
                    42,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-11T17:30:02Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    30,
                    2,
                    4,
                    285,
                    0
                ],
                "title": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals"
                },
                "summary": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language processing (CLP) tasks. We\nobserve consistent improvement in Part-Of-Speech tagging when providing\nadditional information about radicals, suggesting the potential to enhance CLP\nby integrating sub-character information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language processing (CLP) tasks. We\nobserve consistent improvement in Part-Of-Speech tagging when providing\nadditional information about radicals, suggesting the potential to enhance CLP\nby integrating sub-character information."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Karl Stratos"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09013v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09013v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18012v1",
                "updated": "2025-01-29T21:56:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    56,
                    38,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T21:56:38Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    56,
                    38,
                    2,
                    29,
                    0
                ],
                "title": "When less is more: evolving large neural networks from small ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When less is more: evolving large neural networks from small ones"
                },
                "summary": "In contrast to conventional artificial neural networks, which are large and\nstructurally static, we study feed-forward neural networks that are small and\ndynamic, whose nodes can be added (or subtracted) during training. A single\nneuronal weight in the network controls the network's size, while the weight\nitself is optimized by the same gradient-descent algorithm that optimizes the\nnetwork's other weights and biases, but with a size-dependent objective or loss\nfunction. We train and evaluate such Nimble Neural Networks on nonlinear\nregression and classification tasks where they outperform the corresponding\nstatic networks. Growing networks to minimal, appropriate, or optimal sizes\nwhile training elucidates network dynamics and contrasts with pruning large\nnetworks after training but before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to conventional artificial neural networks, which are large and\nstructurally static, we study feed-forward neural networks that are small and\ndynamic, whose nodes can be added (or subtracted) during training. A single\nneuronal weight in the network controls the network's size, while the weight\nitself is optimized by the same gradient-descent algorithm that optimizes the\nnetwork's other weights and biases, but with a size-dependent objective or loss\nfunction. We train and evaluate such Nimble Neural Networks on nonlinear\nregression and classification tasks where they outperform the corresponding\nstatic networks. Growing networks to minimal, appropriate, or optimal sizes\nwhile training elucidates network dynamics and contrasts with pruning large\nnetworks after training but before deployment."
                },
                "authors": [
                    {
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "name": "John F. Lindner"
                    },
                    {
                        "name": "Scott T. Miller"
                    },
                    {
                        "name": "Sudeshna Sinha"
                    },
                    {
                        "name": "William L. Ditto"
                    }
                ],
                "author_detail": {
                    "name": "William L. Ditto"
                },
                "author": "William L. Ditto",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18009v1",
                "updated": "2025-01-29T21:51:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    51,
                    17,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T21:51:17Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    21,
                    51,
                    17,
                    2,
                    29,
                    0
                ],
                "title": "Large Language Models Think Too Fast To Explore Effectively",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Think Too Fast To Explore Effectively"
                },
                "summary": "Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability."
                },
                "authors": [
                    {
                        "name": "Lan Pan"
                    },
                    {
                        "name": "Hanbo Xie"
                    },
                    {
                        "name": "Robert C. Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Wilson"
                },
                "author": "Robert C. Wilson",
                "arxiv_comment": "16 pages, 13 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]