[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.19816v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19816v1",
                "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling"
                },
                "updated": "2026-02-23T13:13:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    41,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19816v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:13:41Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    41,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yungang Yi"
                    }
                ],
                "author_detail": {
                    "name": "Yungang Yi"
                },
                "author": "Yungang Yi"
            },
            {
                "id": "http://arxiv.org/abs/2602.19811v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19811v1",
                "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
                },
                "updated": "2026-02-23T13:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19811v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:12:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "12 pages, 2 figures, 5 tables. Extended version of the short paper published at DOLAP 2026 (co-located with EDBT/ICDT 2026)",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2602.19784v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19784v1",
                "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation"
                },
                "updated": "2026-02-23T12:40:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19784v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:40:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bang Huang"
                    },
                    {
                        "name": "Eddine Youcef Belmekki"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini"
            },
            {
                "id": "http://arxiv.org/abs/2602.14934v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14934v2",
                "title": "Activation-Space Uncertainty Quantification for Pretrained Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-Space Uncertainty Quantification for Pretrained Networks"
                },
                "updated": "2026-02-23T10:54:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14934v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T17:17:08Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    17,
                    17,
                    8,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Richard Bergna"
                    },
                    {
                        "name": "Stefan Depeweg"
                    },
                    {
                        "name": "Sergio Calvo-Ordo침ez"
                    },
                    {
                        "name": "Jonathan Plenk"
                    },
                    {
                        "name": "Alvaro Cartea"
                    },
                    {
                        "name": "Jose Miguel Hern치ndez-Lobato"
                    }
                ],
                "author_detail": {
                    "name": "Jose Miguel Hern치ndez-Lobato"
                },
                "author": "Jose Miguel Hern치ndez-Lobato"
            },
            {
                "id": "http://arxiv.org/abs/2602.19626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19626v1",
                "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding"
                },
                "updated": "2026-02-23T09:14:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    14,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:14:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    14,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Roberto Tacconelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Tacconelli"
                },
                "author": "Roberto Tacconelli"
            },
            {
                "id": "http://arxiv.org/abs/2601.17354v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.17354v3",
                "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling"
                },
                "updated": "2026-02-23T08:13:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    8,
                    13,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.17354v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.17354v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-24T07:58:53Z",
                "published_parsed": [
                    2026,
                    1,
                    24,
                    7,
                    58,
                    53,
                    5,
                    24,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenzhi Guo"
                    },
                    {
                        "name": "Guangchi Fang"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Bing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wang"
                },
                "author": "Bing Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19567v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19567v1",
                "title": "Spritz: Path-Aware Load Balancing in Low-Diameter Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spritz: Path-Aware Load Balancing in Low-Diameter Networks"
                },
                "updated": "2026-02-23T07:33:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    7,
                    33,
                    25,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19567v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T07:33:25Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    7,
                    33,
                    25,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "arxiv_journal_ref": "Proc. 40th IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2026",
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler"
            },
            {
                "id": "http://arxiv.org/abs/2602.19506v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19506v1",
                "title": "Relational Feature Caching for Accelerating Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Feature Caching for Accelerating Diffusion Transformers"
                },
                "updated": "2026-02-23T04:45:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    4,
                    45,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19506v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T04:45:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    4,
                    45,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Byunggwan Son"
                    },
                    {
                        "name": "Jeimin Jeon"
                    },
                    {
                        "name": "Jeongwoo Choi"
                    },
                    {
                        "name": "Bumsub Ham"
                    }
                ],
                "author_detail": {
                    "name": "Bumsub Ham"
                },
                "author": "Bumsub Ham"
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03346v3",
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing"
                },
                "updated": "2026-02-22T15:15:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    15,
                    15,
                    44,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03346v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire"
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic"
            },
            {
                "id": "http://arxiv.org/abs/2511.07399v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07399v2",
                "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation"
                },
                "updated": "2026-02-22T13:10:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    13,
                    10,
                    27,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07399v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T18:51:28Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Accepted by MLSys 2026. Project Page: http://streamdiffusionv2.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Keting Yang"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Akio Kodaira"
                    },
                    {
                        "name": "Chenfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenfeng Xu"
                },
                "author": "Chenfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19137v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19137v1",
                "title": "Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs"
                },
                "updated": "2026-02-22T11:44:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    11,
                    44,
                    34,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19137v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-22T11:44:34Z",
                "published_parsed": [
                    2026,
                    2,
                    22,
                    11,
                    44,
                    34,
                    6,
                    53,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.22420v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22420v3",
                "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving"
                },
                "updated": "2026-02-22T03:46:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    3,
                    46,
                    24,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22420v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Existing Speculative Decoding strategies typically rely on static speculative lengths, failing to adapt to fluctuating request loads or identify the optimal moment to halt speculation. The cost of restarting speculative inference also remains unquantified. During traffic surges, the marginal utility of speculation diminishes; yet, the draft model's persistent memory footprint competes for available KV cache. This resource contention limits the maximum batch size, thereby degrading overall system throughput. To overcome this, we propose Nightjar, a resource-aware adaptive speculative framework. It first adjusts to the request load by dynamically selecting the optimal speculative length for different batch sizes. Crucially, upon detecting significant request queuing or KV cache shortages, it disables speculative decoding and offloads the draft model to the CPU. This reclaims memory for the KV cache, thereby facilitating larger batch sizes and maximizing overall system throughput. Experiments show that Nightjar achieves up to 27.29% higher throughput and 12.90% lower latency compared to standard speculative decoding under dynamic request arrival rates in real-time LLM serving scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Existing Speculative Decoding strategies typically rely on static speculative lengths, failing to adapt to fluctuating request loads or identify the optimal moment to halt speculation. The cost of restarting speculative inference also remains unquantified. During traffic surges, the marginal utility of speculation diminishes; yet, the draft model's persistent memory footprint competes for available KV cache. This resource contention limits the maximum batch size, thereby degrading overall system throughput. To overcome this, we propose Nightjar, a resource-aware adaptive speculative framework. It first adjusts to the request load by dynamically selecting the optimal speculative length for different batch sizes. Crucially, upon detecting significant request queuing or KV cache shortages, it disables speculative decoding and offloads the draft model to the CPU. This reclaims memory for the KV cache, thereby facilitating larger batch sizes and maximizing overall system throughput. Experiments show that Nightjar achieves up to 27.29% higher throughput and 12.90% lower latency compared to standard speculative decoding under dynamic request arrival rates in real-time LLM serving scenarios."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T00:57:55Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    0,
                    57,
                    55,
                    5,
                    361,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Huaimin Wang"
                    },
                    {
                        "name": "Xiang Fu"
                    },
                    {
                        "name": "Zhiquan Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiquan Lai"
                },
                "author": "Zhiquan Lai"
            },
            {
                "id": "http://arxiv.org/abs/2602.04595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.04595v2",
                "title": "Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference"
                },
                "updated": "2026-02-22T03:10:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    3,
                    10,
                    11,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.04595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.04595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-04T14:22:08Z",
                "published_parsed": [
                    2026,
                    2,
                    4,
                    14,
                    22,
                    8,
                    2,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Jieyu Li"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Weifeng He"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng He"
                },
                "author": "Weifeng He"
            },
            {
                "id": "http://arxiv.org/abs/2602.18993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18993v1",
                "title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models"
                },
                "updated": "2026-02-22T00:48:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    0,
                    48,
                    3,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-22T00:48:03Z",
                "published_parsed": [
                    2026,
                    2,
                    22,
                    0,
                    48,
                    3,
                    6,
                    53,
                    0
                ],
                "arxiv_comment": "Accepted to CVPR 2026 Main. Project page:https://jiwoogit.github.io/SeaCache",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Byeongju Han"
                    },
                    {
                        "name": "Geonho Cha"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Youngjun Hong"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo"
            },
            {
                "id": "http://arxiv.org/abs/2602.18955v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18955v1",
                "title": "Incremental Transformer Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental Transformer Neural Processes"
                },
                "updated": "2026-02-21T20:30:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    20,
                    30,
                    4,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18955v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T20:30:04Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    20,
                    30,
                    4,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "Code provided at https://github.com/philipmortimer/incTNP-code",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Philip Mortimer"
                    },
                    {
                        "name": "Cristiana Diaconu"
                    },
                    {
                        "name": "Tommy Rochussen"
                    },
                    {
                        "name": "Bruno Mlodozeniec"
                    },
                    {
                        "name": "Richard E. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Richard E. Turner"
                },
                "author": "Richard E. Turner"
            },
            {
                "id": "http://arxiv.org/abs/2602.18922v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18922v1",
                "title": "Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning"
                },
                "updated": "2026-02-21T18:25:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    25,
                    18,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18922v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,\n  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for\n  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,\n  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for\n  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T18:25:18Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    25,
                    18,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "28 pages, 15 figures, 8 tables, 5 appendices",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Abhinaba Basu"
                    }
                ],
                "author_detail": {
                    "name": "Abhinaba Basu"
                },
                "author": "Abhinaba Basu"
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01928v3",
                "title": "Esoteric Language Models: Bridging Autoregressive and Masked Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models: Bridging Autoregressive and Masked Diffusion LLMs"
                },
                "updated": "2026-02-21T18:05:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    5,
                    37,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01928v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and a video tutorial on the project page: https://s-sahoo.com/Eso-LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and a video tutorial on the project page: https://s-sahoo.com/Eso-LMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat"
            },
            {
                "id": "http://arxiv.org/abs/2602.18750v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18750v1",
                "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD"
                },
                "updated": "2026-02-21T08:19:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    8,
                    19,
                    59,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18750v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T08:19:59Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    8,
                    19,
                    59,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "12 pages, 12 figures, under review",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Mingjun Xiao"
                },
                "author": "Mingjun Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08134v4",
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"
                },
                "updated": "2026-02-21T06:41:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    6,
                    41,
                    0,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08134v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026. Project webpage is available at https://follow-your-shape.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma"
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.03771v5",
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "updated": "2026-02-21T02:29:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    2,
                    29,
                    11,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.03771v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.03771v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees for predictable performance. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines with up to 12.5$\\times$ higher cache hit and 26$\\times$ lower error rates. We release the vCache implementation and four benchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees for predictable performance. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines with up to 12.5$\\times$ higher cache hit and 26$\\times$ lower error rates. We release the vCache implementation and four benchmarks to support future research."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "arxiv_comment": "ICLR 2026 (accepted)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez"
            },
            {
                "id": "http://arxiv.org/abs/2602.18434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18434v1",
                "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory"
                },
                "updated": "2026-02-20T18:59:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    18,
                    59,
                    50,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T18:59:50Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    18,
                    59,
                    50,
                    4,
                    51,
                    0
                ],
                "arxiv_comment": "Project page: see https://vatsalag99.github.io/memstream/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Vatsal Agarwal"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Pulkit Kumar"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava"
            },
            {
                "id": "http://arxiv.org/abs/2602.18304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18304v1",
                "title": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators"
                },
                "updated": "2026-02-20T16:01:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    16,
                    1,
                    16,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T16:01:16Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    16,
                    1,
                    16,
                    4,
                    51,
                    0
                ],
                "arxiv_comment": "4 pages, 3 figures, 3 tables, Journal :- IEEE CAL",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz"
            },
            {
                "id": "http://arxiv.org/abs/2602.18232v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18232v1",
                "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning"
                },
                "updated": "2026-02-20T14:13:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    14,
                    13,
                    22,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18232v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T14:13:22Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    14,
                    13,
                    22,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lexiang Tang"
                    },
                    {
                        "name": "Weihao Gao"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Lu Ma"
                    },
                    {
                        "name": "Qiao jin"
                    },
                    {
                        "name": "Bang Yang"
                    },
                    {
                        "name": "Yuexian Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Zou"
                },
                "author": "Yuexian Zou"
            },
            {
                "id": "http://arxiv.org/abs/2602.18196v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18196v1",
                "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference"
                },
                "updated": "2026-02-20T13:09:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    13,
                    9,
                    49,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18196v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T13:09:49Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    13,
                    9,
                    49,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre"
            },
            {
                "id": "http://arxiv.org/abs/2601.09282v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.09282v2",
                "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing"
                },
                "updated": "2026-02-20T11:40:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    11,
                    40,
                    54,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.09282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.09282v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2026.3665989",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration and presents a proof-of-concept design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration and presents a proof-of-concept design."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T08:36:21Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    8,
                    36,
                    21,
                    2,
                    14,
                    0
                ],
                "arxiv_comment": "This is the accepted version of the paper published in IEEE Access (2026). The final version is available at: https://doi.org/10.1109/ACCESS.2026.3665989",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Leszek Sliwko"
                    },
                    {
                        "name": "Jolanta Mizeria-Pietraszko"
                    }
                ],
                "author_detail": {
                    "name": "Jolanta Mizeria-Pietraszko"
                },
                "author": "Jolanta Mizeria-Pietraszko",
                "arxiv_doi": "10.1109/ACCESS.2026.3665989"
            },
            {
                "id": "http://arxiv.org/abs/2602.18093v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18093v1",
                "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers"
                },
                "updated": "2026-02-20T09:33:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    9,
                    33,
                    59,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18093v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T09:33:59Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    9,
                    33,
                    59,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia"
            },
            {
                "id": "http://arxiv.org/abs/2511.19269v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.19269v2",
                "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDLM: Consistency Diffusion Language Models For Faster Sampling"
                },
                "updated": "2026-02-20T02:50:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    2,
                    50,
                    33,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.19269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.19269v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T16:21:25Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    16,
                    21,
                    25,
                    0,
                    328,
                    0
                ],
                "arxiv_comment": "Accepted to MLSys 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04581v2",
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning"
                },
                "updated": "2026-02-19T19:17:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    19,
                    17,
                    31,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04581v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g., low-rank approximation or attention pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in convolutional networks, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices (Q, K, V, O) into shared dictionary atoms, reducing the attention module's parameters by 66.7\\% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement-trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than GQA, low-rank baselines and recent Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification tasks with 66.7\\% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on large pretrained models to reduce their number of parameters without experiencing any significant drop in their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g., low-rank approximation or attention pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in convolutional networks, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices (Q, K, V, O) into shared dictionary atoms, reducing the attention module's parameters by 66.7\\% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement-trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than GQA, low-rank baselines and recent Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification tasks with 66.7\\% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on large pretrained models to reduce their number of parameters without experiencing any significant drop in their performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "arxiv_comment": "This work has been accepted and presented at AAAI 2026 in Singapore",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis"
            },
            {
                "id": "http://arxiv.org/abs/2602.17518v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17518v1",
                "title": "A Picture of Agentic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Picture of Agentic Search"
                },
                "updated": "2026-02-19T16:32:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    16,
                    32,
                    34,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17518v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T16:32:34Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    16,
                    32,
                    34,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "7 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Francesca Pezzuti"
                    },
                    {
                        "name": "Ophir Frieder"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Nicola Tonellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Tonellotto"
                },
                "author": "Nicola Tonellotto"
            },
            {
                "id": "http://arxiv.org/abs/2602.17414v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17414v1",
                "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models"
                },
                "updated": "2026-02-19T14:43:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    43,
                    59,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17414v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle."
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T14:43:59Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    43,
                    59,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "26 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "stat.CO"
                },
                "authors": [
                    {
                        "name": "David Yallup"
                    }
                ],
                "author_detail": {
                    "name": "David Yallup"
                },
                "author": "David Yallup"
            },
            {
                "id": "http://arxiv.org/abs/2602.17387v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17387v1",
                "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition"
                },
                "updated": "2026-02-19T14:12:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    12,
                    7,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17387v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T14:12:07Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    12,
                    7,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "Submitted to Pattern Recognition, 11 pages + 2-page appendix, 7 figures, 12 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Changhun Kim"
                    },
                    {
                        "name": "Martin Mayr"
                    },
                    {
                        "name": "Thomas Gorges"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Mathias Seuret"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Vincent Christlein"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Christlein"
                },
                "author": "Vincent Christlein"
            },
            {
                "id": "http://arxiv.org/abs/2512.03870v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03870v3",
                "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
                },
                "updated": "2026-02-19T05:41:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    5,
                    41,
                    27,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03870v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T15:22:00Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Xinmiao Zhang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yunlong Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.07474v2",
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "updated": "2026-02-18T21:45:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    45,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.07474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.07474v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/PhysRevB.111.165134",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to K, Rb, Cs) has stimulated widespread research interest due to its interplay of non-trivial topology and unconventional correlated physics including charge-density waves (CDW) and superconductivity. The essential prerequisite to understanding the microscopic mechanisms of this complex electronic landscape is to unveil the configuration and symmetry of the charge-density wave order. As to now, little consensus has been made on what symmetry is broken. Herein, we clarify the microscopic structure and symmetry breaking of the CDW phase in RbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our approach is based on extracting coherent phonon spectra induced by three-dimensional CDW and comparing them to calculated phonon frequencies via density-functional theory. The combination of these experimental results and calculations provides compelling evidence that the CDW structure of both compounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2 staggered inverse Star-of-David pattern with interlayer $$ phase shift, in which the six-fold rotational symmetry is broken. These observations thus corroborate six-fold rotational symmetry breaking throughout the CDW phase of RbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to K, Rb, Cs) has stimulated widespread research interest due to its interplay of non-trivial topology and unconventional correlated physics including charge-density waves (CDW) and superconductivity. The essential prerequisite to understanding the microscopic mechanisms of this complex electronic landscape is to unveil the configuration and symmetry of the charge-density wave order. As to now, little consensus has been made on what symmetry is broken. Herein, we clarify the microscopic structure and symmetry breaking of the CDW phase in RbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our approach is based on extracting coherent phonon spectra induced by three-dimensional CDW and comparing them to calculated phonon frequencies via density-functional theory. The combination of these experimental results and calculations provides compelling evidence that the CDW structure of both compounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2 staggered inverse Star-of-David pattern with interlayer $$ phase shift, in which the six-fold rotational symmetry is broken. These observations thus corroborate six-fold rotational symmetry breaking throughout the CDW phase of RbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "arxiv_journal_ref": "Phys. Rev. B 111, 165134 (2025)",
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "arxiv_doi": "10.1103/PhysRevB.111.165134"
            },
            {
                "id": "http://arxiv.org/abs/2602.16839v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16839v1",
                "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding"
                },
                "updated": "2026-02-18T20:03:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    3,
                    38,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16839v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T20:03:38Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    3,
                    38,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "ICLR 2026, 15 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao"
            },
            {
                "id": "http://arxiv.org/abs/2602.16564v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16564v1",
                "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Approach to Solving Simulation-Based Network Security Games"
                },
                "updated": "2026-02-18T16:07:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16564v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:07:01Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Michael Lanier"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeniy Vorobeychik"
                },
                "author": "Yevgeniy Vorobeychik"
            },
            {
                "id": "http://arxiv.org/abs/2602.16512v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16512v1",
                "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs"
                },
                "updated": "2026-02-18T14:58:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16512v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:58:25Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Felix Fricke"
                    },
                    {
                        "name": "Simon Malberg"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh"
            },
            {
                "id": "http://arxiv.org/abs/2602.16284v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16284v1",
                "title": "Fast KV Compaction via Attention Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast KV Compaction via Attention Matching"
                },
                "updated": "2026-02-18T09:06:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16284v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:06:53Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Xinghong Fu"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2502.01258v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01258v4",
                "title": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics"
                },
                "updated": "2026-02-18T07:41:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    41,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01258v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $풮_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $풮_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T11:31:12Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    31,
                    12,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Zhaobo Zhou"
                    },
                    {
                        "name": "Sangeeta Sharma"
                    },
                    {
                        "name": "John Kay Dewhurst"
                    },
                    {
                        "name": "Junjie He"
                    }
                ],
                "author_detail": {
                    "name": "Junjie He"
                },
                "author": "Junjie He"
            },
            {
                "id": "http://arxiv.org/abs/2602.17726v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17726v1",
                "title": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention"
                },
                "updated": "2026-02-18T06:33:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    6,
                    33,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17726v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T06:33:03Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    6,
                    33,
                    3,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "23 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Qness Ndlovu"
                    }
                ],
                "author_detail": {
                    "name": "Qness Ndlovu"
                },
                "author": "Qness Ndlovu"
            },
            {
                "id": "http://arxiv.org/abs/2602.08242v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08242v4",
                "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications"
                },
                "updated": "2026-02-18T04:00:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    4,
                    0,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08242v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T03:39:45Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    3,
                    39,
                    45,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "18+ pages, 5 figures, 3 tables. Code and data: https://github.com/amughalbscs16/network-layer-quality-testing",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ali Hassaan Mughal"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Noor Fatima"
                    }
                ],
                "author_detail": {
                    "name": "Noor Fatima"
                },
                "author": "Noor Fatima"
            },
            {
                "id": "http://arxiv.org/abs/2601.11837v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.11837v2",
                "title": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment"
                },
                "updated": "2026-02-18T02:02:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    2,
                    2,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.11837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.11837v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-17T00:03:03Z",
                "published_parsed": [
                    2026,
                    1,
                    17,
                    0,
                    3,
                    3,
                    5,
                    17,
                    0
                ],
                "arxiv_comment": "Proceedings of LIght Detection In Noble Elements - LIDINE 2025",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "Ludovico Luzzi"
                    }
                ],
                "author_detail": {
                    "name": "Ludovico Luzzi"
                },
                "author": "Ludovico Luzzi"
            },
            {
                "id": "http://arxiv.org/abs/2602.16132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16132v1",
                "title": "CHAI: CacHe Attention Inference for text2video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAI: CacHe Attention Inference for text2video"
                },
                "updated": "2026-02-18T01:53:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T01:53:29Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joel Mathew Cherian"
                    },
                    {
                        "name": "Ashutosh Muralidhara Bharadwaj"
                    },
                    {
                        "name": "Vima Gupta"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2602.02958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02958v2",
                "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization"
                },
                "updated": "2026-02-17T23:49:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    49,
                    23,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T00:54:32Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    0,
                    54,
                    32,
                    1,
                    34,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Xingyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhuoyang Zhang"
                    },
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Zhiying Xu"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Kurt Keutzer"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Keutzer"
                },
                "author": "Kurt Keutzer"
            },
            {
                "id": "http://arxiv.org/abs/2602.16092v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16092v1",
                "title": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff"
                },
                "updated": "2026-02-17T23:39:39Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16092v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T23:39:39Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Patrick Pynadath"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16054v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16054v1",
                "title": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill"
                },
                "updated": "2026-02-17T22:08:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16054v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T22:08:16Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bradley McDanel"
                    },
                    {
                        "name": "Steven Li"
                    },
                    {
                        "name": "Harshit Khaitan"
                    }
                ],
                "author_detail": {
                    "name": "Harshit Khaitan"
                },
                "author": "Harshit Khaitan"
            },
            {
                "id": "http://arxiv.org/abs/2602.16727v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16727v1",
                "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation"
                },
                "updated": "2026-02-17T15:39:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    39,
                    51,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16727v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T15:39:51Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    39,
                    51,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hua Yan"
                    },
                    {
                        "name": "Heng Tan"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yang"
                },
                "author": "Yu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.15311v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.15311v3",
                "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents"
                },
                "updated": "2026-02-17T15:21:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    21,
                    45,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.15311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.15311v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T15:23:22Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    15,
                    23,
                    22,
                    2,
                    14,
                    0
                ],
                "arxiv_comment": "v3: Production hardening. Added INT8 quantization (5.6x dot product speedup, 3.1x compression), crash recovery via decoupled WAL (<1% overhead), unlimited text storage via sidecar blob arena with generational GC, and epoch-based reclamation for lock-free reads (P99 750ns under 16-thread contention). Revised for systems engineering clarity",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Mustafa Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa Arslan"
                },
                "author": "Mustafa Arslan"
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.00539v2",
                "title": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs"
                },
                "updated": "2026-02-17T13:11:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    13,
                    11,
                    5,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.00539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.00539v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "HaoYuan Hu"
                    },
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "XinYa Wu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Taoye Yin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "KeZun Zhang"
                    },
                    {
                        "name": "KaiLai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Feng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wang"
                },
                "author": "Feng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01110v4",
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory"
                },
                "updated": "2026-02-17T12:40:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    12,
                    40,
                    7,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01110v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas K칬hler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Mattia D'Urso"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger"
            },
            {
                "id": "http://arxiv.org/abs/2602.02108v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02108v3",
                "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts"
                },
                "updated": "2026-02-17T11:41:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    11,
                    41,
                    0,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02108v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T13:52:40Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    13,
                    52,
                    40,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Zimu Liao"
                    }
                ],
                "author_detail": {
                    "name": "Zimu Liao"
                },
                "author": "Zimu Liao"
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11695v3",
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization"
                },
                "updated": "2026-02-17T05:04:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    5,
                    4,
                    13,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11695v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11695v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab"
            },
            {
                "id": "http://arxiv.org/abs/2602.15318v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15318v1",
                "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs"
                },
                "updated": "2026-02-17T02:51:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15318v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T02:51:36Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages , 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Wangyang Hong"
                    },
                    {
                        "name": "Peng Qiao"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.14492v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14492v2",
                "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model"
                },
                "updated": "2026-02-17T02:44:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    44,
                    8,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14492v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T06:09:31Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    9,
                    31,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Yike Xu"
                    },
                    {
                        "name": "Jinyong Wen"
                    },
                    {
                        "name": "Baokun Wang"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Xiaotong Lin"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhongle Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhongle Xie"
                },
                "author": "Zhongle Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.01872v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01872v2",
                "title": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training"
                },
                "updated": "2026-02-16T21:24:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    21,
                    24,
                    43,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01872v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T09:44:12Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    9,
                    44,
                    12,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chongyang Xu"
                    },
                    {
                        "name": "Christoph Siebenbrunner"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2602.14683v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14683v1",
                "title": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $$-Divergences: Unfolding-Free Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $$-Divergences: Unfolding-Free Updates"
                },
                "updated": "2026-02-16T12:16:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14683v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study majorization-minimization methods for nonnegative tensor decompositions under the $$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study majorization-minimization methods for nonnegative tensor decompositions under the $$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T12:16:01Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Valentin Leplat"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Leplat"
                },
                "author": "Valentin Leplat"
            },
            {
                "id": "http://arxiv.org/abs/2601.01112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01112v2",
                "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation"
                },
                "updated": "2026-02-16T10:23:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    10,
                    23,
                    22,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T08:25:58Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference. This latest version corrects and updates certain personal information",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanbo Lu"
                    },
                    {
                        "name": "Zheda Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheda Liu"
                },
                "author": "Zheda Liu"
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16909v3",
                "title": "Understanding GPU Resource Interference One Level Deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding GPU Resource Interference One Level Deeper"
                },
                "updated": "2026-02-16T09:26:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    9,
                    26,
                    53,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16909v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic"
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23094v2",
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "updated": "2026-02-16T06:46:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    46,
                    0,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23094v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026, 21 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01068v6",
                "title": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration"
                },
                "updated": "2026-02-16T06:37:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    37,
                    44,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01068v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01068v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.14381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14381v1",
                "title": "Adapting VACE for Real-Time Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting VACE for Real-Time Autoregressive Video Diffusion"
                },
                "updated": "2026-02-16T01:13:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T01:13:33Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ryan Fosdick"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Fosdick"
                },
                "arxiv_affiliation": "Daydream",
                "author": "Ryan Fosdick"
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22876v3",
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "updated": "2026-02-15T22:53:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    22,
                    53,
                    25,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22876v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.14262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14262v1",
                "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute"
                },
                "updated": "2026-02-15T18:19:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T18:19:06Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Siddhartha Raman Sundara Raman"
                    },
                    {
                        "name": "Jaydeep P. Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep P. Kulkarni"
                },
                "author": "Jaydeep P. Kulkarni"
            },
            {
                "id": "http://arxiv.org/abs/2602.14236v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14236v1",
                "title": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models"
                },
                "updated": "2026-02-15T17:06:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14236v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T17:06:02Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Vishnu Sai"
                    },
                    {
                        "name": "Dheeraj Sai"
                    },
                    {
                        "name": "Srinath B"
                    },
                    {
                        "name": "Girish Varma"
                    },
                    {
                        "name": "Priyesh Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Priyesh Shukla"
                },
                "author": "Priyesh Shukla"
            },
            {
                "id": "http://arxiv.org/abs/2602.14209v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14209v1",
                "title": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM"
                },
                "updated": "2026-02-15T16:07:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14209v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T16:07:51Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Omin Kwon"
                    },
                    {
                        "name": "Yeonjae Kim"
                    },
                    {
                        "name": "Doyeon Kim"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jae W. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae W. Lee"
                },
                "author": "Jae W. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.14162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14162v1",
                "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering"
                },
                "updated": "2026-02-15T14:23:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T14:23:50Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "arxiv_comment": "24 pages, 9 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xu"
                },
                "author": "Tao Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.13993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13993v1",
                "title": "Elastic Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic Diffusion Transformer"
                },
                "updated": "2026-02-15T05:19:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T05:19:17Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jiayi Guo"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo"
            },
            {
                "id": "http://arxiv.org/abs/2602.13692v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13692v1",
                "title": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System"
                },
                "updated": "2026-02-14T09:26:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13692v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-14T09:26:41Z",
                "published_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Weili Xu"
                    },
                    {
                        "name": "Yinfang Chen"
                    },
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Simran Arora"
                    }
                ],
                "author_detail": {
                    "name": "Simran Arora"
                },
                "author": "Simran Arora"
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10746v2",
                "title": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing"
                },
                "updated": "2026-02-14T07:14:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    7,
                    14,
                    40,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10746v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen"
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02634v5",
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider"
                },
                "updated": "2026-02-14T05:32:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    5,
                    32,
                    46,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02634v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02634v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.03983v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.03983v2",
                "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement"
                },
                "updated": "2026-02-14T03:09:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    3,
                    9,
                    51,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.03983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.03983v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T20:17:47Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    20,
                    17,
                    47,
                    1,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Weikang Qiu"
                    },
                    {
                        "name": "Tinglin Huang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.10568v3",
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "updated": "2026-02-14T02:23:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    2,
                    23,
                    24,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.10568v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.10568v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13434v1",
                "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory"
                },
                "updated": "2026-02-13T20:22:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    20,
                    22,
                    44,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T20:22:44Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    20,
                    22,
                    44,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Maccoy Merrell"
                    },
                    {
                        "name": "Daniel Puckett"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Jeffrey Stuecheli"
                    },
                    {
                        "name": "Stavros Kalafatis"
                    },
                    {
                        "name": "Paul V. Gratz"
                    }
                ],
                "author_detail": {
                    "name": "Paul V. Gratz"
                },
                "author": "Paul V. Gratz"
            },
            {
                "id": "http://arxiv.org/abs/2602.13172v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13172v1",
                "title": "LongStream: Long-Sequence Streaming Autoregressive Visual Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongStream: Long-Sequence Streaming Autoregressive Visual Geometry"
                },
                "updated": "2026-02-13T18:30:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    30,
                    51,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13172v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:30:51Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    30,
                    51,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chong Cheng"
                    },
                    {
                        "name": "Xianda Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Weiqiang Ren"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Xiaoyuang Guo"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13165v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13165v1",
                "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures"
                },
                "updated": "2026-02-13T18:25:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    25,
                    0,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13165v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:25:00Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    25,
                    0,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Asmit Kumar Singh"
                    },
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Laxmi Naga Santosh Attaluri"
                    },
                    {
                        "name": "Tak Chiam"
                    },
                    {
                        "name": "Weihua Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Weihua Zhu"
                },
                "author": "Weihua Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.07675v3",
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"
                },
                "updated": "2026-02-13T17:03:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    17,
                    3,
                    20,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.07675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.07675v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "Accepted to INFOCOM 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong"
            },
            {
                "id": "http://arxiv.org/abs/2601.20577v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.20577v2",
                "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization"
                },
                "updated": "2026-02-13T09:56:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    9,
                    56,
                    37,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.20577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.20577v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-28T13:15:58Z",
                "published_parsed": [
                    2026,
                    1,
                    28,
                    13,
                    15,
                    58,
                    2,
                    28,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Baiqing Wang"
                    },
                    {
                        "name": "Helei Cui"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Zhiwen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Yu"
                },
                "author": "Zhiwen Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.11605v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11605v2",
                "title": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation"
                },
                "updated": "2026-02-13T09:30:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    9,
                    30,
                    22,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11605v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T05:51:52Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    5,
                    51,
                    52,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "12 pages, 6figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yixiao Chen"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Juntong Yan"
                    },
                    {
                        "name": "Shuojin Yang"
                    },
                    {
                        "name": "Menghao Guo"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13357v1",
                "title": "AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers"
                },
                "updated": "2026-02-13T08:11:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    8,
                    11,
                    54,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \\textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \\textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T08:11:54Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    8,
                    11,
                    54,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu"
            },
            {
                "id": "http://arxiv.org/abs/2602.15902v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15902v1",
                "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-to-LoRA: Learning to Instantly Internalize Contexts"
                },
                "updated": "2026-02-13T06:54:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    6,
                    54,
                    20,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15902v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T06:54:20Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    6,
                    54,
                    20,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rujikorn Charakorn"
                    },
                    {
                        "name": "Edoardo Cetin"
                    },
                    {
                        "name": "Shinnosuke Uesaka"
                    },
                    {
                        "name": "Robert Tjarko Lange"
                    }
                ],
                "author_detail": {
                    "name": "Robert Tjarko Lange"
                },
                "author": "Robert Tjarko Lange"
            },
            {
                "id": "http://arxiv.org/abs/2602.12635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12635v1",
                "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats"
                },
                "updated": "2026-02-13T05:41:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    5,
                    41,
                    31,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T05:41:31Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    5,
                    41,
                    31,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Ziwei Yu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Dong"
                },
                "author": "Zhenhua Dong"
            },
            {
                "id": "http://arxiv.org/abs/2602.12618v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12618v1",
                "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models"
                },
                "updated": "2026-02-13T04:49:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    49,
                    27,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12618v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T04:49:27Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    49,
                    27,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "2025 IEEE International Conference on Big Data (BigData)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Omer Faruk Deniz"
                    },
                    {
                        "name": "Ruiyu Mao"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Yapeng Tian"
                    },
                    {
                        "name": "Latifur Khan"
                    }
                ],
                "author_detail": {
                    "name": "Latifur Khan"
                },
                "author": "Latifur Khan"
            },
            {
                "id": "http://arxiv.org/abs/2602.12596v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12596v1",
                "title": "Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution"
                },
                "updated": "2026-02-13T04:14:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    14,
                    42,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12596v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T04:14:42Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    14,
                    42,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Johnson Umeike"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "arxiv_affiliation": "University of Maryland, College Park",
                "author": "Bahar Asgari"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v3",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2026-02-13T01:46:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    1,
                    46,
                    3,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wei Luo"
                },
                "author": "Wei Luo"
            },
            {
                "id": "http://arxiv.org/abs/2512.07841v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07841v2",
                "title": "Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs"
                },
                "updated": "2026-02-13T00:12:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    0,
                    12,
                    14,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07841v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-22T17:48:25Z",
                "published_parsed": [
                    2025,
                    11,
                    22,
                    17,
                    48,
                    25,
                    5,
                    326,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "v. 1 n. 26 (2025): Revista Junior de Inicia칞칚o Cient칤fica em Ci칡ncias Exatas e Engenharia",
                "authors": [
                    {
                        "name": "Gabriel M. Arantes"
                    },
                    {
                        "name": "Giancarlo Lucca"
                    },
                    {
                        "name": "Eduardo N. Borges"
                    },
                    {
                        "name": "Richard F. Pinto"
                    },
                    {
                        "name": "Bruno L. Dalmazo"
                    },
                    {
                        "name": "Rafael A. Berri"
                    }
                ],
                "author_detail": {
                    "name": "Rafael A. Berri"
                },
                "author": "Rafael A. Berri"
            },
            {
                "id": "http://arxiv.org/abs/2602.12422v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12422v1",
                "title": "CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement"
                },
                "updated": "2026-02-12T21:28:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    21,
                    28,
                    23,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12422v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3779212.3790136",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, \"Why is the memory access associated with PC X causing more evictions?\", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, \"Why is the memory access associated with PC X causing more evictions?\", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T21:28:23Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    21,
                    28,
                    23,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "16 pages, 13 figures, ASPLOS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Kaushal Mhapsekar"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Bita Aslrousta"
                    },
                    {
                        "name": "Samira Mirbagher-Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher-Ajorpaz"
                },
                "author": "Samira Mirbagher-Ajorpaz",
                "arxiv_doi": "10.1145/3779212.3790136"
            },
            {
                "id": "http://arxiv.org/abs/2602.08798v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08798v2",
                "title": "CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse"
                },
                "updated": "2026-02-12T20:32:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    20,
                    32,
                    18,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08798v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T15:38:13Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    15,
                    38,
                    13,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Hedong Zhang"
                    },
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Shweta Pardeshi"
                    },
                    {
                        "name": "Qian Lou"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar"
            },
            {
                "id": "http://arxiv.org/abs/2602.12220v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12220v1",
                "title": "Taming Subpacketization without Sacrificing Communication: A Packet Type-based Framework for D2D Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Subpacketization without Sacrificing Communication: A Packet Type-based Framework for D2D Coded Caching"
                },
                "updated": "2026-02-12T17:58:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    17,
                    58,
                    37,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12220v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finite-length analysis is critical for bringing coded caching closer to practical deployment. In this work, we study the design of communication rate-optimal device-to-device (D2D) coded caching schemes with minimal subpacketization levels, a key bottleneck in finite-length settings. We present a novel \\tit{packet type-based} (PT) design framework that (i) strategically introduces \\tit{asymmetry} into file splitting through user grouping, and (ii) systematically exploits such asymmetry in both cache placement and multicast delivery to create subpacketization reduction opportunities. In particular, the induced asymmetry gives rise to two fundamental forms of subpacketization reduction gains: the \\emph{subfile saving gain}, achieved by eliminating certain types of subfiles through careful user grouping and transmitter selection, and the \\emph{further splitting saving gain}, attained by reducing the splitting granularity for the remaining subfiles. The combined effect of these two reduction gains yields an overall subpacketization improvement over the original Ji-Caire-Molisch (JCM) caching scheme~\\cite{ji2016fundamental}, as well as various state-of-the-art schemes, while preserving optimal communication rates.\n  Under the PT framework, we formulate the caching scheme design as an integer linear program (ILP), where each feasible solution corresponds to a valid rate-optimal D2D coded caching scheme with potentially reduced subpacketization relative to the JCM baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-length analysis is critical for bringing coded caching closer to practical deployment. In this work, we study the design of communication rate-optimal device-to-device (D2D) coded caching schemes with minimal subpacketization levels, a key bottleneck in finite-length settings. We present a novel \\tit{packet type-based} (PT) design framework that (i) strategically introduces \\tit{asymmetry} into file splitting through user grouping, and (ii) systematically exploits such asymmetry in both cache placement and multicast delivery to create subpacketization reduction opportunities. In particular, the induced asymmetry gives rise to two fundamental forms of subpacketization reduction gains: the \\emph{subfile saving gain}, achieved by eliminating certain types of subfiles through careful user grouping and transmitter selection, and the \\emph{further splitting saving gain}, attained by reducing the splitting granularity for the remaining subfiles. The combined effect of these two reduction gains yields an overall subpacketization improvement over the original Ji-Caire-Molisch (JCM) caching scheme~\\cite{ji2016fundamental}, as well as various state-of-the-art schemes, while preserving optimal communication rates.\n  Under the PT framework, we formulate the caching scheme design as an integer linear program (ILP), where each feasible solution corresponds to a valid rate-optimal D2D coded caching scheme with potentially reduced subpacketization relative to the JCM baseline."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T17:58:37Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    17,
                    58,
                    37,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Giuseppe Caire"
                    },
                    {
                        "name": "Mingyue Ji"
                    }
                ],
                "author_detail": {
                    "name": "Mingyue Ji"
                },
                "author": "Mingyue Ji"
            },
            {
                "id": "http://arxiv.org/abs/2602.12029v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12029v1",
                "title": "PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving"
                },
                "updated": "2026-02-12T14:59:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    14,
                    59,
                    50,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12029v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T14:59:50Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    14,
                    59,
                    50,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "Preprint. 13 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Hoseung Kim"
                    },
                    {
                        "name": "Sunghwan Shim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Hyunjoon Jeong"
                    },
                    {
                        "name": "Jeongtae Lee"
                    },
                    {
                        "name": "Joonghoon Kim"
                    },
                    {
                        "name": "Sungjae Lee"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.11937v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11937v1",
                "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration"
                },
                "updated": "2026-02-12T13:36:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    36,
                    19,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11937v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T13:36:19Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    36,
                    19,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Vladimir Anisimov"
                    },
                    {
                        "name": "Tomer Asida"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Mostofa Patwari"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv"
            },
            {
                "id": "http://arxiv.org/abs/2602.11808v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11808v1",
                "title": "Deep Kernel Fusion for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Kernel Fusion for Transformers"
                },
                "updated": "2026-02-12T10:43:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    10,
                    43,
                    59,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11808v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T10:43:59Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    10,
                    43,
                    59,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zixi Zhang"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins"
            },
            {
                "id": "http://arxiv.org/abs/2602.11741v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11741v1",
                "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions"
                },
                "updated": "2026-02-12T09:11:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    9,
                    11,
                    8,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11741v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T09:11:08Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    9,
                    11,
                    8,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "27 pages, 8 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bo Guan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Guan"
                },
                "author": "Bo Guan"
            },
            {
                "id": "http://arxiv.org/abs/2602.11694v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11694v1",
                "title": "Emergent spin-resolved electronic charge density waves and pseudogap phenomena from strong $d$-wave altermagnetism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent spin-resolved electronic charge density waves and pseudogap phenomena from strong $d$-wave altermagnetism"
                },
                "updated": "2026-02-12T08:19:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    19,
                    55,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11694v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inspired by recent discovery of metallic $d$-wave altermagnetism in KV$_2$Se$_2$O, we develop a self-consistent microscopic many-body calculation of density-wave order for an itinerant altermagnetic metal. We show that the strong $d$-wave spin-momentum locking inherent to the altermagnetic band structure reconstructs the Fermi surface into spin-selective quasi-1D open sheets. This unique topology of Fermi surface drives an instability toward spin-resolved electronic charge density waves (CDWs), in which the ordering wave vectors for spin-up and spin-down electrons condense along two mutually orthogonal directions, forming spin-resolved stripe phases. As a consequence, this results in pronounced gap openings near the Fermi surface, and the superposition of these spin-resolved stripe orders leads to a checkerboard CDW in the charge channel and an antiphase spin-density-wave modulation in the spin channel. Upon increasing temperature, the density-wave order melts at $T_c$ due to thermal phase fluctuation while the gap opening persists, giving rise to a robust pseudogap regime, which eventually closes at a higher temperature $T_g$. The resulting simulations quantitatively reproduce the key features observed in the spectroscopic measurements, offering a consistent and generic understanding of the reported phenomena in KV$_2$Se$_2$O and, more broadly, in metallic altermagnets with strong spin-momentum locking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent discovery of metallic $d$-wave altermagnetism in KV$_2$Se$_2$O, we develop a self-consistent microscopic many-body calculation of density-wave order for an itinerant altermagnetic metal. We show that the strong $d$-wave spin-momentum locking inherent to the altermagnetic band structure reconstructs the Fermi surface into spin-selective quasi-1D open sheets. This unique topology of Fermi surface drives an instability toward spin-resolved electronic charge density waves (CDWs), in which the ordering wave vectors for spin-up and spin-down electrons condense along two mutually orthogonal directions, forming spin-resolved stripe phases. As a consequence, this results in pronounced gap openings near the Fermi surface, and the superposition of these spin-resolved stripe orders leads to a checkerboard CDW in the charge channel and an antiphase spin-density-wave modulation in the spin channel. Upon increasing temperature, the density-wave order melts at $T_c$ due to thermal phase fluctuation while the gap opening persists, giving rise to a robust pseudogap regime, which eventually closes at a higher temperature $T_g$. The resulting simulations quantitatively reproduce the key features observed in the spectroscopic measurements, offering a consistent and generic understanding of the reported phenomena in KV$_2$Se$_2$O and, more broadly, in metallic altermagnets with strong spin-momentum locking."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T08:19:55Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    19,
                    55,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "authors": [
                    {
                        "name": "Fei Yang"
                    },
                    {
                        "name": "Guo-Dong Zhao"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Long-Qing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long-Qing Chen"
                },
                "author": "Long-Qing Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.11688v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11688v1",
                "title": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing"
                },
                "updated": "2026-02-12T08:09:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    9,
                    14,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11688v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T08:09:14Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    9,
                    14,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "12 pages, 4 figures. Code: https://github.com/atoniolo76/gotoni/tree/benchmark-load-balancing",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Alessio Ricci Toniolo"
                    },
                    {
                        "name": "Abinaya Dinesh"
                    },
                    {
                        "name": "Rome Thorstenson"
                    }
                ],
                "author_detail": {
                    "name": "Rome Thorstenson"
                },
                "author": "Rome Thorstenson"
            },
            {
                "id": "http://arxiv.org/abs/2602.07837v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.07837v3",
                "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI"
                },
                "updated": "2026-02-12T08:08:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    8,
                    43,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.07837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.07837v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-08T06:23:43Z",
                "published_parsed": [
                    2026,
                    2,
                    8,
                    6,
                    23,
                    43,
                    6,
                    39,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Hongzhi Zang"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Hao Lin"
                    },
                    {
                        "name": "Tianxing Zhou"
                    },
                    {
                        "name": "Zefang Huang"
                    },
                    {
                        "name": "Zhen Guo"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Jiakai Zhou"
                    },
                    {
                        "name": "Yuze Sheng"
                    },
                    {
                        "name": "Shizhe Zhang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Yufeng Yue"
                    },
                    {
                        "name": "Quanlu Zhang"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.09725v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09725v2",
                "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Remote Prefix Fetching with GPU-native Media ASICs"
                },
                "updated": "2026-02-12T03:30:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    35,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09725v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T12:29:02Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    12,
                    29,
                    2,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Liang Mi"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Jinghan Chen"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.11521v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11521v1",
                "title": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System"
                },
                "updated": "2026-02-12T03:30:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    11,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11521v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T03:30:11Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    11,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "15 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Shixin Zhao"
                    },
                    {
                        "name": "Yutian Zhou"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02388v2",
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation"
                },
                "updated": "2026-02-12T02:52:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    2,
                    52,
                    55,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02388v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu"
            },
            {
                "id": "http://arxiv.org/abs/2602.10718v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10718v2",
                "title": "SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining"
                },
                "updated": "2026-02-12T02:38:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    2,
                    38,
                    42,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10718v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T10:24:42Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    10,
                    24,
                    42,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Shuhao Hu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yulei Qian"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai"
            },
            {
                "id": "http://arxiv.org/abs/2602.11470v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11470v1",
                "title": "Cachemir: Fully Homomorphic Encrypted Inference of Generative Large Language Model with KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cachemir: Fully Homomorphic Encrypted Inference of Generative Large Language Model with KV Cache"
                },
                "updated": "2026-02-12T01:01:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    1,
                    1,
                    38,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11470v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative large language models (LLMs) have revolutionized multiple domains. Modern LLMs predominantly rely on an autoregressive decoding strategy, which generates output tokens sequentially and employs a key-value cache (KV cache) to avoid redundant computation. However, the widespread deployment of LLMs has raised serious privacy concerns, as users are feeding all types of data into the model, motivating the development of secure inference frameworks based on fully homomorphic encryption (FHE). A major limitation of existing FHE-based frameworks is their inability to effectively integrate the KV cache, resulting in prohibitively high latency for autoregressive decoding. In this paper, we propose Cachemir, a KV Cache Accelerated Homomorphic Encrypted LLM Inference Regime to overcome this limitation. Cachemir comprises three key technical contributions: 1) a set of novel HE packing algorithms specifically designed to leverage the computational advantages of the KV cache; 2) an interleaved replicated packing algorithm to efficiently compute the vector-matrix multiplications that result from using the KV cache in Transformer linear layers; and 3) an augmented bootstrapping placement strategy that accounts for the KV cache to minimize bootstrapping cost. We demonstrate that Cachemir achieves $48.83\\times$ and $67.16\\times$ speedup over MOAI (ICML'25) and THOR (CCS'25) respectively on CPU and consumes less than 100 seconds on GPU to generate an output token for Llama-3-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have revolutionized multiple domains. Modern LLMs predominantly rely on an autoregressive decoding strategy, which generates output tokens sequentially and employs a key-value cache (KV cache) to avoid redundant computation. However, the widespread deployment of LLMs has raised serious privacy concerns, as users are feeding all types of data into the model, motivating the development of secure inference frameworks based on fully homomorphic encryption (FHE). A major limitation of existing FHE-based frameworks is their inability to effectively integrate the KV cache, resulting in prohibitively high latency for autoregressive decoding. In this paper, we propose Cachemir, a KV Cache Accelerated Homomorphic Encrypted LLM Inference Regime to overcome this limitation. Cachemir comprises three key technical contributions: 1) a set of novel HE packing algorithms specifically designed to leverage the computational advantages of the KV cache; 2) an interleaved replicated packing algorithm to efficiently compute the vector-matrix multiplications that result from using the KV cache in Transformer linear layers; and 3) an augmented bootstrapping placement strategy that accounts for the KV cache to minimize bootstrapping cost. We demonstrate that Cachemir achieves $48.83\\times$ and $67.16\\times$ speedup over MOAI (ICML'25) and THOR (CCS'25) respectively on CPU and consumes less than 100 seconds on GPU to generate an output token for Llama-3-8B."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T01:01:38Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    1,
                    1,
                    38,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "16 pages, 10 figures, 6 tables. Under review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ye Yu"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Pedro Soto"
                    },
                    {
                        "name": "Wenjie Xiong"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.11374v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11374v1",
                "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Aware Distillation for Transformer-SSM Hybrids"
                },
                "updated": "2026-02-11T21:05:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    21,
                    5,
                    0,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11374v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T21:05:00Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    21,
                    5,
                    0,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aviv Bick"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Albert Gu"
                    }
                ],
                "author_detail": {
                    "name": "Albert Gu"
                },
                "author": "Albert Gu"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.20156v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20156v1",
                "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
                },
                "updated": "2026-02-23T18:59:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20156v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko"
            },
            {
                "id": "http://arxiv.org/abs/2602.20153v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20153v1",
                "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks"
                },
                "updated": "2026-02-23T18:59:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    10,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20153v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:10Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    10,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "11 pages + appendix. Preliminary version of an ongoing project that will be expanded with furhter evaluations",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Jakob Heiss"
                    },
                    {
                        "name": "S칬ren Lambrecht"
                    },
                    {
                        "name": "Jakob Weissteiner"
                    },
                    {
                        "name": "Hanna Wutte"
                    },
                    {
                        "name": "콯an 콯uri캜"
                    },
                    {
                        "name": "Josef Teichmann"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yu"
                },
                "author": "Bin Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.20144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20144v1",
                "title": "Agentic AI for Scalable and Robust Optical Systems Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Scalable and Robust Optical Systems Control"
                },
                "updated": "2026-02-23T18:54:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:54:32Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yue-Kai Huang"
                    },
                    {
                        "name": "Philip Ji"
                    },
                    {
                        "name": "Denton Wu"
                    },
                    {
                        "name": "Mahdi Safari"
                    },
                    {
                        "name": "Flemming Holtorf"
                    },
                    {
                        "name": "Kenaish AlQubaisi"
                    },
                    {
                        "name": "Norbert M. Linke"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Dirk Englund"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.03817v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03817v3",
                "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models"
                },
                "updated": "2026-02-23T18:54:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    13,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03817v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-04T14:14:20Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    14,
                    14,
                    20,
                    5,
                    277,
                    0
                ],
                "arxiv_comment": "Published as a conference paper at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Philipp Becker"
                    },
                    {
                        "name": "Niklas Freymuth"
                    },
                    {
                        "name": "Serge Thilges"
                    },
                    {
                        "name": "Fabian Otto"
                    },
                    {
                        "name": "Gerhard Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Neumann"
                },
                "author": "Gerhard Neumann"
            },
            {
                "id": "http://arxiv.org/abs/2602.20140v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20140v1",
                "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment"
                },
                "updated": "2026-02-23T18:52:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    52,
                    13,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20140v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameters given a molecular graph. This lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning PackFlow as a scalable proposal engine within standard CSP pipelines. To explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. Physics alignment improves physical validity without altering inference-time sampling. We validate PackFlow's performance against heuristic baselines through two distinct evaluations. First, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. Second, we assess the full end-to-end workflow on two unseen CSP blind-test case studies, including relaxation and lattice-energy analysis. In both settings, PackFlow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameters given a molecular graph. This lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning PackFlow as a scalable proposal engine within standard CSP pipelines. To explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. Physics alignment improves physical validity without altering inference-time sampling. We validate PackFlow's performance against heuristic baselines through two distinct evaluations. First, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. Second, we assess the full end-to-end workflow on two unseen CSP blind-test case studies, including relaxation and lattice-energy analysis. In both settings, PackFlow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck."
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:52:13Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    52,
                    13,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph"
                },
                "authors": [
                    {
                        "name": "Akshay Subramanian"
                    },
                    {
                        "name": "Elton Pan"
                    },
                    {
                        "name": "Juno Nam"
                    },
                    {
                        "name": "Maurice Weiler"
                    },
                    {
                        "name": "Shuhui Qu"
                    },
                    {
                        "name": "Cheol Woo Park"
                    },
                    {
                        "name": "Tommi S. Jaakkola"
                    },
                    {
                        "name": "Elsa Olivetti"
                    },
                    {
                        "name": "Rafael Gomez-Bombarelli"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Gomez-Bombarelli"
                },
                "author": "Rafael Gomez-Bombarelli"
            },
            {
                "id": "http://arxiv.org/abs/2602.20137v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20137v1",
                "title": "Do Large Language Models Understand Data Visualization Rules?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Data Visualization Rules?"
                },
                "updated": "2026-02-23T18:47:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    47,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20137v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:47:51Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    47,
                    51,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Martin Sinnona"
                    },
                    {
                        "name": "Valentin Bonas"
                    },
                    {
                        "name": "Emmanuel Iarussi"
                    },
                    {
                        "name": "Viviana Siless"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Siless"
                },
                "author": "Viviana Siless"
            },
            {
                "id": "http://arxiv.org/abs/2512.01149v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01149v2",
                "title": "A Benchmark of Causal vs. Correlation AI for Predictive Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark of Causal vs. Correlation AI for Predictive Maintenance"
                },
                "updated": "2026-02-23T18:46:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    56,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01149v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T23:59:37Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    23,
                    59,
                    37,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shaunak Dhande"
                    },
                    {
                        "name": "Chutian Ma"
                    },
                    {
                        "name": "Giacinto Paolo Saggese"
                    },
                    {
                        "name": "Paul Smith"
                    },
                    {
                        "name": "Krishna Taduri"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Taduri"
                },
                "author": "Krishna Taduri"
            },
            {
                "id": "http://arxiv.org/abs/2602.20135v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20135v1",
                "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration"
                },
                "updated": "2026-02-23T18:46:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20135v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:46:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Amanlou"
                    },
                    {
                        "name": "Erfan Shafiee Moghaddam"
                    },
                    {
                        "name": "Yasaman Amou Jafari"
                    },
                    {
                        "name": "Mahdi Noori"
                    },
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Behnam Bahrak"
                    }
                ],
                "author_detail": {
                    "name": "Behnam Bahrak"
                },
                "author": "Behnam Bahrak"
            },
            {
                "id": "http://arxiv.org/abs/2602.20133v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20133v1",
                "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization"
                },
                "updated": "2026-02-23T18:45:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    45,
                    31,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20133v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:45:31Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    45,
                    31,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Shubham Agrawal"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Ashwin Naren"
                    },
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Alex Dimakis"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2602.20132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20132v1",
                "title": "LAD: Learning Advantage Distribution for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAD: Learning Advantage Distribution for Reasoning"
                },
                "updated": "2026-02-23T18:44:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    44,
                    10,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:44:10Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    44,
                    10,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wendi Li"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.20130v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20130v1",
                "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering"
                },
                "updated": "2026-02-23T18:42:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    42,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20130v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:42:50Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    42,
                    50,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yiran Song"
                    },
                    {
                        "name": "Xiaoyi Chen"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Yang Ruan"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20126v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20126v1",
                "title": "Adaptation to Intrinsic Dependence in Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptation to Intrinsic Dependence in Diffusion Language Models"
                },
                "updated": "2026-02-23T18:41:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    41,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20126v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:41:34Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    41,
                    34,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yunxiao Zhao"
                    },
                    {
                        "name": "Changxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Changxiao Cai"
                },
                "author": "Changxiao Cai"
            },
            {
                "id": "http://arxiv.org/abs/2602.20122v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20122v1",
                "title": "NanoKnow: How to Know What Your Language Model Knows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoKnow: How to Know What Your Language Model Knows"
                },
                "updated": "2026-02-23T18:37:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    37,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20122v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:37:49Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    37,
                    49,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lingwei Gu"
                    },
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin"
            },
            {
                "id": "http://arxiv.org/abs/2206.13174v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2206.13174v2",
                "title": "Towards Unifying Perceptual Reasoning and Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Perceptual Reasoning and Logical Reasoning"
                },
                "updated": "2026-02-23T18:36:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    36,
                    24,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2206.13174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2206.13174v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "An increasing number of scientific experiments support the view of perception as Bayesian inference, which is rooted in Helmholtz's view of perception as unconscious inference. Recent study of logic presents a view of logical reasoning as Bayesian inference. In this paper, we give a simple probabilistic model that is applicable to both perceptual reasoning and logical reasoning. We show that the model unifies the two essential processes common in perceptual and logical systems: on the one hand, the process by which perceptual and logical knowledge is derived from another knowledge, and on the other hand, the process by which such knowledge is derived from data. We fully characterise the model in terms of logical consequence relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An increasing number of scientific experiments support the view of perception as Bayesian inference, which is rooted in Helmholtz's view of perception as unconscious inference. Recent study of logic presents a view of logical reasoning as Bayesian inference. In this paper, we give a simple probabilistic model that is applicable to both perceptual reasoning and logical reasoning. We show that the model unifies the two essential processes common in perceptual and logical systems: on the one hand, the process by which perceptual and logical knowledge is derived from another knowledge, and on the other hand, the process by which such knowledge is derived from data. We fully characterise the model in terms of logical consequence relations."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2022-06-27T10:32:47Z",
                "published_parsed": [
                    2022,
                    6,
                    27,
                    10,
                    32,
                    47,
                    0,
                    178,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hiroyuki Kido"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Kido"
                },
                "author": "Hiroyuki Kido"
            },
            {
                "id": "http://arxiv.org/abs/2508.02943v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02943v2",
                "title": "Reliable Non-Leveled Homomorphic Encryption for Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Non-Leveled Homomorphic Encryption for Web Services"
                },
                "updated": "2026-02-23T18:35:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    35,
                    44,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02943v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the ubiquitous deployment of web services, ensuring data confidentiality has become a challenging imperative. Fully Homomorphic Encryption (FHE) presents a powerful solution for processing encrypted data; however, its widespread adoption is severely constrained by two fundamental bottlenecks: substantial computational overhead and the absence of a built-in automatic error correction mechanism. These limitations render the deployment of FHE in real-world, complex network environments impractical.\n  To address this dual challenge, this work puts forward a new FHE framework that enhances computational efficiency and integrates an automatic error correction capability through new encoding techniques and an algebraic reliability layer. Our system was validated across several web workloads, including encrypted inference on MNIST and CIFAR-10, federated aggregation with non-IID data, and streaming analytics on household power consumption data. Experimental results demonstrate significant performance improvements, particularly with large polynomial modulus degrees such as $N=8192$, while maintaining task accuracy within $0.5\\%$ of the plaintext baseline. Furthermore, our error correction mechanism reduces the service failure rate to below $0.5\\%$ even under harsh, bursty network fault conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ubiquitous deployment of web services, ensuring data confidentiality has become a challenging imperative. Fully Homomorphic Encryption (FHE) presents a powerful solution for processing encrypted data; however, its widespread adoption is severely constrained by two fundamental bottlenecks: substantial computational overhead and the absence of a built-in automatic error correction mechanism. These limitations render the deployment of FHE in real-world, complex network environments impractical.\n  To address this dual challenge, this work puts forward a new FHE framework that enhances computational efficiency and integrates an automatic error correction capability through new encoding techniques and an algebraic reliability layer. Our system was validated across several web workloads, including encrypted inference on MNIST and CIFAR-10, federated aggregation with non-IID data, and streaming analytics on household power consumption data. Experimental results demonstrate significant performance improvements, particularly with large polynomial modulus degrees such as $N=8192$, while maintaining task accuracy within $0.5\\%$ of the plaintext baseline. Furthermore, our error correction mechanism reduces the service failure rate to below $0.5\\%$ even under harsh, bursty network fault conditions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T22:53:36Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    22,
                    53,
                    36,
                    0,
                    216,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Baigang Chen"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2602.20114v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20114v1",
                "title": "Benchmarking Unlearning for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Unlearning for Vision Transformers"
                },
                "updated": "2026-02-23T18:33:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    33,
                    16,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20114v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:33:16Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    33,
                    16,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kairan Zhao"
                    },
                    {
                        "name": "Iurie Luca"
                    },
                    {
                        "name": "Peter Triantafillou"
                    }
                ],
                "author_detail": {
                    "name": "Peter Triantafillou"
                },
                "author": "Peter Triantafillou"
            },
            {
                "id": "http://arxiv.org/abs/2602.20111v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20111v1",
                "title": "Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds"
                },
                "updated": "2026-02-23T18:30:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    30,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20111v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.\n  We resolve this question by proving a matching $풜(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.\n  We resolve this question by proving a matching $풜(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:30:48Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    30,
                    48,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ezra Edelman"
                    },
                    {
                        "name": "Surbhi Goel"
                    }
                ],
                "author_detail": {
                    "name": "Surbhi Goel"
                },
                "author": "Surbhi Goel"
            },
            {
                "id": "http://arxiv.org/abs/2506.07751v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.07751v4",
                "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking"
                },
                "updated": "2026-02-23T18:25:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    25,
                    13,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.07751v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.07751v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T13:34:50Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    13,
                    34,
                    50,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Abbe"
                },
                "author": "Emmanuel Abbe"
            },
            {
                "id": "http://arxiv.org/abs/2602.05165v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.05165v3",
                "title": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization"
                },
                "updated": "2026-02-23T18:23:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    23,
                    57,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.05165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.05165v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-05T00:33:02Z",
                "published_parsed": [
                    2026,
                    2,
                    5,
                    0,
                    33,
                    2,
                    3,
                    36,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kevin Han"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Gedi Zhou"
                    },
                    {
                        "name": "Serena Li"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Lizhu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lizhu Zhang"
                },
                "author": "Lizhu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20102v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20102v1",
                "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BarrierSteer: LLM Safety via Learning Barrier Steering"
                },
                "updated": "2026-02-23T18:19:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    19,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20102v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:19:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    19,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "This paper introduces SafeBarrier, a framework that enforces safety in large language models by steering their latent representations with control barrier functions during inference, reducing adversarial and unsafe outputs",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thanh Q. Tran"
                    },
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Kiwan Wong"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Wei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xiao"
                },
                "author": "Wei Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2601.05500v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05500v3",
                "title": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm"
                },
                "updated": "2026-02-23T18:16:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    16,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05500v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is not just limited to human preferences, but is also consequential even in safety critical domains such as medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how - high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability. Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80\\%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is not just limited to human preferences, but is also consequential even in safety critical domains such as medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how - high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability. Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80\\%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-09T03:19:37Z",
                "published_parsed": [
                    2026,
                    1,
                    9,
                    3,
                    19,
                    37,
                    4,
                    9,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Aparna Elangovan"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Mahsa Elyasi"
                    },
                    {
                        "name": "Ismail Akdulum"
                    },
                    {
                        "name": "Mehmet Aksakal"
                    },
                    {
                        "name": "Enes Gurun"
                    },
                    {
                        "name": "Brian Hur"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Ravid Shwartz Ziv"
                    },
                    {
                        "name": "Karin Verspoor"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth"
            },
            {
                "id": "http://arxiv.org/abs/2601.17258v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.17258v2",
                "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding"
                },
                "updated": "2026-02-23T18:12:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    12,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.17258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.17258v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-24T02:17:07Z",
                "published_parsed": [
                    2026,
                    1,
                    24,
                    2,
                    17,
                    7,
                    5,
                    24,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jo칚o Pereira"
                    },
                    {
                        "name": "Vasco Lopes"
                    },
                    {
                        "name": "Jo칚o Neves"
                    },
                    {
                        "name": "David Semedo"
                    }
                ],
                "author_detail": {
                    "name": "David Semedo"
                },
                "author": "David Semedo"
            },
            {
                "id": "http://arxiv.org/abs/2507.17842v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.17842v2",
                "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
                },
                "updated": "2026-02-23T18:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.17842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.17842v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-23T18:10:43Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    18,
                    10,
                    43,
                    2,
                    204,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026. The project page is available at https://damon-demon.github.io/shop-r1.html",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Sinong Zhan"
                    },
                    {
                        "name": "Vianne Gao"
                    },
                    {
                        "name": "Ruochen Jiao"
                    },
                    {
                        "name": "Junze Liu"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yuxin Tang"
                    },
                    {
                        "name": "Ran Xue"
                    },
                    {
                        "name": "Houyu Zhang"
                    },
                    {
                        "name": "Qingjun Cui"
                    },
                    {
                        "name": "Yufan Guo"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20094v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20094v1",
                "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching"
                },
                "updated": "2026-02-23T18:06:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    6,
                    15,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20094v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:06:15Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    6,
                    15,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "8 pages plus references, 3 figures, 3 tables. Under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuzhe Wang"
                    },
                    {
                        "name": "Yaochen Zhu"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.13632v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13632v2",
                "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Gap Between Text and Speech Understanding in LLMs"
                },
                "updated": "2026-02-23T18:05:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    5,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13632v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Santiago Cuervo"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maureen de Seyssel"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    }
                ],
                "author_detail": {
                    "name": "Zakaria Aldeneh"
                },
                "author": "Zakaria Aldeneh"
            },
            {
                "id": "http://arxiv.org/abs/2602.20091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20091v1",
                "title": "How Retrieved Context Shapes Internal Representations in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Retrieved Context Shapes Internal Representations in RAG"
                },
                "updated": "2026-02-23T18:02:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    2,
                    4,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:02:04Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    2,
                    4,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.00759v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.00759v6",
                "title": "Proof by Mechanization: Cubic Diophantine Equation Satisfiability is $풖^0_1$-Complete",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof by Mechanization: Cubic Diophantine Equation Satisfiability is $풖^0_1$-Complete"
                },
                "updated": "2026-02-23T18:00:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    0,
                    7,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.00759v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.00759v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Following a mechanization in $\\mathsf{Rocq}$, we prove that satisfiability of single Diophantine equations of total degree $\\le 3$ over $\\mathbb{N}$ (with a computably bounded number of variables depending on the instance) is $풖^0_1$-complete, and hence undecidable, in the sense of Hilbert's Tenth Problem. The core construction is a uniform primitive recursive compiler mapping arithmetic sentence codes to finite structured cubic constraint systems, such that provability in $\\mathrm{I}풊_0 + \\mathrm{B}풖_1$ is equivalent to solvability of the emitted system. Conjunction is eliminated without squaring: all emitted constraints are merged into a single cubic Diophantine equation via a Fibonacci-banded two-channel equality that preserves the cubic bound without sum-of-squares blow-up. All syntactic and inference checks compile to quadratic constraints; the only cubic terms come from linear selector variables multiplying those quadratic obligations. The mechanized core is constructive and requires no classical axioms: uniform computability, checker-constraint equivalence, and the degree-$3$ bound are verified by structural induction. From this base, undecidability is obtained by a classical Diagonal Argument: any total correct decider for cubic satisfiability would compose with the compiler to decide arithmetic theoremhood, and a fixed-point construction yields an instance on which it fails. Consequently, cubic satisfiability is $풖^0_1$-complete.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following a mechanization in $\\mathsf{Rocq}$, we prove that satisfiability of single Diophantine equations of total degree $\\le 3$ over $\\mathbb{N}$ (with a computably bounded number of variables depending on the instance) is $풖^0_1$-complete, and hence undecidable, in the sense of Hilbert's Tenth Problem. The core construction is a uniform primitive recursive compiler mapping arithmetic sentence codes to finite structured cubic constraint systems, such that provability in $\\mathrm{I}풊_0 + \\mathrm{B}풖_1$ is equivalent to solvability of the emitted system. Conjunction is eliminated without squaring: all emitted constraints are merged into a single cubic Diophantine equation via a Fibonacci-banded two-channel equality that preserves the cubic bound without sum-of-squares blow-up. All syntactic and inference checks compile to quadratic constraints; the only cubic terms come from linear selector variables multiplying those quadratic obligations. The mechanized core is constructive and requires no classical axioms: uniform computability, checker-constraint equivalence, and the degree-$3$ bound are verified by structural induction. From this base, undecidability is obtained by a classical Diagonal Argument: any total correct decider for cubic satisfiability would compose with the compiler to decide arithmetic theoremhood, and a fixed-point construction yields an instance on which it fails. Consequently, cubic satisfiability is $풖^0_1$-complete."
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-01T10:45:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    45,
                    14,
                    2,
                    274,
                    0
                ],
                "arxiv_comment": "32 pages. Formalized in Rocq; includes certified reduction from Hilbert proof checking to cubic Diophantine systems and consequences for single equations",
                "arxiv_primary_category": {
                    "term": "math.LO"
                },
                "authors": [
                    {
                        "name": "Milan Rosko"
                    }
                ],
                "author_detail": {
                    "name": "Milan Rosko"
                },
                "author": "Milan Rosko"
            },
            {
                "id": "http://arxiv.org/abs/2511.20648v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20648v2",
                "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight"
                },
                "updated": "2026-02-23T17:59:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    59,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20648v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 38.90 AP_3D, surpassing the previous best by +13.98 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 38.90 AP_3D, surpassing the previous best by +13.98 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T18:59:45Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    18,
                    59,
                    45,
                    1,
                    329,
                    0
                ],
                "arxiv_comment": "Tech report. Project page: https://nvlabs.github.io/LocateAnything3D/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Guowen Zhang"
                    },
                    {
                        "name": "Johan Bjorck"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Liang-Yan Gui"
                    },
                    {
                        "name": "Jim Fan"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Zhiding Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiding Yu"
                },
                "author": "Zhiding Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.20084v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20084v1",
                "title": "Do Large Language Models Understand Data Visualization Principles?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Data Visualization Principles?"
                },
                "updated": "2026-02-23T17:51:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    51,
                    6,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20084v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:51:06Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    51,
                    6,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Martin Sinnona"
                    },
                    {
                        "name": "Valentin Bonas"
                    },
                    {
                        "name": "Viviana Siless"
                    },
                    {
                        "name": "Emmanuel Iarussi"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Iarussi"
                },
                "author": "Emmanuel Iarussi"
            },
            {
                "id": "http://arxiv.org/abs/2602.20082v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20082v1",
                "title": "Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)"
                },
                "updated": "2026-02-23T17:48:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    48,
                    24,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20082v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:48:24Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    48,
                    24,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Zoe Paraskevopoulou"
                    }
                ],
                "author_detail": {
                    "name": "Zoe Paraskevopoulou"
                },
                "author": "Zoe Paraskevopoulou"
            },
            {
                "id": "http://arxiv.org/abs/2602.20065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20065v1",
                "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees"
                },
                "updated": "2026-02-23T17:22:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:22:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "36 pages, 3 figures, 2 tables, 4 supplementary tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Natalia Moskvina"
                    },
                    {
                        "name": "Raquel Montero"
                    },
                    {
                        "name": "Masaya Yoshida"
                    },
                    {
                        "name": "Ferdy Hubers"
                    },
                    {
                        "name": "Paolo Morosi"
                    },
                    {
                        "name": "Walid Irhaymi"
                    },
                    {
                        "name": "Jin Yan"
                    },
                    {
                        "name": "Tamara Serrano"
                    },
                    {
                        "name": "Elena Pagliarini"
                    },
                    {
                        "name": "Fritz G칲nther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada"
            },
            {
                "id": "http://arxiv.org/abs/2602.20064v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20064v1",
                "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow"
                },
                "updated": "2026-02-23T17:22:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    35,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20064v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:22:35Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    35,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Zac Garby"
                    },
                    {
                        "name": "Andrew D. Gordon"
                    },
                    {
                        "name": "David Sands"
                    }
                ],
                "author_detail": {
                    "name": "David Sands"
                },
                "author": "David Sands"
            },
            {
                "id": "http://arxiv.org/abs/2602.20060v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20060v1",
                "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving"
                },
                "updated": "2026-02-23T17:17:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    17,
                    26,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20060v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:17:26Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    17,
                    26,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Junli Wang"
                    },
                    {
                        "name": "Xueyi Liu"
                    },
                    {
                        "name": "Yinan Zheng"
                    },
                    {
                        "name": "Zebing Xing"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Guang Li"
                    },
                    {
                        "name": "Kun Ma"
                    },
                    {
                        "name": "Guang Chen"
                    },
                    {
                        "name": "Hangjun Ye"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qichao Zhang"
                },
                "author": "Qichao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20059v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20059v1",
                "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction Theater: A case of LLM Agents Interacting at Scale"
                },
                "updated": "2026-02-23T17:14:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    14,
                    29,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20059v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:14:29Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    14,
                    29,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sarath Shekkizhar"
                    },
                    {
                        "name": "Adam Earle"
                    }
                ],
                "author_detail": {
                    "name": "Adam Earle"
                },
                "author": "Adam Earle"
            },
            {
                "id": "http://arxiv.org/abs/2602.20055v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20055v1",
                "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation"
                },
                "updated": "2026-02-23T17:10:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    10,
                    0,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20055v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:10:00Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    10,
                    0,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Apoorva Vashisth"
                    },
                    {
                        "name": "Manav Kulshrestha"
                    },
                    {
                        "name": "Pranav Bakshi"
                    },
                    {
                        "name": "Damon Conover"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "arxiv_affiliation": "Purdue University",
                "author": "Aniket Bera"
            },
            {
                "id": "http://arxiv.org/abs/2506.08119v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08119v2",
                "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents"
                },
                "updated": "2026-02-23T17:05:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    5,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08119v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents struggle to execute complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Existing benchmarks fail to capture the procedural complexity and tool orchestration demands of real-world workflows. We introduce SOP-Bench, a benchmark of 2,000+ tasks from human expert-authored SOPs across 12 business domains (healthcare, logistics, finance, content moderation, etc.). Using a human-AI collaborative framework, experts crafted authentic SOPs while AI generated artifacts (tools, APIs, datasets), all human-validated, yielding realistic tasks with executable interfaces and ground-truth outputs.\n  SOP-Bench serves as a research enabler for systematically investigating agent architectures, model capabilities, and deployment considerations across diverse procedural tasks. We demonstrate its utility through illustrative experiments with a subset of frontier models across Function-Calling (FC) and ReAct agents, revealing critical insights. For example, (1) newer models do not guarantee better performance - Claude 4 family outperforms Claude 4.5 family on ReAct tasks (Claude 4 Opus: 72.4% vs. Claude 4.5 Sonnet: 63.3% task success rate), demonstrating that production upgrades require validation; (2) no single model-agent combination dominates: best performances range from 57% to 100% depending on domain. These examples illustrate how SOP-Bench enables isolating and studying specific dimensions of agent performance without costly production experiments. Our goal is not to rank model capabilities or build optimal agents, but to provide a rigorous evaluation framework that enables the researchers and practitioners to systematically investigate agent design choices, model selection, and deployment strategies. We release the benchmark at https://github.com/amazon-science/sop-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents struggle to execute complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Existing benchmarks fail to capture the procedural complexity and tool orchestration demands of real-world workflows. We introduce SOP-Bench, a benchmark of 2,000+ tasks from human expert-authored SOPs across 12 business domains (healthcare, logistics, finance, content moderation, etc.). Using a human-AI collaborative framework, experts crafted authentic SOPs while AI generated artifacts (tools, APIs, datasets), all human-validated, yielding realistic tasks with executable interfaces and ground-truth outputs.\n  SOP-Bench serves as a research enabler for systematically investigating agent architectures, model capabilities, and deployment considerations across diverse procedural tasks. We demonstrate its utility through illustrative experiments with a subset of frontier models across Function-Calling (FC) and ReAct agents, revealing critical insights. For example, (1) newer models do not guarantee better performance - Claude 4 family outperforms Claude 4.5 family on ReAct tasks (Claude 4 Opus: 72.4% vs. Claude 4.5 Sonnet: 63.3% task success rate), demonstrating that production upgrades require validation; (2) no single model-agent combination dominates: best performances range from 57% to 100% depending on domain. These examples illustrate how SOP-Bench enables isolating and studying specific dimensions of agent performance without costly production experiments. Our goal is not to rank model capabilities or build optimal agents, but to provide a rigorous evaluation framework that enables the researchers and practitioners to systematically investigate agent design choices, model selection, and deployment strategies. We release the benchmark at https://github.com/amazon-science/sop-bench."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T18:20:12Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    20,
                    12,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Subhrangshu Nandi"
                    },
                    {
                        "name": "Arghya Datta"
                    },
                    {
                        "name": "Rohith Nama"
                    },
                    {
                        "name": "Udita Patel"
                    },
                    {
                        "name": "Nikhil Vichare"
                    },
                    {
                        "name": "Indranil Bhattacharya"
                    },
                    {
                        "name": "Prince Grover"
                    },
                    {
                        "name": "Shivam Asija"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Arushi Gupta"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Huzefa Raja"
                    },
                    {
                        "name": "Shayan Ray"
                    },
                    {
                        "name": "Aaron Chan"
                    },
                    {
                        "name": "Esther Xu Fei"
                    },
                    {
                        "name": "Gaoyuan Du"
                    },
                    {
                        "name": "Zuhaib Akhtar"
                    },
                    {
                        "name": "Harshita Asnani"
                    },
                    {
                        "name": "Weian Chan"
                    },
                    {
                        "name": "Ming Xiong"
                    },
                    {
                        "name": "Francesco Carbone"
                    },
                    {
                        "name": "Jeetu Mirchandani"
                    }
                ],
                "author_detail": {
                    "name": "Jeetu Mirchandani"
                },
                "author": "Jeetu Mirchandani"
            },
            {
                "id": "http://arxiv.org/abs/2602.20052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20052v1",
                "title": "Entropy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy in Large Language Models"
                },
                "updated": "2026-02-23T17:02:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    2,
                    45,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:02:45Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    2,
                    45,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "7 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Marco Scharringhausen"
                    }
                ],
                "author_detail": {
                    "name": "Marco Scharringhausen"
                },
                "author": "Marco Scharringhausen"
            },
            {
                "id": "http://arxiv.org/abs/2602.20049v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20049v1",
                "title": "noDice: Inference for Discrete Probabilistic Programs with Nondeterminism and Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "noDice: Inference for Discrete Probabilistic Programs with Nondeterminism and Conditioning"
                },
                "updated": "2026-02-23T16:59:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    59,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20049v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Probabilistic programming languages (PPLs) are an expressive and intuitive means of representing complex probability distributions. In that realm, languages like Dice target an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis. Another important feature in the world of probabilistic modeling are nondeterministic choices as found in Markov Decision Processes (MDPs) which play a major role in reinforcement learning. Modern PPLs usually lack support for nondeterminism. We address this gap with the introduction of noDice, which extends the discrete probabilistic inference engine Dice. noDice performs inference on loop-free programs by constructing an MDP so that the distributions modeled by the program correspond to schedulers in the MDP. Furthermore, decision diagrams are used as an intermediate step to exploit the program structure and drastically reduce the state space of the MDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic programming languages (PPLs) are an expressive and intuitive means of representing complex probability distributions. In that realm, languages like Dice target an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis. Another important feature in the world of probabilistic modeling are nondeterministic choices as found in Markov Decision Processes (MDPs) which play a major role in reinforcement learning. Modern PPLs usually lack support for nondeterminism. We address this gap with the introduction of noDice, which extends the discrete probabilistic inference engine Dice. noDice performs inference on loop-free programs by constructing an MDP so that the distributions modeled by the program correspond to schedulers in the MDP. Furthermore, decision diagrams are used as an intermediate step to exploit the program structure and drastically reduce the state space of the MDP."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:59:37Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    59,
                    37,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "46 pages, 7 figures, accepted to OOPSLA 2026 R1",
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Tobias G칲rtler"
                    },
                    {
                        "name": "Benjamin Lucien Kaminski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Lucien Kaminski"
                },
                "author": "Benjamin Lucien Kaminski"
            },
            {
                "id": "http://arxiv.org/abs/2602.20045v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20045v1",
                "title": "Dual Security for MIMO-OFDM ISAC Systems: Artificial Ghosts or Artificial Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Security for MIMO-OFDM ISAC Systems: Artificial Ghosts or Artificial Noise"
                },
                "updated": "2026-02-23T16:57:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    57,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20045v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Integrated sensing and communication (ISAC) enables the efficient sharing of wireless resources to support emerging applications, but it also gives rise to new sensing-based security vulnerabilities. Here, potential communication security threats whereby confidential messages intended for legitimate users are intercepted, but also unauthorized receivers (Eves) can passively exploit target echoes to infer sensing parameters without users being aware. Despite these risks, the joint protection of sensing and communication security in ISAC systems remains unexplored. To address this challenge, this paper proposes a two-layer dual-secure ISAC framework that simultaneously protects sensing and communication against passive sensing Eves and communication Eves, without requiring their channel state information (CSI). Specifically, transmit beamformers are jointly designed to inject artificial noise (AN) to introduce interference to communication Eves, while deliberately distorting the reference signal available to sensing Eves to impair their sensing capability. Furthermore, the proposed design generates artificial ghosts (AGs) with fake angle-range-velocity profiles observable by all receivers. Legitimate receivers can suppress these AGs, whereas sensing Eves cannot, thereby significantly reducing their probability of correctly detecting the true targets. Numerical results demonstrate that the proposed framework effectively enhances both communication and sensing security, while preserving the performance of communication users and legitimate sensing receivers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) enables the efficient sharing of wireless resources to support emerging applications, but it also gives rise to new sensing-based security vulnerabilities. Here, potential communication security threats whereby confidential messages intended for legitimate users are intercepted, but also unauthorized receivers (Eves) can passively exploit target echoes to infer sensing parameters without users being aware. Despite these risks, the joint protection of sensing and communication security in ISAC systems remains unexplored. To address this challenge, this paper proposes a two-layer dual-secure ISAC framework that simultaneously protects sensing and communication against passive sensing Eves and communication Eves, without requiring their channel state information (CSI). Specifically, transmit beamformers are jointly designed to inject artificial noise (AN) to introduce interference to communication Eves, while deliberately distorting the reference signal available to sensing Eves to impair their sensing capability. Furthermore, the proposed design generates artificial ghosts (AGs) with fake angle-range-velocity profiles observable by all receivers. Legitimate receivers can suppress these AGs, whereas sensing Eves cannot, thereby significantly reducing their probability of correctly detecting the true targets. Numerical results demonstrate that the proposed framework effectively enhances both communication and sensing security, while preserving the performance of communication users and legitimate sensing receivers."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:57:34Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    57,
                    34,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Journal on Selected Areas in Communications",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Yinchao Yang"
                    },
                    {
                        "name": "Prabhat Raj Gautam"
                    },
                    {
                        "name": "Yathreb Bouazizi"
                    },
                    {
                        "name": "Michael Breza"
                    },
                    {
                        "name": "Julie McCann"
                    }
                ],
                "author_detail": {
                    "name": "Julie McCann"
                },
                "author": "Julie McCann"
            },
            {
                "id": "http://arxiv.org/abs/2509.06326v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06326v2",
                "title": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs"
                },
                "updated": "2026-02-23T16:56:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    56,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06326v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T04:17:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    4,
                    17,
                    2,
                    0,
                    251,
                    0
                ],
                "arxiv_comment": "accept to DAC 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ruisi Zhang"
                    },
                    {
                        "name": "Yifei Zhao"
                    },
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar"
            },
            {
                "id": "http://arxiv.org/abs/2602.20040v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20040v1",
                "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization"
                },
                "updated": "2026-02-23T16:49:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    49,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20040v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:49:37Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    49,
                    37,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Fahmida Liza Piya"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti"
            },
            {
                "id": "http://arxiv.org/abs/2602.11962v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11962v2",
                "title": "Wisdom of the LLM Crowd: A Large Scale Benchmark of Multi-Label U.S. Election-Related Harmful Social Media Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom of the LLM Crowd: A Large Scale Benchmark of Multi-Label U.S. Election-Related Harmful Social Media Content"
                },
                "updated": "2026-02-23T16:43:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    43,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11962v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The spread of election misinformation and harmful political content conveys misleading narratives and poses a serious threat to democratic integrity. Detecting harmful content at early stages is essential for understanding and potentially mitigating its downstream spread. In this study, we introduce USE24-XD, a large-scale dataset of nearly 100k posts collected from X (formerly Twitter) during the 2024 U.S. presidential election cycle, enriched with spatio-temporal metadata. To substantially reduce the cost of manual annotation while enabling scalable categorization, we employ six large language models (LLMs) to systematically annotate posts across five nuanced categories: Conspiracy, Sensationalism, Hate Speech, Speculation, and Satire. We validate LLM annotations with crowdsourcing (n = 34) and benchmark them against human annotators. Inter-rater reliability analyses show comparable agreement patterns between LLMs and humans, with LLMs exhibiting higher internal consistency and achieving up to 0.90 recall on Speculation. We apply a wisdom-of-the-crowd approach across LLMs to aggregate annotations and curate a robust multi-label dataset. 60% of posts receive at least one label. We further analyze how human annotator demographics, including political ideology and affiliation, shape labeling behavior, highlighting systematic sources of subjectivity in judgments of harmful content. The USE24-XD dataset is publicly released to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of election misinformation and harmful political content conveys misleading narratives and poses a serious threat to democratic integrity. Detecting harmful content at early stages is essential for understanding and potentially mitigating its downstream spread. In this study, we introduce USE24-XD, a large-scale dataset of nearly 100k posts collected from X (formerly Twitter) during the 2024 U.S. presidential election cycle, enriched with spatio-temporal metadata. To substantially reduce the cost of manual annotation while enabling scalable categorization, we employ six large language models (LLMs) to systematically annotate posts across five nuanced categories: Conspiracy, Sensationalism, Hate Speech, Speculation, and Satire. We validate LLM annotations with crowdsourcing (n = 34) and benchmark them against human annotators. Inter-rater reliability analyses show comparable agreement patterns between LLMs and humans, with LLMs exhibiting higher internal consistency and achieving up to 0.90 recall on Speculation. We apply a wisdom-of-the-crowd approach across LLMs to aggregate annotations and curate a robust multi-label dataset. 60% of posts receive at least one label. We further analyze how human annotator demographics, including political ideology and affiliation, shape labeling behavior, highlighting systematic sources of subjectivity in judgments of harmful content. The USE24-XD dataset is publicly released to support future research."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T13:57:12Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    57,
                    12,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Qile Wang"
                    },
                    {
                        "name": "Prerana Khatiwada"
                    },
                    {
                        "name": "Carolina Coimbra Vieira"
                    },
                    {
                        "name": "Benjamin E. Bagozzi"
                    },
                    {
                        "name": "Kenneth E. Barner"
                    },
                    {
                        "name": "Matthew Louis Mauriello"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Louis Mauriello"
                },
                "author": "Matthew Louis Mauriello"
            },
            {
                "id": "http://arxiv.org/abs/2506.16824v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.16824v2",
                "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs"
                },
                "updated": "2026-02-23T16:38:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    38,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.16824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.16824v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-20T08:26:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thomas Marwitz"
                    },
                    {
                        "name": "Alexander Colsmann"
                    },
                    {
                        "name": "Ben Breitung"
                    },
                    {
                        "name": "Christoph Brabec"
                    },
                    {
                        "name": "Christoph Kirchlechner"
                    },
                    {
                        "name": "Eva Blasco"
                    },
                    {
                        "name": "Gabriel Cadilha Marques"
                    },
                    {
                        "name": "Horst Hahn"
                    },
                    {
                        "name": "Michael Hirtz"
                    },
                    {
                        "name": "Pavel A. Levkin"
                    },
                    {
                        "name": "Yolita M. Eggeler"
                    },
                    {
                        "name": "Tobias Schl칬der"
                    },
                    {
                        "name": "Pascal Friederich"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Friederich"
                },
                "author": "Pascal Friederich"
            },
            {
                "id": "http://arxiv.org/abs/2602.20008v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20008v1",
                "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation"
                },
                "updated": "2026-02-23T16:15:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    15,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20008v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:15:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    15,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Louis Fabrice Tshimanga"
                    },
                    {
                        "name": "Andrea Zanola"
                    },
                    {
                        "name": "Federico Del Pup"
                    },
                    {
                        "name": "Manfredo Atzori"
                    }
                ],
                "author_detail": {
                    "name": "Manfredo Atzori"
                },
                "author": "Manfredo Atzori"
            },
            {
                "id": "http://arxiv.org/abs/2602.10604v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10604v2",
                "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters"
                },
                "updated": "2026-02-23T16:07:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    7,
                    40,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10604v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T07:53:51Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    7,
                    53,
                    51,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "Technical report for Step 3.5 Flash",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Dong"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Brian Li"
                    },
                    {
                        "name": "Buyun Ma"
                    },
                    {
                        "name": "Chang Su"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Chao Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengting Feng"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Di Qi"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Guoliang Cao"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Han Cheng"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Hekun Lv"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Hongbo Huang"
                    },
                    {
                        "name": "Hongbo Peng"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Houyong Chen"
                    },
                    {
                        "name": "Huangxi Zhu"
                    },
                    {
                        "name": "Huimin Wu"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiashu Lv"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Jishi Zhou"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Kai Liang"
                    },
                    {
                        "name": "Kaibo Liu"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Lv"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Luck Ma"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Michael Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Mingming Zhang"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Mitt Huang"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinxin Du"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rongqiu Yang"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruixin Han"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shan Wang"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shaowen Yang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiwei Li"
                    },
                    {
                        "name": "Shuangshuang Tian"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianchi Yue"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianning Li"
                    },
                    {
                        "name": "Tingdan Luo"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Weibo Wu"
                    },
                    {
                        "name": "Weihao Xie"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiangfeng Wang"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaolan Yuan"
                    },
                    {
                        "name": "Xiaoran Jiao"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xinran Wang"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Xuanti Feng"
                    },
                    {
                        "name": "Xuedan Cai"
                    },
                    {
                        "name": "Xuqiang Zhou"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Yanlin Lai"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yichen Lv"
                    },
                    {
                        "name": "Yicheng Cao"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yikun Yang"
                    },
                    {
                        "name": "Yin Zhao"
                    },
                    {
                        "name": "Yingxiu Zhao"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Yiyang Chen"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Yongshen Long"
                    },
                    {
                        "name": "Yongyao Wang"
                    },
                    {
                        "name": "Yousong Guan"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yudi Zhao"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yueqiang Lin"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yurong Zhang"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zetao Hong"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zheng Zeng"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhiheng Hu"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Ren"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Zixuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Wang"
                },
                "author": "Zixuan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19994v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19994v1",
                "title": "RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather"
                },
                "updated": "2026-02-23T16:01:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    1,
                    31,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19994v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:01:31Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    1,
                    31,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted to 2026 IEEE Intelligent Vehicles Symposium (IV)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Christof Leitgeb"
                    },
                    {
                        "name": "Thomas Puchleitner"
                    },
                    {
                        "name": "Max Peter Ronecker"
                    },
                    {
                        "name": "Daniel Watzenig"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Watzenig"
                },
                "author": "Daniel Watzenig"
            },
            {
                "id": "http://arxiv.org/abs/2408.07543v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.07543v6",
                "title": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts"
                },
                "updated": "2026-02-23T15:56:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    56,
                    57,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.07543v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.07543v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-14T13:23:43Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    23,
                    43,
                    2,
                    227,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Minxuan Zhou"
                    },
                    {
                        "name": "Zirong Chen"
                    },
                    {
                        "name": "Meiyi Qiang"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.01428v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01428v2",
                "title": "Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models"
                },
                "updated": "2026-02-23T15:55:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    55,
                    7,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01428v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-01T20:30:59Z",
                "published_parsed": [
                    2026,
                    2,
                    1,
                    20,
                    30,
                    59,
                    6,
                    32,
                    0
                ],
                "arxiv_comment": "Accepted at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Qi Long"
                    }
                ],
                "author_detail": {
                    "name": "Qi Long"
                },
                "author": "Qi Long"
            },
            {
                "id": "http://arxiv.org/abs/2602.19980v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19980v1",
                "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks"
                },
                "updated": "2026-02-23T15:47:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    47,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19980v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:47:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    47,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Itamar Trainin"
                    },
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "Omri Abend"
                    },
                    {
                        "name": "Amir Feder"
                    }
                ],
                "author_detail": {
                    "name": "Amir Feder"
                },
                "author": "Amir Feder"
            },
            {
                "id": "http://arxiv.org/abs/2602.11936v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11936v2",
                "title": "Probing Dynamical Dark Energy with Late-Time Data: Evidence, Tensions, and the Limits of the $w_0w_a$CDM Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Dynamical Dark Energy with Late-Time Data: Evidence, Tensions, and the Limits of the $w_0w_a$CDM Framework"
                },
                "updated": "2026-02-23T15:38:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    38,
                    1,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11936v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We test the dynamical dark-energy $w_0w_a$CDM (CPL) framework against $풑$CDM using CMB anisotropies and lensing together with late-time distance probes: DESI DR2 BAO, the completed SDSS-IV BAO consensus compilation, a transverse/angular BAO compilation (BAOtr), and the Cepheid-calibrated PantheonPlus SN~Ia likelihood (PP\\&SH0ES). We find that CPL inferences are strongly dataset-dependent. With CMB data alone, the broad geometric degeneracy in $(H_0,풜_{\\rm m},w_0,w_a)$ admits an extrapolation tail that can extend to $q_0<-1$ (super-acceleration), whereas adding DESI DR2 BAO pulls the reconstruction toward a weakly accelerating or nearly coasting present-day Universe ($q_0\\simeq 0$). In contrast, combining CMB with PP\\&SH0ES and BAOtr yields a conventional moderately accelerating expansion ($-1<q_0\\lesssim 0$) and substantially reduces the Hubble tension. Across all combinations, $w(z\\to\\infty)=w_0+w_a<-1$, while at post-recombination redshifts the expansion remains matter dominated ($q\\to1/2$). The origin of this behavior can be traced to low-redshift distance information: BAOtr and DESI prefer different BAO distance ratios at $z\\lesssim 0.5$, which propagates into divergent expansion histories in CPL. In all cases, $r_{\\rm d}$ stays nearly unchanged, indicating that shifts in $H_0$ arise from late-time expansion freedom rather than early-Universe physics. Bayesian evidence mirrors this contingency: it is strong for CPL mainly when PP\\&SH0ES and/or BAOtr are included, while it is inconclusive for CMB-only and CMB+DESI and moderately favors $풑$CDM for CMB+SDSS. Overall, our results show that the apparent support for CPL and its ability to ease the Hubble tension are not universal but depend sensitively on the adopted low-redshift distance data, motivating either more flexible late-time models or closer scrutiny of residual systematics in current BAO determinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test the dynamical dark-energy $w_0w_a$CDM (CPL) framework against $풑$CDM using CMB anisotropies and lensing together with late-time distance probes: DESI DR2 BAO, the completed SDSS-IV BAO consensus compilation, a transverse/angular BAO compilation (BAOtr), and the Cepheid-calibrated PantheonPlus SN~Ia likelihood (PP\\&SH0ES). We find that CPL inferences are strongly dataset-dependent. With CMB data alone, the broad geometric degeneracy in $(H_0,풜_{\\rm m},w_0,w_a)$ admits an extrapolation tail that can extend to $q_0<-1$ (super-acceleration), whereas adding DESI DR2 BAO pulls the reconstruction toward a weakly accelerating or nearly coasting present-day Universe ($q_0\\simeq 0$). In contrast, combining CMB with PP\\&SH0ES and BAOtr yields a conventional moderately accelerating expansion ($-1<q_0\\lesssim 0$) and substantially reduces the Hubble tension. Across all combinations, $w(z\\to\\infty)=w_0+w_a<-1$, while at post-recombination redshifts the expansion remains matter dominated ($q\\to1/2$). The origin of this behavior can be traced to low-redshift distance information: BAOtr and DESI prefer different BAO distance ratios at $z\\lesssim 0.5$, which propagates into divergent expansion histories in CPL. In all cases, $r_{\\rm d}$ stays nearly unchanged, indicating that shifts in $H_0$ arise from late-time expansion freedom rather than early-Universe physics. Bayesian evidence mirrors this contingency: it is strong for CPL mainly when PP\\&SH0ES and/or BAOtr are included, while it is inconclusive for CMB-only and CMB+DESI and moderately favors $풑$CDM for CMB+SDSS. Overall, our results show that the apparent support for CPL and its ability to ease the Hubble tension are not universal but depend sensitively on the adopted low-redshift distance data, motivating either more flexible late-time models or closer scrutiny of residual systematics in current BAO determinations."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T13:35:54Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    35,
                    54,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "18 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Tengpeng Xu"
                    },
                    {
                        "name": "Suresh Kumar"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Abra칚o J. S. Capistrano"
                    },
                    {
                        "name": "칐zg칲r Akarsu"
                    }
                ],
                "author_detail": {
                    "name": "칐zg칲r Akarsu"
                },
                "author": "칐zg칲r Akarsu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19969v1",
                "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting"
                },
                "updated": "2026-02-23T15:30:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    52,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:30:52Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    52,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted by EACL2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuxing Tian"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Yun Nie"
                },
                "author": "Jian-Yun Nie"
            },
            {
                "id": "http://arxiv.org/abs/2504.15077v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.15077v4",
                "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL"
                },
                "updated": "2026-02-23T15:30:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.15077v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.15077v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-21T13:05:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "arxiv_comment": "26 pages, work in progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Simone Papicchio"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Luca Cagliero"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti"
            },
            {
                "id": "http://arxiv.org/abs/2602.19964v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19964v1",
                "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference"
                },
                "updated": "2026-02-23T15:28:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    28,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19964v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:28:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    28,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "8 pages, 1 Figure",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Moritz A. Zanger"
                    },
                    {
                        "name": "Yijun Wu"
                    },
                    {
                        "name": "Pascal R. Van der Vaart"
                    },
                    {
                        "name": "Wendelin B칬hmer"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan"
            },
            {
                "id": "http://arxiv.org/abs/2602.19959v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19959v1",
                "title": "Deploying a Hybrid PVFinder Algorithm for Primary Vertex Reconstruction in LHCb's GPU-Resident HLT1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying a Hybrid PVFinder Algorithm for Primary Vertex Reconstruction in LHCb's GPU-Resident HLT1"
                },
                "updated": "2026-02-23T15:26:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    26,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19959v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LHCb's Run 3 upgrade introduced a fully software-based trigger system operating at 30~MHz, processing an average of 5.6 proton-proton collision vertices per bunch crossing (event). This work presents the development of an inference engine for PVFinder, a hybrid deep neural network for finding primary vertices, the proton-proton collision points from which all subsequent particle decays originate into Allen, LHCb's High Level Trigger (HLT1) framework. The integration addresses critical real-time constraints including fixed memory pools, single-stream execution, and sub-400~$풮$s per-event processing budgets on NVIDIA GPUs. We introduce a translation layer that bridges Allen's Structure-of-Arrays (SoA) data layout with cuDNN's tensor format while maintaining zero-copy semantics and deterministic behavior. Current performance shows the CNN stage contributes significant throughput overhead. We present a roadmap targeting order-of-magnitude improvements through mixed-precision computing, model compression and other techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LHCb's Run 3 upgrade introduced a fully software-based trigger system operating at 30~MHz, processing an average of 5.6 proton-proton collision vertices per bunch crossing (event). This work presents the development of an inference engine for PVFinder, a hybrid deep neural network for finding primary vertices, the proton-proton collision points from which all subsequent particle decays originate into Allen, LHCb's High Level Trigger (HLT1) framework. The integration addresses critical real-time constraints including fixed memory pools, single-stream execution, and sub-400~$풮$s per-event processing budgets on NVIDIA GPUs. We introduce a translation layer that bridges Allen's Structure-of-Arrays (SoA) data layout with cuDNN's tensor format while maintaining zero-copy semantics and deterministic behavior. Current performance shows the CNN stage contributes significant throughput overhead. We present a roadmap targeting order-of-magnitude improvements through mixed-precision computing, model compression and other techniques."
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:26:49Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    26,
                    49,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Contribution to the proceedings of the Connecting the Dots Workshop",
                "arxiv_primary_category": {
                    "term": "hep-ex"
                },
                "authors": [
                    {
                        "name": "Simon Akar"
                    },
                    {
                        "name": "Mohamed Elashri"
                    },
                    {
                        "name": "Conor Henderson"
                    },
                    {
                        "name": "Michael Sokoloff"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sokoloff"
                },
                "author": "Michael Sokoloff"
            },
            {
                "id": "http://arxiv.org/abs/2602.19955v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19955v1",
                "title": "Electronic structure of Graphene/Co interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic structure of Graphene/Co interfaces"
                },
                "updated": "2026-02-23T15:22:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    22,
                    58,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19955v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/physrevb.90.195446",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Photoemission, from core levels and valence band, and low-energy electron diffraction (LEED) have been employed to investigate the electronic and structural properties of novel graphene-ferromagnetic (G-FM) systems,obtained by intercalation of one mono-layer (1ML) and several layers (4ML) of Co on G grown on Ir(111). Upon intercalation of 1ML of Co, the Co lattice is resized to match the Ir-Ir lattice parameter, resulting in a mismatched G/Co/Ir(111) system. The intercalation of further Co layers leads to a relaxation of the Co lattice and a progressive formation of a commensurate G layer lying on top. We show the C 1s line shape and the band structure of G in the two artificial phases, mismatched and commensurate G/Co, through a comparison with the electronic structure of G grown directly on a Co thick film. Our results show that while the G valence band mainly reflects the hybridization with the d states of Co, regardless of the structural phase, the C 1s line shape is very sensitive to the rumpling of the G layer and the coordination of carbon atoms with the underlying Co. Even in the commensurate (1x1) G/Co phase, where graphene is in register with the Co film, from the angular dependence of the C 1s core level we infer the presence of a double component, due to in-equivalent adsorption sites of carbon sub-lattices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoemission, from core levels and valence band, and low-energy electron diffraction (LEED) have been employed to investigate the electronic and structural properties of novel graphene-ferromagnetic (G-FM) systems,obtained by intercalation of one mono-layer (1ML) and several layers (4ML) of Co on G grown on Ir(111). Upon intercalation of 1ML of Co, the Co lattice is resized to match the Ir-Ir lattice parameter, resulting in a mismatched G/Co/Ir(111) system. The intercalation of further Co layers leads to a relaxation of the Co lattice and a progressive formation of a commensurate G layer lying on top. We show the C 1s line shape and the band structure of G in the two artificial phases, mismatched and commensurate G/Co, through a comparison with the electronic structure of G grown directly on a Co thick film. Our results show that while the G valence band mainly reflects the hybridization with the d states of Co, regardless of the structural phase, the C 1s line shape is very sensitive to the rumpling of the G layer and the coordination of carbon atoms with the underlying Co. Even in the commensurate (1x1) G/Co phase, where graphene is in register with the Co film, from the angular dependence of the C 1s core level we infer the presence of a double component, due to in-equivalent adsorption sites of carbon sub-lattices."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:22:58Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    22,
                    58,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "arxiv_journal_ref": "Phys. Rev. B 90, 195446 (2014)",
                "authors": [
                    {
                        "name": "Daniela Pacil칠"
                    },
                    {
                        "name": "Simone Lisi"
                    },
                    {
                        "name": "Iolanda Di Bernardo"
                    },
                    {
                        "name": "M. Papagno"
                    },
                    {
                        "name": "L. Ferrari"
                    },
                    {
                        "name": "Michele Pisarra"
                    },
                    {
                        "name": "Marco Caputo"
                    },
                    {
                        "name": "S. K. Mahatha"
                    },
                    {
                        "name": "P. M. Sheverdyaeva"
                    },
                    {
                        "name": "P. Moras"
                    },
                    {
                        "name": "P. Lacovig"
                    },
                    {
                        "name": "Silvano Lizzit"
                    },
                    {
                        "name": "Alessandro Baraldi"
                    },
                    {
                        "name": "Maria Grazia Betti"
                    },
                    {
                        "name": "Carlo Carbone"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Carbone"
                },
                "author": "Carlo Carbone",
                "arxiv_doi": "10.1103/physrevb.90.195446"
            },
            {
                "id": "http://arxiv.org/abs/2602.19948v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19948v1",
                "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming"
                },
                "updated": "2026-02-23T15:17:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    17,
                    18,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19948v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:17:18Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    17,
                    18,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "This paper is a condensed version of the first author's Ph.D. dissertation submitted to Northeastern University",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ian Steenstra"
                    },
                    {
                        "name": "Paola Pedrelli"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Stacy Marsella"
                    },
                    {
                        "name": "Timothy W. Bickmore"
                    }
                ],
                "author_detail": {
                    "name": "Timothy W. Bickmore"
                },
                "author": "Timothy W. Bickmore"
            },
            {
                "id": "http://arxiv.org/abs/2602.09609v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09609v2",
                "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing"
                },
                "updated": "2026-02-23T15:14:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    14,
                    47,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09609v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T10:01:16Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    10,
                    1,
                    16,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialun Liu"
                    },
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Xiao Cao"
                    },
                    {
                        "name": "Yukuo Ma"
                    },
                    {
                        "name": "Gonghu Shang"
                    },
                    {
                        "name": "Haibin Huang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xiangzhen Chang"
                    },
                    {
                        "name": "Zhiyong Huang"
                    },
                    {
                        "name": "Jiakui Hu"
                    },
                    {
                        "name": "Zuoxin Li"
                    },
                    {
                        "name": "Yuanzhi Liang"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haitong Tang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Liying Yang"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Shiwen Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.19941v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19941v1",
                "title": "Probing Dust in the MWC 480 Disk from Millimeter to Centimeter Wavelengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Dust in the MWC 480 Disk from Millimeter to Centimeter Wavelengths"
                },
                "updated": "2026-02-23T15:12:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    12,
                    22,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19941v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present deep, high-resolution ($\\sim$100 mas) Karl G. Jansky Very Large Array (VLA) Ka-band (9.1 mm) observations of the disk around MWC 480, and infer dust properties through a combined analysis with archival Atacama Large Millimeter/submillimeter Array (ALMA) data at 0.87, 1.17, 1.33, and 3.0 mm. The prominent dust ring at 95 au (B95) is detected at 9.1 mm for the first time, while the faint outer ring at 160 au is not revealed. Through non-parametric visibility modeling, we identified two new annular features: a plateau within 20-50 au across all wavelengths, and a shoulder exterior to the B95 ring at 0.87, 1.17 and 1.33 mm, consistent with signatures of planet-disk interaction. We find that the width of the B95 ring remains constant across wavelengths, suggesting that fragmentation dominates over radial diffusion or that unresolved substructure is present within the ring. Resolved spectral modeling yields two families of dust solutions that reproduce the observations equally well: compact grains or highly porous (90\\%) grains, with carbonaceous components dominated by refractory organics or amorphous carbon, respectively. The inferred maximum grain sizes peak at the locations of the two rings and reach centimeter within the B95 ring. The total dust masses are $860^{+95}_{-78}\\rm~M_\\oplus$/$1500^{+440}_{-330}\\rm~M_\\oplus$ (large/small-grain solution in inner disk) and $230^{+14}_{-13}\\rm~M_\\oplus$ for the two dust mixtures. The B95 ring alone contains $100^{+5}_{-5}\\rm~M_\\oplus$ and $43^{+2}_{-2}\\rm~M_\\oplus$, respectively, sufficient to assemble the cores of giant planets. Finally, we highlight the power of broadband, multi-wavelength observations in placing better constraints on dust composition and porosity in protoplanetary disks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present deep, high-resolution ($\\sim$100 mas) Karl G. Jansky Very Large Array (VLA) Ka-band (9.1 mm) observations of the disk around MWC 480, and infer dust properties through a combined analysis with archival Atacama Large Millimeter/submillimeter Array (ALMA) data at 0.87, 1.17, 1.33, and 3.0 mm. The prominent dust ring at 95 au (B95) is detected at 9.1 mm for the first time, while the faint outer ring at 160 au is not revealed. Through non-parametric visibility modeling, we identified two new annular features: a plateau within 20-50 au across all wavelengths, and a shoulder exterior to the B95 ring at 0.87, 1.17 and 1.33 mm, consistent with signatures of planet-disk interaction. We find that the width of the B95 ring remains constant across wavelengths, suggesting that fragmentation dominates over radial diffusion or that unresolved substructure is present within the ring. Resolved spectral modeling yields two families of dust solutions that reproduce the observations equally well: compact grains or highly porous (90\\%) grains, with carbonaceous components dominated by refractory organics or amorphous carbon, respectively. The inferred maximum grain sizes peak at the locations of the two rings and reach centimeter within the B95 ring. The total dust masses are $860^{+95}_{-78}\\rm~M_\\oplus$/$1500^{+440}_{-330}\\rm~M_\\oplus$ (large/small-grain solution in inner disk) and $230^{+14}_{-13}\\rm~M_\\oplus$ for the two dust mixtures. The B95 ring alone contains $100^{+5}_{-5}\\rm~M_\\oplus$ and $43^{+2}_{-2}\\rm~M_\\oplus$, respectively, sufficient to assemble the cores of giant planets. Finally, we highlight the power of broadband, multi-wavelength observations in placing better constraints on dust composition and porosity in protoplanetary disks."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:12:22Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    12,
                    22,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "29 pages, 18 figures, 4 tables. Accepted for publication in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Yangfan Shi"
                    },
                    {
                        "name": "Feng Long"
                    },
                    {
                        "name": "Enrique Mac칤as"
                    },
                    {
                        "name": "Gregory J. Herczeg"
                    },
                    {
                        "name": "Paola Pinilla"
                    },
                    {
                        "name": "Sean M. Andrews"
                    },
                    {
                        "name": "David J. Wilner"
                    },
                    {
                        "name": "Haochang Jiang"
                    },
                    {
                        "name": "Ruobing Dong"
                    },
                    {
                        "name": "Richard Teague"
                    },
                    {
                        "name": "Ilaria Pascucci"
                    },
                    {
                        "name": "Claudia Toci"
                    },
                    {
                        "name": "Yuri Aikawa"
                    },
                    {
                        "name": "Daniel Harsono"
                    },
                    {
                        "name": "Yao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Liu"
                },
                "author": "Yao Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.08550v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08550v2",
                "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing"
                },
                "updated": "2026-02-23T15:12:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    12,
                    1,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08550v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T11:50:29Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    11,
                    50,
                    29,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shih-Fang Chen"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    },
                    {
                        "name": "I-Hong Jhuo"
                    },
                    {
                        "name": "Yen-Yu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Yu Lin"
                },
                "author": "Yen-Yu Lin"
            },
            {
                "id": "http://arxiv.org/abs/2602.19938v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19938v1",
                "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs"
                },
                "updated": "2026-02-23T15:11:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    11,
                    16,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19938v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:11:16Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    11,
                    16,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Mingfu Liang"
                    },
                    {
                        "name": "Luke Simon"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.19936v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19936v1",
                "title": "Guiding Peptide Kinetics via Collective-Variable Tuning of Free-Energy Barriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Peptide Kinetics via Collective-Variable Tuning of Free-Energy Barriers"
                },
                "updated": "2026-02-23T15:09:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    9,
                    31,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19936v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While recent advances in AI have transformed protein structure prediction, protein function is also often strongly influenced by the thermodynamic and kinetic features encoded in its underlying free-energy surface. Here, we propose a framework to rationally modify these surfaces in order to control conformational transition rates, built on the Collective Variables for Free Energy Surface Tailoring (CV-FEST) framework, and validate it on point mutations of the miniprotein Chignolin. The framework relies on Harmonic Linear Discriminant Analysis (HLDA) based collective variables (CVs) constructed from short molecular dynamics trajectories restricted to the metastable states, requiring only limited sampling within each state. Notably, the HLDA CV derived solely from the wild-type system already provides residue-level scores that predict whether mutations at specific positions are likely to accelerate or slow conformational transitions. Furthermore, we find that the leading HLDA eigenvalue associated with the derived CV, a quantitative measure of the one-dimensional statistical separation between folded and unfolded ensembles, correlates strongly with conformational transition rates across mutations. Together, these results show that kinetic effects of point mutations can be inferred from minimal local sampling, providing a practical route to the rational engineering of conformational transition rates without exhaustive simulations or large training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advances in AI have transformed protein structure prediction, protein function is also often strongly influenced by the thermodynamic and kinetic features encoded in its underlying free-energy surface. Here, we propose a framework to rationally modify these surfaces in order to control conformational transition rates, built on the Collective Variables for Free Energy Surface Tailoring (CV-FEST) framework, and validate it on point mutations of the miniprotein Chignolin. The framework relies on Harmonic Linear Discriminant Analysis (HLDA) based collective variables (CVs) constructed from short molecular dynamics trajectories restricted to the metastable states, requiring only limited sampling within each state. Notably, the HLDA CV derived solely from the wild-type system already provides residue-level scores that predict whether mutations at specific positions are likely to accelerate or slow conformational transitions. Furthermore, we find that the leading HLDA eigenvalue associated with the derived CV, a quantitative measure of the one-dimensional statistical separation between folded and unfolded ensembles, correlates strongly with conformational transition rates across mutations. Together, these results show that kinetic effects of point mutations can be inferred from minimal local sampling, providing a practical route to the rational engineering of conformational transition rates without exhaustive simulations or large training datasets."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:09:31Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    9,
                    31,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Alexander Zhilkin"
                    },
                    {
                        "name": "Muralika Medaparambath"
                    },
                    {
                        "name": "Dan Mendels"
                    }
                ],
                "author_detail": {
                    "name": "Dan Mendels"
                },
                "author": "Dan Mendels"
            },
            {
                "id": "http://arxiv.org/abs/2602.19929v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19929v1",
                "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models"
                },
                "updated": "2026-02-23T15:06:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    6,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19929v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:06:32Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    6,
                    32,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "We propose a novel end-to-end generative framework for beam prediction by using vision-language models",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Chenran Kou"
                    },
                    {
                        "name": "Changsheng You"
                    },
                    {
                        "name": "Mingjiang Wu"
                    },
                    {
                        "name": "Dingzhu Wen"
                    },
                    {
                        "name": "Zezhong Zhang"
                    },
                    {
                        "name": "Chengwen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chengwen Xing"
                },
                "author": "Chengwen Xing"
            },
            {
                "id": "http://arxiv.org/abs/2602.19926v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19926v1",
                "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models"
                },
                "updated": "2026-02-23T15:05:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    5,
                    28,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19926v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($풧= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($풧= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:05:28Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    5,
                    28,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jin Liu"
                    },
                    {
                        "name": "Yinbin Miao"
                    },
                    {
                        "name": "Ning Xi"
                    },
                    {
                        "name": "Junkang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junkang Liu"
                },
                "author": "Junkang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19919v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19919v1",
                "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling"
                },
                "updated": "2026-02-23T14:58:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19919v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:58:51Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    51,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zikai Wei"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Wanyun Zhou"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19918v1",
                "title": "RobPI: Robust Private Inference against Malicious Client",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobPI: Robust Private Inference against Malicious Client"
                },
                "updated": "2026-02-23T14:58:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    8,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:58:08Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    8,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted by SaTML 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiaqi Xue"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou"
            },
            {
                "id": "http://arxiv.org/abs/2602.19915v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19915v1",
                "title": "Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction"
                },
                "updated": "2026-02-23T14:55:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    55,
                    28,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19915v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:55:28Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    55,
                    28,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "24 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Michael Trimboli"
                    },
                    {
                        "name": "Mohammed Alsubaie"
                    },
                    {
                        "name": "Sirani M. Perera"
                    },
                    {
                        "name": "Ke-Gang Wang"
                    },
                    {
                        "name": "Xianqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xianqi Li"
                },
                "author": "Xianqi Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.19914v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19914v1",
                "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning"
                },
                "updated": "2026-02-23T14:54:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    54,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19914v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:54:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    54,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "51 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Thatchawin Leelawat"
                    },
                    {
                        "name": "Lewis D Griffin"
                    }
                ],
                "author_detail": {
                    "name": "Lewis D Griffin"
                },
                "author": "Lewis D Griffin"
            },
            {
                "id": "http://arxiv.org/abs/2508.00017v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00017v3",
                "title": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation"
                },
                "updated": "2026-02-23T14:37:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    20,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00017v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Generative Logic (GL), a deterministic architecture that starts from user-supplied axiomatic definitions, written in a minimalist Mathematical Programming Language (MPL), and systematically explores a configurable region of their deductive neighborhood. A defining feature of the architecture is its unified hash-based inference engine, which executes both algebraic manipulations and deterministic logical transformations. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; whenever the premises of an inference rule unify, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs. Experimental validation is performed on Elementary Number Theory (ENT) utilizing a batched execution strategy. Starting from foundational axioms and definitions, the system first develops first-order Peano arithmetic, which is subsequently applied to autonomously derive and prove Gauss's summation formula as a main result. To manage combinatorial explosion, GL algorithmically enumerates conjectures and applies normalization, type constraints, and counterexample (CE) filtering. On commodity hardware, an end-to-end run completes in under 7 minutes. Generated proofs export as navigable HTML so that every inference step can be inspected independently. We outline a hardware-software co-design path toward massively parallel realizations and describe future integration with large language models (LLMs) for auto-formalization and conjecture seeding. The Python, C++, and MPL code to reproduce these experiments, along with the full proof graphs in HTML as well as machine-readable text format, are available in the project's GitHub repository at github.com/Generative-Logic/GL commit 1771330 and are permanently archived at doi:10.5281/zenodo.17206386.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Generative Logic (GL), a deterministic architecture that starts from user-supplied axiomatic definitions, written in a minimalist Mathematical Programming Language (MPL), and systematically explores a configurable region of their deductive neighborhood. A defining feature of the architecture is its unified hash-based inference engine, which executes both algebraic manipulations and deterministic logical transformations. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; whenever the premises of an inference rule unify, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs. Experimental validation is performed on Elementary Number Theory (ENT) utilizing a batched execution strategy. Starting from foundational axioms and definitions, the system first develops first-order Peano arithmetic, which is subsequently applied to autonomously derive and prove Gauss's summation formula as a main result. To manage combinatorial explosion, GL algorithmically enumerates conjectures and applies normalization, type constraints, and counterexample (CE) filtering. On commodity hardware, an end-to-end run completes in under 7 minutes. Generated proofs export as navigable HTML so that every inference step can be inspected independently. We outline a hardware-software co-design path toward massively parallel realizations and describe future integration with large language models (LLMs) for auto-formalization and conjecture seeding. The Python, C++, and MPL code to reproduce these experiments, along with the full proof graphs in HTML as well as machine-readable text format, are available in the project's GitHub repository at github.com/Generative-Logic/GL commit 1771330 and are permanently archived at doi:10.5281/zenodo.17206386."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-25T17:29:19Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    29,
                    19,
                    4,
                    206,
                    0
                ],
                "arxiv_comment": "v3: Added derivation of Gauss summation formula, Logical Transformer section and batched workflow. Updated code/artifact links. 20 pages, 6 figures. Code and HTML proof graphs archived at Zenodo (DOI: 10.5281/zenodo.17206386)",
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Nikolai Sergeev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolai Sergeev"
                },
                "author": "Nikolai Sergeev"
            },
            {
                "id": "http://arxiv.org/abs/2602.19895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19895v1",
                "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning"
                },
                "updated": "2026-02-23T14:37:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    1,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:37:01Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    1,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Yun Shen"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Donghao Zhou"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zixuan Zhong"
                    },
                    {
                        "name": "Peizhou Huang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19888v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19888v1",
                "title": "A Physics-Regularized Neural Network and Kirchhoff Markov Random Field Framework for Inferring Internal Electrochemical States from Operando Spectromicroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physics-Regularized Neural Network and Kirchhoff Markov Random Field Framework for Inferring Internal Electrochemical States from Operando Spectromicroscopy"
                },
                "updated": "2026-02-23T14:32:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    32,
                    30,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19888v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantitative understanding of coupled reaction and transport processes in lithium-ion battery (LIB) composite electrodes remains challenging because key internal states cannot be measured directly. In this study, we develop a physics-integrated, data-driven analysis pipeline to estimate internal electrochemical states from operando microscopic X-ray absorption fine structure ($풮$-XAFS) hyperspectral data of LIB cathodes with LiPF$_6$ electrolyte. State-of-charge (SOC) maps are first constructed from Co K-edge spectra. To resolve ambiguities in the two-phase reaction region, a physics-regularized three-layer neural network is introduced, enforcing spatial continuity of SOC and current conservation. The inferred SOC dynamics are then incorporated into a Kirchhoff-based Markov random field framework that integrates Kirchhoff's current and voltage laws, Ohm's law, and a symmetric Butler-Volmer relation to estimate interfacial current, ionic current, electrolyte potential, and effective ionic conductivity. Application to composite electrodes with different initial electrolyte concentrations (0.3, 1, and 2M LiPF$_6$) reveals distinct reaction propagation behaviors governed by electrolyte concentration-dependent conductivity. The inferred electrolyte concentration distributions show qualitative agreement with independent operando X-ray transmission imaging performed on LIB composite cathodes employing a LiAsF$_6$ electrolyte. This framework enables quantitative visualization of otherwise inaccessible internal transport phenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative understanding of coupled reaction and transport processes in lithium-ion battery (LIB) composite electrodes remains challenging because key internal states cannot be measured directly. In this study, we develop a physics-integrated, data-driven analysis pipeline to estimate internal electrochemical states from operando microscopic X-ray absorption fine structure ($풮$-XAFS) hyperspectral data of LIB cathodes with LiPF$_6$ electrolyte. State-of-charge (SOC) maps are first constructed from Co K-edge spectra. To resolve ambiguities in the two-phase reaction region, a physics-regularized three-layer neural network is introduced, enforcing spatial continuity of SOC and current conservation. The inferred SOC dynamics are then incorporated into a Kirchhoff-based Markov random field framework that integrates Kirchhoff's current and voltage laws, Ohm's law, and a symmetric Butler-Volmer relation to estimate interfacial current, ionic current, electrolyte potential, and effective ionic conductivity. Application to composite electrodes with different initial electrolyte concentrations (0.3, 1, and 2M LiPF$_6$) reveals distinct reaction propagation behaviors governed by electrolyte concentration-dependent conductivity. The inferred electrolyte concentration distributions show qualitative agreement with independent operando X-ray transmission imaging performed on LIB composite cathodes employing a LiAsF$_6$ electrolyte. This framework enables quantitative visualization of otherwise inaccessible internal transport phenomena."
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:32:30Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    32,
                    30,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "46 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "physics.chem-ph"
                },
                "authors": [
                    {
                        "name": "Naoki Wada"
                    },
                    {
                        "name": "Yuta Kimura"
                    },
                    {
                        "name": "Masaichiro Mizumaki"
                    },
                    {
                        "name": "Koji Amezawa"
                    },
                    {
                        "name": "Ichiro Akai"
                    },
                    {
                        "name": "Toru Aonishi"
                    }
                ],
                "author_detail": {
                    "name": "Toru Aonishi"
                },
                "author": "Toru Aonishi"
            },
            {
                "id": "http://arxiv.org/abs/2512.02840v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02840v2",
                "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "promptolution: A Unified, Modular Framework for Prompt Optimization"
                },
                "updated": "2026-02-23T14:32:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    32,
                    6,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02840v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T14:53:23Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    14,
                    53,
                    23,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Timo Hei"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Matthias A른nmacher"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer"
            },
            {
                "id": "http://arxiv.org/abs/2602.08535v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08535v4",
                "title": "Causal Schr칬dinger Bridges: Constrained Optimal Transport on Structural Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Schr칬dinger Bridges: Constrained Optimal Transport on Structural Manifolds"
                },
                "updated": "2026-02-23T14:27:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    27,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08535v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08535v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and the pathology of anticipatory control. In this work, we introduce the Causal Schrodinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. By leveraging diffusion processes (SDEs), CSB enables probability mass to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes exactly into local, robust transitions. This theorem provides a principled resolution to the Information Bottleneck that plagues monolithic architectures in high dimensions. We empirically validate CSB on a full-rank causal system (d=10^5, intrinsic rank 10^5), where standard structure-blind MLPs fail to converge (MSE ~0.31). By physically implementing the structural decomposition, CSB achieves high-fidelity transport (MSE ~0.06) in just 73.73 seconds on a single GPU. This stands in stark contrast to structure-agnostic O(d^3) baselines, estimated to require over 6 years. Our results demonstrate that CSB breaks the Curse of Dimensionality through structural intelligence, offering a scalable foundation for high-stakes causal discovery in 10^5-node systems. Code is available at: https://github.com/cochran1/causal-schrodinger-bridge",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and the pathology of anticipatory control. In this work, we introduce the Causal Schrodinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. By leveraging diffusion processes (SDEs), CSB enables probability mass to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes exactly into local, robust transitions. This theorem provides a principled resolution to the Information Bottleneck that plagues monolithic architectures in high dimensions. We empirically validate CSB on a full-rank causal system (d=10^5, intrinsic rank 10^5), where standard structure-blind MLPs fail to converge (MSE ~0.31). By physically implementing the structural decomposition, CSB achieves high-fidelity transport (MSE ~0.06) in just 73.73 seconds on a single GPU. This stands in stark contrast to structure-agnostic O(d^3) baselines, estimated to require over 6 years. Our results demonstrate that CSB breaks the Curse of Dimensionality through structural intelligence, offering a scalable foundation for high-stakes causal discovery in 10^5-node systems. Code is available at: https://github.com/cochran1/causal-schrodinger-bridge"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T11:33:12Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    11,
                    33,
                    12,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rui Wu"
                    },
                    {
                        "name": "Li YongJun"
                    }
                ],
                "author_detail": {
                    "name": "Li YongJun"
                },
                "author": "Li YongJun"
            },
            {
                "id": "http://arxiv.org/abs/2602.19870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19870v1",
                "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs"
                },
                "updated": "2026-02-23T14:15:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    15,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:15:37Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    15,
                    37,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "CVPR2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qiankun Ma"
                    },
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Haofei Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhen Song"
                    },
                    {
                        "name": "Hairong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hairong Zheng"
                },
                "author": "Hairong Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2602.19864v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19864v1",
                "title": "A Bayesian Method for Air-Shower Reconstruction using Information Field Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Method for Air-Shower Reconstruction using Information Field Theory"
                },
                "updated": "2026-02-23T14:09:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    9,
                    10,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19864v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The radio detection of extensive air showers provides a powerful method for studying the origin of high-energy cosmic rays. The Low-Frequency Array (LOFAR) offers unprecedentedly detailed measurements of the radio emission footprint. However, fully exploiting this information requires advanced reconstruction techniques. In this paper, we introduce a novel framework for air shower reconstruction based on Bayesian inference and Information Field Theory (IFT). Our method is built on a fully differentiable forward model of the radio signal, which incorporates a physical emission parameterization and a precise wavefront model. Additionally, we augment this physical model with Gaussian processes to account for systematic uncertainties in both the signal fluence and arrival timing. By leveraging gradient information, our approach enables efficient (three orders of magnitude acceleration w.r.t.\\ the legacy method) and robust inference of the underlying physical shower parameters, such as primary energy and the depth of shower maximum, $X_\\text{max}$. This work provides not only point estimates but also a rigorous quantification of uncertainties. We achieve a resolution in $X_\\text{max}$ of $25\\,\\mathrm{g/cm^2}$ and a radiation energy resolution of $12\\%$ on simulations for LOFAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The radio detection of extensive air showers provides a powerful method for studying the origin of high-energy cosmic rays. The Low-Frequency Array (LOFAR) offers unprecedentedly detailed measurements of the radio emission footprint. However, fully exploiting this information requires advanced reconstruction techniques. In this paper, we introduce a novel framework for air shower reconstruction based on Bayesian inference and Information Field Theory (IFT). Our method is built on a fully differentiable forward model of the radio signal, which incorporates a physical emission parameterization and a precise wavefront model. Additionally, we augment this physical model with Gaussian processes to account for systematic uncertainties in both the signal fluence and arrival timing. By leveraging gradient information, our approach enables efficient (three orders of magnitude acceleration w.r.t.\\ the legacy method) and robust inference of the underlying physical shower parameters, such as primary energy and the depth of shower maximum, $X_\\text{max}$. This work provides not only point estimates but also a rigorous quantification of uncertainties. We achieve a resolution in $X_\\text{max}$ of $25\\,\\mathrm{g/cm^2}$ and a radiation energy resolution of $12\\%$ on simulations for LOFAR."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:09:10Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    9,
                    10,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "19 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Karen Terveer"
                    },
                    {
                        "name": "Sjoerd Bouma"
                    },
                    {
                        "name": "Stijn Buitink"
                    },
                    {
                        "name": "Arthur Corstanje"
                    },
                    {
                        "name": "Vital De Henau"
                    },
                    {
                        "name": "Vincent Eberle"
                    },
                    {
                        "name": "Torsten A. En륿in"
                    },
                    {
                        "name": "Philipp Frank"
                    },
                    {
                        "name": "Tim Huege"
                    },
                    {
                        "name": "Philipp Laub"
                    },
                    {
                        "name": "Katharine Mulrey"
                    },
                    {
                        "name": "Anna Nelles"
                    },
                    {
                        "name": "Simon Str칛hnz"
                    },
                    {
                        "name": "Satyendra Thoudam"
                    },
                    {
                        "name": "Keito Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Keito Watanabe"
                },
                "author": "Keito Watanabe"
            },
            {
                "id": "http://arxiv.org/abs/2601.19657v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.19657v4",
                "title": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token"
                },
                "updated": "2026-02-23T14:06:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    6,
                    12,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.19657v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.19657v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-27T14:32:36Z",
                "published_parsed": [
                    2026,
                    1,
                    27,
                    14,
                    32,
                    36,
                    1,
                    27,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihou Zhang"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Li Zhong"
                    },
                    {
                        "name": "Haifeng Liu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaosheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Shaosheng Cao"
                },
                "author": "Shaosheng Cao"
            },
            {
                "id": "http://arxiv.org/abs/2602.19848v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19848v1",
                "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation"
                },
                "updated": "2026-02-23T13:52:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    52,
                    28,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19848v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:52:28Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    52,
                    28,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "4 pages, 2 figures, 1 table, isbi2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Francisco Filho"
                    },
                    {
                        "name": "Kelvin Cunha"
                    },
                    {
                        "name": "F치bio Papais"
                    },
                    {
                        "name": "Emanoel dos Santos"
                    },
                    {
                        "name": "Rodrigo Mota"
                    },
                    {
                        "name": "Thales Bezerra"
                    },
                    {
                        "name": "Erico Medeiros"
                    },
                    {
                        "name": "Paulo Borba"
                    },
                    {
                        "name": "Tsang Ing Ren"
                    }
                ],
                "author_detail": {
                    "name": "Tsang Ing Ren"
                },
                "author": "Tsang Ing Ren"
            },
            {
                "id": "http://arxiv.org/abs/2509.11851v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11851v2",
                "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Memory Gap: Users Misremember What They Created With AI or Without"
                },
                "updated": "2026-02-23T13:51:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    51,
                    33,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11851v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3791494",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T12:31:00Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "22 pages, 10 figures, 10 tables, ACM CHI 2026",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Sven Goller"
                    },
                    {
                        "name": "Daniela Fernandes"
                    },
                    {
                        "name": "Robin Welsch"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_doi": "10.1145/3772318.3791494"
            },
            {
                "id": "http://arxiv.org/abs/2602.19844v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19844v1",
                "title": "LLM-enabled Applications Require System-Level Threat Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled Applications Require System-Level Threat Monitoring"
                },
                "updated": "2026-02-23T13:48:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    48,
                    36,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19844v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:48:36Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    48,
                    36,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "26 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yedi Zhang"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Xianglin Yang"
                    },
                    {
                        "name": "Jin Song Dong"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun"
            },
            {
                "id": "http://arxiv.org/abs/2602.19843v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19843v1",
                "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems"
                },
                "updated": "2026-02-23T13:47:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    47,
                    43,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19843v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:47:43Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    47,
                    43,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jin Jia"
                    },
                    {
                        "name": "Zhiling Deng"
                    },
                    {
                        "name": "Zhuangbin Chen"
                    },
                    {
                        "name": "Yingqi Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.24943v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24943v2",
                "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment"
                },
                "updated": "2026-02-23T13:41:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    41,
                    40,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24943v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T16:09:08Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    16,
                    9,
                    8,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Chenji Lu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Zhenyi Wang"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Jian Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xu"
                },
                "author": "Jian Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19840v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19840v1",
                "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation"
                },
                "updated": "2026-02-23T13:40:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    40,
                    44,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19840v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:40:44Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    40,
                    44,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingzhuo Wu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Keyan Jin"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Junbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Wang"
                },
                "author": "Junbo Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19831v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19831v1",
                "title": "An Explainable Memory Forensics Approach for Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Memory Forensics Approach for Malware Analysis"
                },
                "updated": "2026-02-23T13:30:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    30,
                    4,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19831v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:30:04Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    30,
                    4,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Silvia Lucia Sanna"
                    },
                    {
                        "name": "Davide Maiorca"
                    },
                    {
                        "name": "Giorgio Giacinto"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Giacinto"
                },
                "author": "Giorgio Giacinto"
            },
            {
                "id": "http://arxiv.org/abs/2602.19828v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19828v1",
                "title": "TextShield-R1: Reinforced Reasoning for Tampered Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextShield-R1: Reinforced Reasoning for Tampered Text Detection"
                },
                "updated": "2026-02-23T13:26:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    26,
                    18,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19828v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:26:18Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    26,
                    18,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chenfan Qu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Xuekang Zhu"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin"
            },
            {
                "id": "http://arxiv.org/abs/2602.19811v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19811v1",
                "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
                },
                "updated": "2026-02-23T13:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19811v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:12:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "12 pages, 2 figures, 5 tables. Extended version of the short paper published at DOLAP 2026 (co-located with EDBT/ICDT 2026)",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2412.17596v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.17596v4",
                "title": "Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context"
                },
                "updated": "2026-02-23T13:11:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    11,
                    55,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.17596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.17596v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-23T14:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "arxiv_comment": "Updated manuscript and title",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun"
            },
            {
                "id": "http://arxiv.org/abs/2602.19806v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19806v1",
                "title": "String Diagrams for Monoidal Categories, in Rocq",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "String Diagrams for Monoidal Categories, in Rocq"
                },
                "updated": "2026-02-23T13:04:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    4,
                    24,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19806v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a Rocq library for monoidal categories, which includes a decision procedure for proving equality of morphisms as well as notations that make it possible to reason as if they were strict, inferring MacLane isomorphims automatically in the background. Together with an external tool for visualising and editing string diagrams, this make it possible to perform rewriting steps in monoidal categories graphically, and to translate them into textual formal proofs which are concise and readable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Rocq library for monoidal categories, which includes a decision procedure for proving equality of morphisms as well as notations that make it possible to reason as if they were strict, inferring MacLane isomorphims automatically in the background. Together with an external tool for visualising and editing string diagrams, this make it possible to perform rewriting steps in monoidal categories graphically, and to translate them into textual formal proofs which are concise and readable."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:04:24Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    4,
                    24,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Damien Pous"
                    }
                ],
                "author_detail": {
                    "name": "Damien Pous"
                },
                "arxiv_affiliation": "PLUME, LIP",
                "author": "Damien Pous"
            },
            {
                "id": "http://arxiv.org/abs/2505.11057v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11057v2",
                "title": "Locally Consistent K-relations: Entailment and Axioms of Functional Dependence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locally Consistent K-relations: Entailment and Axioms of Functional Dependence"
                },
                "updated": "2026-02-23T12:48:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    48,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11057v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Local consistency arises in diverse areas, including Bayesian statistics, relational databases, and quantum foundations, and so does the notion of functional dependence. We adopt a general approach to study logical inference in a setting that enables both global inconsistency and local consistency. Our approach builds upon pairwise consistent families of K-relations, i.e, relations with tuples annotated with elements of some positive commutative monoid. The framework covers, e.g., families of probability distributions arising from quantum experiments and their possibilistic counterparts. As a first step, we investigate the entailment problem for functional dependencies (FDs) in this setting. Notably, the transitivity rule for FDs is no longer sound, but can be replaced by two novel axiom schemas. We provide a complete axiomatisation for, and establish NL-completeness of, the entailment problem of unary FDs, and demonstrate that even this restricted case exhibits context-dependent subtleties. In addition, we explore when contextual families over the Booleans have realisations as contextual families over various monoids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local consistency arises in diverse areas, including Bayesian statistics, relational databases, and quantum foundations, and so does the notion of functional dependence. We adopt a general approach to study logical inference in a setting that enables both global inconsistency and local consistency. Our approach builds upon pairwise consistent families of K-relations, i.e, relations with tuples annotated with elements of some positive commutative monoid. The framework covers, e.g., families of probability distributions arising from quantum experiments and their possibilistic counterparts. As a first step, we investigate the entailment problem for functional dependencies (FDs) in this setting. Notably, the transitivity rule for FDs is no longer sound, but can be replaced by two novel axiom schemas. We provide a complete axiomatisation for, and establish NL-completeness of, the entailment problem of unary FDs, and demonstrate that even this restricted case exhibits context-dependent subtleties. In addition, we explore when contextual families over the Booleans have realisations as contextual families over various monoids."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T09:55:34Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    55,
                    34,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Timon Barlag"
                    },
                    {
                        "name": "Miika Hannula"
                    },
                    {
                        "name": "Juha Kontinen"
                    },
                    {
                        "name": "Nina Pardal"
                    },
                    {
                        "name": "Jonni Virtema"
                    }
                ],
                "author_detail": {
                    "name": "Jonni Virtema"
                },
                "author": "Jonni Virtema"
            },
            {
                "id": "http://arxiv.org/abs/2602.19792v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19792v1",
                "title": "Unlocking photodetection for quantum sensing with Bayesian likelihood-free methods and deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking photodetection for quantum sensing with Bayesian likelihood-free methods and deep learning"
                },
                "updated": "2026-02-23T12:47:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    47,
                    41,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19792v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To operate quantum sensors at their quantum limit in real time, it is crucial to identify efficient data inference tools for rapid parameter estimation. In photodetection, the key challenge is the fast interpretation of click-patterns that exhibit non-classical statistics -- the very features responsible for the quantum enhancement of precision. We achieve this goal by comparing Bayesian likelihood-free methods with ones based on deep learning (DL). While the former are more conceptually intuitive, the latter, once trained, provide significantly faster estimates with comparable precision and yield similar predictions of the associated errors, challenging a common misconception that DL lacks such capabilities. We first verify both approaches for an analytically tractable, yet multiparameter, scenario of a two-level system emitting uncorrelated photons. Our main result, however, is the application to a driven nonlinear optomechanical device emitting non-classical light with complex multiclick correlations; in this case, our methods are essential for fast inference and, hence, unlock the possibility of distinguishing different photon statistics in real time. Our results pave the way for dynamical control of quantum sensors that leverage non-classical effects in photodetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To operate quantum sensors at their quantum limit in real time, it is crucial to identify efficient data inference tools for rapid parameter estimation. In photodetection, the key challenge is the fast interpretation of click-patterns that exhibit non-classical statistics -- the very features responsible for the quantum enhancement of precision. We achieve this goal by comparing Bayesian likelihood-free methods with ones based on deep learning (DL). While the former are more conceptually intuitive, the latter, once trained, provide significantly faster estimates with comparable precision and yield similar predictions of the associated errors, challenging a common misconception that DL lacks such capabilities. We first verify both approaches for an analytically tractable, yet multiparameter, scenario of a two-level system emitting uncorrelated photons. Our main result, however, is the application to a driven nonlinear optomechanical device emitting non-classical light with complex multiclick correlations; in this case, our methods are essential for fast inference and, hence, unlock the possibility of distinguishing different photon statistics in real time. Our results pave the way for dynamical control of quantum sensors that leverage non-classical effects in photodetection."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:47:41Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    47,
                    41,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "16 pages, 11 figures, comments are welcome",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Mateusz Molenda"
                    },
                    {
                        "name": "Lewis A. Clark"
                    },
                    {
                        "name": "Marcin P켹odzie켻"
                    },
                    {
                        "name": "Jan Kolodynski"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kolodynski"
                },
                "author": "Jan Kolodynski"
            },
            {
                "id": "http://arxiv.org/abs/2602.15888v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15888v2",
                "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing"
                },
                "updated": "2026-02-23T12:47:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    47,
                    29,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15888v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-06T13:16:28Z",
                "published_parsed": [
                    2026,
                    2,
                    6,
                    13,
                    16,
                    28,
                    4,
                    37,
                    0
                ],
                "arxiv_comment": "14 pages, 5 figures, under review at Physiological Measurement",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Boyu Li"
                    },
                    {
                        "name": "Xingchun Zhu"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu"
            },
            {
                "id": "http://arxiv.org/abs/2305.11098v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2305.11098v2",
                "title": "A Simple Generative Model of Logical Reasoning and Statistical Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Generative Model of Logical Reasoning and Statistical Learning"
                },
                "updated": "2026-02-23T12:33:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    33,
                    45,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2305.11098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2305.11098v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Statistical learning and logical reasoning are two major fields of AI expected to be unified for human-like machine intelligence. Most existing work considers how to combine existing logical and statistical systems. However, there is no theory of inference so far explaining how basic approaches to statistical learning and logical reasoning stem from a common principle. Inspired by the fact that much empirical work in neuroscience suggests Bayesian (or probabilistic generative) approaches to brain function including learning and reasoning, we here propose a simple Bayesian model of logical reasoning and statistical learning. The theory is statistically correct as it satisfies Kolmogorov's axioms, is consistent with both Fenstad's representation theorem and maximum likelihood estimation and performs exact Bayesian inference with a linear-time complexity. The theory is logically correct as it is a data-driven generalisation of uncertain reasoning from consistency, possibility, inconsistency and impossibility. The theory is correct in terms of machine learning as its solution to generation and prediction tasks on the MNIST dataset is not only empirically reasonable but also theoretically correct against the K nearest neighbour method. We simply model how data causes symbolic knowledge in terms of its satisfiability in formal logic. Symbolic reasoning emerges as a result of the process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation in formal logic, respectively. The inverse interpretation differentiates our work from the mainstream often referred to as inverse entailment, inverse deduction or inverse resolution. The perspective gives new insights into learning and reasoning towards human-like machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical learning and logical reasoning are two major fields of AI expected to be unified for human-like machine intelligence. Most existing work considers how to combine existing logical and statistical systems. However, there is no theory of inference so far explaining how basic approaches to statistical learning and logical reasoning stem from a common principle. Inspired by the fact that much empirical work in neuroscience suggests Bayesian (or probabilistic generative) approaches to brain function including learning and reasoning, we here propose a simple Bayesian model of logical reasoning and statistical learning. The theory is statistically correct as it satisfies Kolmogorov's axioms, is consistent with both Fenstad's representation theorem and maximum likelihood estimation and performs exact Bayesian inference with a linear-time complexity. The theory is logically correct as it is a data-driven generalisation of uncertain reasoning from consistency, possibility, inconsistency and impossibility. The theory is correct in terms of machine learning as its solution to generation and prediction tasks on the MNIST dataset is not only empirically reasonable but also theoretically correct against the K nearest neighbour method. We simply model how data causes symbolic knowledge in terms of its satisfiability in formal logic. Symbolic reasoning emerges as a result of the process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation in formal logic, respectively. The inverse interpretation differentiates our work from the mainstream often referred to as inverse entailment, inverse deduction or inverse resolution. The perspective gives new insights into learning and reasoning towards human-like machine intelligence."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-18T16:34:51Z",
                "published_parsed": [
                    2023,
                    5,
                    18,
                    16,
                    34,
                    51,
                    3,
                    138,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hiroyuki Kido"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Kido"
                },
                "author": "Hiroyuki Kido"
            },
            {
                "id": "http://arxiv.org/abs/2602.19775v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19775v1",
                "title": "Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization"
                },
                "updated": "2026-02-23T12:29:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    29,
                    43,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19775v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains."
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:29:43Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    29,
                    43,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "28 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "q-bio.QM"
                },
                "authors": [
                    {
                        "name": "Jose M. G. Vilar"
                    },
                    {
                        "name": "Leonor Saiz"
                    }
                ],
                "author_detail": {
                    "name": "Leonor Saiz"
                },
                "author": "Leonor Saiz"
            },
            {
                "id": "http://arxiv.org/abs/2507.11230v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.11230v3",
                "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages"
                },
                "updated": "2026-02-23T12:16:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    16,
                    53,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.11230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.11230v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-15T12:00:30Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    0,
                    30,
                    1,
                    196,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lyzander Marciano Andrylie"
                    },
                    {
                        "name": "Inaya Rahmanisa"
                    },
                    {
                        "name": "Mahardika Krisna Ihsani"
                    },
                    {
                        "name": "Alfan Farizki Wicaksono"
                    },
                    {
                        "name": "Haryo Akbarianto Wibowo"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji"
            },
            {
                "id": "http://arxiv.org/abs/2602.19764v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19764v1",
                "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling"
                },
                "updated": "2026-02-23T12:12:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    12,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19764v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:12:51Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    12,
                    51,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yirui Sun"
                    },
                    {
                        "name": "Guangyu Zhuge"
                    },
                    {
                        "name": "Keliang Liu"
                    },
                    {
                        "name": "Jie Gu"
                    },
                    {
                        "name": "Zhihao xia"
                    },
                    {
                        "name": "Qionglin Ren"
                    },
                    {
                        "name": "Chunxu tian"
                    },
                    {
                        "name": "Zhongxue Ga"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxue Ga"
                },
                "author": "Zhongxue Ga"
            },
            {
                "id": "http://arxiv.org/abs/2402.08646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.08646v2",
                "title": "Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data"
                },
                "updated": "2026-02-23T12:10:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    10,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.08646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.08646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-13T18:24:23Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    18,
                    24,
                    23,
                    1,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hiroyuki Kido"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Kido"
                },
                "author": "Hiroyuki Kido"
            },
            {
                "id": "http://arxiv.org/abs/2602.19755v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19755v1",
                "title": "Young system development in a cometary globule: An investigation into the eccentric disk around AT Pyx in terms of planet-formation and interaction with its surrounding environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young system development in a cometary globule: An investigation into the eccentric disk around AT Pyx in terms of planet-formation and interaction with its surrounding environment"
                },
                "updated": "2026-02-23T12:03:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    3,
                    54,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19755v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents new data and analyses of the AT Pyx system, a disk-hosting young star located in a cometary globule in the Gum Nebula. This radiation-driven structure is an unusual environment for observations of planet formation and differs greatly from the low-mass star-forming regions disks are most commonly observed in. Aided by a collection of visual and spectroscopic data available for this system, this paper aims to infer the possibility of embedded planets existing within the disk and how the system's environment may affect its disk morphology. Using data from the VLT's instruments XSHOOTER, ESPRESSO and - most prominently - SPHERE along with data from ALMA, we make a variety of measurements (geometric, photometric and otherwise) to characterise the observed disk features and attributes such as spiral arms and eccentricity. Mapping of the velocity components is also undertaken using the ALMA gas line data to characterise disk orientation and determine the likelihood that the system is experiencing a late-stage infall event. The disk is found to be eccentric when deprojected. Under the assumption that the formation of a planet is wholly responsible for the primary and secondary spiral arms, we find the mass of such a planet can range between 0.004 and 3 Jupiter masses. Measurement of the velocities associated with nearby globule cloud material returns reasonable velocities for a late-stage infall event. We estimate far-ultraviolet field strength at AT Pyx's location to be low in comparison to other surveyed disks. We also find that AT Pyx is possibly a binary system. AT Pyx is the first disk within a cometary globule to be spatially resolved, and is now the first such disk to be investigated to this extent. The work of this paper could potentially be a first step into the further study of disks in the moderate-FUV environment of the Gum Nebula and its globules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents new data and analyses of the AT Pyx system, a disk-hosting young star located in a cometary globule in the Gum Nebula. This radiation-driven structure is an unusual environment for observations of planet formation and differs greatly from the low-mass star-forming regions disks are most commonly observed in. Aided by a collection of visual and spectroscopic data available for this system, this paper aims to infer the possibility of embedded planets existing within the disk and how the system's environment may affect its disk morphology. Using data from the VLT's instruments XSHOOTER, ESPRESSO and - most prominently - SPHERE along with data from ALMA, we make a variety of measurements (geometric, photometric and otherwise) to characterise the observed disk features and attributes such as spiral arms and eccentricity. Mapping of the velocity components is also undertaken using the ALMA gas line data to characterise disk orientation and determine the likelihood that the system is experiencing a late-stage infall event. The disk is found to be eccentric when deprojected. Under the assumption that the formation of a planet is wholly responsible for the primary and secondary spiral arms, we find the mass of such a planet can range between 0.004 and 3 Jupiter masses. Measurement of the velocities associated with nearby globule cloud material returns reasonable velocities for a late-stage infall event. We estimate far-ultraviolet field strength at AT Pyx's location to be low in comparison to other surveyed disks. We also find that AT Pyx is possibly a binary system. AT Pyx is the first disk within a cometary globule to be spatially resolved, and is now the first such disk to be investigated to this extent. The work of this paper could potentially be a first step into the further study of disks in the moderate-FUV environment of the Gum Nebula and its globules."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:03:54Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    3,
                    54,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "17 pages, 21 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Dan McLachlan"
                    },
                    {
                        "name": "Christian Ginski"
                    },
                    {
                        "name": "Jake Byrne"
                    },
                    {
                        "name": "Chloe Lawlor"
                    },
                    {
                        "name": "Justyn Campbell-White"
                    },
                    {
                        "name": "Rik Claes"
                    },
                    {
                        "name": "Bin Ren"
                    },
                    {
                        "name": "Anibal Sierra"
                    }
                ],
                "author_detail": {
                    "name": "Anibal Sierra"
                },
                "author": "Anibal Sierra"
            },
            {
                "id": "http://arxiv.org/abs/2602.19753v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19753v1",
                "title": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing"
                },
                "updated": "2026-02-23T12:02:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    2,
                    3,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19753v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:02:03Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    2,
                    3,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted by CVPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kaifa Yang"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Yiling Xu"
                    },
                    {
                        "name": "Zhu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Li"
                },
                "author": "Zhu Li"
            },
            {
                "id": "http://arxiv.org/abs/2412.04272v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.04272v4",
                "title": "PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables"
                },
                "updated": "2026-02-23T11:53:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    53,
                    2,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.04272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.04272v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-05T15:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Rui Li"
                    }
                ],
                "author_detail": {
                    "name": "Rui Li"
                },
                "author": "Rui Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.01450v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01450v3",
                "title": "The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT"
                },
                "updated": "2026-02-23T11:43:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    43,
                    23,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01450v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01450v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.\n  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.\n  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-01T21:39:36Z",
                "published_parsed": [
                    2026,
                    2,
                    1,
                    21,
                    39,
                    36,
                    6,
                    32,
                    0
                ],
                "arxiv_comment": "This paper has been accepted at The ACM Web Conference 2026",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Abhisek Dash"
                    },
                    {
                        "name": "Soumi Das"
                    },
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Qinyuan Wu"
                    },
                    {
                        "name": "Sai Keerthana Karnam"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Thorsten Holz"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    },
                    {
                        "name": "Savvas Zannettou"
                    }
                ],
                "author_detail": {
                    "name": "Savvas Zannettou"
                },
                "author": "Savvas Zannettou"
            },
            {
                "id": "http://arxiv.org/abs/2602.19743v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19743v1",
                "title": "NILE: Formalizing Natural-Language Descriptions of Formal Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NILE: Formalizing Natural-Language Descriptions of Formal Languages"
                },
                "updated": "2026-02-23T11:42:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    56,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19743v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations."
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T11:42:56Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    56,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL"
                },
                "authors": [
                    {
                        "name": "Tristan Kneisel"
                    },
                    {
                        "name": "Marko Schmellenkamp"
                    },
                    {
                        "name": "Fabian Vehlken"
                    },
                    {
                        "name": "Thomas Zeume"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Zeume"
                },
                "author": "Thomas Zeume"
            },
            {
                "id": "http://arxiv.org/abs/2510.13614v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13614v3",
                "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning"
                },
                "updated": "2026-02-23T11:42:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13614v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T14:43:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "arxiv_comment": "Accepted by The Web Conference 2026 (WWW, 2026)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.20161v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20161v1",
                "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device"
                },
                "updated": "2026-02-23T18:59:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    58,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20161v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:58Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    58,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Project page: https://amshaker.github.io/Mobile-O/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Jaseel Muhammad"
                    },
                    {
                        "name": "Ritesh Thawkar"
                    },
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Senmao Li"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan"
            },
            {
                "id": "http://arxiv.org/abs/2602.20156v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20156v1",
                "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
                },
                "updated": "2026-02-23T18:59:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20156v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko"
            },
            {
                "id": "http://arxiv.org/abs/2602.20144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20144v1",
                "title": "Agentic AI for Scalable and Robust Optical Systems Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Scalable and Robust Optical Systems Control"
                },
                "updated": "2026-02-23T18:54:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:54:32Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yue-Kai Huang"
                    },
                    {
                        "name": "Philip Ji"
                    },
                    {
                        "name": "Denton Wu"
                    },
                    {
                        "name": "Mahdi Safari"
                    },
                    {
                        "name": "Flemming Holtorf"
                    },
                    {
                        "name": "Kenaish AlQubaisi"
                    },
                    {
                        "name": "Norbert M. Linke"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Dirk Englund"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.03817v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03817v3",
                "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models"
                },
                "updated": "2026-02-23T18:54:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    54,
                    13,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03817v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-04T14:14:20Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    14,
                    14,
                    20,
                    5,
                    277,
                    0
                ],
                "arxiv_comment": "Published as a conference paper at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Philipp Becker"
                    },
                    {
                        "name": "Niklas Freymuth"
                    },
                    {
                        "name": "Serge Thilges"
                    },
                    {
                        "name": "Fabian Otto"
                    },
                    {
                        "name": "Gerhard Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Neumann"
                },
                "author": "Gerhard Neumann"
            },
            {
                "id": "http://arxiv.org/abs/2602.20137v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20137v1",
                "title": "Do Large Language Models Understand Data Visualization Rules?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Data Visualization Rules?"
                },
                "updated": "2026-02-23T18:47:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    47,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20137v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:47:51Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    47,
                    51,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Martin Sinnona"
                    },
                    {
                        "name": "Valentin Bonas"
                    },
                    {
                        "name": "Emmanuel Iarussi"
                    },
                    {
                        "name": "Viviana Siless"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Siless"
                },
                "author": "Viviana Siless"
            },
            {
                "id": "http://arxiv.org/abs/2505.16547v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.16547v3",
                "title": "Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation"
                },
                "updated": "2026-02-23T18:46:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    55,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.16547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.16547v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T11:37:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    37,
                    39,
                    3,
                    142,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Nitesh Subedi"
                    },
                    {
                        "name": "Hsin-Jung Yang"
                    },
                    {
                        "name": "Devesh K. Jha"
                    },
                    {
                        "name": "Soumik Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Soumik Sarkar"
                },
                "author": "Soumik Sarkar"
            },
            {
                "id": "http://arxiv.org/abs/2602.20135v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20135v1",
                "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration"
                },
                "updated": "2026-02-23T18:46:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20135v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:46:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    46,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Amanlou"
                    },
                    {
                        "name": "Erfan Shafiee Moghaddam"
                    },
                    {
                        "name": "Yasaman Amou Jafari"
                    },
                    {
                        "name": "Mahdi Noori"
                    },
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Behnam Bahrak"
                    }
                ],
                "author_detail": {
                    "name": "Behnam Bahrak"
                },
                "author": "Behnam Bahrak"
            },
            {
                "id": "http://arxiv.org/abs/2602.20133v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20133v1",
                "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization"
                },
                "updated": "2026-02-23T18:45:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    45,
                    31,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20133v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:45:31Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    45,
                    31,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Shubham Agrawal"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Ashwin Naren"
                    },
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Alex Dimakis"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2602.20132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20132v1",
                "title": "LAD: Learning Advantage Distribution for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAD: Learning Advantage Distribution for Reasoning"
                },
                "updated": "2026-02-23T18:44:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    44,
                    10,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:44:10Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    44,
                    10,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wendi Li"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.20130v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20130v1",
                "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering"
                },
                "updated": "2026-02-23T18:42:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    42,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20130v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:42:50Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    42,
                    50,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yiran Song"
                    },
                    {
                        "name": "Xiaoyi Chen"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Yang Ruan"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20122v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20122v1",
                "title": "NanoKnow: How to Know What Your Language Model Knows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoKnow: How to Know What Your Language Model Knows"
                },
                "updated": "2026-02-23T18:37:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    37,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20122v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:37:49Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    37,
                    49,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lingwei Gu"
                    },
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin"
            },
            {
                "id": "http://arxiv.org/abs/2508.02943v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02943v2",
                "title": "Reliable Non-Leveled Homomorphic Encryption for Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Non-Leveled Homomorphic Encryption for Web Services"
                },
                "updated": "2026-02-23T18:35:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    35,
                    44,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02943v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the ubiquitous deployment of web services, ensuring data confidentiality has become a challenging imperative. Fully Homomorphic Encryption (FHE) presents a powerful solution for processing encrypted data; however, its widespread adoption is severely constrained by two fundamental bottlenecks: substantial computational overhead and the absence of a built-in automatic error correction mechanism. These limitations render the deployment of FHE in real-world, complex network environments impractical.\n  To address this dual challenge, this work puts forward a new FHE framework that enhances computational efficiency and integrates an automatic error correction capability through new encoding techniques and an algebraic reliability layer. Our system was validated across several web workloads, including encrypted inference on MNIST and CIFAR-10, federated aggregation with non-IID data, and streaming analytics on household power consumption data. Experimental results demonstrate significant performance improvements, particularly with large polynomial modulus degrees such as $N=8192$, while maintaining task accuracy within $0.5\\%$ of the plaintext baseline. Furthermore, our error correction mechanism reduces the service failure rate to below $0.5\\%$ even under harsh, bursty network fault conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ubiquitous deployment of web services, ensuring data confidentiality has become a challenging imperative. Fully Homomorphic Encryption (FHE) presents a powerful solution for processing encrypted data; however, its widespread adoption is severely constrained by two fundamental bottlenecks: substantial computational overhead and the absence of a built-in automatic error correction mechanism. These limitations render the deployment of FHE in real-world, complex network environments impractical.\n  To address this dual challenge, this work puts forward a new FHE framework that enhances computational efficiency and integrates an automatic error correction capability through new encoding techniques and an algebraic reliability layer. Our system was validated across several web workloads, including encrypted inference on MNIST and CIFAR-10, federated aggregation with non-IID data, and streaming analytics on household power consumption data. Experimental results demonstrate significant performance improvements, particularly with large polynomial modulus degrees such as $N=8192$, while maintaining task accuracy within $0.5\\%$ of the plaintext baseline. Furthermore, our error correction mechanism reduces the service failure rate to below $0.5\\%$ even under harsh, bursty network fault conditions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T22:53:36Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    22,
                    53,
                    36,
                    0,
                    216,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Baigang Chen"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2602.20120v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20120v1",
                "title": "Enhancing Capstone Program Workflow: A Case Study on a Platform for Managing Academic-Industry Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Capstone Program Workflow: A Case Study on a Platform for Managing Academic-Industry Projects"
                },
                "updated": "2026-02-23T18:35:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    35,
                    20,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20120v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.5281/zenodo.15850131",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Capstone projects are widely adopted by universities around the world as a culminating assessment in bachelor's degree programs. These projects typically involve student teams tackling complex, real-world problems proposed by external stakeholders, such as companies, NGOs, or research centers. Although they offer valuable hands-on experience, managing Capstone projects can be challenging due to their multiple stages and demands. The process typically begins by identifying students' interests, followed by sourcing and selecting potential projects from external organizations. After presenting these options to students, groups must be formed based on various criteria, including academic ranking, GPA, previous experience, and individual skill sets.\n  In this paper, we detail a web-based tool designed to streamline the management of Capstone projects at Insper, with an emphasis on project sourcing and group formation. We also discuss the technological solutions and the challenges encountered throughout development and deployment. Furthermore, we present usage data from recent years, offering insights that may prove valuable for institutions or teams developing similar tools in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capstone projects are widely adopted by universities around the world as a culminating assessment in bachelor's degree programs. These projects typically involve student teams tackling complex, real-world problems proposed by external stakeholders, such as companies, NGOs, or research centers. Although they offer valuable hands-on experience, managing Capstone projects can be challenging due to their multiple stages and demands. The process typically begins by identifying students' interests, followed by sourcing and selecting potential projects from external organizations. After presenting these options to students, groups must be formed based on various criteria, including academic ranking, GPA, previous experience, and individual skill sets.\n  In this paper, we detail a web-based tool designed to streamline the management of Capstone projects at Insper, with an emphasis on project sourcing and group formation. We also discuss the technological solutions and the challenges encountered throughout development and deployment. Furthermore, we present usage data from recent years, offering insights that may prove valuable for institutions or teams developing similar tools in the future."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:35:20Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    35,
                    20,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "arxiv_journal_ref": "2025, PAEE-ALE, Porto",
                "authors": [
                    {
                        "name": "Rafael Corsi Ferrao"
                    },
                    {
                        "name": "Luciano Pereira Soares"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Pereira Soares"
                },
                "author": "Luciano Pereira Soares",
                "arxiv_doi": "10.5281/zenodo.15850131"
            },
            {
                "id": "http://arxiv.org/abs/2602.20114v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20114v1",
                "title": "Benchmarking Unlearning for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Unlearning for Vision Transformers"
                },
                "updated": "2026-02-23T18:33:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    33,
                    16,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20114v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:33:16Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    33,
                    16,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kairan Zhao"
                    },
                    {
                        "name": "Iurie Luca"
                    },
                    {
                        "name": "Peter Triantafillou"
                    }
                ],
                "author_detail": {
                    "name": "Peter Triantafillou"
                },
                "author": "Peter Triantafillou"
            },
            {
                "id": "http://arxiv.org/abs/2506.07751v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.07751v4",
                "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking"
                },
                "updated": "2026-02-23T18:25:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    25,
                    13,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.07751v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.07751v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T13:34:50Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    13,
                    34,
                    50,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Abbe"
                },
                "author": "Emmanuel Abbe"
            },
            {
                "id": "http://arxiv.org/abs/2602.05165v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.05165v3",
                "title": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization"
                },
                "updated": "2026-02-23T18:23:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    23,
                    57,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.05165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.05165v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-05T00:33:02Z",
                "published_parsed": [
                    2026,
                    2,
                    5,
                    0,
                    33,
                    2,
                    3,
                    36,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kevin Han"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Gedi Zhou"
                    },
                    {
                        "name": "Serena Li"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Lizhu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lizhu Zhang"
                },
                "author": "Lizhu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20102v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20102v1",
                "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BarrierSteer: LLM Safety via Learning Barrier Steering"
                },
                "updated": "2026-02-23T18:19:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    19,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20102v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:19:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    19,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "This paper introduces SafeBarrier, a framework that enforces safety in large language models by steering their latent representations with control barrier functions during inference, reducing adversarial and unsafe outputs",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thanh Q. Tran"
                    },
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Kiwan Wong"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Wei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xiao"
                },
                "author": "Wei Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2601.05500v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05500v3",
                "title": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm"
                },
                "updated": "2026-02-23T18:16:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    16,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05500v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is not just limited to human preferences, but is also consequential even in safety critical domains such as medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how - high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability. Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80\\%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is not just limited to human preferences, but is also consequential even in safety critical domains such as medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how - high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability. Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80\\%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-09T03:19:37Z",
                "published_parsed": [
                    2026,
                    1,
                    9,
                    3,
                    19,
                    37,
                    4,
                    9,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Aparna Elangovan"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Mahsa Elyasi"
                    },
                    {
                        "name": "Ismail Akdulum"
                    },
                    {
                        "name": "Mehmet Aksakal"
                    },
                    {
                        "name": "Enes Gurun"
                    },
                    {
                        "name": "Brian Hur"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Ravid Shwartz Ziv"
                    },
                    {
                        "name": "Karin Verspoor"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth"
            },
            {
                "id": "http://arxiv.org/abs/2601.17258v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.17258v2",
                "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding"
                },
                "updated": "2026-02-23T18:12:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    12,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.17258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.17258v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-24T02:17:07Z",
                "published_parsed": [
                    2026,
                    1,
                    24,
                    2,
                    17,
                    7,
                    5,
                    24,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jo칚o Pereira"
                    },
                    {
                        "name": "Vasco Lopes"
                    },
                    {
                        "name": "Jo칚o Neves"
                    },
                    {
                        "name": "David Semedo"
                    }
                ],
                "author_detail": {
                    "name": "David Semedo"
                },
                "author": "David Semedo"
            },
            {
                "id": "http://arxiv.org/abs/2507.17842v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.17842v2",
                "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning"
                },
                "updated": "2026-02-23T18:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.17842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.17842v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-23T18:10:43Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    18,
                    10,
                    43,
                    2,
                    204,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026. The project page is available at https://damon-demon.github.io/shop-r1.html",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Sinong Zhan"
                    },
                    {
                        "name": "Vianne Gao"
                    },
                    {
                        "name": "Ruochen Jiao"
                    },
                    {
                        "name": "Junze Liu"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yuxin Tang"
                    },
                    {
                        "name": "Ran Xue"
                    },
                    {
                        "name": "Houyu Zhang"
                    },
                    {
                        "name": "Qingjun Cui"
                    },
                    {
                        "name": "Yufan Guo"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20094v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20094v1",
                "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching"
                },
                "updated": "2026-02-23T18:06:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    6,
                    15,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20094v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:06:15Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    6,
                    15,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "8 pages plus references, 3 figures, 3 tables. Under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuzhe Wang"
                    },
                    {
                        "name": "Yaochen Zhu"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.13632v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13632v2",
                "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Gap Between Text and Speech Understanding in LLMs"
                },
                "updated": "2026-02-23T18:05:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    5,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13632v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Santiago Cuervo"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maureen de Seyssel"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    }
                ],
                "author_detail": {
                    "name": "Zakaria Aldeneh"
                },
                "author": "Zakaria Aldeneh"
            },
            {
                "id": "http://arxiv.org/abs/2602.20091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20091v1",
                "title": "How Retrieved Context Shapes Internal Representations in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Retrieved Context Shapes Internal Representations in RAG"
                },
                "updated": "2026-02-23T18:02:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    2,
                    4,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:02:04Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    2,
                    4,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.20084v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20084v1",
                "title": "Do Large Language Models Understand Data Visualization Principles?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Understand Data Visualization Principles?"
                },
                "updated": "2026-02-23T17:51:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    51,
                    6,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20084v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:51:06Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    51,
                    6,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Martin Sinnona"
                    },
                    {
                        "name": "Valentin Bonas"
                    },
                    {
                        "name": "Viviana Siless"
                    },
                    {
                        "name": "Emmanuel Iarussi"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Iarussi"
                },
                "author": "Emmanuel Iarussi"
            },
            {
                "id": "http://arxiv.org/abs/2602.20082v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20082v1",
                "title": "Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)"
                },
                "updated": "2026-02-23T17:48:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    48,
                    24,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20082v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:48:24Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    48,
                    24,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Zoe Paraskevopoulou"
                    }
                ],
                "author_detail": {
                    "name": "Zoe Paraskevopoulou"
                },
                "author": "Zoe Paraskevopoulou"
            },
            {
                "id": "http://arxiv.org/abs/2602.03596v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.03596v2",
                "title": "SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network"
                },
                "updated": "2026-02-23T17:35:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    35,
                    26,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.03596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.03596v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks \\textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks \\textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T14:50:19Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    14,
                    50,
                    19,
                    1,
                    34,
                    0
                ],
                "arxiv_comment": "ITASEC-2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Cristian Manca"
                    },
                    {
                        "name": "Christian Scano"
                    },
                    {
                        "name": "Giorgio Piras"
                    },
                    {
                        "name": "Fabio Brau"
                    },
                    {
                        "name": "Maura Pintor"
                    },
                    {
                        "name": "Battista Biggio"
                    }
                ],
                "author_detail": {
                    "name": "Battista Biggio"
                },
                "author": "Battista Biggio"
            },
            {
                "id": "http://arxiv.org/abs/2602.20065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20065v1",
                "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees"
                },
                "updated": "2026-02-23T17:22:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:22:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "36 pages, 3 figures, 2 tables, 4 supplementary tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Natalia Moskvina"
                    },
                    {
                        "name": "Raquel Montero"
                    },
                    {
                        "name": "Masaya Yoshida"
                    },
                    {
                        "name": "Ferdy Hubers"
                    },
                    {
                        "name": "Paolo Morosi"
                    },
                    {
                        "name": "Walid Irhaymi"
                    },
                    {
                        "name": "Jin Yan"
                    },
                    {
                        "name": "Tamara Serrano"
                    },
                    {
                        "name": "Elena Pagliarini"
                    },
                    {
                        "name": "Fritz G칲nther"
                    },
                    {
                        "name": "Evelina Leivada"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Leivada"
                },
                "author": "Evelina Leivada"
            },
            {
                "id": "http://arxiv.org/abs/2602.20064v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20064v1",
                "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow"
                },
                "updated": "2026-02-23T17:22:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    35,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20064v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:22:35Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    22,
                    35,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Zac Garby"
                    },
                    {
                        "name": "Andrew D. Gordon"
                    },
                    {
                        "name": "David Sands"
                    }
                ],
                "author_detail": {
                    "name": "David Sands"
                },
                "author": "David Sands"
            },
            {
                "id": "http://arxiv.org/abs/2602.20059v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20059v1",
                "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction Theater: A case of LLM Agents Interacting at Scale"
                },
                "updated": "2026-02-23T17:14:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    14,
                    29,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20059v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:14:29Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    14,
                    29,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sarath Shekkizhar"
                    },
                    {
                        "name": "Adam Earle"
                    }
                ],
                "author_detail": {
                    "name": "Adam Earle"
                },
                "author": "Adam Earle"
            },
            {
                "id": "http://arxiv.org/abs/2602.20055v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20055v1",
                "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation"
                },
                "updated": "2026-02-23T17:10:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    10,
                    0,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20055v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:10:00Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    10,
                    0,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Apoorva Vashisth"
                    },
                    {
                        "name": "Manav Kulshrestha"
                    },
                    {
                        "name": "Pranav Bakshi"
                    },
                    {
                        "name": "Damon Conover"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "arxiv_affiliation": "Purdue University",
                "author": "Aniket Bera"
            },
            {
                "id": "http://arxiv.org/abs/2506.08119v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08119v2",
                "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents"
                },
                "updated": "2026-02-23T17:05:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    5,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08119v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents struggle to execute complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Existing benchmarks fail to capture the procedural complexity and tool orchestration demands of real-world workflows. We introduce SOP-Bench, a benchmark of 2,000+ tasks from human expert-authored SOPs across 12 business domains (healthcare, logistics, finance, content moderation, etc.). Using a human-AI collaborative framework, experts crafted authentic SOPs while AI generated artifacts (tools, APIs, datasets), all human-validated, yielding realistic tasks with executable interfaces and ground-truth outputs.\n  SOP-Bench serves as a research enabler for systematically investigating agent architectures, model capabilities, and deployment considerations across diverse procedural tasks. We demonstrate its utility through illustrative experiments with a subset of frontier models across Function-Calling (FC) and ReAct agents, revealing critical insights. For example, (1) newer models do not guarantee better performance - Claude 4 family outperforms Claude 4.5 family on ReAct tasks (Claude 4 Opus: 72.4% vs. Claude 4.5 Sonnet: 63.3% task success rate), demonstrating that production upgrades require validation; (2) no single model-agent combination dominates: best performances range from 57% to 100% depending on domain. These examples illustrate how SOP-Bench enables isolating and studying specific dimensions of agent performance without costly production experiments. Our goal is not to rank model capabilities or build optimal agents, but to provide a rigorous evaluation framework that enables the researchers and practitioners to systematically investigate agent design choices, model selection, and deployment strategies. We release the benchmark at https://github.com/amazon-science/sop-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents struggle to execute complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Existing benchmarks fail to capture the procedural complexity and tool orchestration demands of real-world workflows. We introduce SOP-Bench, a benchmark of 2,000+ tasks from human expert-authored SOPs across 12 business domains (healthcare, logistics, finance, content moderation, etc.). Using a human-AI collaborative framework, experts crafted authentic SOPs while AI generated artifacts (tools, APIs, datasets), all human-validated, yielding realistic tasks with executable interfaces and ground-truth outputs.\n  SOP-Bench serves as a research enabler for systematically investigating agent architectures, model capabilities, and deployment considerations across diverse procedural tasks. We demonstrate its utility through illustrative experiments with a subset of frontier models across Function-Calling (FC) and ReAct agents, revealing critical insights. For example, (1) newer models do not guarantee better performance - Claude 4 family outperforms Claude 4.5 family on ReAct tasks (Claude 4 Opus: 72.4% vs. Claude 4.5 Sonnet: 63.3% task success rate), demonstrating that production upgrades require validation; (2) no single model-agent combination dominates: best performances range from 57% to 100% depending on domain. These examples illustrate how SOP-Bench enables isolating and studying specific dimensions of agent performance without costly production experiments. Our goal is not to rank model capabilities or build optimal agents, but to provide a rigorous evaluation framework that enables the researchers and practitioners to systematically investigate agent design choices, model selection, and deployment strategies. We release the benchmark at https://github.com/amazon-science/sop-bench."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T18:20:12Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    18,
                    20,
                    12,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Subhrangshu Nandi"
                    },
                    {
                        "name": "Arghya Datta"
                    },
                    {
                        "name": "Rohith Nama"
                    },
                    {
                        "name": "Udita Patel"
                    },
                    {
                        "name": "Nikhil Vichare"
                    },
                    {
                        "name": "Indranil Bhattacharya"
                    },
                    {
                        "name": "Prince Grover"
                    },
                    {
                        "name": "Shivam Asija"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Arushi Gupta"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Huzefa Raja"
                    },
                    {
                        "name": "Shayan Ray"
                    },
                    {
                        "name": "Aaron Chan"
                    },
                    {
                        "name": "Esther Xu Fei"
                    },
                    {
                        "name": "Gaoyuan Du"
                    },
                    {
                        "name": "Zuhaib Akhtar"
                    },
                    {
                        "name": "Harshita Asnani"
                    },
                    {
                        "name": "Weian Chan"
                    },
                    {
                        "name": "Ming Xiong"
                    },
                    {
                        "name": "Francesco Carbone"
                    },
                    {
                        "name": "Jeetu Mirchandani"
                    }
                ],
                "author_detail": {
                    "name": "Jeetu Mirchandani"
                },
                "author": "Jeetu Mirchandani"
            },
            {
                "id": "http://arxiv.org/abs/2602.20052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20052v1",
                "title": "Entropy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy in Large Language Models"
                },
                "updated": "2026-02-23T17:02:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    2,
                    45,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T17:02:45Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    17,
                    2,
                    45,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "7 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Marco Scharringhausen"
                    }
                ],
                "author_detail": {
                    "name": "Marco Scharringhausen"
                },
                "author": "Marco Scharringhausen"
            },
            {
                "id": "http://arxiv.org/abs/2509.06326v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06326v2",
                "title": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs"
                },
                "updated": "2026-02-23T16:56:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    56,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06326v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T04:17:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    4,
                    17,
                    2,
                    0,
                    251,
                    0
                ],
                "arxiv_comment": "accept to DAC 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ruisi Zhang"
                    },
                    {
                        "name": "Yifei Zhao"
                    },
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar"
            },
            {
                "id": "http://arxiv.org/abs/2602.20040v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20040v1",
                "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization"
                },
                "updated": "2026-02-23T16:49:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    49,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20040v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:49:37Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    49,
                    37,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Fahmida Liza Piya"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti"
            },
            {
                "id": "http://arxiv.org/abs/2602.11962v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11962v2",
                "title": "Wisdom of the LLM Crowd: A Large Scale Benchmark of Multi-Label U.S. Election-Related Harmful Social Media Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom of the LLM Crowd: A Large Scale Benchmark of Multi-Label U.S. Election-Related Harmful Social Media Content"
                },
                "updated": "2026-02-23T16:43:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    43,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11962v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The spread of election misinformation and harmful political content conveys misleading narratives and poses a serious threat to democratic integrity. Detecting harmful content at early stages is essential for understanding and potentially mitigating its downstream spread. In this study, we introduce USE24-XD, a large-scale dataset of nearly 100k posts collected from X (formerly Twitter) during the 2024 U.S. presidential election cycle, enriched with spatio-temporal metadata. To substantially reduce the cost of manual annotation while enabling scalable categorization, we employ six large language models (LLMs) to systematically annotate posts across five nuanced categories: Conspiracy, Sensationalism, Hate Speech, Speculation, and Satire. We validate LLM annotations with crowdsourcing (n = 34) and benchmark them against human annotators. Inter-rater reliability analyses show comparable agreement patterns between LLMs and humans, with LLMs exhibiting higher internal consistency and achieving up to 0.90 recall on Speculation. We apply a wisdom-of-the-crowd approach across LLMs to aggregate annotations and curate a robust multi-label dataset. 60% of posts receive at least one label. We further analyze how human annotator demographics, including political ideology and affiliation, shape labeling behavior, highlighting systematic sources of subjectivity in judgments of harmful content. The USE24-XD dataset is publicly released to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of election misinformation and harmful political content conveys misleading narratives and poses a serious threat to democratic integrity. Detecting harmful content at early stages is essential for understanding and potentially mitigating its downstream spread. In this study, we introduce USE24-XD, a large-scale dataset of nearly 100k posts collected from X (formerly Twitter) during the 2024 U.S. presidential election cycle, enriched with spatio-temporal metadata. To substantially reduce the cost of manual annotation while enabling scalable categorization, we employ six large language models (LLMs) to systematically annotate posts across five nuanced categories: Conspiracy, Sensationalism, Hate Speech, Speculation, and Satire. We validate LLM annotations with crowdsourcing (n = 34) and benchmark them against human annotators. Inter-rater reliability analyses show comparable agreement patterns between LLMs and humans, with LLMs exhibiting higher internal consistency and achieving up to 0.90 recall on Speculation. We apply a wisdom-of-the-crowd approach across LLMs to aggregate annotations and curate a robust multi-label dataset. 60% of posts receive at least one label. We further analyze how human annotator demographics, including political ideology and affiliation, shape labeling behavior, highlighting systematic sources of subjectivity in judgments of harmful content. The USE24-XD dataset is publicly released to support future research."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T13:57:12Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    57,
                    12,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Qile Wang"
                    },
                    {
                        "name": "Prerana Khatiwada"
                    },
                    {
                        "name": "Carolina Coimbra Vieira"
                    },
                    {
                        "name": "Benjamin E. Bagozzi"
                    },
                    {
                        "name": "Kenneth E. Barner"
                    },
                    {
                        "name": "Matthew Louis Mauriello"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Louis Mauriello"
                },
                "author": "Matthew Louis Mauriello"
            },
            {
                "id": "http://arxiv.org/abs/2506.16824v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.16824v2",
                "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs"
                },
                "updated": "2026-02-23T16:38:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    38,
                    34,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.16824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.16824v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-20T08:26:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thomas Marwitz"
                    },
                    {
                        "name": "Alexander Colsmann"
                    },
                    {
                        "name": "Ben Breitung"
                    },
                    {
                        "name": "Christoph Brabec"
                    },
                    {
                        "name": "Christoph Kirchlechner"
                    },
                    {
                        "name": "Eva Blasco"
                    },
                    {
                        "name": "Gabriel Cadilha Marques"
                    },
                    {
                        "name": "Horst Hahn"
                    },
                    {
                        "name": "Michael Hirtz"
                    },
                    {
                        "name": "Pavel A. Levkin"
                    },
                    {
                        "name": "Yolita M. Eggeler"
                    },
                    {
                        "name": "Tobias Schl칬der"
                    },
                    {
                        "name": "Pascal Friederich"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Friederich"
                },
                "author": "Pascal Friederich"
            },
            {
                "id": "http://arxiv.org/abs/2602.20021v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20021v1",
                "title": "Agents of Chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents of Chaos"
                },
                "updated": "2026-02-23T16:28:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    28,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20021v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:28:48Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    28,
                    48,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Natalie Shapira"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Avery Yen"
                    },
                    {
                        "name": "Gabriele Sarti"
                    },
                    {
                        "name": "Koyena Pal"
                    },
                    {
                        "name": "Olivia Floody"
                    },
                    {
                        "name": "Adam Belfki"
                    },
                    {
                        "name": "Alex Loftus"
                    },
                    {
                        "name": "Aditya Ratan Jannali"
                    },
                    {
                        "name": "Nikhil Prakash"
                    },
                    {
                        "name": "Jasmine Cui"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Can Rager"
                    },
                    {
                        "name": "Amir Zur"
                    },
                    {
                        "name": "Michael Ripa"
                    },
                    {
                        "name": "Aruna Sankaranarayanan"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Rohit Gandikota"
                    },
                    {
                        "name": "Jaden Fiotto-Kaufman"
                    },
                    {
                        "name": "EunJeong Hwang"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "P Sam Sahil"
                    },
                    {
                        "name": "Negev Taglicht"
                    },
                    {
                        "name": "Tomer Shabtay"
                    },
                    {
                        "name": "Atai Ambus"
                    },
                    {
                        "name": "Nitay Alon"
                    },
                    {
                        "name": "Shiri Oron"
                    },
                    {
                        "name": "Ayelet Gordon-Tapiero"
                    },
                    {
                        "name": "Yotam Kaplan"
                    },
                    {
                        "name": "Vered Shwartz"
                    },
                    {
                        "name": "Tamar Rott Shaham"
                    },
                    {
                        "name": "Christoph Riedl"
                    },
                    {
                        "name": "Reuth Mirsky"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "David Manheim"
                    },
                    {
                        "name": "Tomer Ullman"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau"
            },
            {
                "id": "http://arxiv.org/abs/2602.20008v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20008v1",
                "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation"
                },
                "updated": "2026-02-23T16:15:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    15,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20008v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T16:15:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    16,
                    15,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Louis Fabrice Tshimanga"
                    },
                    {
                        "name": "Andrea Zanola"
                    },
                    {
                        "name": "Federico Del Pup"
                    },
                    {
                        "name": "Manfredo Atzori"
                    }
                ],
                "author_detail": {
                    "name": "Manfredo Atzori"
                },
                "author": "Manfredo Atzori"
            },
            {
                "id": "http://arxiv.org/abs/2408.07543v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.07543v6",
                "title": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts"
                },
                "updated": "2026-02-23T15:56:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    56,
                    57,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.07543v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.07543v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-14T13:23:43Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    23,
                    43,
                    2,
                    227,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Minxuan Zhou"
                    },
                    {
                        "name": "Zirong Chen"
                    },
                    {
                        "name": "Meiyi Qiang"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.01428v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01428v2",
                "title": "Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models"
                },
                "updated": "2026-02-23T15:55:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    55,
                    7,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01428v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-01T20:30:59Z",
                "published_parsed": [
                    2026,
                    2,
                    1,
                    20,
                    30,
                    59,
                    6,
                    32,
                    0
                ],
                "arxiv_comment": "Accepted at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Weiqing He"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Qi Long"
                    }
                ],
                "author_detail": {
                    "name": "Qi Long"
                },
                "author": "Qi Long"
            },
            {
                "id": "http://arxiv.org/abs/2602.19983v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19983v1",
                "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Safety Reasoning and Grounding for Open-World Robots"
                },
                "updated": "2026-02-23T15:51:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    51,
                    23,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19983v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:51:23Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    51,
                    23,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zachary Ravichadran"
                    },
                    {
                        "name": "David Snyder"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "George J. Pappas"
                    }
                ],
                "author_detail": {
                    "name": "George J. Pappas"
                },
                "author": "George J. Pappas"
            },
            {
                "id": "http://arxiv.org/abs/2602.19982v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19982v1",
                "title": "A Computationally Efficient Multidimensional Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computationally Efficient Multidimensional Vision Transformer"
                },
                "updated": "2026-02-23T15:49:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    49,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19982v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and structured feature representations. We develop the\n  theoretical foundations of the tensor cosine product, analyze its algebraic\n  properties, and integrate it into a new Cproduct-based Vision Transformer\n  architecture (TCP-ViT). Numerical experiments on standard classification and\n  segmentation benchmarks demonstrate that the proposed method achieves a uniform\n  1/C parameter reduction (where C is the number of channels) while\n  maintaining competitive accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and structured feature representations. We develop the\n  theoretical foundations of the tensor cosine product, analyze its algebraic\n  properties, and integrate it into a new Cproduct-based Vision Transformer\n  architecture (TCP-ViT). Numerical experiments on standard classification and\n  segmentation benchmarks demonstrate that the proposed method achieves a uniform\n  1/C parameter reduction (where C is the number of channels) while\n  maintaining competitive accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:49:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    49,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Alaa El Ichi"
                    },
                    {
                        "name": "Khalide Jbilou"
                    }
                ],
                "author_detail": {
                    "name": "Khalide Jbilou"
                },
                "author": "Khalide Jbilou"
            },
            {
                "id": "http://arxiv.org/abs/2602.19969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19969v1",
                "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting"
                },
                "updated": "2026-02-23T15:30:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    52,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:30:52Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    52,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted by EACL2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuxing Tian"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Yun Nie"
                },
                "author": "Jian-Yun Nie"
            },
            {
                "id": "http://arxiv.org/abs/2504.15077v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.15077v4",
                "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL"
                },
                "updated": "2026-02-23T15:30:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    30,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.15077v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.15077v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-21T13:05:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "arxiv_comment": "26 pages, work in progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Simone Papicchio"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Luca Cagliero"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti"
            },
            {
                "id": "http://arxiv.org/abs/2602.19964v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19964v1",
                "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference"
                },
                "updated": "2026-02-23T15:28:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    28,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19964v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:28:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    28,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "8 pages, 1 Figure",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Moritz A. Zanger"
                    },
                    {
                        "name": "Yijun Wu"
                    },
                    {
                        "name": "Pascal R. Van der Vaart"
                    },
                    {
                        "name": "Wendelin B칬hmer"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan"
            },
            {
                "id": "http://arxiv.org/abs/2602.19948v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19948v1",
                "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming"
                },
                "updated": "2026-02-23T15:17:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    17,
                    18,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19948v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:17:18Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    17,
                    18,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "This paper is a condensed version of the first author's Ph.D. dissertation submitted to Northeastern University",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ian Steenstra"
                    },
                    {
                        "name": "Paola Pedrelli"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Stacy Marsella"
                    },
                    {
                        "name": "Timothy W. Bickmore"
                    }
                ],
                "author_detail": {
                    "name": "Timothy W. Bickmore"
                },
                "author": "Timothy W. Bickmore"
            },
            {
                "id": "http://arxiv.org/abs/2602.19942v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19942v1",
                "title": "Secure Communications, Sensing, and Computing Towards Next-Generation Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Communications, Sensing, and Computing Towards Next-Generation Networks"
                },
                "updated": "2026-02-23T15:13:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    13,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19942v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Next-generation wireless networks are progressing beyond conventional connectivity to incorporate emerging sensing and computing capabilities. This convergence gives rise to integrated systems that enable not only uninterrupted communication, but also environmental awareness, intelligent decision-making, and novel applications that take advantage of these combined features. At the same time, this integration brings substantial security challenges. As computing, sensing, and communication become more tightly intertwined, the overall complexity of the system increases, creating new vulnerabilities and expanding the attack surface. The widespread deployment of data-heavy artificial intelligence applications further amplifies concerns regarding data security and privacy. This paper presents a comprehensive survey of security and privacy threats, along with potential countermeasures, in integrated wireless systems. We first review physical-layer security techniques for communication networks, and then investigate the security and privacy implications of semantic and pragmatic communications and their associated cross-layer design methodologies. For sensing functionalities, we pinpoint security and privacy risks at the levels of signal sources, propagation channels, and sensing targets, and summarize state-of-the-art defense strategies for each. The growing computational requirements of these applications drive the need for distributed computing over the network, which introduces additional risks such as data leakage, weak authentication, and multiple points of failure. We subsequently discuss secure coded computing approaches that can help overcome several of these challenges. Finally, we introduce unified security frameworks tailored to integrated communication-sensing-computing architectures, offering an end-to-end perspective on protecting future wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation wireless networks are progressing beyond conventional connectivity to incorporate emerging sensing and computing capabilities. This convergence gives rise to integrated systems that enable not only uninterrupted communication, but also environmental awareness, intelligent decision-making, and novel applications that take advantage of these combined features. At the same time, this integration brings substantial security challenges. As computing, sensing, and communication become more tightly intertwined, the overall complexity of the system increases, creating new vulnerabilities and expanding the attack surface. The widespread deployment of data-heavy artificial intelligence applications further amplifies concerns regarding data security and privacy. This paper presents a comprehensive survey of security and privacy threats, along with potential countermeasures, in integrated wireless systems. We first review physical-layer security techniques for communication networks, and then investigate the security and privacy implications of semantic and pragmatic communications and their associated cross-layer design methodologies. For sensing functionalities, we pinpoint security and privacy risks at the levels of signal sources, propagation channels, and sensing targets, and summarize state-of-the-art defense strategies for each. The growing computational requirements of these applications drive the need for distributed computing over the network, which introduces additional risks such as data leakage, weak authentication, and multiple points of failure. We subsequently discuss secure coded computing approaches that can help overcome several of these challenges. Finally, we introduce unified security frameworks tailored to integrated communication-sensing-computing architectures, offering an end-to-end perspective on protecting future wireless systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:13:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    13,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "28 pages, 7 figures, IEEE Journal on Selected Areas in Communications",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Ruiqi Liu"
                    },
                    {
                        "name": "Beixiong Zheng"
                    },
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Si-Hyeon Lee"
                    },
                    {
                        "name": "Georges Kaddoum"
                    },
                    {
                        "name": "Onur G칲nl칲"
                    },
                    {
                        "name": "Deniz G칲nd칲z"
                    }
                ],
                "author_detail": {
                    "name": "Deniz G칲nd칲z"
                },
                "author": "Deniz G칲nd칲z"
            },
            {
                "id": "http://arxiv.org/abs/2602.19938v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19938v1",
                "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs"
                },
                "updated": "2026-02-23T15:11:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    11,
                    16,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19938v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:11:16Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    11,
                    16,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Mingfu Liang"
                    },
                    {
                        "name": "Luke Simon"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.19929v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19929v1",
                "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models"
                },
                "updated": "2026-02-23T15:06:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    6,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19929v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:06:32Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    6,
                    32,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "We propose a novel end-to-end generative framework for beam prediction by using vision-language models",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Chenran Kou"
                    },
                    {
                        "name": "Changsheng You"
                    },
                    {
                        "name": "Mingjiang Wu"
                    },
                    {
                        "name": "Dingzhu Wen"
                    },
                    {
                        "name": "Zezhong Zhang"
                    },
                    {
                        "name": "Chengwen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chengwen Xing"
                },
                "author": "Chengwen Xing"
            },
            {
                "id": "http://arxiv.org/abs/2602.19926v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19926v1",
                "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models"
                },
                "updated": "2026-02-23T15:05:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    5,
                    28,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19926v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($풧= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($풧= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T15:05:28Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    15,
                    5,
                    28,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jin Liu"
                    },
                    {
                        "name": "Yinbin Miao"
                    },
                    {
                        "name": "Ning Xi"
                    },
                    {
                        "name": "Junkang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junkang Liu"
                },
                "author": "Junkang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19919v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19919v1",
                "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling"
                },
                "updated": "2026-02-23T14:58:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19919v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:58:51Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    51,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zikai Wei"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Wanyun Zhou"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Penglei Sun"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19918v1",
                "title": "RobPI: Robust Private Inference against Malicious Client",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobPI: Robust Private Inference against Malicious Client"
                },
                "updated": "2026-02-23T14:58:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    8,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:58:08Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    58,
                    8,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted by SaTML 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiaqi Xue"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou"
            },
            {
                "id": "http://arxiv.org/abs/2509.12441v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.12441v3",
                "title": "Automatic Network Planning with Digital Radio Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Network Planning with Digital Radio Twin"
                },
                "updated": "2026-02-23T14:57:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    57,
                    29,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.12441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.12441v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Network planning seeks to determine base station parameters that maximize coverage and capacity in cellular networks. However, achieving optimal planning remains challenging due to the diversity of deployment scenarios and the significant simulation-to-reality discrepancy. In this paper, we propose \\emph{AutoPlan}, a new automatic network planning framework by leveraging digital radio twin (DRT) techniques. We derive the DRT by finetuning the parameters of building materials to reduce the sim-to-real discrepancy based on crowdsource real-world user data. Leveraging the DRT, we design a Bayesian optimization based algorithm to optimize the deployment parameters of base stations efficiently. Using the field measurement from Husker-Net, we extensively evaluate \\emph{AutoPlan} under various deployment scenarios, in terms of both coverage and capacity. The evaluation results show that \\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance comparable to exhaustive search, while requiring less than 2\\% of its computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network planning seeks to determine base station parameters that maximize coverage and capacity in cellular networks. However, achieving optimal planning remains challenging due to the diversity of deployment scenarios and the significant simulation-to-reality discrepancy. In this paper, we propose \\emph{AutoPlan}, a new automatic network planning framework by leveraging digital radio twin (DRT) techniques. We derive the DRT by finetuning the parameters of building materials to reduce the sim-to-real discrepancy based on crowdsource real-world user data. Leveraging the DRT, we design a Bayesian optimization based algorithm to optimize the deployment parameters of base stations efficiently. Using the field measurement from Husker-Net, we extensively evaluate \\emph{AutoPlan} under various deployment scenarios, in terms of both coverage and capacity. The evaluation results show that \\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance comparable to exhaustive search, while requiring less than 2\\% of its computation time."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T20:48:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    20,
                    48,
                    50,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "This paper is accepted by IEEE ICC 2026",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Xiaomeng Li"
                    },
                    {
                        "name": "Yuru Zhang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Mehmet Can Vuran"
                    },
                    {
                        "name": "Nathan Huynh"
                    },
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Mizan Rahman"
                    },
                    {
                        "name": "Eren Erman Ozguven"
                    }
                ],
                "author_detail": {
                    "name": "Eren Erman Ozguven"
                },
                "author": "Eren Erman Ozguven"
            },
            {
                "id": "http://arxiv.org/abs/2602.19914v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19914v1",
                "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning"
                },
                "updated": "2026-02-23T14:54:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    54,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19914v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:54:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    54,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "51 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Thatchawin Leelawat"
                    },
                    {
                        "name": "Lewis D Griffin"
                    }
                ],
                "author_detail": {
                    "name": "Lewis D Griffin"
                },
                "author": "Lewis D Griffin"
            },
            {
                "id": "http://arxiv.org/abs/2508.00017v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00017v3",
                "title": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation"
                },
                "updated": "2026-02-23T14:37:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    20,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00017v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Generative Logic (GL), a deterministic architecture that starts from user-supplied axiomatic definitions, written in a minimalist Mathematical Programming Language (MPL), and systematically explores a configurable region of their deductive neighborhood. A defining feature of the architecture is its unified hash-based inference engine, which executes both algebraic manipulations and deterministic logical transformations. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; whenever the premises of an inference rule unify, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs. Experimental validation is performed on Elementary Number Theory (ENT) utilizing a batched execution strategy. Starting from foundational axioms and definitions, the system first develops first-order Peano arithmetic, which is subsequently applied to autonomously derive and prove Gauss's summation formula as a main result. To manage combinatorial explosion, GL algorithmically enumerates conjectures and applies normalization, type constraints, and counterexample (CE) filtering. On commodity hardware, an end-to-end run completes in under 7 minutes. Generated proofs export as navigable HTML so that every inference step can be inspected independently. We outline a hardware-software co-design path toward massively parallel realizations and describe future integration with large language models (LLMs) for auto-formalization and conjecture seeding. The Python, C++, and MPL code to reproduce these experiments, along with the full proof graphs in HTML as well as machine-readable text format, are available in the project's GitHub repository at github.com/Generative-Logic/GL commit 1771330 and are permanently archived at doi:10.5281/zenodo.17206386.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Generative Logic (GL), a deterministic architecture that starts from user-supplied axiomatic definitions, written in a minimalist Mathematical Programming Language (MPL), and systematically explores a configurable region of their deductive neighborhood. A defining feature of the architecture is its unified hash-based inference engine, which executes both algebraic manipulations and deterministic logical transformations. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; whenever the premises of an inference rule unify, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs. Experimental validation is performed on Elementary Number Theory (ENT) utilizing a batched execution strategy. Starting from foundational axioms and definitions, the system first develops first-order Peano arithmetic, which is subsequently applied to autonomously derive and prove Gauss's summation formula as a main result. To manage combinatorial explosion, GL algorithmically enumerates conjectures and applies normalization, type constraints, and counterexample (CE) filtering. On commodity hardware, an end-to-end run completes in under 7 minutes. Generated proofs export as navigable HTML so that every inference step can be inspected independently. We outline a hardware-software co-design path toward massively parallel realizations and describe future integration with large language models (LLMs) for auto-formalization and conjecture seeding. The Python, C++, and MPL code to reproduce these experiments, along with the full proof graphs in HTML as well as machine-readable text format, are available in the project's GitHub repository at github.com/Generative-Logic/GL commit 1771330 and are permanently archived at doi:10.5281/zenodo.17206386."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-25T17:29:19Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    17,
                    29,
                    19,
                    4,
                    206,
                    0
                ],
                "arxiv_comment": "v3: Added derivation of Gauss summation formula, Logical Transformer section and batched workflow. Updated code/artifact links. 20 pages, 6 figures. Code and HTML proof graphs archived at Zenodo (DOI: 10.5281/zenodo.17206386)",
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Nikolai Sergeev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolai Sergeev"
                },
                "author": "Nikolai Sergeev"
            },
            {
                "id": "http://arxiv.org/abs/2602.19895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19895v1",
                "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning"
                },
                "updated": "2026-02-23T14:37:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    1,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:37:01Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    37,
                    1,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Yun Shen"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Donghao Zhou"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zixuan Zhong"
                    },
                    {
                        "name": "Peizhou Huang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19891v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19891v1",
                "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images"
                },
                "updated": "2026-02-23T14:33:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    33,
                    24,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19891v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:33:24Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    33,
                    24,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Wen-Liang Lin"
                    },
                    {
                        "name": "Yun-Chien Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Chien Cheng"
                },
                "author": "Yun-Chien Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.02840v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02840v2",
                "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "promptolution: A Unified, Modular Framework for Prompt Optimization"
                },
                "updated": "2026-02-23T14:32:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    32,
                    6,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02840v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T14:53:23Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    14,
                    53,
                    23,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Timo Hei"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Matthias A른nmacher"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer"
            },
            {
                "id": "http://arxiv.org/abs/2602.19870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19870v1",
                "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs"
                },
                "updated": "2026-02-23T14:15:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    15,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T14:15:37Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    15,
                    37,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "CVPR2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qiankun Ma"
                    },
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Haofei Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhen Song"
                    },
                    {
                        "name": "Hairong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hairong Zheng"
                },
                "author": "Hairong Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2509.06685v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06685v4",
                "title": "MOGS: Monocular Object-guided Gaussian Splatting in Large Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOGS: Monocular Object-guided Gaussian Splatting in Large Scenes"
                },
                "updated": "2026-02-23T14:07:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    14,
                    7,
                    37,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06685v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06685v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in 3D Gaussian Splatting (3DGS) deliver striking photorealism, and extending it to large scenes opens new opportunities for semantic reasoning and prediction in applications such as autonomous driving. Today's state-of-the-art systems for large scenes primarily originate from LiDAR-based pipelines that utilize long-range depth sensing. However, they require costly high-channel sensors whose dense point clouds strain memory and computation, limiting scalability, fleet deployment, and optimization speed. We present MOGS, a monocular 3DGS framework that replaces active LiDAR depth with object-anchored, metrized dense depth derived from sparse visual-inertial (VI) structure-from-motion (SfM) cues. Our key idea is to exploit image semantics to hypothesize per-object shape priors, anchor them with sparse but metrically reliable SfM points, and propagate the resulting metric constraints across each object to produce dense depth. To address two key challenges, i.e., insufficient SfM coverage within objects and cross-object geometric inconsistency, MOGS introduces (1) a multi-scale shape consensus module that adaptively merges small segments into coarse objects best supported by SfM and fits them with parametric shape models, and (2) a cross-object depth refinement module that optimizes per-pixel depth under a combinatorial objective combining geometric consistency, prior anchoring, and edge-aware smoothness. Experiments on public datasets show that, with a low-cost VI sensor suite, MOGS reduces training time by up to 30.4% and memory consumption by 19.8%, while achieving high-quality rendering competitive with costly LiDAR-based approaches in large scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 3D Gaussian Splatting (3DGS) deliver striking photorealism, and extending it to large scenes opens new opportunities for semantic reasoning and prediction in applications such as autonomous driving. Today's state-of-the-art systems for large scenes primarily originate from LiDAR-based pipelines that utilize long-range depth sensing. However, they require costly high-channel sensors whose dense point clouds strain memory and computation, limiting scalability, fleet deployment, and optimization speed. We present MOGS, a monocular 3DGS framework that replaces active LiDAR depth with object-anchored, metrized dense depth derived from sparse visual-inertial (VI) structure-from-motion (SfM) cues. Our key idea is to exploit image semantics to hypothesize per-object shape priors, anchor them with sparse but metrically reliable SfM points, and propagate the resulting metric constraints across each object to produce dense depth. To address two key challenges, i.e., insufficient SfM coverage within objects and cross-object geometric inconsistency, MOGS introduces (1) a multi-scale shape consensus module that adaptively merges small segments into coarse objects best supported by SfM and fits them with parametric shape models, and (2) a cross-object depth refinement module that optimizes per-pixel depth under a combinatorial objective combining geometric consistency, prior anchoring, and edge-aware smoothness. Experiments on public datasets show that, with a low-cost VI sensor suite, MOGS reduces training time by up to 30.4% and memory consumption by 19.8%, while achieving high-quality rendering competitive with costly LiDAR-based approaches in large scenes."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T13:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    41,
                    10,
                    0,
                    251,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shengkai Zhang"
                    },
                    {
                        "name": "Yuhe Liu"
                    },
                    {
                        "name": "Jianhua He"
                    },
                    {
                        "name": "Xuedou Xiao"
                    },
                    {
                        "name": "Mozi Chen"
                    },
                    {
                        "name": "Kezhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kezhong Liu"
                },
                "author": "Kezhong Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19857v1",
                "title": "Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions"
                },
                "updated": "2026-02-23T13:56:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    56,
                    49,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:56:49Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    56,
                    49,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "4 pages, 5 figures, 1 table, isbi2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rodrigo Mota"
                    },
                    {
                        "name": "Kelvin Cunha"
                    },
                    {
                        "name": "Emanoel dos Santos"
                    },
                    {
                        "name": "F치bio Papais"
                    },
                    {
                        "name": "Francisco Filho"
                    },
                    {
                        "name": "Thales Bezerra"
                    },
                    {
                        "name": "Erico Medeiros"
                    },
                    {
                        "name": "Paulo Borba"
                    },
                    {
                        "name": "Tsang Ing Ren"
                    }
                ],
                "author_detail": {
                    "name": "Tsang Ing Ren"
                },
                "author": "Tsang Ing Ren"
            },
            {
                "id": "http://arxiv.org/abs/2602.19848v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19848v1",
                "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation"
                },
                "updated": "2026-02-23T13:52:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    52,
                    28,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19848v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:52:28Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    52,
                    28,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "4 pages, 2 figures, 1 table, isbi2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Francisco Filho"
                    },
                    {
                        "name": "Kelvin Cunha"
                    },
                    {
                        "name": "F치bio Papais"
                    },
                    {
                        "name": "Emanoel dos Santos"
                    },
                    {
                        "name": "Rodrigo Mota"
                    },
                    {
                        "name": "Thales Bezerra"
                    },
                    {
                        "name": "Erico Medeiros"
                    },
                    {
                        "name": "Paulo Borba"
                    },
                    {
                        "name": "Tsang Ing Ren"
                    }
                ],
                "author_detail": {
                    "name": "Tsang Ing Ren"
                },
                "author": "Tsang Ing Ren"
            },
            {
                "id": "http://arxiv.org/abs/2509.11851v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11851v2",
                "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Memory Gap: Users Misremember What They Created With AI or Without"
                },
                "updated": "2026-02-23T13:51:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    51,
                    33,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11851v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3791494",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T12:31:00Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    31,
                    0,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "22 pages, 10 figures, 10 tables, ACM CHI 2026",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Tim Zindulka"
                    },
                    {
                        "name": "Sven Goller"
                    },
                    {
                        "name": "Daniela Fernandes"
                    },
                    {
                        "name": "Robin Welsch"
                    },
                    {
                        "name": "Daniel Buschek"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Buschek"
                },
                "author": "Daniel Buschek",
                "arxiv_doi": "10.1145/3772318.3791494"
            },
            {
                "id": "http://arxiv.org/abs/2602.19844v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19844v1",
                "title": "LLM-enabled Applications Require System-Level Threat Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled Applications Require System-Level Threat Monitoring"
                },
                "updated": "2026-02-23T13:48:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    48,
                    36,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19844v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:48:36Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    48,
                    36,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "26 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yedi Zhang"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Xianglin Yang"
                    },
                    {
                        "name": "Jin Song Dong"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun"
            },
            {
                "id": "http://arxiv.org/abs/2602.19843v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19843v1",
                "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems"
                },
                "updated": "2026-02-23T13:47:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    47,
                    43,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19843v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:47:43Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    47,
                    43,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jin Jia"
                    },
                    {
                        "name": "Zhiling Deng"
                    },
                    {
                        "name": "Zhuangbin Chen"
                    },
                    {
                        "name": "Yingqi Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.24943v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24943v2",
                "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment"
                },
                "updated": "2026-02-23T13:41:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    41,
                    40,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24943v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T16:09:08Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    16,
                    9,
                    8,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Chenji Lu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Zhenyi Wang"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Jian Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xu"
                },
                "author": "Jian Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19840v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19840v1",
                "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation"
                },
                "updated": "2026-02-23T13:40:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    40,
                    44,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19840v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:40:44Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    40,
                    44,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingzhuo Wu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Keyan Jin"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Junbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Wang"
                },
                "author": "Junbo Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19831v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19831v1",
                "title": "An Explainable Memory Forensics Approach for Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Memory Forensics Approach for Malware Analysis"
                },
                "updated": "2026-02-23T13:30:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    30,
                    4,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19831v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:30:04Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    30,
                    4,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Silvia Lucia Sanna"
                    },
                    {
                        "name": "Davide Maiorca"
                    },
                    {
                        "name": "Giorgio Giacinto"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Giacinto"
                },
                "author": "Giorgio Giacinto"
            },
            {
                "id": "http://arxiv.org/abs/2505.05295v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.05295v2",
                "title": "Performance Estimation in Binary Classification Using Calibrated Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Estimation in Binary Classification Using Calibrated Confidence"
                },
                "updated": "2026-02-23T13:16:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    16,
                    47,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.05295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.05295v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Model monitoring is a critical component of the machine learning lifecycle, safeguarding against undetected drops in the model's performance after deployment. Traditionally, performance monitoring has required access to ground truth labels, which are not always readily available. This can result in unacceptable latency or render performance monitoring altogether impossible. Recently, methods designed to estimate the accuracy of classifier models without access to labels have shown promising results. However, there are various other metrics that might be more suitable for assessing model performance in many cases. Until now, none of these important metrics has received similar interest from the scientific community. In this work, we address this gap by presenting CBPE, a novel method that can estimate any binary classification metric defined using the confusion matrix. In particular, we choose four metrics from this large family: accuracy, precision, recall, and F$_1$, to demonstrate our method. CBPE treats the elements of the confusion matrix as random variables and leverages calibrated confidence scores of the model to estimate their distributions. The desired metric is then also treated as a random variable, whose full probability distribution can be derived from the estimated confusion matrix. CBPE is shown to produce estimates that come with strong theoretical guarantees and valid confidence intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model monitoring is a critical component of the machine learning lifecycle, safeguarding against undetected drops in the model's performance after deployment. Traditionally, performance monitoring has required access to ground truth labels, which are not always readily available. This can result in unacceptable latency or render performance monitoring altogether impossible. Recently, methods designed to estimate the accuracy of classifier models without access to labels have shown promising results. However, there are various other metrics that might be more suitable for assessing model performance in many cases. Until now, none of these important metrics has received similar interest from the scientific community. In this work, we address this gap by presenting CBPE, a novel method that can estimate any binary classification metric defined using the confusion matrix. In particular, we choose four metrics from this large family: accuracy, precision, recall, and F$_1$, to demonstrate our method. CBPE treats the elements of the confusion matrix as random variables and leverages calibrated confidence scores of the model to estimate their distributions. The desired metric is then also treated as a random variable, whose full probability distribution can be derived from the estimated confusion matrix. CBPE is shown to produce estimates that come with strong theoretical guarantees and valid confidence intervals."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-08T14:34:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    34,
                    44,
                    3,
                    128,
                    0
                ],
                "arxiv_comment": "Accepted for publication in Machine Learning, (ACML 2025 Journal Track). Presented at the 17th Asian Conference on Machine Learning",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Juhani Kivim칛ki"
                    },
                    {
                        "name": "Jakub Bia켹ek"
                    },
                    {
                        "name": "Wojtek Kuberski"
                    },
                    {
                        "name": "Jukka K. Nurminen"
                    }
                ],
                "author_detail": {
                    "name": "Jukka K. Nurminen"
                },
                "author": "Jukka K. Nurminen"
            },
            {
                "id": "http://arxiv.org/abs/2602.19815v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19815v1",
                "title": "Keyboards for the Endangered Idu Mishmi Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyboards for the Endangered Idu Mishmi Language"
                },
                "updated": "2026-02-23T13:13:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    40,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19815v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:13:40Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    40,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Akhilesh Kakolu Ramarao"
                    }
                ],
                "author_detail": {
                    "name": "Akhilesh Kakolu Ramarao"
                },
                "author": "Akhilesh Kakolu Ramarao"
            },
            {
                "id": "http://arxiv.org/abs/2602.19811v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19811v1",
                "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
                },
                "updated": "2026-02-23T13:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19811v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:12:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "12 pages, 2 figures, 5 tables. Extended version of the short paper published at DOLAP 2026 (co-located with EDBT/ICDT 2026)",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2412.17596v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.17596v4",
                "title": "Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context"
                },
                "updated": "2026-02-23T13:11:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    11,
                    55,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.17596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.17596v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-23T14:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "arxiv_comment": "Updated manuscript and title",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun"
            },
            {
                "id": "http://arxiv.org/abs/2602.15888v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15888v2",
                "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing"
                },
                "updated": "2026-02-23T12:47:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    47,
                    29,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15888v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-06T13:16:28Z",
                "published_parsed": [
                    2026,
                    2,
                    6,
                    13,
                    16,
                    28,
                    4,
                    37,
                    0
                ],
                "arxiv_comment": "14 pages, 5 figures, under review at Physiological Measurement",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Boyu Li"
                    },
                    {
                        "name": "Xingchun Zhu"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19788v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19788v1",
                "title": "Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings"
                },
                "updated": "2026-02-23T12:44:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    44,
                    22,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19788v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:44:22Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    44,
                    22,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "27 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Lotta M칛kinen"
                    },
                    {
                        "name": "Jorge Lor칤a"
                    },
                    {
                        "name": "Samuel Kaski"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kaski"
                },
                "author": "Samuel Kaski"
            },
            {
                "id": "http://arxiv.org/abs/2602.19784v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19784v1",
                "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation"
                },
                "updated": "2026-02-23T12:40:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19784v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:40:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bang Huang"
                    },
                    {
                        "name": "Eddine Youcef Belmekki"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini"
            },
            {
                "id": "http://arxiv.org/abs/2509.15886v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.15886v4",
                "title": "RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation"
                },
                "updated": "2026-02-23T12:38:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    38,
                    59,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.15886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.15886v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-19T11:33:10Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    10,
                    4,
                    262,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Paul Julius K칲hn"
                    },
                    {
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "name": "Arjan Kuijper"
                    },
                    {
                        "name": "Saptarshi Neil Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saptarshi Neil Sinha"
                },
                "author": "Saptarshi Neil Sinha"
            },
            {
                "id": "http://arxiv.org/abs/2507.11230v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.11230v3",
                "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages"
                },
                "updated": "2026-02-23T12:16:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    16,
                    53,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.11230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.11230v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-15T12:00:30Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    0,
                    30,
                    1,
                    196,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lyzander Marciano Andrylie"
                    },
                    {
                        "name": "Inaya Rahmanisa"
                    },
                    {
                        "name": "Mahardika Krisna Ihsani"
                    },
                    {
                        "name": "Alfan Farizki Wicaksono"
                    },
                    {
                        "name": "Haryo Akbarianto Wibowo"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji"
            },
            {
                "id": "http://arxiv.org/abs/2602.19762v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19762v1",
                "title": "Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)"
                },
                "updated": "2026-02-23T12:12:39Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    12,
                    39,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19762v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:12:39Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    12,
                    39,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Mohammed Javed Absar"
                    },
                    {
                        "name": "Muthu Baskaran"
                    },
                    {
                        "name": "Abhikrant Sharma"
                    },
                    {
                        "name": "Abhilash Bhandari"
                    },
                    {
                        "name": "Ankit Aggarwal"
                    },
                    {
                        "name": "Arun Rangasamy"
                    },
                    {
                        "name": "Dibyendu Das"
                    },
                    {
                        "name": "Fateme Hosseini"
                    },
                    {
                        "name": "Franck Slama"
                    },
                    {
                        "name": "Iulian Brumar"
                    },
                    {
                        "name": "Jyotsna Verma"
                    },
                    {
                        "name": "Krishnaprasad Bindumadhavan"
                    },
                    {
                        "name": "Mitesh Kothari"
                    },
                    {
                        "name": "Mohit Gupta"
                    },
                    {
                        "name": "Ravishankar Kolachana"
                    },
                    {
                        "name": "Richard Lethin"
                    },
                    {
                        "name": "Samarth Narang"
                    },
                    {
                        "name": "Sanjay Motilal Ladwa"
                    },
                    {
                        "name": "Shalini Jain"
                    },
                    {
                        "name": "Snigdha Suresh Dalvi"
                    },
                    {
                        "name": "Tasmia Rahman"
                    },
                    {
                        "name": "Venkat Rasagna Reddy Komatireddy"
                    },
                    {
                        "name": "Vivek Vasudevbhai Pandya"
                    },
                    {
                        "name": "Xiyue Shi"
                    },
                    {
                        "name": "Zachary Zipper"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Zipper"
                },
                "author": "Zachary Zipper"
            },
            {
                "id": "http://arxiv.org/abs/2509.19405v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.19405v2",
                "title": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation"
                },
                "updated": "2026-02-23T11:57:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    57,
                    41,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.19405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.19405v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance with respect to state-of-the-art approaches, reducing the median positioning error by up to 30% in the most sparsely sampled or structurally complex regions. We also observe region-dependent saturation effects, which emerge most rapidly in scenarios with high user density where the information gain from additional synthetic samples quickly diminishes. Overall, the framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance with respect to state-of-the-art approaches, reducing the median positioning error by up to 30% in the most sparsely sampled or structurally complex regions. We also observe region-dependent saturation effects, which emerge most rapidly in scenarios with high user density where the information gain from additional synthetic samples quickly diminishes. Overall, the framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T09:09:45Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    9,
                    45,
                    1,
                    266,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Tony Chahoud"
                    },
                    {
                        "name": "Lorenzo Mario Amorosa"
                    },
                    {
                        "name": "Riccardo Marini"
                    },
                    {
                        "name": "Luca De Nardis"
                    }
                ],
                "author_detail": {
                    "name": "Luca De Nardis"
                },
                "author": "Luca De Nardis"
            },
            {
                "id": "http://arxiv.org/abs/2412.04272v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.04272v4",
                "title": "PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables"
                },
                "updated": "2026-02-23T11:53:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    53,
                    2,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.04272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.04272v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-05T15:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Rui Li"
                    }
                ],
                "author_detail": {
                    "name": "Rui Li"
                },
                "author": "Rui Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.19743v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19743v1",
                "title": "NILE: Formalizing Natural-Language Descriptions of Formal Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NILE: Formalizing Natural-Language Descriptions of Formal Languages"
                },
                "updated": "2026-02-23T11:42:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    56,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19743v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations."
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T11:42:56Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    56,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL"
                },
                "authors": [
                    {
                        "name": "Tristan Kneisel"
                    },
                    {
                        "name": "Marko Schmellenkamp"
                    },
                    {
                        "name": "Fabian Vehlken"
                    },
                    {
                        "name": "Thomas Zeume"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Zeume"
                },
                "author": "Thomas Zeume"
            },
            {
                "id": "http://arxiv.org/abs/2510.13614v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13614v3",
                "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning"
                },
                "updated": "2026-02-23T11:42:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    42,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13614v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T14:43:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "arxiv_comment": "Accepted by The Web Conference 2026 (WWW, 2026)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19723v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19723v1",
                "title": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets"
                },
                "updated": "2026-02-23T11:20:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    20,
                    27,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19723v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T11:20:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    20,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "19 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Zhizheng Zhuo"
                    },
                    {
                        "name": "Siyao Xu"
                    },
                    {
                        "name": "Shan Lv"
                    },
                    {
                        "name": "Zhaoxi Liu"
                    },
                    {
                        "name": "Jun Qiu"
                    },
                    {
                        "name": "Qiuli Wang"
                    },
                    {
                        "name": "Yaou Liu"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2506.03867v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.03867v3",
                "title": "EuroGEST: Investigating gender stereotypes in multilingual language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroGEST: Investigating gender stereotypes in multilingual language models"
                },
                "updated": "2026-02-23T11:19:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    19,
                    0,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.03867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.03867v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.18653/v1/2025.emnlp-main.1632",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are 'beautiful', 'empathetic' and 'neat' and men are 'leaders', 'strong, tough' and 'professional'. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are 'beautiful', 'empathetic' and 'neat' and men are 'leaders', 'strong, tough' and 'professional'. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T11:58:18Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    58,
                    18,
                    2,
                    155,
                    0
                ],
                "arxiv_comment": "In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 32074-32096, Suzhou, China. Association for Computational Linguistics. 9 pages, 5 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jacqueline Rowe"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Shannon Vallor"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_doi": "10.18653/v1/2025.emnlp-main.1632"
            },
            {
                "id": "http://arxiv.org/abs/2602.16902v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16902v3",
                "title": "LLM-WikiRace Benchmark: How Far Can LLMs Plan over Real-World Knowledge Graphs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-WikiRace Benchmark: How Far Can LLMs Plan over Real-World Knowledge Graphs?"
                },
                "updated": "2026-02-23T11:03:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    11,
                    3,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16902v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T21:33:59Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    33,
                    59,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Juliusz Ziomek"
                    },
                    {
                        "name": "William Bankes"
                    },
                    {
                        "name": "Lorenz Wolf"
                    },
                    {
                        "name": "Shyam Sundhar Ramesh"
                    },
                    {
                        "name": "Xiaohang Tang"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    }
                ],
                "author_detail": {
                    "name": "Ilija Bogunovic"
                },
                "author": "Ilija Bogunovic"
            },
            {
                "id": "http://arxiv.org/abs/2602.19700v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19700v1",
                "title": "Reversible Information Transformation via Quantum Reservoir Computing: Conditions, Protocol, and Noise Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reversible Information Transformation via Quantum Reservoir Computing: Conditions, Protocol, and Noise Resilience"
                },
                "updated": "2026-02-23T10:47:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    47,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19700v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantum reservoir computing (QRC) exploits fixed quantum dynamics and a trainable linear readout to process temporal data, yet reversing the transformation -- reconstructing the input from the reservoir output -- has been considered intractable owing to the recursive nonlinearity of sequential quantum state evolution. Here we propose a four-equation encode-decode protocol with cross-key pairing and constructively show that quantum reservoir and key combinations satisfying all four equations exist. Using a full XYZ Hamiltonian reservoir with 10 data qubits, we expand the feature dimension to 76 without increasing qubit count and achieve machine-precision reconstruction (mean-squared error $\\mathrm{MSE} \\sim 10^{-17}$) for data lengths up to 30 under ideal conditions; the rank condition $\\mathrm{dim}(V) \\geq N_c$ is identified as a necessary criterion. A comprehensive noise analysis across seven conditions and four baseline methods reveals a clear hierarchy: shot noise dominates, depolarizing noise adds a moderate factor, and asymmetric resource allocation -- 10 shots for encoding, $10^5$ for decoding -- yields approximately two orders of magnitude MSE improvement by exploiting the asymmetric noise roles of the encryption and decryption feature matrices. Under realistic noise the MSE degrades to $10^{-3}$-$10^{-1}$, indicating that error mitigation is needed before practical deployment, but our results establish the feasibility of bidirectional reversible information transformation within QRC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum reservoir computing (QRC) exploits fixed quantum dynamics and a trainable linear readout to process temporal data, yet reversing the transformation -- reconstructing the input from the reservoir output -- has been considered intractable owing to the recursive nonlinearity of sequential quantum state evolution. Here we propose a four-equation encode-decode protocol with cross-key pairing and constructively show that quantum reservoir and key combinations satisfying all four equations exist. Using a full XYZ Hamiltonian reservoir with 10 data qubits, we expand the feature dimension to 76 without increasing qubit count and achieve machine-precision reconstruction (mean-squared error $\\mathrm{MSE} \\sim 10^{-17}$) for data lengths up to 30 under ideal conditions; the rank condition $\\mathrm{dim}(V) \\geq N_c$ is identified as a necessary criterion. A comprehensive noise analysis across seven conditions and four baseline methods reveals a clear hierarchy: shot noise dominates, depolarizing noise adds a moderate factor, and asymmetric resource allocation -- 10 shots for encoding, $10^5$ for decoding -- yields approximately two orders of magnitude MSE improvement by exploiting the asymmetric noise roles of the encryption and decryption feature matrices. Under realistic noise the MSE degrades to $10^{-3}$-$10^{-1}$, indicating that error mitigation is needed before practical deployment, but our results establish the feasibility of bidirectional reversible information transformation within QRC."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T10:47:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    47,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Hikaru Wakaura"
                    },
                    {
                        "name": "Taiki Tanimae"
                    }
                ],
                "author_detail": {
                    "name": "Taiki Tanimae"
                },
                "author": "Taiki Tanimae"
            },
            {
                "id": "http://arxiv.org/abs/2602.19694v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19694v1",
                "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs"
                },
                "updated": "2026-02-23T10:42:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    42,
                    25,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19694v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks."
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T10:42:25Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    42,
                    25,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.ET"
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Zhu Xiao"
                    },
                    {
                        "name": "Ruihui Li"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Zhuo Tang"
                    },
                    {
                        "name": "Kenli Li"
                    }
                ],
                "author_detail": {
                    "name": "Kenli Li"
                },
                "author": "Kenli Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.19690v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19690v1",
                "title": "\"The explanation makes sense\": An Empirical Study on LLM Performance in News Classification and its Influence on Judgment in Human-AI Collaborative Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"The explanation makes sense\": An Empirical Study on LLM Performance in News Classification and its Influence on Judgment in Human-AI Collaborative Annotation"
                },
                "updated": "2026-02-23T10:37:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    37,
                    55,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19690v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The spread of media bias is a significant concern as political discourse shapes beliefs and opinions. Addressing this challenge computationally requires improved methods for interpreting news. While large language models (LLMs) can scale classification tasks, concerns remain about their trustworthiness. To advance human-AI collaboration, we investigate the feasibility of using LLMs to classify U.S. news by political ideology and examine their effect on user decision-making. We first compared GPT models with prompt engineering to state-of-the-art supervised machine learning on a 34k public dataset. We then collected 17k news articles and tested GPT-4 predictions with brief and detailed explanations. In a between-subjects study (N=124), we evaluated how LLM-generated explanations influence human annotation, judgment, and confidence. Results show that AI assistance significantly increases confidence ($p<.001$), with detailed explanations more persuasive and more likely to alter decisions. We highlight recommendations for AI explanations through thematic analysis and provide our dataset for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of media bias is a significant concern as political discourse shapes beliefs and opinions. Addressing this challenge computationally requires improved methods for interpreting news. While large language models (LLMs) can scale classification tasks, concerns remain about their trustworthiness. To advance human-AI collaboration, we investigate the feasibility of using LLMs to classify U.S. news by political ideology and examine their effect on user decision-making. We first compared GPT models with prompt engineering to state-of-the-art supervised machine learning on a 34k public dataset. We then collected 17k news articles and tested GPT-4 predictions with brief and detailed explanations. In a between-subjects study (N=124), we evaluated how LLM-generated explanations influence human annotation, judgment, and confidence. Results show that AI assistance significantly increases confidence ($p<.001$), with detailed explanations more persuasive and more likely to alter decisions. We highlight recommendations for AI explanations through thematic analysis and provide our dataset for further research."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T10:37:55Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    37,
                    55,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Qile Wang"
                    },
                    {
                        "name": "Prerana Khatiwada"
                    },
                    {
                        "name": "Avinash Chouhan"
                    },
                    {
                        "name": "Ashrey Mahesh"
                    },
                    {
                        "name": "Joy Mwaria"
                    },
                    {
                        "name": "Duy Duc Tran"
                    },
                    {
                        "name": "Kenneth E. Barner"
                    },
                    {
                        "name": "Matthew Louis Mauriello"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Louis Mauriello"
                },
                "author": "Matthew Louis Mauriello"
            },
            {
                "id": "http://arxiv.org/abs/2507.23465v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.23465v3",
                "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations"
                },
                "updated": "2026-02-23T10:22:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    22,
                    6,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.23465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.23465v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-31T11:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    11,
                    41,
                    4,
                    3,
                    212,
                    0
                ],
                "arxiv_comment": "AACL 2025 - Main",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Saeed Almheiri"
                    },
                    {
                        "name": "Yerulan Kongrat"
                    },
                    {
                        "name": "Adrian Santosh"
                    },
                    {
                        "name": "Ruslan Tasmukhanov"
                    },
                    {
                        "name": "Josemaria Loza Vera"
                    },
                    {
                        "name": "Muhammad Dehan Al Kautsar"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto"
            },
            {
                "id": "http://arxiv.org/abs/2602.19672v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19672v1",
                "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkillOrchestra: Learning to Route Agents via Skill Transfer"
                },
                "updated": "2026-02-23T10:17:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    17,
                    25,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19672v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T10:17:25Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    17,
                    25,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Aws Albarghouthi"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala"
            },
            {
                "id": "http://arxiv.org/abs/2602.19661v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19661v1",
                "title": "PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information"
                },
                "updated": "2026-02-23T10:09:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    9,
                    50,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19661v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T10:09:50Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    9,
                    50,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "26 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kihyuk Yoon"
                    },
                    {
                        "name": "Lingchao Mao"
                    },
                    {
                        "name": "Catherine Chong"
                    },
                    {
                        "name": "Todd J. Schwedt"
                    },
                    {
                        "name": "Chia-Chun Chiang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li"
            },
            {
                "id": "http://arxiv.org/abs/2509.22387v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22387v2",
                "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly"
                },
                "updated": "2026-02-23T10:05:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    5,
                    41,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22387v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T14:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    15,
                    44,
                    4,
                    269,
                    0
                ],
                "arxiv_comment": "Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Narada Maugin"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave"
            },
            {
                "id": "http://arxiv.org/abs/2602.19654v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19654v1",
                "title": "NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region"
                },
                "updated": "2026-02-23T09:56:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    56,
                    22,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19654v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:56:22Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    56,
                    22,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "18 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rampunit Kumar"
                    },
                    {
                        "name": "Aditya Maheshwari"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Maheshwari"
                },
                "author": "Aditya Maheshwari"
            },
            {
                "id": "http://arxiv.org/abs/2602.19652v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19652v1",
                "title": "Hardware-Accelerated Geometrical Simulation of Biological and Engineered In-Air Ultrasonic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Accelerated Geometrical Simulation of Biological and Engineered In-Air Ultrasonic Systems"
                },
                "updated": "2026-02-23T09:53:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    53,
                    55,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19652v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of in-air acoustic sensors for industrial monitoring and autonomous robotics has grown significantly, often drawing inspiration from biological echolocation. However, developing and validating these systems in existing simulation frameworks remains challenging due to the computational cost of simulating high-frequency wave propagation in large, dynamic, and complex environments. While wave-based methods offer high accuracy, they scale poorly with frequency and volume. Conversely, existing geometric acoustic solvers often lack support for dynamic scenes, complex diffraction, or closed-loop robotic integration. In this work, we introduce SonoTraceUE, a high-fidelity acoustic simulation framework built as a plugin for Unreal Engine. By using a hardware-accelerated ray tracing-based specular reflection model, and a curvature-based Monte Carlo diffraction model, the system enables near real-time simulation of active and passive acoustic sensing in dynamic, multi-material environments. We validate the framework through two distinct experimental domains: a bioacoustic study and a robotics experiment. Our results demonstrate that SonoTraceUE achieves high correlation with real-world spectral and spatial data. The framework provides a versatile platform for synthetic data generation, hypothesis testing in bioacoustics, and the rapid prototyping of closed-loop robotic systems that use acoustic sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of in-air acoustic sensors for industrial monitoring and autonomous robotics has grown significantly, often drawing inspiration from biological echolocation. However, developing and validating these systems in existing simulation frameworks remains challenging due to the computational cost of simulating high-frequency wave propagation in large, dynamic, and complex environments. While wave-based methods offer high accuracy, they scale poorly with frequency and volume. Conversely, existing geometric acoustic solvers often lack support for dynamic scenes, complex diffraction, or closed-loop robotic integration. In this work, we introduce SonoTraceUE, a high-fidelity acoustic simulation framework built as a plugin for Unreal Engine. By using a hardware-accelerated ray tracing-based specular reflection model, and a curvature-based Monte Carlo diffraction model, the system enables near real-time simulation of active and passive acoustic sensing in dynamic, multi-material environments. We validate the framework through two distinct experimental domains: a bioacoustic study and a robotics experiment. Our results demonstrate that SonoTraceUE achieves high correlation with real-world spectral and spatial data. The framework provides a versatile platform for synthetic data generation, hypothesis testing in bioacoustics, and the rapid prototyping of closed-loop robotic systems that use acoustic sensing."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:53:55Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    53,
                    55,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Wouter Jansen"
                    },
                    {
                        "name": "Jan Steckel"
                    }
                ],
                "author_detail": {
                    "name": "Jan Steckel"
                },
                "author": "Jan Steckel"
            },
            {
                "id": "http://arxiv.org/abs/2602.05695v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.05695v2",
                "title": "SweetSpot: An Analytical Model for Predicting Energy Efficiency of LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SweetSpot: An Analytical Model for Predicting Energy Efficiency of LLM Inference"
                },
                "updated": "2026-02-23T09:49:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    49,
                    19,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.05695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.05695v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) inference is central to modern AI applications, dominating worldwide datacenter workloads, making it critical to predict its energy footprint. Existing approaches estimate energy consumption as a simple linear function of input and output sequence. However, by analyzing the autoregressive structure of Transformers, which implies a fundamentally non-linear relationship between input and output sequence lengths and energy consumption, we demonstrate the existence of a generation energy minima. Peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs. Consequently, we propose SweetSpot, an analytical model derived from the computational and memory-access complexity of the Transformer architecture, which accurately characterizes the efficiency curve as a function of input and output lengths. To assess accuracy, we measure energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite. We test input and output lengths from 64 to 4096 tokens and achieve a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"sweet spots\" reduce energy usage, up to 33.41x, enabling informed truncation, summarization, and adaptive generation strategies in production systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) inference is central to modern AI applications, dominating worldwide datacenter workloads, making it critical to predict its energy footprint. Existing approaches estimate energy consumption as a simple linear function of input and output sequence. However, by analyzing the autoregressive structure of Transformers, which implies a fundamentally non-linear relationship between input and output sequence lengths and energy consumption, we demonstrate the existence of a generation energy minima. Peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs. Consequently, we propose SweetSpot, an analytical model derived from the computational and memory-access complexity of the Transformer architecture, which accurately characterizes the efficiency curve as a function of input and output lengths. To assess accuracy, we measure energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite. We test input and output lengths from 64 to 4096 tokens and achieve a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"sweet spots\" reduce energy usage, up to 33.41x, enabling informed truncation, summarization, and adaptive generation strategies in production systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-05T14:21:00Z",
                "published_parsed": [
                    2026,
                    2,
                    5,
                    14,
                    21,
                    0,
                    3,
                    36,
                    0
                ],
                "arxiv_comment": "To appear at ICPE 2026 (International Conference on Performance Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Andrea Proia"
                    },
                    {
                        "name": "Giacomo Madella"
                    },
                    {
                        "name": "Giovanni B. Esposito"
                    },
                    {
                        "name": "Francesco Antici"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Zeynep Kiziltan"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini"
            },
            {
                "id": "http://arxiv.org/abs/2602.19643v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19643v1",
                "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge"
                },
                "updated": "2026-02-23T09:41:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    41,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19643v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:41:46Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    41,
                    46,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "EACL 2026 Findings",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Alex Robertson"
                    },
                    {
                        "name": "Huizhi Liang"
                    },
                    {
                        "name": "Mahbub Gani"
                    },
                    {
                        "name": "Rohit Kumar"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    }
                ],
                "author_detail": {
                    "name": "Srijith Rajamohan"
                },
                "author": "Srijith Rajamohan"
            },
            {
                "id": "http://arxiv.org/abs/2505.24183v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24183v5",
                "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation"
                },
                "updated": "2026-02-23T09:24:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    24,
                    51,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24183v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24183v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T03:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    3,
                    51,
                    6,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Hanqi Lyu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jinghua Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.19626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19626v1",
                "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding"
                },
                "updated": "2026-02-23T09:14:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    14,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:14:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    14,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Roberto Tacconelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Tacconelli"
                },
                "author": "Roberto Tacconelli"
            },
            {
                "id": "http://arxiv.org/abs/2602.19612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19612v1",
                "title": "Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning"
                },
                "updated": "2026-02-23T08:58:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    8,
                    58,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T08:58:48Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    8,
                    58,
                    48,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Borisiuk Anna"
                    },
                    {
                        "name": "Andrey Savchenko"
                    },
                    {
                        "name": "Alexander Panchecko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina"
            },
            {
                "id": "http://arxiv.org/abs/2602.13033v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13033v2",
                "title": "Buy versus Build an LLM: A Decision Framework for Governments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Buy versus Build an LLM: A Decision Framework for Governments"
                },
                "updated": "2026-02-23T08:47:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    8,
                    47,
                    46,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13033v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.\n  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.\n  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T15:39:31Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    15,
                    39,
                    31,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "The short version of this document is published as an ACM TechBrief at https://dl.acm.org/doi/epdf/10.1145/3797946, and this document is published as an ACM Technology Policy Council white paper at https://www.acm.org/binaries/content/assets/public-policy/buildvsbuyai.pdf",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Jiahao Lu"
                    },
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "William Tjhi"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli"
            }
        ]
    }
]