[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v2",
                "updated": "2024-12-03T04:51:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    51,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qichen Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v1",
                "updated": "2024-11-29T09:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v1",
                "updated": "2024-11-28T21:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Ravi Netravali"
                    },
                    {
                        "name": "Yida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yida Wang"
                },
                "author": "Yida Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sébastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.02687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02687v1",
                "updated": "2024-12-03T18:56:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    32,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:56:32Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    32,
                    1,
                    338,
                    0
                ],
                "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper\n  Guidance"
                },
                "summary": "Recent approaches have yielded promising results in distilling multi-step\ntext-to-image diffusion models into one-step ones. The state-of-the-art\nefficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the\nteacher model's performance with limited resources. However, our study reveals\nits instability when handling different diffusion model backbones due to using\na fixed guidance scale within the Variational Score Distillation (VSD) loss.\nAnother weakness of the existing one-step diffusion models is the missing\nsupport for negative prompt guidance, which is crucial in practical image\ngeneration. This paper presents SNOOPI, a novel framework designed to address\nthese limitations by enhancing the guidance in one-step diffusion models during\nboth training and inference. First, we effectively enhance training stability\nthrough Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale\nclassifier-free guidance approach. By varying the guidance scale of both\nteacher models, we broaden their output distributions, resulting in a more\nrobust VSD loss that enables SB to perform effectively across diverse backbones\nwhile maintaining competitive performance. Second, we propose a training-free\nmethod called Negative-Away Steer Attention (NASA), which integrates negative\nprompts into one-step diffusion models via cross-attention to suppress\nundesired elements in generated images. Our experimental results show that our\nproposed methods significantly improve baseline models across various metrics.\nRemarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art\nbenchmark for one-step diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent approaches have yielded promising results in distilling multi-step\ntext-to-image diffusion models into one-step ones. The state-of-the-art\nefficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the\nteacher model's performance with limited resources. However, our study reveals\nits instability when handling different diffusion model backbones due to using\na fixed guidance scale within the Variational Score Distillation (VSD) loss.\nAnother weakness of the existing one-step diffusion models is the missing\nsupport for negative prompt guidance, which is crucial in practical image\ngeneration. This paper presents SNOOPI, a novel framework designed to address\nthese limitations by enhancing the guidance in one-step diffusion models during\nboth training and inference. First, we effectively enhance training stability\nthrough Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale\nclassifier-free guidance approach. By varying the guidance scale of both\nteacher models, we broaden their output distributions, resulting in a more\nrobust VSD loss that enables SB to perform effectively across diverse backbones\nwhile maintaining competitive performance. Second, we propose a training-free\nmethod called Negative-Away Steer Attention (NASA), which integrates negative\nprompts into one-step diffusion models via cross-attention to suppress\nundesired elements in generated images. Our experimental results show that our\nproposed methods significantly improve baseline models across various metrics.\nRemarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art\nbenchmark for one-step diffusion models."
                },
                "authors": [
                    {
                        "name": "Viet Nguyen"
                    },
                    {
                        "name": "Anh Aengus Nguyen"
                    },
                    {
                        "name": "Trung Dao"
                    },
                    {
                        "name": "Khoi Nguyen"
                    },
                    {
                        "name": "Cuong Pham"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Anh Tran"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tran"
                },
                "author": "Anh Tran",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02685v1",
                "updated": "2024-12-03T18:56:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    7,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:56:07Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    7,
                    1,
                    338,
                    0
                ],
                "title": "T-REG: Preference Optimization with Token-Level Reward Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-REG: Preference Optimization with Token-Level Reward Regularization"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Tao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Tao Meng"
                },
                "author": "Tao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02684v1",
                "updated": "2024-12-03T18:55:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    55,
                    39,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:55:39Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    55,
                    39,
                    1,
                    338,
                    0
                ],
                "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent\n  Gaussian Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent\n  Gaussian Reconstruction"
                },
                "summary": "Generating animatable human avatars from a single image is essential for\nvarious digital human modeling applications. Existing 3D reconstruction methods\noften struggle to capture fine details in animatable models, while generative\napproaches for controllable animation, though avoiding explicit 3D modeling,\nsuffer from viewpoint inconsistencies in extreme poses and computational\ninefficiencies. In this paper, we address these challenges by leveraging the\npower of generative models to produce detailed multi-view canonical pose\nimages, which help resolve ambiguities in animatable human reconstruction. We\nthen propose a robust method for 3D reconstruction of inconsistent images,\nenabling real-time rendering during inference. Specifically, we adapt a\ntransformer-based video generation model to generate multi-view canonical pose\nimages and normal maps, pretraining on a large-scale video dataset to improve\ngeneralization. To handle view inconsistencies, we recast the reconstruction\nproblem as a 4D task and introduce an efficient 3D modeling approach using 4D\nGaussian Splatting. Experiments demonstrate that our method achieves\nphotorealistic, real-time animation of 3D human avatars from in-the-wild\nimages, showcasing its effectiveness and generalization capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating animatable human avatars from a single image is essential for\nvarious digital human modeling applications. Existing 3D reconstruction methods\noften struggle to capture fine details in animatable models, while generative\napproaches for controllable animation, though avoiding explicit 3D modeling,\nsuffer from viewpoint inconsistencies in extreme poses and computational\ninefficiencies. In this paper, we address these challenges by leveraging the\npower of generative models to produce detailed multi-view canonical pose\nimages, which help resolve ambiguities in animatable human reconstruction. We\nthen propose a robust method for 3D reconstruction of inconsistent images,\nenabling real-time rendering during inference. Specifically, we adapt a\ntransformer-based video generation model to generate multi-view canonical pose\nimages and normal maps, pretraining on a large-scale video dataset to improve\ngeneralization. To handle view inconsistencies, we recast the reconstruction\nproblem as a 4D task and introduce an efficient 3D modeling approach using 4D\nGaussian Splatting. Experiments demonstrate that our method achieves\nphotorealistic, real-time animation of 3D human avatars from in-the-wild\nimages, showcasing its effectiveness and generalization capability."
                },
                "authors": [
                    {
                        "name": "Lingteng Qiu"
                    },
                    {
                        "name": "Shenhao Zhu"
                    },
                    {
                        "name": "Qi Zuo"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Yuan Dong"
                    },
                    {
                        "name": "Junfei Zhang"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Weihao Yuan"
                    },
                    {
                        "name": "Liefeng Bo"
                    },
                    {
                        "name": "Guanying Chen"
                    },
                    {
                        "name": "Zilong Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Dong"
                },
                "author": "Zilong Dong",
                "arxiv_comment": "Project Page: https://lingtengqiu.github.io/2024/AniGS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14052v2",
                "updated": "2024-12-03T18:48:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    48,
                    0,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-17T21:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    47,
                    11,
                    3,
                    291,
                    0
                ],
                "title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs"
                },
                "summary": "Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management."
                },
                "authors": [
                    {
                        "name": "Alireza Rezazadeh"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    }
                ],
                "author_detail": {
                    "name": "Yujia Bao"
                },
                "author": "Yujia Bao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02674v1",
                "updated": "2024-12-03T18:47:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models"
                },
                "summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Udaya Ghai"
                    }
                ],
                "author_detail": {
                    "name": "Udaya Ghai"
                },
                "author": "Udaya Ghai",
                "arxiv_comment": "41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02655v1",
                "updated": "2024-12-03T18:29:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    29,
                    37,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:29:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    29,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation\n  with Instructional Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation\n  with Instructional Inputs"
                },
                "summary": "Autonomous navigation guided by natural language instructions is essential\nfor improving human-robot interaction and enabling complex operations in\ndynamic environments. While large language models (LLMs) are not inherently\ndesigned for planning, they can significantly enhance planning efficiency by\nproviding guidance and informing constraints to ensure safety. This paper\nintroduces a planning framework that integrates LLMs with 2D occupancy grid\nmaps and natural language commands to improve spatial reasoning and task\nexecution in resource-limited settings. By decomposing high-level commands and\nreal-time environmental data, the system generates structured navigation plans\nfor pick-and-place tasks, including obstacle avoidance, goal prioritization,\nand adaptive behaviors. The framework dynamically recalculates paths to address\nenvironmental changes and aligns with implicit social norms for seamless\nhuman-robot interaction. Our results demonstrates the potential of LLMs to\ndesign context-aware system to enhance navigation efficiency and safety in\nindustrial and dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation guided by natural language instructions is essential\nfor improving human-robot interaction and enabling complex operations in\ndynamic environments. While large language models (LLMs) are not inherently\ndesigned for planning, they can significantly enhance planning efficiency by\nproviding guidance and informing constraints to ensure safety. This paper\nintroduces a planning framework that integrates LLMs with 2D occupancy grid\nmaps and natural language commands to improve spatial reasoning and task\nexecution in resource-limited settings. By decomposing high-level commands and\nreal-time environmental data, the system generates structured navigation plans\nfor pick-and-place tasks, including obstacle avoidance, goal prioritization,\nand adaptive behaviors. The framework dynamically recalculates paths to address\nenvironmental changes and aligns with implicit social norms for seamless\nhuman-robot interaction. Our results demonstrates the potential of LLMs to\ndesign context-aware system to enhance navigation efficiency and safety in\nindustrial and dynamic environments."
                },
                "authors": [
                    {
                        "name": "Pranav Doma"
                    },
                    {
                        "name": "Aliasghar Arab"
                    },
                    {
                        "name": "Xuesu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xuesu Xiao"
                },
                "author": "Xuesu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02651v1",
                "updated": "2024-12-03T18:24:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    24,
                    56,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:24:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    24,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: a Review of Acceleration Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: a Review of Acceleration Methods"
                },
                "summary": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence events every year with substantially\nhigher SNR and longer signal duration, presenting significant computational\nchallenges. In this study, we systematically evaluate the computational costs\nof source parameter estimation (PE) in the 3G era by modeling the PE time cost\nas a function of SNR and signal duration. We examine the standard PE method\nalongside acceleration methods including relative binning, multibanding, and\nreduced order quadrature. We predict that PE for a one-month-observation\ncatalog with 3G detectors could require billions to quadrillions of CPU core\nhours with the standard PE method, whereas acceleration techniques can reduce\nthis demand to millions of core hours. These findings highlight the necessity\nfor more efficient PE methods to enable cost-effective and environmentally\nsustainable data analysis for 3G detectors. In addition, we assess the accuracy\nof accelerated PE methods, emphasizing the need for careful treatment in\nhigh-SNR scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence events every year with substantially\nhigher SNR and longer signal duration, presenting significant computational\nchallenges. In this study, we systematically evaluate the computational costs\nof source parameter estimation (PE) in the 3G era by modeling the PE time cost\nas a function of SNR and signal duration. We examine the standard PE method\nalongside acceleration methods including relative binning, multibanding, and\nreduced order quadrature. We predict that PE for a one-month-observation\ncatalog with 3G detectors could require billions to quadrillions of CPU core\nhours with the standard PE method, whereas acceleration techniques can reduce\nthis demand to millions of core hours. These findings highlight the necessity\nfor more efficient PE methods to enable cost-effective and environmentally\nsustainable data analysis for 3G detectors. In addition, we assess the accuracy\nof accelerated PE methods, emphasizing the need for careful treatment in\nhigh-SNR scenarios."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "13 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02638v1",
                "updated": "2024-12-03T18:10:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    10,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:10:31Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    10,
                    31,
                    1,
                    338,
                    0
                ],
                "title": "QA-TOOLBOX: Conversational Question-Answering for process task guidance\n  in manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-TOOLBOX: Conversational Question-Answering for process task guidance\n  in manufacturing"
                },
                "summary": "In this work we explore utilizing LLMs for data augmentation for\nmanufacturing task guidance system. The dataset consists of representative\nsamples of interactions with technicians working in an advanced manufacturing\nsetting. The purpose of this work to explore the task, data augmentation for\nthe supported tasks and evaluating the performance of the existing LLMs. We\nobserve that that task is complex requiring understanding from procedure\nspecification documents, actions and objects sequenced temporally. The dataset\nconsists of 200,000+ question/answer pairs that refer to the spec document and\nare grounded in narrations and/or video demonstrations. We compared the\nperformance of several popular open-sourced LLMs by developing a baseline using\neach LLM and then compared the responses in a reference-free setting using\nLLM-as-a-judge and compared the ratings with crowd-workers whilst validating\nthe ratings with experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we explore utilizing LLMs for data augmentation for\nmanufacturing task guidance system. The dataset consists of representative\nsamples of interactions with technicians working in an advanced manufacturing\nsetting. The purpose of this work to explore the task, data augmentation for\nthe supported tasks and evaluating the performance of the existing LLMs. We\nobserve that that task is complex requiring understanding from procedure\nspecification documents, actions and objects sequenced temporally. The dataset\nconsists of 200,000+ question/answer pairs that refer to the spec document and\nare grounded in narrations and/or video demonstrations. We compared the\nperformance of several popular open-sourced LLMs by developing a baseline using\neach LLM and then compared the responses in a reference-free setting using\nLLM-as-a-judge and compared the ratings with crowd-workers whilst validating\nthe ratings with experts."
                },
                "authors": [
                    {
                        "name": "Ramesh Manuvinakurike"
                    },
                    {
                        "name": "Elizabeth Watkins"
                    },
                    {
                        "name": "Celal Savur"
                    },
                    {
                        "name": "Anthony Rhodes"
                    },
                    {
                        "name": "Sovan Biswas"
                    },
                    {
                        "name": "Gesem Gudino Mejia"
                    },
                    {
                        "name": "Richard Beckwith"
                    },
                    {
                        "name": "Saurav Sahay"
                    },
                    {
                        "name": "Giuseppe Raffa"
                    },
                    {
                        "name": "Lama Nachman"
                    }
                ],
                "author_detail": {
                    "name": "Lama Nachman"
                },
                "author": "Lama Nachman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v1",
                "updated": "2024-12-03T17:54:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_journal_ref": "Varun, Y., Madhavan, R., Addepalli, S., Suggala, A., Shanmugam,\n  K., & Jain, P. Time-Reversal Provides Unsupervised Feedback to LLMs. In The\n  Thirty-Eighth Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02622v1",
                "updated": "2024-12-03T17:50:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    50,
                    49,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:50:49Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    50,
                    49,
                    1,
                    338,
                    0
                ],
                "title": "High-z stellar masses can be recovered robustly with JWST photometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-z stellar masses can be recovered robustly with JWST photometry"
                },
                "summary": "Robust inference of galaxy stellar masses from photometry is crucial for\nconstraints on galaxy assembly across cosmic time. Here, we test a\ncommonly-used Spectral Energy Distribution (SED) fitting code, using simulated\ngalaxies from the SPHINX20 cosmological radiation hydrodynamics simulation,\nwith JWST NIRCam photometry forward-modelled with radiative transfer. Fitting\nthe synthetic photometry with various star formation history models, we show\nthat recovered stellar masses are, encouragingly, generally robust to within a\nfactor of ~3 for galaxies in the range M*~10^7-10^9M_sol at z=5-10. These\nresults are in stark contrast to recent work claiming that stellar masses can\nbe underestimated by as much as an order of magnitude in these mass and\nredshift ranges. However, while >90% of masses are recovered to within 0.5dex,\nthere are notable systematic trends, with stellar masses typically\noverestimated for low-mass galaxies (M*<~10^8M_sol) and slightly underestimated\nfor high-mass galaxies (M*>~10^9M_sol). We demonstrate that these trends arise\ndue to the SED fitting code poorly modelling the impact of strong emission\nlines on broadband photometry. These systematic trends, which exist for all\nstar formation history parametrisations tested, have a tilting effect on the\ninferred stellar mass function, with number densities of massive galaxies\nunderestimated (particularly at the lowest redshifts studied) and number\ndensities of lower-mass galaxies typically overestimated. Overall, this work\nsuggests that we should be optimistic about our ability to infer the masses of\nhigh-z galaxies observed with JWST (notwithstanding contamination from AGN) but\ncareful when modelling the impact of strong emission lines on broadband\nphotometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust inference of galaxy stellar masses from photometry is crucial for\nconstraints on galaxy assembly across cosmic time. Here, we test a\ncommonly-used Spectral Energy Distribution (SED) fitting code, using simulated\ngalaxies from the SPHINX20 cosmological radiation hydrodynamics simulation,\nwith JWST NIRCam photometry forward-modelled with radiative transfer. Fitting\nthe synthetic photometry with various star formation history models, we show\nthat recovered stellar masses are, encouragingly, generally robust to within a\nfactor of ~3 for galaxies in the range M*~10^7-10^9M_sol at z=5-10. These\nresults are in stark contrast to recent work claiming that stellar masses can\nbe underestimated by as much as an order of magnitude in these mass and\nredshift ranges. However, while >90% of masses are recovered to within 0.5dex,\nthere are notable systematic trends, with stellar masses typically\noverestimated for low-mass galaxies (M*<~10^8M_sol) and slightly underestimated\nfor high-mass galaxies (M*>~10^9M_sol). We demonstrate that these trends arise\ndue to the SED fitting code poorly modelling the impact of strong emission\nlines on broadband photometry. These systematic trends, which exist for all\nstar formation history parametrisations tested, have a tilting effect on the\ninferred stellar mass function, with number densities of massive galaxies\nunderestimated (particularly at the lowest redshifts studied) and number\ndensities of lower-mass galaxies typically overestimated. Overall, this work\nsuggests that we should be optimistic about our ability to infer the masses of\nhigh-z galaxies observed with JWST (notwithstanding contamination from AGN) but\ncareful when modelling the impact of strong emission lines on broadband\nphotometry."
                },
                "authors": [
                    {
                        "name": "R. K. Cochrane"
                    },
                    {
                        "name": "H. Katz"
                    },
                    {
                        "name": "R. Begley"
                    },
                    {
                        "name": "C. C. Hayward"
                    },
                    {
                        "name": "P. N. Best"
                    }
                ],
                "author_detail": {
                    "name": "P. N. Best"
                },
                "author": "P. N. Best",
                "arxiv_comment": "18 pages, 7 figures. Accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00608v2",
                "updated": "2024-12-03T17:49:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    49,
                    2,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-30T23:11:44Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    23,
                    11,
                    44,
                    5,
                    335,
                    0
                ],
                "title": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation"
                },
                "summary": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadeq Abolhasani"
                    },
                    {
                        "name": "Rong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Rong Pan"
                },
                "author": "Rong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02611v1",
                "updated": "2024-12-03T17:41:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    41,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:41:23Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    41,
                    23,
                    1,
                    338,
                    0
                ],
                "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?"
                },
                "summary": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Kaixiong Gong"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Bohao Li"
                    },
                    {
                        "name": "Yibing Wang"
                    },
                    {
                        "name": "Mofan Cheng"
                    },
                    {
                        "name": "Shijia Yang"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Yutong Bai"
                    },
                    {
                        "name": "Zhuoran Yang"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Project page: https://av-odyssey.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v2",
                "updated": "2024-12-03T17:38:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    38,
                    54,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02605v1",
                "updated": "2024-12-03T17:34:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    34,
                    50,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:34:50Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    34,
                    50,
                    1,
                    338,
                    0
                ],
                "title": "Interpretable Company Similarity with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Company Similarity with Sparse Autoencoders"
                },
                "summary": "Determining company similarity is a vital task in finance, underpinning\nhedging, risk management, portfolio diversification, and more. Practitioners\noften rely on sector and industry classifications to gauge similarity, such as\nSIC-codes and GICS-codes, the former being used by the U.S. Securities and\nExchange Commission (SEC), and the latter widely used by the investment\ncommunity. Clustering embeddings of company descriptions has been proposed as a\npotential technique for determining company similarity, but the lack of\ninterpretability in token embeddings poses a significant barrier to adoption in\nhigh-stakes contexts. Sparse Autoencoders have shown promise in enhancing the\ninterpretability of Large Language Models by decomposing LLM activations into\ninterpretable features. In this paper, we explore the use of SAE features in\nmeasuring company similarity and benchmark them against (1) SIC codes and (2)\nMajor Group codes. We conclude that SAE features can reproduce and even surpass\nsector classifications in quantifying fundamental characteristics of companies,\nevaluated by the correlation of monthly returns, a proxy for similarity, and\nPnL from cointegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining company similarity is a vital task in finance, underpinning\nhedging, risk management, portfolio diversification, and more. Practitioners\noften rely on sector and industry classifications to gauge similarity, such as\nSIC-codes and GICS-codes, the former being used by the U.S. Securities and\nExchange Commission (SEC), and the latter widely used by the investment\ncommunity. Clustering embeddings of company descriptions has been proposed as a\npotential technique for determining company similarity, but the lack of\ninterpretability in token embeddings poses a significant barrier to adoption in\nhigh-stakes contexts. Sparse Autoencoders have shown promise in enhancing the\ninterpretability of Large Language Models by decomposing LLM activations into\ninterpretable features. In this paper, we explore the use of SAE features in\nmeasuring company similarity and benchmark them against (1) SIC codes and (2)\nMajor Group codes. We conclude that SAE features can reproduce and even surpass\nsector classifications in quantifying fundamental characteristics of companies,\nevaluated by the correlation of monthly returns, a proxy for similarity, and\nPnL from cointegration."
                },
                "authors": [
                    {
                        "name": "Marco Molinari"
                    },
                    {
                        "name": "Vladimir Tregubiak"
                    },
                    {
                        "name": "Victor Shao"
                    },
                    {
                        "name": "Abhimanyu Pandey"
                    },
                    {
                        "name": "Mateusz Mikolajczak"
                    },
                    {
                        "name": "Sebastião Kuznetsov Ryder Torres Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Sebastião Kuznetsov Ryder Torres Pereira"
                },
                "author": "Sebastião Kuznetsov Ryder Torres Pereira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02602v1",
                "updated": "2024-12-03T17:32:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    32,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:32:47Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    32,
                    47,
                    1,
                    338,
                    0
                ],
                "title": "CEGI: Measuring the trade-off between efficiency and carbon emissions\n  for SLMs and VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEGI: Measuring the trade-off between efficiency and carbon emissions\n  for SLMs and VLMs"
                },
                "summary": "This paper analyzes the performance of Small Language Models (SLMs) and\nVision Language Models (VLMs) and evaluates the trade-off between model\nperformance and carbon emissions across 4 essential tasks: Image Captioning,\nVisual Question Answering (VQA), Dialogue Summarization and Text-to-SQL\nconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of\nparameters, quantization level and fine-tuning parameters are evaluated. The\nmodel variant's performance and carbon emissions are calculated. To quantify\nthe trade-off between model performance and carbon emissions, we introduce a\nnovel metric called CEGI (Carbon Efficient Gain Index). This metric represents\nthe carbon emission per unit percentage gain per million trainable parameters .\nThis metric provides a normalized measure to compare model's efficiency in\nterms of performance improvement relative to their environmental cost. The\nexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve\nperformance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal\ngains in accuracy from larger models do not justify the substantial increase in\ncarbon emissions. Leveraging lower-bit quantization levels, the proposed metric\nfurther enhances energy efficiency without compromising performance. This study\nhighlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for\nenvironmentally-friendly AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper analyzes the performance of Small Language Models (SLMs) and\nVision Language Models (VLMs) and evaluates the trade-off between model\nperformance and carbon emissions across 4 essential tasks: Image Captioning,\nVisual Question Answering (VQA), Dialogue Summarization and Text-to-SQL\nconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of\nparameters, quantization level and fine-tuning parameters are evaluated. The\nmodel variant's performance and carbon emissions are calculated. To quantify\nthe trade-off between model performance and carbon emissions, we introduce a\nnovel metric called CEGI (Carbon Efficient Gain Index). This metric represents\nthe carbon emission per unit percentage gain per million trainable parameters .\nThis metric provides a normalized measure to compare model's efficiency in\nterms of performance improvement relative to their environmental cost. The\nexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve\nperformance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal\ngains in accuracy from larger models do not justify the substantial increase in\ncarbon emissions. Leveraging lower-bit quantization levels, the proposed metric\nfurther enhances energy efficiency without compromising performance. This study\nhighlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for\nenvironmentally-friendly AI development."
                },
                "authors": [
                    {
                        "name": "Abhas Kumar"
                    },
                    {
                        "name": "Kapil Pathak"
                    },
                    {
                        "name": "Rajesh Kavuru"
                    },
                    {
                        "name": "Prabhakar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Prabhakar Srinivasan"
                },
                "author": "Prabhakar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02594v1",
                "updated": "2024-12-03T17:26:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    26,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:26:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    26,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "PrefixLLM: LLM-aided Prefix Circuit Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixLLM: LLM-aided Prefix Circuit Design"
                },
                "summary": "Prefix circuits are fundamental components in digital adders, widely used in\ndigital systems due to their efficiency in calculating carry signals.\nSynthesizing prefix circuits with minimized area and delay is crucial for\nenhancing the performance of modern computing systems. Recently, large language\nmodels (LLMs) have demonstrated a surprising ability to perform text generation\ntasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.\nPrefixLLM transforms the prefix circuit synthesis task into a structured text\ngeneration problem, termed the Structured Prefix Circuit Representation (SPCR),\nand introduces an iterative framework to automatically and accurately generate\nvalid SPCRs. We further present a design space exploration (DSE) framework that\nuses LLMs to iteratively search for area and delay optimized prefix circuits.\nCompared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the\nsame delay constraint. This work highlights the use of LLMs in the synthesis of\narithmetic circuits, which can be transformed into the structured text\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix circuits are fundamental components in digital adders, widely used in\ndigital systems due to their efficiency in calculating carry signals.\nSynthesizing prefix circuits with minimized area and delay is crucial for\nenhancing the performance of modern computing systems. Recently, large language\nmodels (LLMs) have demonstrated a surprising ability to perform text generation\ntasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.\nPrefixLLM transforms the prefix circuit synthesis task into a structured text\ngeneration problem, termed the Structured Prefix Circuit Representation (SPCR),\nand introduces an iterative framework to automatically and accurately generate\nvalid SPCRs. We further present a design space exploration (DSE) framework that\nuses LLMs to iteratively search for area and delay optimized prefix circuits.\nCompared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the\nsame delay constraint. This work highlights the use of LLMs in the synthesis of\narithmetic circuits, which can be transformed into the structured text\ngeneration."
                },
                "authors": [
                    {
                        "name": "Weihua Xiao"
                    },
                    {
                        "name": "Venkata Sai Charan Putrevu"
                    },
                    {
                        "name": "Raghu Vamshi Hemadri"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02592v1",
                "updated": "2024-12-03T17:23:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    23,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:23:47Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    23,
                    47,
                    1,
                    338,
                    0
                ],
                "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench"
                },
                "authors": [
                    {
                        "name": "Junyuan Zhang"
                    },
                    {
                        "name": "Qintong Zhang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Linke Ouyang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Ka-Ho Chow"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19636v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19636v3",
                "updated": "2024-12-03T17:17:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    17,
                    45,
                    1,
                    338,
                    0
                ],
                "published": "2024-04-30T15:38:01Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    15,
                    38,
                    1,
                    1,
                    121,
                    0
                ],
                "title": "Bayesian calibration of bubble size dynamics applied to CO2 gas\n  fermenters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian calibration of bubble size dynamics applied to CO2 gas\n  fermenters"
                },
                "summary": "To accelerate the scale-up of gaseous CO2 fermentation reactors,\ncomputational models need to predict gas-to-liquid mass transfer which requires\ncapturing the bubble size dynamics, i.e. bubble breakup and coalescence.\nHowever, the applicability of existing models beyond air-water mixtures remains\nto be established. Here, an inverse modeling approach, accelerated with a\nneural network surrogate, calibrates the breakup and coalescence closure\nmodels, that are used in class methods for population balance modeling (PBM).\nThe calibration is performed based on experimental results obtained in a\nCO2-air-water-coflowing bubble column reactor. Bayesian inference is used to\naccount for noise in the experimental dataset and bias in the simulation\nresults. To accurately capture gas holdup and interphase mass transfer, the\nresults show that the breakage rate needs to be increased by one order of\nmagnitude. The inferred model parameters are then used on a separate\nconfiguration and shown to also improve bubble size distribution predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accelerate the scale-up of gaseous CO2 fermentation reactors,\ncomputational models need to predict gas-to-liquid mass transfer which requires\ncapturing the bubble size dynamics, i.e. bubble breakup and coalescence.\nHowever, the applicability of existing models beyond air-water mixtures remains\nto be established. Here, an inverse modeling approach, accelerated with a\nneural network surrogate, calibrates the breakup and coalescence closure\nmodels, that are used in class methods for population balance modeling (PBM).\nThe calibration is performed based on experimental results obtained in a\nCO2-air-water-coflowing bubble column reactor. Bayesian inference is used to\naccount for noise in the experimental dataset and bias in the simulation\nresults. To accurately capture gas holdup and interphase mass transfer, the\nresults show that the breakage rate needs to be increased by one order of\nmagnitude. The inferred model parameters are then used on a separate\nconfiguration and shown to also improve bubble size distribution predictions."
                },
                "authors": [
                    {
                        "name": "Malik Hassanaly"
                    },
                    {
                        "name": "John M. Parra-Alvarez"
                    },
                    {
                        "name": "Mohammad J. Rahimi"
                    },
                    {
                        "name": "Hariswaran Sitaraman"
                    }
                ],
                "author_detail": {
                    "name": "Hariswaran Sitaraman"
                },
                "author": "Hariswaran Sitaraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19636v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02588v1",
                "updated": "2024-12-03T17:17:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    17,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:17:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    17,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Explainable CTR Prediction via LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable CTR Prediction via LLM Reasoning"
                },
                "summary": "Recommendation Systems have become integral to modern user experiences, but\nlack transparency in their decision-making processes. Existing explainable\nrecommendation methods are hindered by reliance on a post-hoc paradigm, wherein\nexplanation generators are trained independently of the underlying recommender\nmodels. This paradigm necessitates substantial human effort in data\nconstruction and raises concerns about explanation reliability. In this paper,\nwe present ExpCTR, a novel framework that integrates large language model based\nexplanation generation directly into the CTR prediction process. Inspired by\nrecent advances in reinforcement learning, we employ two carefully designed\nreward mechanisms, LC alignment, which ensures explanations reflect user\nintentions, and IC alignment, which maintains consistency with traditional\nID-based CTR models. Our approach incorporates an efficient training paradigm\nwith LoRA and a three-stage iterative process. ExpCTR circumvents the need for\nextensive explanation datasets while fostering synergy between CTR prediction\nand explanation generation. Experimental results demonstrate that ExpCTR\nsignificantly enhances both recommendation accuracy and interpretability across\nthree real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation Systems have become integral to modern user experiences, but\nlack transparency in their decision-making processes. Existing explainable\nrecommendation methods are hindered by reliance on a post-hoc paradigm, wherein\nexplanation generators are trained independently of the underlying recommender\nmodels. This paradigm necessitates substantial human effort in data\nconstruction and raises concerns about explanation reliability. In this paper,\nwe present ExpCTR, a novel framework that integrates large language model based\nexplanation generation directly into the CTR prediction process. Inspired by\nrecent advances in reinforcement learning, we employ two carefully designed\nreward mechanisms, LC alignment, which ensures explanations reflect user\nintentions, and IC alignment, which maintains consistency with traditional\nID-based CTR models. Our approach incorporates an efficient training paradigm\nwith LoRA and a three-stage iterative process. ExpCTR circumvents the need for\nextensive explanation datasets while fostering synergy between CTR prediction\nand explanation generation. Experimental results demonstrate that ExpCTR\nsignificantly enhances both recommendation accuracy and interpretability across\nthree real-world datasets."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Chong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chong Chen"
                },
                "author": "Chong Chen",
                "arxiv_comment": "WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02575v1",
                "updated": "2024-12-03T17:02:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    2,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:02:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    2,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Copy-Move Forgery Detection and Question Answering for Remote Sensing\n  Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copy-Move Forgery Detection and Question Answering for Remote Sensing\n  Image"
                },
                "summary": "This paper introduces the task of Remote Sensing Copy-Move Question Answering\n(RSCMQA). Unlike traditional Remote Sensing Visual Question Answering (RSVQA),\nRSCMQA focuses on interpreting complex tampering scenarios and inferring\nrelationships between objects. Based on the practical needs of national defense\nsecurity and land resource monitoring, we have developed an accurate and\ncomprehensive global dataset for remote sensing image copy-move question\nanswering, named RS-CMQA-2.1M. These images were collected from 29 different\nregions across 14 countries. Additionally, we have refined a balanced dataset,\nRS-CMQA-B, to address the long-standing issue of long-tail data in the remote\nsensing field. Furthermore, we propose a region-discriminative guided\nmultimodal CMQA model, which enhances the accuracy of answering questions about\ntampered images by leveraging prompt about the differences and connections\nbetween the source and tampered domains. Extensive experiments demonstrate that\nour method provides a stronger benchmark for RS-CMQA compared to general VQA\nand RSVQA models. Our dataset and code are available at\nhttps://github.com/shenyedepisa/RSCMQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the task of Remote Sensing Copy-Move Question Answering\n(RSCMQA). Unlike traditional Remote Sensing Visual Question Answering (RSVQA),\nRSCMQA focuses on interpreting complex tampering scenarios and inferring\nrelationships between objects. Based on the practical needs of national defense\nsecurity and land resource monitoring, we have developed an accurate and\ncomprehensive global dataset for remote sensing image copy-move question\nanswering, named RS-CMQA-2.1M. These images were collected from 29 different\nregions across 14 countries. Additionally, we have refined a balanced dataset,\nRS-CMQA-B, to address the long-standing issue of long-tail data in the remote\nsensing field. Furthermore, we propose a region-discriminative guided\nmultimodal CMQA model, which enhances the accuracy of answering questions about\ntampered images by leveraging prompt about the differences and connections\nbetween the source and tampered domains. Extensive experiments demonstrate that\nour method provides a stronger benchmark for RS-CMQA compared to general VQA\nand RSVQA models. Our dataset and code are available at\nhttps://github.com/shenyedepisa/RSCMQA."
                },
                "authors": [
                    {
                        "name": "Ze Zhang"
                    },
                    {
                        "name": "Enyuan Zhao"
                    },
                    {
                        "name": "Ziyi Wan"
                    },
                    {
                        "name": "Jie Nie"
                    },
                    {
                        "name": "Xinyue Liang"
                    },
                    {
                        "name": "Lei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Huang"
                },
                "author": "Lei Huang",
                "arxiv_comment": "7 figs, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v2",
                "updated": "2024-12-03T16:55:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    55,
                    24,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02565v1",
                "updated": "2024-12-03T16:53:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    53,
                    58,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:53:58Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    53,
                    58,
                    1,
                    338,
                    0
                ],
                "title": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SJTU:Spatial judgments in multimodal models towards unified segmentation\n  through coordinate detection"
                },
                "summary": "Despite advances in vision-language understanding, implementing image\nsegmentation within multimodal architectures remains a fundamental challenge in\nmodern artificial intelligence systems. Existing vision-language models, which\nprimarily rely on backbone architectures or CLIP-based embedding learning,\ndemonstrate inherent limitations in fine-grained spatial localization and\noperational capabilities. This paper introduces SJTU: Spatial Judgments in\nmultimodal models - Towards Unified segmentation through coordinate detection,\na novel framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework proposes a\nnovel approach for integrating segmentation techniques with vision-language\nmodels based on multimodal spatial inference. By leveraging normalized\ncoordinate detection for bounding boxes and translating it into actionable\nsegmentation outputs, we explore the possibility of integrating multimodal\nspatial and language representations. Based on the proposed technical approach,\nthe framework demonstrates superior performance on various benchmark datasets\nas well as accurate object segmentation. Results on the COCO 2017 dataset for\ngeneral object detection and Pascal VOC datasets for semantic segmentation\ndemonstrate the generalization capabilities of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in vision-language understanding, implementing image\nsegmentation within multimodal architectures remains a fundamental challenge in\nmodern artificial intelligence systems. Existing vision-language models, which\nprimarily rely on backbone architectures or CLIP-based embedding learning,\ndemonstrate inherent limitations in fine-grained spatial localization and\noperational capabilities. This paper introduces SJTU: Spatial Judgments in\nmultimodal models - Towards Unified segmentation through coordinate detection,\na novel framework that leverages spatial coordinate understanding to bridge\nvision-language interaction and precise segmentation, enabling accurate target\nidentification through natural language instructions. The framework proposes a\nnovel approach for integrating segmentation techniques with vision-language\nmodels based on multimodal spatial inference. By leveraging normalized\ncoordinate detection for bounding boxes and translating it into actionable\nsegmentation outputs, we explore the possibility of integrating multimodal\nspatial and language representations. Based on the proposed technical approach,\nthe framework demonstrates superior performance on various benchmark datasets\nas well as accurate object segmentation. Results on the COCO 2017 dataset for\ngeneral object detection and Pascal VOC datasets for semantic segmentation\ndemonstrate the generalization capabilities of the framework."
                },
                "authors": [
                    {
                        "name": "Joongwon Chae"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Peiwu Qin"
                    }
                ],
                "author_detail": {
                    "name": "Peiwu Qin"
                },
                "author": "Peiwu Qin",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02563v1",
                "updated": "2024-12-03T16:52:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:52:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "Semantic Tokens in Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Tokens in Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered\nsignificant attention for their ability to improve truth grounding and\ncoherence in natural language processing tasks. However, the reliability of RAG\nsystems in producing accurate answers diminishes as the volume of data they\naccess increases. Even with smaller datasets, these systems occasionally fail\nto address simple queries. This issue arises from their dependence on\nstate-of-the-art large language models (LLMs), which can introduce uncertainty\ninto the system's outputs. In this work, I propose a novel Comparative RAG\nsystem that introduces an evaluator module to bridge the gap between\nprobabilistic RAG systems and deterministically verifiable responses. The\nevaluator compares external recommendations with the retrieved document chunks,\nadding a decision-making layer that enhances the system's reliability. This\napproach ensures that the chunks retrieved are both semantically relevant and\nlogically consistent with deterministic insights, thereby improving the\naccuracy and overall efficiency of RAG systems. This framework paves the way\nfor more reliable and scalable question-answering applications in domains\nrequiring high precision and verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) architectures have recently garnered\nsignificant attention for their ability to improve truth grounding and\ncoherence in natural language processing tasks. However, the reliability of RAG\nsystems in producing accurate answers diminishes as the volume of data they\naccess increases. Even with smaller datasets, these systems occasionally fail\nto address simple queries. This issue arises from their dependence on\nstate-of-the-art large language models (LLMs), which can introduce uncertainty\ninto the system's outputs. In this work, I propose a novel Comparative RAG\nsystem that introduces an evaluator module to bridge the gap between\nprobabilistic RAG systems and deterministically verifiable responses. The\nevaluator compares external recommendations with the retrieved document chunks,\nadding a decision-making layer that enhances the system's reliability. This\napproach ensures that the chunks retrieved are both semantically relevant and\nlogically consistent with deterministic insights, thereby improving the\naccuracy and overall efficiency of RAG systems. This framework paves the way\nfor more reliable and scalable question-answering applications in domains\nrequiring high precision and verifiability."
                },
                "authors": [
                    {
                        "name": "Joel Suro"
                    }
                ],
                "author_detail": {
                    "name": "Joel Suro"
                },
                "author": "Joel Suro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02549v1",
                "updated": "2024-12-03T16:43:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    43,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:43:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    43,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "Patent-CR: A Dataset for Patent Claim Revision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent-CR: A Dataset for Patent Claim Revision"
                },
                "summary": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Pascal A Scherz"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_comment": "15 pages, 6 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v2",
                "updated": "2024-12-03T16:37:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    37,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Wenyi Zhao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02540v1",
                "updated": "2024-12-03T16:33:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    33,
                    17,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:33:17Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    33,
                    17,
                    1,
                    338,
                    0
                ],
                "title": "Automatic State Machine Inference for Binary Protocol Reverse\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic State Machine Inference for Binary Protocol Reverse\n  Engineering"
                },
                "summary": "Protocol Reverse Engineering (PRE) is used to analyze protocols by inferring\ntheir structure and behavior. However, current PRE methods mainly focus on\nfield identification within a single protocol and neglect Protocol State\nMachine (PSM) analysis in mixed protocol environments. This results in\ninsufficient analysis of protocols' abnormal behavior and potential\nvulnerabilities, which are crucial for detecting and defending against new\nattack patterns. To address these challenges, we propose an automatic PSM\ninference framework for unknown protocols, including a fuzzy membership-based\nauto-converging DBSCAN algorithm for protocol format clustering, followed by a\nsession clustering algorithm based on Needleman-Wunsch and K-Medoids algorithms\nto classify sessions by protocol type. Finally, we refine a probabilistic PSM\nalgorithm to infer protocol states and the transition conditions between these\nstates. Experimental results show that, compared with existing PRE techniques,\nour method can infer PSMs while enabling more precise classification of\nprotocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protocol Reverse Engineering (PRE) is used to analyze protocols by inferring\ntheir structure and behavior. However, current PRE methods mainly focus on\nfield identification within a single protocol and neglect Protocol State\nMachine (PSM) analysis in mixed protocol environments. This results in\ninsufficient analysis of protocols' abnormal behavior and potential\nvulnerabilities, which are crucial for detecting and defending against new\nattack patterns. To address these challenges, we propose an automatic PSM\ninference framework for unknown protocols, including a fuzzy membership-based\nauto-converging DBSCAN algorithm for protocol format clustering, followed by a\nsession clustering algorithm based on Needleman-Wunsch and K-Medoids algorithms\nto classify sessions by protocol type. Finally, we refine a probabilistic PSM\nalgorithm to infer protocol states and the transition conditions between these\nstates. Experimental results show that, compared with existing PRE techniques,\nour method can infer PSMs while enabling more precise classification of\nprotocols."
                },
                "authors": [
                    {
                        "name": "Junhai Yang"
                    },
                    {
                        "name": "Fenghua Li"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Junhao Zhang"
                    },
                    {
                        "name": "Liang Fang"
                    },
                    {
                        "name": "Yunchuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yunchuan Guo"
                },
                "author": "Yunchuan Guo",
                "arxiv_comment": "4 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02525v1",
                "updated": "2024-12-03T16:18:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:18:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "LLMForecaster: Improving Seasonal Event Forecasts with Unstructured\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMForecaster: Improving Seasonal Event Forecasts with Unstructured\n  Textual Data"
                },
                "summary": "Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Chuck Arvin"
                    },
                    {
                        "name": "Dmitry Efimov"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Dominique Perrault-Joncas"
                    },
                    {
                        "name": "Shankar Ramasubramanian"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    },
                    {
                        "name": "Malcolm Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Malcolm Wolff"
                },
                "author": "Malcolm Wolff",
                "arxiv_comment": "Presented at NeurIPS Time Series in the Age of Large Models (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16019v2",
                "updated": "2024-12-03T16:18:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-04-24T17:51:36Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    17,
                    51,
                    36,
                    2,
                    115,
                    0
                ],
                "title": "The PRISM Alignment Dataset: What Participatory, Representative and\n  Individualised Human Feedback Reveals About the Subjective and Multicultural\n  Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PRISM Alignment Dataset: What Participatory, Representative and\n  Individualised Human Feedback Reveals About the Subjective and Multicultural\n  Alignment of Large Language Models"
                },
                "summary": "Human feedback is central to the alignment of Large Language Models (LLMs).\nHowever, open questions remain about methods (how), domains (where), people\n(who) and objectives (to what end) of feedback processes. To navigate these\nquestions, we introduce PRISM, a dataset that maps the sociodemographics and\nstated preferences of 1,500 diverse participants from 75 countries, to their\ncontextual preferences and fine-grained feedback in 8,011 live conversations\nwith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic\nparticipation in feedback; (ii) census-representative samples for two countries\n(UK, US); and (iii) individualised ratings that link to detailed participant\nprofiles, permitting personalisation and attribution of sample artefacts. We\ntarget subjective and multicultural perspectives on value-laden and\ncontroversial issues, where we expect interpersonal and cross-cultural\ndisagreement. We use PRISM in three case studies to demonstrate the need for\ncareful consideration of which humans provide what alignment data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback is central to the alignment of Large Language Models (LLMs).\nHowever, open questions remain about methods (how), domains (where), people\n(who) and objectives (to what end) of feedback processes. To navigate these\nquestions, we introduce PRISM, a dataset that maps the sociodemographics and\nstated preferences of 1,500 diverse participants from 75 countries, to their\ncontextual preferences and fine-grained feedback in 8,011 live conversations\nwith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic\nparticipation in feedback; (ii) census-representative samples for two countries\n(UK, US); and (iii) individualised ratings that link to detailed participant\nprofiles, permitting personalisation and attribution of sample artefacts. We\ntarget subjective and multicultural perspectives on value-laden and\ncontroversial issues, where we expect interpersonal and cross-cultural\ndisagreement. We use PRISM in three case studies to demonstrate the need for\ncareful consideration of which humans provide what alignment data."
                },
                "authors": [
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Alexander Whitefield"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Andrew Bean"
                    },
                    {
                        "name": "Katerina Margatina"
                    },
                    {
                        "name": "Juan Ciro"
                    },
                    {
                        "name": "Rafael Mosquera"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Scott A. Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A. Hale"
                },
                "author": "Scott A. Hale",
                "arxiv_journal_ref": "The Thirty-eight Conference on Neural Information Processing\n  Systems Datasets and Benchmarks Track (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12317v2",
                "updated": "2024-12-03T15:56:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    15,
                    56,
                    26,
                    1,
                    338,
                    0
                ],
                "published": "2024-02-19T17:37:28Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    17,
                    37,
                    28,
                    0,
                    50,
                    0
                ],
                "title": "EVOR: Evolving Retrieval for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOR: Evolving Retrieval for Code Generation"
                },
                "summary": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io."
                },
                "authors": [
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yuhang Lai"
                    },
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Boao Shi"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Yu"
                },
                "author": "Tao Yu",
                "arxiv_comment": "Retrieval-augmented code generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18644v2",
                "updated": "2024-12-03T15:22:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    15,
                    22,
                    38,
                    1,
                    338,
                    0
                ],
                "published": "2023-11-30T15:53:02Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    15,
                    53,
                    2,
                    3,
                    334,
                    0
                ],
                "title": "Exploring the hierarchical structure of human plans via program\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the hierarchical structure of human plans via program\n  generation"
                },
                "summary": "Human behavior is often assumed to be hierarchically structured, made up of\nabstract actions that can be decomposed into concrete actions. However,\nbehavior is typically measured as a sequence of actions, which makes it\ndifficult to infer its hierarchical structure. In this paper, we explore how\npeople form hierarchically structured plans, using an experimental paradigm\nwith observable hierarchical representations: participants create programs that\nproduce sequences of actions in a language with explicit hierarchical\nstructure. This task lets us test two well-established principles of human\nbehavior: utility maximization (i.e. using fewer actions) and minimum\ndescription length (MDL; i.e. having a shorter program). We find that humans\nare sensitive to both metrics, but that both accounts fail to predict a\nqualitative feature of human-created programs, namely that people prefer\nprograms with reuse over and above the predictions of MDL. We formalize this\npreference for reuse by extending the MDL account into a generative model over\nprograms, modeling hierarchy choice as the induction of a grammar over actions.\nOur account can explain the preference for reuse and provides better\npredictions of human behavior, going beyond simple accounts of compressibility\nto highlight a principle that guides hierarchical planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human behavior is often assumed to be hierarchically structured, made up of\nabstract actions that can be decomposed into concrete actions. However,\nbehavior is typically measured as a sequence of actions, which makes it\ndifficult to infer its hierarchical structure. In this paper, we explore how\npeople form hierarchically structured plans, using an experimental paradigm\nwith observable hierarchical representations: participants create programs that\nproduce sequences of actions in a language with explicit hierarchical\nstructure. This task lets us test two well-established principles of human\nbehavior: utility maximization (i.e. using fewer actions) and minimum\ndescription length (MDL; i.e. having a shorter program). We find that humans\nare sensitive to both metrics, but that both accounts fail to predict a\nqualitative feature of human-created programs, namely that people prefer\nprograms with reuse over and above the predictions of MDL. We formalize this\npreference for reuse by extending the MDL account into a generative model over\nprograms, modeling hierarchy choice as the induction of a grammar over actions.\nOur account can explain the preference for reuse and provides better\npredictions of human behavior, going beyond simple accounts of compressibility\nto highlight a principle that guides hierarchical planning."
                },
                "authors": [
                    {
                        "name": "Carlos G. Correa"
                    },
                    {
                        "name": "Sophia Sanborn"
                    },
                    {
                        "name": "Mark K. Ho"
                    },
                    {
                        "name": "Frederick Callaway"
                    },
                    {
                        "name": "Nathaniel D. Daw"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "arxiv_doi": "10.1016/j.cognition.2024.105990",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.cognition.2024.105990",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.18644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00710v3",
                "updated": "2024-12-03T14:45:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    45,
                    3,
                    1,
                    338,
                    0
                ],
                "published": "2023-12-01T16:42:57Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    16,
                    42,
                    57,
                    4,
                    335,
                    0
                ],
                "title": "SpaCE: The Spatial Confounding Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpaCE: The Spatial Confounding Environment"
                },
                "summary": "Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions."
                },
                "authors": [
                    {
                        "name": "Mauricio Tec"
                    },
                    {
                        "name": "Ana Trisovic"
                    },
                    {
                        "name": "Michelle Audirac"
                    },
                    {
                        "name": "Sophie Woodward"
                    },
                    {
                        "name": "Jie Kate Hu"
                    },
                    {
                        "name": "Naeem Khoshnevis"
                    },
                    {
                        "name": "Francesca Dominici"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Dominici"
                },
                "author": "Francesca Dominici",
                "arxiv_journal_ref": "Published at ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v4",
                "updated": "2024-12-03T14:31:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    31,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning."
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v2",
                "updated": "2024-12-03T14:17:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    17,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02467v1",
                "updated": "2024-12-03T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators"
                },
                "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose \\ours, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose \\ours, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."
                },
                "authors": [
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; G.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02454v1",
                "updated": "2024-12-03T13:43:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    43,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T13:43:36Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    43,
                    36,
                    1,
                    338,
                    0
                ],
                "title": "Gracefully Filtering Backdoor Samples for Generative Large Language\n  Models without Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gracefully Filtering Backdoor Samples for Generative Large Language\n  Models without Retraining"
                },
                "summary": "Backdoor attacks remain significant security threats to generative large\nlanguage models (LLMs). Since generative LLMs output sequences of\nhigh-dimensional token logits instead of low-dimensional classification logits,\nmost existing backdoor defense methods designed for discriminative models like\nBERT are ineffective for generative LLMs. Inspired by the observed differences\nin learning behavior between backdoor and clean mapping in the frequency space,\nwe transform gradients of each training sample, directly influencing parameter\nupdates, into the frequency space. Our findings reveal a distinct separation\nbetween the gradients of backdoor and clean samples in the frequency space.\nBased on this phenomenon, we propose Gradient Clustering in the Frequency Space\nfor Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients\nin the frequency space to effectively identify backdoor samples without\nrequiring retraining LLMs. Experimental results show that GraCeFul outperforms\nbaselines significantly. Notably, GraCeFul exhibits remarkable computational\nefficiency, achieving nearly 100% recall and F1 scores in identifying backdoor\nsamples, reducing the average success rate of various backdoor attacks to 0%\nwith negligible drops in clean accuracy across multiple free-style question\nanswering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna.\nThe codes are publicly available at https://github.com/ZrW00/GraceFul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks remain significant security threats to generative large\nlanguage models (LLMs). Since generative LLMs output sequences of\nhigh-dimensional token logits instead of low-dimensional classification logits,\nmost existing backdoor defense methods designed for discriminative models like\nBERT are ineffective for generative LLMs. Inspired by the observed differences\nin learning behavior between backdoor and clean mapping in the frequency space,\nwe transform gradients of each training sample, directly influencing parameter\nupdates, into the frequency space. Our findings reveal a distinct separation\nbetween the gradients of backdoor and clean samples in the frequency space.\nBased on this phenomenon, we propose Gradient Clustering in the Frequency Space\nfor Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients\nin the frequency space to effectively identify backdoor samples without\nrequiring retraining LLMs. Experimental results show that GraCeFul outperforms\nbaselines significantly. Notably, GraCeFul exhibits remarkable computational\nefficiency, achieving nearly 100% recall and F1 scores in identifying backdoor\nsamples, reducing the average success rate of various backdoor attacks to 0%\nwith negligible drops in clean accuracy across multiple free-style question\nanswering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna.\nThe codes are publicly available at https://github.com/ZrW00/GraceFul."
                },
                "authors": [
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Lingyong Fang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02441v1",
                "updated": "2024-12-03T13:25:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    25,
                    18,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T13:25:18Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    25,
                    18,
                    1,
                    338,
                    0
                ],
                "title": "Artificial Expert Intelligence through PAC-reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Expert Intelligence through PAC-reasoning"
                },
                "summary": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of\nboth Artificial General Intelligence (AGI) and narrow AI by integrating\ndomain-specific expertise with critical, precise reasoning capabilities akin to\nthose of top human experts. Existing AI systems often excel at predefined tasks\nbut struggle with adaptability and precision in novel problem-solving. To\novercome this, AEI introduces a framework for ``Probably Approximately Correct\n(PAC) Reasoning\". This paradigm provides robust theoretical guarantees for\nreliably decomposing complex problems, with a practical mechanism for\ncontrolling reasoning precision. In reference to the division of human thought\ninto System 1 for intuitive thinking and System 2 for reflective\nreasoning~\\citep{tversky1974judgment}, we refer to this new type of reasoning\nas System 3 for precise reasoning, inspired by the rigor of the scientific\nmethod. AEI thus establishes a foundation for error-bounded, inference-time\nlearning."
                },
                "authors": [
                    {
                        "name": "Shai Shalev-Shwartz"
                    },
                    {
                        "name": "Amnon Shashua"
                    },
                    {
                        "name": "Gal Beniamini"
                    },
                    {
                        "name": "Yoav Levine"
                    },
                    {
                        "name": "Or Sharir"
                    },
                    {
                        "name": "Noam Wies"
                    },
                    {
                        "name": "Ido Ben-Shaul"
                    },
                    {
                        "name": "Tomer Nussbaum"
                    },
                    {
                        "name": "Shir Granot Peled"
                    }
                ],
                "author_detail": {
                    "name": "Shir Granot Peled"
                },
                "author": "Shir Granot Peled",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02439v1",
                "updated": "2024-12-03T13:21:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    21,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T13:21:09Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    21,
                    9,
                    1,
                    338,
                    0
                ],
                "title": "Nature versus nurture in galaxy formation: the effect of environment on\n  star formation with causal machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature versus nurture in galaxy formation: the effect of environment on\n  star formation with causal machine learning"
                },
                "summary": "Understanding how galaxies form and evolve is at the heart of modern\nastronomy. With the advent of large-scale surveys and simulations, remarkable\nprogress has been made in the last few decades. Despite this, the physical\nprocesses behind the phenomena, and particularly their importance, remain far\nfrom known, as correlations have primarily been established rather than the\nunderlying causality. We address this challenge by applying the causal\ninference framework. Specifically, we tackle the fundamental open question of\nwhether galaxy formation and evolution depends more on nature (i.e., internal\nprocesses) or nurture (i.e., external processes), by estimating the causal\neffect of environment on star-formation rate in the IllustrisTNG simulations.\nTo do so, we develop a comprehensive causal model and employ cutting-edge\ntechniques from epidemiology to overcome the long-standing problem of\ndisentangling nature and nurture. We find that the causal effect is negative\nand substantial, with environment suppressing the SFR by a maximal factor of\n$\\sim100$. While the overall effect at $z=0$ is negative, in the early\nuniverse, environment is discovered to have a positive impact, boosting star\nformation by a factor of $\\sim10$ at $z\\sim1$ and by even greater amounts at\nhigher redshifts. Furthermore, we show that: (i) nature also plays an important\nrole, as ignoring it underestimates the causal effect in intermediate-density\nenvironments by a factor of $\\sim2$, (ii) controlling for the stellar mass at a\nsnapshot in time, as is common in the literature, is not only insufficient to\ndisentangle nature and nurture but actually has an adverse effect, though (iii)\nstellar mass is an adequate proxy of the effects of nature. Finally, this work\nmay prove a useful blueprint for extracting causal insights in other fields\nthat deal with dynamical systems with closed feedback loops, such as the\nEarth's climate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how galaxies form and evolve is at the heart of modern\nastronomy. With the advent of large-scale surveys and simulations, remarkable\nprogress has been made in the last few decades. Despite this, the physical\nprocesses behind the phenomena, and particularly their importance, remain far\nfrom known, as correlations have primarily been established rather than the\nunderlying causality. We address this challenge by applying the causal\ninference framework. Specifically, we tackle the fundamental open question of\nwhether galaxy formation and evolution depends more on nature (i.e., internal\nprocesses) or nurture (i.e., external processes), by estimating the causal\neffect of environment on star-formation rate in the IllustrisTNG simulations.\nTo do so, we develop a comprehensive causal model and employ cutting-edge\ntechniques from epidemiology to overcome the long-standing problem of\ndisentangling nature and nurture. We find that the causal effect is negative\nand substantial, with environment suppressing the SFR by a maximal factor of\n$\\sim100$. While the overall effect at $z=0$ is negative, in the early\nuniverse, environment is discovered to have a positive impact, boosting star\nformation by a factor of $\\sim10$ at $z\\sim1$ and by even greater amounts at\nhigher redshifts. Furthermore, we show that: (i) nature also plays an important\nrole, as ignoring it underestimates the causal effect in intermediate-density\nenvironments by a factor of $\\sim2$, (ii) controlling for the stellar mass at a\nsnapshot in time, as is common in the literature, is not only insufficient to\ndisentangle nature and nurture but actually has an adverse effect, though (iii)\nstellar mass is an adequate proxy of the effects of nature. Finally, this work\nmay prove a useful blueprint for extracting causal insights in other fields\nthat deal with dynamical systems with closed feedback loops, such as the\nEarth's climate."
                },
                "authors": [
                    {
                        "name": "Sunil Mucesh"
                    },
                    {
                        "name": "William G. Hartley"
                    },
                    {
                        "name": "Ciarán M. Gilligan-Lee"
                    },
                    {
                        "name": "Ofer Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ofer Lahav"
                },
                "author": "Ofer Lahav",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02437v1",
                "updated": "2024-12-03T13:19:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    19,
                    21,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T13:19:21Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    19,
                    21,
                    1,
                    338,
                    0
                ],
                "title": "Reproduction of AdEx dynamics on neuromorphic hardware through data\n  embedding and simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproduction of AdEx dynamics on neuromorphic hardware through data\n  embedding and simulation-based inference"
                },
                "summary": "The development of mechanistic models of physical systems is essential for\nunderstanding their behavior and formulating predictions that can be validated\nexperimentally. Calibration of these models, especially for complex systems,\nrequires automated optimization methods due to the impracticality of manual\nparameter tuning. In this study, we use an autoencoder to automatically extract\nrelevant features from the membrane trace of a complex neuron model emulated on\nthe BrainScaleS-2 neuromorphic system, and subsequently leverage sequential\nneural posterior estimation (SNPE), a simulation-based inference algorithm, to\napproximate the posterior distribution of neuron parameters. Our results\ndemonstrate that the autoencoder is able to extract essential features from the\nobserved membrane traces, with which the SNPE algorithm is able to find an\napproximation of the posterior distribution. This suggests that the combination\nof an autoencoder with the SNPE algorithm is a promising optimization method\nfor complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of mechanistic models of physical systems is essential for\nunderstanding their behavior and formulating predictions that can be validated\nexperimentally. Calibration of these models, especially for complex systems,\nrequires automated optimization methods due to the impracticality of manual\nparameter tuning. In this study, we use an autoencoder to automatically extract\nrelevant features from the membrane trace of a complex neuron model emulated on\nthe BrainScaleS-2 neuromorphic system, and subsequently leverage sequential\nneural posterior estimation (SNPE), a simulation-based inference algorithm, to\napproximate the posterior distribution of neuron parameters. Our results\ndemonstrate that the autoencoder is able to extract essential features from the\nobserved membrane traces, with which the SNPE algorithm is able to find an\napproximation of the posterior distribution. This suggests that the combination\nof an autoencoder with the SNPE algorithm is a promising optimization method\nfor complex systems."
                },
                "authors": [
                    {
                        "name": "Jakob Huhle"
                    },
                    {
                        "name": "Jakob Kaiser"
                    },
                    {
                        "name": "Eric Müller"
                    },
                    {
                        "name": "Johannes Schemmel"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Schemmel"
                },
                "author": "Johannes Schemmel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02427v1",
                "updated": "2024-12-03T12:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    46,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T12:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    46,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "GerPS-Compare: Comparing NER methods for legal norm analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GerPS-Compare: Comparing NER methods for legal norm analysis"
                },
                "summary": "We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems."
                },
                "authors": [
                    {
                        "name": "Sarah T. Bachinger"
                    },
                    {
                        "name": "Christoph Unger"
                    },
                    {
                        "name": "Robin Erd"
                    },
                    {
                        "name": "Leila Feddoul"
                    },
                    {
                        "name": "Clara Lachenmaier"
                    },
                    {
                        "name": "Sina Zarrieß"
                    },
                    {
                        "name": "Birgitta König-Ries"
                    }
                ],
                "author_detail": {
                    "name": "Birgitta König-Ries"
                },
                "author": "Birgitta König-Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02410v1",
                "updated": "2024-12-03T12:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    5,
                    56,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T12:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    5,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "A Multi-Agent Framework for Extensible Structured Text Generation in\n  PLCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Framework for Extensible Structured Text Generation in\n  PLCs"
                },
                "summary": "Programmable Logic Controllers (PLCs) are microcomputers essential for\nautomating factory operations. Structured Text (ST), a high-level language\nadhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to\nexpress logic succinctly and to seamlessly integrate with other languages\nwithin the same standard. However, vendors develop their own customized\nversions of ST, and the lack of comprehensive and standardized documentation\nfor the full semantics of ST has contributed to inconsistencies in how the\nlanguage is implemented. Consequently, the steep learning curve associated with\nST, combined with ever-evolving industrial requirements, presents significant\nchallenges for developers. In response to these issues, we present AutoPLC, an\nLLM-based approach designed to automate the generation of vendor-specific ST\ncode. To facilitate effective code generation, we first built a comprehensive\nknowledge base, including Rq2ST Case Library (requirements and corresponding\nimplementations) and Instruction libraries. Then we developed a retrieval\nmodule to incorporate the domain-specific knowledge by identifying pertinent\ncases and instructions, guiding the LLM to generate code that meets the\nrequirements. In order to verify and improve the quality of the generated code,\nwe designed an adaptable code checker. If errors are detected, we initiate an\niterative self-improvement process to instruct the LLM to revise the generated\ncode. We evaluate AutoPLC's performance against seven state-of-the-art\nbaselines using three benchmarks, one for open-source basic ST and two for\ncommercial Structured Control Language (SCL) from Siemens. The results show\nthat our approach consistently achieves superior performance across all\nbenchmarks. Ablation study emphasizes the significance of our modules. Further\nmanual analysis confirm the practical utility of the ST code generated by\nAutoPLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmable Logic Controllers (PLCs) are microcomputers essential for\nautomating factory operations. Structured Text (ST), a high-level language\nadhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to\nexpress logic succinctly and to seamlessly integrate with other languages\nwithin the same standard. However, vendors develop their own customized\nversions of ST, and the lack of comprehensive and standardized documentation\nfor the full semantics of ST has contributed to inconsistencies in how the\nlanguage is implemented. Consequently, the steep learning curve associated with\nST, combined with ever-evolving industrial requirements, presents significant\nchallenges for developers. In response to these issues, we present AutoPLC, an\nLLM-based approach designed to automate the generation of vendor-specific ST\ncode. To facilitate effective code generation, we first built a comprehensive\nknowledge base, including Rq2ST Case Library (requirements and corresponding\nimplementations) and Instruction libraries. Then we developed a retrieval\nmodule to incorporate the domain-specific knowledge by identifying pertinent\ncases and instructions, guiding the LLM to generate code that meets the\nrequirements. In order to verify and improve the quality of the generated code,\nwe designed an adaptable code checker. If errors are detected, we initiate an\niterative self-improvement process to instruct the LLM to revise the generated\ncode. We evaluate AutoPLC's performance against seven state-of-the-art\nbaselines using three benchmarks, one for open-source basic ST and two for\ncommercial Structured Control Language (SCL) from Siemens. The results show\nthat our approach consistently achieves superior performance across all\nbenchmarks. Ablation study emphasizes the significance of our modules. Further\nmanual analysis confirm the practical utility of the ST code generated by\nAutoPLC."
                },
                "authors": [
                    {
                        "name": "Donghao Yang"
                    },
                    {
                        "name": "Aolang Wu"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Yuming Ren"
                    },
                    {
                        "name": "Jiaji Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jiaji Tian"
                },
                "author": "Jiaji Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11373v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11373v3",
                "updated": "2024-12-03T11:03:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    11,
                    3,
                    28,
                    1,
                    338,
                    0
                ],
                "published": "2024-04-17T13:31:51Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    13,
                    31,
                    51,
                    2,
                    108,
                    0
                ],
                "title": "Simulation-based inference of black hole ringdowns in the time domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference of black hole ringdowns in the time domain"
                },
                "summary": "Gravitational waves emitted by a ringing black hole allow us to perform\nprecision tests of general relativity in the strong field regime. With\nimprovements to our current gravitational wave detectors and upcoming\nnext-generation detectors, developing likelihood-free parameter inference\ninfrastructure is critical as we will face complications like nonstandard noise\nproperties, partial data and incomplete signal modeling that may not allow for\nan analytically tractable likelihood function. In this work, we present a\nproof-of-concept strategy to perform likelihood-free Bayesian inference on\nringdown gravitational waves using simulation based inference. Specifically,\nour method is based on truncated sequential neural posterior estimation, which\ntrains a neural density estimator of the posterior for a specific observed data\nsegment. We setup the ringdown parameter estimation directly in the time\ndomain. We show that the parameter estimation results obtained using our\ntrained networks are in agreement with well-established Markov-chain methods\nfor simulated injections as well as analysis on real detector data\ncorresponding to GW150914. Additionally, to assess our approach's internal\nconsistency, we show that the density estimators pass a Bayesian coverage test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves emitted by a ringing black hole allow us to perform\nprecision tests of general relativity in the strong field regime. With\nimprovements to our current gravitational wave detectors and upcoming\nnext-generation detectors, developing likelihood-free parameter inference\ninfrastructure is critical as we will face complications like nonstandard noise\nproperties, partial data and incomplete signal modeling that may not allow for\nan analytically tractable likelihood function. In this work, we present a\nproof-of-concept strategy to perform likelihood-free Bayesian inference on\nringdown gravitational waves using simulation based inference. Specifically,\nour method is based on truncated sequential neural posterior estimation, which\ntrains a neural density estimator of the posterior for a specific observed data\nsegment. We setup the ringdown parameter estimation directly in the time\ndomain. We show that the parameter estimation results obtained using our\ntrained networks are in agreement with well-established Markov-chain methods\nfor simulated injections as well as analysis on real detector data\ncorresponding to GW150914. Additionally, to assess our approach's internal\nconsistency, we show that the density estimators pass a Bayesian coverage test."
                },
                "authors": [
                    {
                        "name": "Costantino Pacilio"
                    },
                    {
                        "name": "Swetha Bhagwat"
                    },
                    {
                        "name": "Roberto Cotesta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Cotesta"
                },
                "author": "Roberto Cotesta",
                "arxiv_doi": "10.1103/PhysRevD.110.083010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.083010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11373v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11373v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 7 figures; v2: minor changes, matches published version;\n  v3: metadata added",
                "arxiv_journal_ref": "Phys. Rev. D 110 (2024) no.8, 083010",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02372v1",
                "updated": "2024-12-03T10:58:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    58,
                    34,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:58:34Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    58,
                    34,
                    1,
                    338,
                    0
                ],
                "title": "HERO: Hint-Based Efficient and Reliable Query Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERO: Hint-Based Efficient and Reliable Query Optimizer"
                },
                "summary": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Sergey Iazov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Iazov"
                },
                "author": "Sergey Iazov",
                "arxiv_comment": "Submitted to VLDB 2025; 13 pages; 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02368v1",
                "updated": "2024-12-03T10:52:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:52:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?"
                },
                "summary": "Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts."
                },
                "authors": [
                    {
                        "name": "Leixin Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Yinjie Cheng"
                    },
                    {
                        "name": "Weihe Zhai"
                    },
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Christoph Leiter"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Fahimeh Moafian"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01623v2",
                "updated": "2024-12-03T10:47:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    47,
                    39,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T15:43:04Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    43,
                    4,
                    0,
                    337,
                    0
                ],
                "title": "Evolution of the UV slope of galaxies at cosmic morning (z > 4): the\n  properties of extremely blue galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of the UV slope of galaxies at cosmic morning (z > 4): the\n  properties of extremely blue galaxies"
                },
                "summary": "We present an analysis of the UV continuum slope, beta, using a sample of 733\ngalaxies selected from a mixture of JWST ERS/GTO/GO observational programs and\nwith z > 4. We consider spectroscopic data obtained with the low resolution\nPRISM/CLEAR NIRSpec configuration. Studying the correlation of beta with M_UV\nwe find a decreasing trend of beta = (-0.056 +- 0.017) M_UV - (3.01 +- 0.34),\nconsistent with brighter galaxies having redder beta as found in previous\nworks. However, analysing the trend in separate redshift bins, we find that at\nhigh redshift the relation becomes much flatter, consistent with a flat slope.\nFurthermore, we find that beta decreases with redshift with an evolution as\nbeta = (-0.075 +- 0.010) z - (1.496 +- 0.056), consistent with most previous\nresults that show a steepening of the spectra going at higher z. We then select\na sample of galaxies with extremely blue slopes (beta < -2.6): such slopes are\nsteeper than what is predicted by stellar evolution models, even for dust free,\nyoung, metal poor populations, when the contribution of nebular emission is\nincluded. We select 51 extremely blue galaxies (XBGs) and we investigate the\npossible physical origin of their steep slopes, comparing them to a sub-sample\nof redder galaxies (matched in redshift and M_UV). We find that XBGs have\nyounger stellar populations, stronger ionization fields, lower dust\nattenuation, and lower but not pristine metallicity (~ 10% solar) compared to\nred galaxies. However, these properties alone cannot explain the extreme beta\nvalues. By using indirect inference of Lyman continuum escape, using the most\nrecent models, we estimate escape fractions f_esc > 10% in at least 25% of\nXBGs, while all the red sources have smaller f_esc. A reduced nebular continuum\ncontribution as due to either a high escape fraction or to a bursty\nstar-formation history is likely the origin of the extremely blue slopes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of the UV continuum slope, beta, using a sample of 733\ngalaxies selected from a mixture of JWST ERS/GTO/GO observational programs and\nwith z > 4. We consider spectroscopic data obtained with the low resolution\nPRISM/CLEAR NIRSpec configuration. Studying the correlation of beta with M_UV\nwe find a decreasing trend of beta = (-0.056 +- 0.017) M_UV - (3.01 +- 0.34),\nconsistent with brighter galaxies having redder beta as found in previous\nworks. However, analysing the trend in separate redshift bins, we find that at\nhigh redshift the relation becomes much flatter, consistent with a flat slope.\nFurthermore, we find that beta decreases with redshift with an evolution as\nbeta = (-0.075 +- 0.010) z - (1.496 +- 0.056), consistent with most previous\nresults that show a steepening of the spectra going at higher z. We then select\na sample of galaxies with extremely blue slopes (beta < -2.6): such slopes are\nsteeper than what is predicted by stellar evolution models, even for dust free,\nyoung, metal poor populations, when the contribution of nebular emission is\nincluded. We select 51 extremely blue galaxies (XBGs) and we investigate the\npossible physical origin of their steep slopes, comparing them to a sub-sample\nof redder galaxies (matched in redshift and M_UV). We find that XBGs have\nyounger stellar populations, stronger ionization fields, lower dust\nattenuation, and lower but not pristine metallicity (~ 10% solar) compared to\nred galaxies. However, these properties alone cannot explain the extreme beta\nvalues. By using indirect inference of Lyman continuum escape, using the most\nrecent models, we estimate escape fractions f_esc > 10% in at least 25% of\nXBGs, while all the red sources have smaller f_esc. A reduced nebular continuum\ncontribution as due to either a high escape fraction or to a bursty\nstar-formation history is likely the origin of the extremely blue slopes."
                },
                "authors": [
                    {
                        "name": "D. Dottorini"
                    },
                    {
                        "name": "A. Calabrò"
                    },
                    {
                        "name": "L. Pentericci"
                    },
                    {
                        "name": "S. Mascia"
                    },
                    {
                        "name": "M. Llerena"
                    },
                    {
                        "name": "L. Napolitano"
                    },
                    {
                        "name": "P. Santini"
                    },
                    {
                        "name": "G. Roberts-Borsani"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "R. Amorín"
                    },
                    {
                        "name": "M. Dickinson"
                    },
                    {
                        "name": "A. Fontana"
                    },
                    {
                        "name": "N. Hathi"
                    },
                    {
                        "name": "M. Hirschmann"
                    },
                    {
                        "name": "A. Koekemoer"
                    },
                    {
                        "name": "R. A. Lucas"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "A. Morales"
                    },
                    {
                        "name": "F. Pacucci"
                    },
                    {
                        "name": "S. Wilkins"
                    },
                    {
                        "name": "P. Arrabal Haro"
                    },
                    {
                        "name": "M. Bagley"
                    },
                    {
                        "name": "S. Finkelstein"
                    },
                    {
                        "name": "J. Kartaltepe"
                    },
                    {
                        "name": "C. Papovich"
                    },
                    {
                        "name": "N. Pirzkal"
                    }
                ],
                "author_detail": {
                    "name": "N. Pirzkal"
                },
                "author": "N. Pirzkal",
                "arxiv_comment": "Submitted to A&A (12 pages, 6 figures, and 2 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02359v1",
                "updated": "2024-12-03T10:32:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    32,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:32:41Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    32,
                    41,
                    1,
                    338,
                    0
                ],
                "title": "Realistic Surgical Simulation from Monocular Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic Surgical Simulation from Monocular Videos"
                },
                "summary": "This paper tackles the challenge of automatically performing realistic\nsurgical simulations from readily available surgical videos. Recent efforts\nhave successfully integrated physically grounded dynamics within 3D Gaussians\nto perform high-fidelity simulations in well-reconstructed simulation\nenvironments from static scenes. However, they struggle with the geometric\ninconsistency in reconstructing simulation environments and unrealistic\nphysical deformations in simulations of soft tissues when it comes to dynamic\nand complex surgical processes. In this paper, we propose SurgiSim, a novel\nautomatic simulation system to overcome these limitations. To build a surgical\nsimulation environment, we maintain a canonical 3D scene composed of 3D\nGaussians coupled with a deformation field to represent a dynamic surgical\nscene. This process involves a multi-stage optimization with trajectory and\nanisotropic regularization, enhancing the geometry consistency of the canonical\nscene, which serves as the simulation environment. To achieve realistic\nphysical simulations in this environment, we implement a Visco-Elastic\ndeformation model based on the Maxwell model, effectively restoring the complex\ndeformations of tissues. Additionally, we infer the physical parameters of\ntissues by minimizing the discrepancies between the input video and simulation\nresults guided by estimated tissue motion, ensuring realistic simulation\noutcomes. Experiments on various surgical scenarios and interactions\ndemonstrate SurgiSim's ability to perform realistic simulation of soft tissues\namong surgical procedures, showing its enormous potential for enhancing\nsurgical training, planning, and robotic surgery systems. The project page is\nat https://namaenashibot.github.io/SurgiSim/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the challenge of automatically performing realistic\nsurgical simulations from readily available surgical videos. Recent efforts\nhave successfully integrated physically grounded dynamics within 3D Gaussians\nto perform high-fidelity simulations in well-reconstructed simulation\nenvironments from static scenes. However, they struggle with the geometric\ninconsistency in reconstructing simulation environments and unrealistic\nphysical deformations in simulations of soft tissues when it comes to dynamic\nand complex surgical processes. In this paper, we propose SurgiSim, a novel\nautomatic simulation system to overcome these limitations. To build a surgical\nsimulation environment, we maintain a canonical 3D scene composed of 3D\nGaussians coupled with a deformation field to represent a dynamic surgical\nscene. This process involves a multi-stage optimization with trajectory and\nanisotropic regularization, enhancing the geometry consistency of the canonical\nscene, which serves as the simulation environment. To achieve realistic\nphysical simulations in this environment, we implement a Visco-Elastic\ndeformation model based on the Maxwell model, effectively restoring the complex\ndeformations of tissues. Additionally, we infer the physical parameters of\ntissues by minimizing the discrepancies between the input video and simulation\nresults guided by estimated tissue motion, ensuring realistic simulation\noutcomes. Experiments on various surgical scenarios and interactions\ndemonstrate SurgiSim's ability to perform realistic simulation of soft tissues\namong surgical procedures, showing its enormous potential for enhancing\nsurgical training, planning, and robotic surgery systems. The project page is\nat https://namaenashibot.github.io/SurgiSim/."
                },
                "authors": [
                    {
                        "name": "Kailing Wang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Keyang Zhao"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Wei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shen"
                },
                "author": "Wei Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02344v1",
                "updated": "2024-12-03T10:04:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    4,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:04:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    4,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices"
                },
                "summary": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications"
                },
                "authors": [
                    {
                        "name": "Seul-Ki Yeom"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "13 Pages, 8 Tables, 7 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.05125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.05125v2",
                "updated": "2024-12-03T09:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    55,
                    18,
                    1,
                    338,
                    0
                ],
                "published": "2023-02-10T09:14:21Z",
                "published_parsed": [
                    2023,
                    2,
                    10,
                    9,
                    14,
                    21,
                    4,
                    41,
                    0
                ],
                "title": "Cosmic primordial density fluctuations and Bell inequalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic primordial density fluctuations and Bell inequalities"
                },
                "summary": "The temperature measurements, $T$, of the perturbed cosmic microwave\nbackground, performed by the cosmic background explorer satellite (COBE), are\nconsidered. A dichotomist function, $f = \\pm 1$, is defined such that $f =+1$\nif $\\delta T > 0$ and $f =-1$ if $\\delta T < 0$, where $\\delta T$ stands for\nthe fluctuation of $T$. Then, it is assumed that behind the appearance of these\nfluctuations there is local realism. Under this assumption, some specific\nClauser-Horne-Shimony-Holt (CHSH) inequalities are proved for these fluctuation\ntemperatures measured by COBE in the different sky directions. The calculation\nof these inequalities from the actual temperature measurements shows that these\ninequalities are not violated. This result cannot be anticipated by calculating\nthe commutators of the cosmic density quantum operators. This must be remarked\nhere since, in the case of a system of two entangled spin ${\\textstyle{1 \\over\n2}}$ particles, its CHSH inequalities violation can be inferred from the\nnonvanishing value of the corresponding spin measurement commutators. The above\nnonviolation of the observed cosmic CHSH inequalities is compatible with the\nexistence of local realism behind the cosmic measurement results. Nevertheless,\nassuming again local realism, some new cosmic CHSH inequalities can be derived\nfor the case of the WMAP measurements whose accuracy is better than the one of\nthe above considered COBE measurements. More specifically, in the WMAP case,\nsome significant cross correlations between the temperature and polarization\nmaps are detected, and the new cosmic CHSH inequalities are the ones built with\nthese cross correlations. Now, the occasional violation of these CHSH\ninequalities would mean the failure of the assumed local realism in accordance\nwith the quantum origin of the primordial temperature and polarization\nfluctuations in the framework of standard inflation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The temperature measurements, $T$, of the perturbed cosmic microwave\nbackground, performed by the cosmic background explorer satellite (COBE), are\nconsidered. A dichotomist function, $f = \\pm 1$, is defined such that $f =+1$\nif $\\delta T > 0$ and $f =-1$ if $\\delta T < 0$, where $\\delta T$ stands for\nthe fluctuation of $T$. Then, it is assumed that behind the appearance of these\nfluctuations there is local realism. Under this assumption, some specific\nClauser-Horne-Shimony-Holt (CHSH) inequalities are proved for these fluctuation\ntemperatures measured by COBE in the different sky directions. The calculation\nof these inequalities from the actual temperature measurements shows that these\ninequalities are not violated. This result cannot be anticipated by calculating\nthe commutators of the cosmic density quantum operators. This must be remarked\nhere since, in the case of a system of two entangled spin ${\\textstyle{1 \\over\n2}}$ particles, its CHSH inequalities violation can be inferred from the\nnonvanishing value of the corresponding spin measurement commutators. The above\nnonviolation of the observed cosmic CHSH inequalities is compatible with the\nexistence of local realism behind the cosmic measurement results. Nevertheless,\nassuming again local realism, some new cosmic CHSH inequalities can be derived\nfor the case of the WMAP measurements whose accuracy is better than the one of\nthe above considered COBE measurements. More specifically, in the WMAP case,\nsome significant cross correlations between the temperature and polarization\nmaps are detected, and the new cosmic CHSH inequalities are the ones built with\nthese cross correlations. Now, the occasional violation of these CHSH\ninequalities would mean the failure of the assumed local realism in accordance\nwith the quantum origin of the primordial temperature and polarization\nfluctuations in the framework of standard inflation."
                },
                "authors": [
                    {
                        "name": "Roberto Dale"
                    },
                    {
                        "name": "Ramon Lapiedra"
                    },
                    {
                        "name": "Juan Antonio Morales-Lladosa"
                    }
                ],
                "author_detail": {
                    "name": "Juan Antonio Morales-Lladosa"
                },
                "author": "Juan Antonio Morales-Lladosa",
                "arxiv_doi": "10.1103/PhysRevD.107.023506",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.107.023506",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2302.05125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.05125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 4 figures",
                "arxiv_journal_ref": "Phys. Rev. D 107, 023506 (2023)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04710v2",
                "updated": "2024-12-03T09:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    51,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-07T07:38:33Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    7,
                    38,
                    33,
                    4,
                    159,
                    0
                ],
                "title": "Morescient GAI for Software Engineering (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morescient GAI for Software Engineering (Extended Version)"
                },
                "summary": "The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science."
                },
                "authors": [
                    {
                        "name": "Marcus Kessel"
                    },
                    {
                        "name": "Colin Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Colin Atkinson"
                },
                "author": "Colin Atkinson",
                "arxiv_comment": "To appear in ACM Transactions on Software Engineering and\n  Methodology, Special Issue \"2030 Roadmap Software Engineering\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; I.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10583v2",
                "updated": "2024-12-03T09:43:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    43,
                    55,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-15T21:09:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    21,
                    9,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Personalization of Code Readability Evaluation Based on LLM Using\n  Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization of Code Readability Evaluation Based on LLM Using\n  Collaborative Filtering"
                },
                "summary": "Code readability is an important indicator of software maintenance as it can\nsignificantly impact maintenance efforts. Recently, LLM (large language models)\nhave been utilized for code readability evaluation. However, readability\nevaluation differs among developers, so personalization of the evaluation by\nLLM is needed. This study proposes a method which calibrates the evaluation,\nusing collaborative filtering. Our preliminary analysis suggested that the\nmethod effectively enhances the accuracy of the readability evaluation using\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code readability is an important indicator of software maintenance as it can\nsignificantly impact maintenance efforts. Recently, LLM (large language models)\nhave been utilized for code readability evaluation. However, readability\nevaluation differs among developers, so personalization of the evaluation by\nLLM is needed. This study proposes a method which calibrates the evaluation,\nusing collaborative filtering. Our preliminary analysis suggested that the\nmethod effectively enhances the accuracy of the readability evaluation using\nLLMs."
                },
                "authors": [
                    {
                        "name": "Buntaro Hiraki"
                    },
                    {
                        "name": "Kensei Hamamoto"
                    },
                    {
                        "name": "Ami Kimura"
                    },
                    {
                        "name": "Masateru Tsunoda"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Kwabena Ebo Bennin"
                    },
                    {
                        "name": "Akito Monden"
                    },
                    {
                        "name": "Keitaro Nakasai"
                    }
                ],
                "author_detail": {
                    "name": "Keitaro Nakasai"
                },
                "author": "Keitaro Nakasai",
                "arxiv_comment": "2 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11284v2",
                "updated": "2024-12-03T09:41:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    41,
                    22,
                    1,
                    338,
                    0
                ],
                "published": "2024-05-18T13:09:33Z",
                "published_parsed": [
                    2024,
                    5,
                    18,
                    13,
                    9,
                    33,
                    5,
                    139,
                    0
                ],
                "title": "The Logic of Counterfactuals and the Epistemology of Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Logic of Counterfactuals and the Epistemology of Causal Inference"
                },
                "summary": "The 2021 Nobel Prize in Economics recognizes a type of causal model known as\nthe Rubin causal model, or potential outcome framework, which deserves far more\nattention from philosophers than it currently receives. To spark philosophers'\ninterest, I develop a dialectic connecting the Rubin causal model to the\nLewis-Stalnaker debate on a logical principle of counterfactuals: Conditional\nExcluded Middle (CEM). I begin by playing good cop for CEM, developing a new\nargument in its favor -- a Quine-Putnam-style indispensability argument. This\nargument is based on the observation that CEM seems to be indispensable to the\nRubin causal model, which underpins our best scientific theory of causal\ninference in health and social sciences -- a Nobel Prize-winning theory.\nIndeed, CEM has long remained a core assumption of the Rubin causal model,\ndespite challenges from within the statistics and economics communities over\ntwenty years ago. I then switch sides to play bad cop for CEM, undermining the\nindispensability argument by developing a new theory of causal inference that\ndispenses with CEM while preserving the successes of the original theory\n(thanks to a new theorem proved here). The key, somewhat surprisingly, is to\nintegrate two approaches to causal modeling: the Rubin causal model, more\nfamiliar in health and social sciences, and the causal Bayes net, more familiar\nin philosophy. The good cop/bad cop dialectic is concluded with a connection to\nbroader philosophical issues, including intertheory relations, the revisability\nof logic, and the role of background assumptions in justifying scientific\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 2021 Nobel Prize in Economics recognizes a type of causal model known as\nthe Rubin causal model, or potential outcome framework, which deserves far more\nattention from philosophers than it currently receives. To spark philosophers'\ninterest, I develop a dialectic connecting the Rubin causal model to the\nLewis-Stalnaker debate on a logical principle of counterfactuals: Conditional\nExcluded Middle (CEM). I begin by playing good cop for CEM, developing a new\nargument in its favor -- a Quine-Putnam-style indispensability argument. This\nargument is based on the observation that CEM seems to be indispensable to the\nRubin causal model, which underpins our best scientific theory of causal\ninference in health and social sciences -- a Nobel Prize-winning theory.\nIndeed, CEM has long remained a core assumption of the Rubin causal model,\ndespite challenges from within the statistics and economics communities over\ntwenty years ago. I then switch sides to play bad cop for CEM, undermining the\nindispensability argument by developing a new theory of causal inference that\ndispenses with CEM while preserving the successes of the original theory\n(thanks to a new theorem proved here). The key, somewhat surprisingly, is to\nintegrate two approaches to causal modeling: the Rubin causal model, more\nfamiliar in health and social sciences, and the causal Bayes net, more familiar\nin philosophy. The good cop/bad cop dialectic is concluded with a connection to\nbroader philosophical issues, including intertheory relations, the revisability\nof logic, and the role of background assumptions in justifying scientific\ninference."
                },
                "authors": [
                    {
                        "name": "Hanti Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hanti Lin"
                },
                "author": "Hanti Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02311v1",
                "updated": "2024-12-03T09:28:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    28,
                    5,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T09:28:05Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    28,
                    5,
                    1,
                    338,
                    0
                ],
                "title": "Simulation-based inference has its own Dodelson-Schneider effect (but it\n  knows that it does)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference has its own Dodelson-Schneider effect (but it\n  knows that it does)"
                },
                "summary": "Making inferences about physical properties of the Universe requires\nknowledge of the data likelihood. A Gaussian distribution is commonly assumed\nfor the uncertainties with a covariance matrix estimated from a set of\nsimulations. The noise in such covariance estimates causes two problems: it\ndistorts the width of the parameter contours, and it adds scatter to the\nlocation of those contours which is not captured by the widths themselves. For\nnon-Gaussian likelihoods, an approximation may be derived via Simulation-Based\nInference (SBI). It is often implicitly assumed that parameter constraints from\nSBI analyses, which do not use covariance matrices, are not affected by the\nsame problems as parameter estimation with a covariance matrix estimated from\nsimulations. We investigate whether SBI suffers from effects similar to those\nof covariance estimation in Gaussian likelihoods. We use Neural Posterior and\nLikelihood Estimation with continuous and masked autoregressive normalizing\nflows for density estimation. We fit our approximate posterior models to\nsimulations drawn from a Gaussian linear model, so that the SBI result can be\ncompared to the true posterior. We test linear and neural network based\ncompression, demonstrating that neither methods circumvent the issues of\ncovariance estimation. SBI suffers an inflation of posterior variance that is\nequal or greater than the analytical result in covariance estimation for\nGaussian likelihoods for the same number of simulations. The assumption that\nSBI requires a smaller number of simulations than covariance estimation for a\nGaussian likelihood analysis is inaccurate. The limitations of traditional\nlikelihood analysis with simulation-based covariance remain for SBI with a\nfinite simulation budget. Despite these issues, we show that SBI correctly\ndraws the true posterior contour given enough simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making inferences about physical properties of the Universe requires\nknowledge of the data likelihood. A Gaussian distribution is commonly assumed\nfor the uncertainties with a covariance matrix estimated from a set of\nsimulations. The noise in such covariance estimates causes two problems: it\ndistorts the width of the parameter contours, and it adds scatter to the\nlocation of those contours which is not captured by the widths themselves. For\nnon-Gaussian likelihoods, an approximation may be derived via Simulation-Based\nInference (SBI). It is often implicitly assumed that parameter constraints from\nSBI analyses, which do not use covariance matrices, are not affected by the\nsame problems as parameter estimation with a covariance matrix estimated from\nsimulations. We investigate whether SBI suffers from effects similar to those\nof covariance estimation in Gaussian likelihoods. We use Neural Posterior and\nLikelihood Estimation with continuous and masked autoregressive normalizing\nflows for density estimation. We fit our approximate posterior models to\nsimulations drawn from a Gaussian linear model, so that the SBI result can be\ncompared to the true posterior. We test linear and neural network based\ncompression, demonstrating that neither methods circumvent the issues of\ncovariance estimation. SBI suffers an inflation of posterior variance that is\nequal or greater than the analytical result in covariance estimation for\nGaussian likelihoods for the same number of simulations. The assumption that\nSBI requires a smaller number of simulations than covariance estimation for a\nGaussian likelihood analysis is inaccurate. The limitations of traditional\nlikelihood analysis with simulation-based covariance remain for SBI with a\nfinite simulation budget. Despite these issues, we show that SBI correctly\ndraws the true posterior contour given enough simulations."
                },
                "authors": [
                    {
                        "name": "Jed Homer"
                    },
                    {
                        "name": "Oliver Friedrich"
                    },
                    {
                        "name": "Daniel Gruen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gruen"
                },
                "author": "Daniel Gruen",
                "arxiv_comment": "21 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v2",
                "updated": "2024-12-03T09:25:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    25,
                    11,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02306v1",
                "updated": "2024-12-03T09:21:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    21,
                    4,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T09:21:04Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    21,
                    4,
                    1,
                    338,
                    0
                ],
                "title": "Partial Non-rigid Deformations and interpolations of Human Body Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial Non-rigid Deformations and interpolations of Human Body Surfaces"
                },
                "summary": "Non-rigid shape deformations pose significant challenges, and most existing\nmethods struggle to handle partial deformations effectively. We present Partial\nNon-rigid Deformations and interpolations of the human body Surfaces (PaNDAS),\na new method to learn local and global deformations of 3D surface meshes by\nbuilding on recent deep models. Unlike previous approaches, our method enables\nrestricting deformations to specific parts of the shape in a versatile way and\nallows for mixing and combining various poses from the database, all while not\nrequiring any optimization at inference time. We demonstrate that the proposed\nframework can be used to generate new shapes, interpolate between parts of\nshapes, and perform other shape manipulation tasks with state-of-the-art\naccuracy and greater locality across various types of human surface data. Code\nand data will be made available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-rigid shape deformations pose significant challenges, and most existing\nmethods struggle to handle partial deformations effectively. We present Partial\nNon-rigid Deformations and interpolations of the human body Surfaces (PaNDAS),\na new method to learn local and global deformations of 3D surface meshes by\nbuilding on recent deep models. Unlike previous approaches, our method enables\nrestricting deformations to specific parts of the shape in a versatile way and\nallows for mixing and combining various poses from the database, all while not\nrequiring any optimization at inference time. We demonstrate that the proposed\nframework can be used to generate new shapes, interpolate between parts of\nshapes, and perform other shape manipulation tasks with state-of-the-art\naccuracy and greater locality across various types of human surface data. Code\nand data will be made available soon."
                },
                "authors": [
                    {
                        "name": "Thomas Besnier"
                    },
                    {
                        "name": "Emery Pierson"
                    },
                    {
                        "name": "Sylvain Arguillere"
                    },
                    {
                        "name": "Mohamed Daoudi"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Daoudi"
                },
                "author": "Mohamed Daoudi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19146v2",
                "updated": "2024-12-03T09:06:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    6,
                    33,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-28T13:45:42Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    45,
                    42,
                    3,
                    333,
                    0
                ],
                "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir adoption is limited by high computational costs during inference. While\nincreasing parameter counts enhances accuracy, it also widens the gap between\nstate-of-the-art capabilities and practical deployability. We present Puzzle, a\nframework to accelerate LLM inference on specific hardware while preserving\ntheir capabilities. Through an innovative application of neural architecture\nsearch (NAS) at an unprecedented scale, Puzzle systematically optimizes models\nwith tens of billions of parameters under hardware constraints. Our approach\nutilizes blockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We demonstrate the real-world impact of our framework through\nLlama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model\nderived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4%\nof the original model's capabilities. Nemotron-51B currently stands as the most\naccurate language model capable of inference on a single GPU with large batch\nsizes. Remarkably, this transformation required just 45B training tokens,\ncompared to over 15T tokens used for the 70B model it was derived from. This\nestablishes a new paradigm where powerful models can be optimized for efficient\ndeployment with only negligible compromise of their capabilities, demonstrating\nthat inference performance, not parameter count alone, should guide model\nselection. With the release of Nemotron-51B and the presentation of the Puzzle\nframework, we provide practitioners immediate access to state-of-the-art\nlanguage modeling capabilities at significantly reduced computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir adoption is limited by high computational costs during inference. While\nincreasing parameter counts enhances accuracy, it also widens the gap between\nstate-of-the-art capabilities and practical deployability. We present Puzzle, a\nframework to accelerate LLM inference on specific hardware while preserving\ntheir capabilities. Through an innovative application of neural architecture\nsearch (NAS) at an unprecedented scale, Puzzle systematically optimizes models\nwith tens of billions of parameters under hardware constraints. Our approach\nutilizes blockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We demonstrate the real-world impact of our framework through\nLlama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model\nderived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4%\nof the original model's capabilities. Nemotron-51B currently stands as the most\naccurate language model capable of inference on a single GPU with large batch\nsizes. Remarkably, this transformation required just 45B training tokens,\ncompared to over 15T tokens used for the 70B model it was derived from. This\nestablishes a new paradigm where powerful models can be optimized for efficient\ndeployment with only negligible compromise of their capabilities, demonstrating\nthat inference performance, not parameter count alone, should guide model\nselection. With the release of Nemotron-51B and the presentation of the Puzzle\nframework, we provide practitioners immediate access to state-of-the-art\nlanguage modeling capabilities at significantly reduced computational costs."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ran Rubin"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v4",
                "updated": "2024-12-03T09:05:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    5,
                    50,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02279v1",
                "updated": "2024-12-03T08:54:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    54,
                    17,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:54:17Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    54,
                    17,
                    1,
                    338,
                    0
                ],
                "title": "A Comprehensive Evaluation of Large Language Models on Aspect-Based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Large Language Models on Aspect-Based\n  Sentiment Analysis"
                },
                "summary": "Recently, Large Language Models (LLMs) have garnered increasing attention in\nthe field of natural language processing, revolutionizing numerous downstream\ntasks with powerful reasoning and generation abilities. For example, In-Context\nLearning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box\nLLMs to execute downstream tasks by analogy learning without any fine-tuning.\nBesides, in a fine-tuning-dependent paradigm where substantial training data\nexists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,\nenable LLMs to achieve excellent performance comparable to full fine-tuning.\n  However, these fascinating techniques employed by LLMs have not been fully\nexploited in the ABSA field. Previous works probe LLMs in ABSA by merely using\nrandomly selected input-output pairs as demonstrations in ICL, resulting in an\nincomplete and superficial evaluation. In this paper, we shed light on a\ncomprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8\nABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation\nto unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''\nFor the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using\ninstruction-based multi-task learning. For the fine-tuning-free paradigm, we\npropose 3 demonstration selection strategies to stimulate the few-shot\nabilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a\nnew state-of-the-art performance compared to fine-tuned Small Language Models\n(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the\nfine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still\nshowcase impressive potential and even compete with fine-tuned SLMs on some\nABSA subtasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have garnered increasing attention in\nthe field of natural language processing, revolutionizing numerous downstream\ntasks with powerful reasoning and generation abilities. For example, In-Context\nLearning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box\nLLMs to execute downstream tasks by analogy learning without any fine-tuning.\nBesides, in a fine-tuning-dependent paradigm where substantial training data\nexists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,\nenable LLMs to achieve excellent performance comparable to full fine-tuning.\n  However, these fascinating techniques employed by LLMs have not been fully\nexploited in the ABSA field. Previous works probe LLMs in ABSA by merely using\nrandomly selected input-output pairs as demonstrations in ICL, resulting in an\nincomplete and superficial evaluation. In this paper, we shed light on a\ncomprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8\nABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation\nto unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''\nFor the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using\ninstruction-based multi-task learning. For the fine-tuning-free paradigm, we\npropose 3 demonstration selection strategies to stimulate the few-shot\nabilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a\nnew state-of-the-art performance compared to fine-tuned Small Language Models\n(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the\nfine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still\nshowcase impressive potential and even compete with fine-tuned SLMs on some\nABSA subtasks."
                },
                "authors": [
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Dandan Song"
                    },
                    {
                        "name": "Yuhang Tian"
                    },
                    {
                        "name": "Zhijing Wu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Shuhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuhao Zhang"
                },
                "author": "Shuhao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02271v1",
                "updated": "2024-12-03T08:41:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    41,
                    13,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:41:13Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    41,
                    13,
                    1,
                    338,
                    0
                ],
                "title": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News\n  Headlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News\n  Headlines"
                },
                "summary": "In this paper, we introduce the MediaSpin dataset aiming to help in the\ndevelopment of models that can detect different forms of media bias present in\nnews headlines, developed through human-supervised and -validated Large\nLanguage Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs\nof news headlines and annotations with explanations of the 13 distinct types of\nmedia bias categories assigned. We demonstrate the usefulness of our dataset\nfor automated bias detection in news edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the MediaSpin dataset aiming to help in the\ndevelopment of models that can detect different forms of media bias present in\nnews headlines, developed through human-supervised and -validated Large\nLanguage Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs\nof news headlines and annotations with explanations of the 13 distinct types of\nmedia bias categories assigned. We demonstrate the usefulness of our dataset\nfor automated bias detection in news edits."
                },
                "authors": [
                    {
                        "name": "Preetika Verma"
                    },
                    {
                        "name": "Kokil Jaidka"
                    }
                ],
                "author_detail": {
                    "name": "Kokil Jaidka"
                },
                "author": "Kokil Jaidka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02263v1",
                "updated": "2024-12-03T08:35:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:35:51Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "title": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence"
                },
                "summary": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future."
                },
                "authors": [
                    {
                        "name": "Youquan Xian"
                    },
                    {
                        "name": "Xueying Zeng"
                    },
                    {
                        "name": "Duancheng Xuan"
                    },
                    {
                        "name": "Danping Yang"
                    },
                    {
                        "name": "Chunpei Li"
                    },
                    {
                        "name": "Peng Fan"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02261v1",
                "updated": "2024-12-03T08:34:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    34,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:34:41Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    34,
                    41,
                    1,
                    338,
                    0
                ],
                "title": "Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis"
                },
                "summary": "Human motion generation is a long-standing problem, and scene-aware motion\nsynthesis has been widely researched recently due to its numerous applications.\nPrevailing methods rely heavily on paired motion-scene data whose quantity is\nlimited. Meanwhile, it is difficult to generalize to diverse scenes when\ntrained only on a few specific ones. Thus, we propose a unified framework,\ntermed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where\npaired motion-scene data are no longer necessary. In this framework, we\ndisentangle human-scene interaction from motion synthesis during training and\nthen introduce an interaction-based implicit policy into motion diffusion\nduring inference. Synthesized motion can be derived through iterative diffusion\ndenoising and implicit policy optimization, thus motion naturalness and\ninteraction plausibility can be maintained simultaneously. The proposed\nimplicit policy optimizes the intermediate noised motion in a GAN Inversion\nmanner to maintain motion continuity and control keyframe poses though the\nControlNet branch and motion inpainting. For long-term motion synthesis, we\nintroduce motion blending for stable transitions between multiple sub-tasks,\nwhere motions are fused in rotation power space and translation linear space.\nThe proposed method is evaluated on synthesized scenes with ShapeNet furniture,\nand real scenes from PROX and Replica. Results show that our framework presents\nbetter motion naturalness and interaction plausibility than cutting-edge\nmethods. This also indicates the feasibility of utilizing the DIP for motion\nsynthesis in more general tasks and versatile scenes.\nhttps://jingyugong.github.io/DiffusionImplicitPolicy/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation is a long-standing problem, and scene-aware motion\nsynthesis has been widely researched recently due to its numerous applications.\nPrevailing methods rely heavily on paired motion-scene data whose quantity is\nlimited. Meanwhile, it is difficult to generalize to diverse scenes when\ntrained only on a few specific ones. Thus, we propose a unified framework,\ntermed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where\npaired motion-scene data are no longer necessary. In this framework, we\ndisentangle human-scene interaction from motion synthesis during training and\nthen introduce an interaction-based implicit policy into motion diffusion\nduring inference. Synthesized motion can be derived through iterative diffusion\ndenoising and implicit policy optimization, thus motion naturalness and\ninteraction plausibility can be maintained simultaneously. The proposed\nimplicit policy optimizes the intermediate noised motion in a GAN Inversion\nmanner to maintain motion continuity and control keyframe poses though the\nControlNet branch and motion inpainting. For long-term motion synthesis, we\nintroduce motion blending for stable transitions between multiple sub-tasks,\nwhere motions are fused in rotation power space and translation linear space.\nThe proposed method is evaluated on synthesized scenes with ShapeNet furniture,\nand real scenes from PROX and Replica. Results show that our framework presents\nbetter motion naturalness and interaction plausibility than cutting-edge\nmethods. This also indicates the feasibility of utilizing the DIP for motion\nsynthesis in more general tasks and versatile scenes.\nhttps://jingyugong.github.io/DiffusionImplicitPolicy/"
                },
                "authors": [
                    {
                        "name": "Jingyu Gong"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Fengqi Liu"
                    },
                    {
                        "name": "Ke Fan"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Zhizhong Zhang"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14026v3",
                "updated": "2024-12-03T08:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    3,
                    25,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-20T06:46:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    46,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations"
                },
                "summary": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/"
                },
                "authors": [
                    {
                        "name": "Xisen Jin"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "10 pages; preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15316v3",
                "updated": "2024-12-03T07:57:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    57,
                    33,
                    1,
                    338,
                    0
                ],
                "published": "2023-11-26T14:35:23Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    14,
                    35,
                    23,
                    6,
                    330,
                    0
                ],
                "title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference"
                },
                "summary": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses."
                },
                "authors": [
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Huan Liu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02228v1",
                "updated": "2024-12-03T07:51:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    51,
                    14,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T07:51:14Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    51,
                    14,
                    1,
                    338,
                    0
                ],
                "title": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition"
                },
                "summary": "Despite the recent success of two-stage prototypical networks in few-shot\nnamed entity recognition (NER), challenges such as over/under-detected false\nspans in the span detection stage and unaligned entity prototypes in the type\nclassification stage persist. Additionally, LLMs have not proven to be\neffective few-shot information extractors in general. In this paper, we propose\nan approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to\naddress these issues. We introduce a boundary-aware contrastive learning\nstrategy to enhance the LLM's ability to perceive entity boundaries for\ngeneralized entity spans. Additionally, we utilize LoRAHub to align information\nfrom the target domain to the source domain, thereby enhancing adaptive\ncross-domain classification capabilities. Extensive experiments across various\nbenchmarks demonstrate that our framework outperforms prior methods, validating\nits effectiveness. In particular, the proposed strategies demonstrate\neffectiveness across a range of LLM architectures. The code and data are\nreleased on https://github.com/UESTC-GQJ/BANER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of two-stage prototypical networks in few-shot\nnamed entity recognition (NER), challenges such as over/under-detected false\nspans in the span detection stage and unaligned entity prototypes in the type\nclassification stage persist. Additionally, LLMs have not proven to be\neffective few-shot information extractors in general. In this paper, we propose\nan approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to\naddress these issues. We introduce a boundary-aware contrastive learning\nstrategy to enhance the LLM's ability to perceive entity boundaries for\ngeneralized entity spans. Additionally, we utilize LoRAHub to align information\nfrom the target domain to the source domain, thereby enhancing adaptive\ncross-domain classification capabilities. Extensive experiments across various\nbenchmarks demonstrate that our framework outperforms prior methods, validating\nits effectiveness. In particular, the proposed strategies demonstrate\neffectiveness across a range of LLM architectures. The code and data are\nreleased on https://github.com/UESTC-GQJ/BANER."
                },
                "authors": [
                    {
                        "name": "Quanjiang Guo"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ling Tian"
                    },
                    {
                        "name": "Zhao Kang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Sijie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sijie Wang"
                },
                "author": "Sijie Wang",
                "arxiv_comment": "Appear on COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06638v2",
                "updated": "2024-12-03T07:40:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    40,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-11T00:18:54Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    0,
                    18,
                    54,
                    0,
                    316,
                    0
                ],
                "title": "Model Editing for LLMs4Code: How Far are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Editing for LLMs4Code: How Far are We?"
                },
                "summary": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Weimin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhang"
                },
                "author": "Weimin Zhang",
                "arxiv_comment": "Accepted by ICSE2025. The code is available at:\n  https://github.com/xpq-tech/code-llmedit.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08978v2",
                "updated": "2024-12-03T07:36:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    36,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-03-13T22:06:03Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    22,
                    6,
                    3,
                    2,
                    73,
                    0
                ],
                "title": "AutoGuide: Automated Generation and Selection of Context-Aware\n  Guidelines for Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoGuide: Automated Generation and Selection of Context-Aware\n  Guidelines for Large Language Model Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have empowered AI agents\ncapable of performing various sequential decision-making tasks. However,\neffectively guiding LLMs to perform well in unfamiliar domains like web\nnavigation, where they lack sufficient knowledge, has proven to be difficult\nwith the demonstration-based in-context learning paradigm. In this paper, we\nintroduce a novel framework, called AutoGuide, which addresses this limitation\nby automatically generating context-aware guidelines from offline experiences.\nImportantly, each context-aware guideline is expressed in concise natural\nlanguage and follows a conditional structure, clearly describing the context\nwhere it is applicable. As a result, our guidelines facilitate the provision of\nrelevant knowledge for the agent's current decision-making process, overcoming\nthe limitations of the conventional demonstration-based learning paradigm. Our\nevaluation demonstrates that AutoGuide significantly outperforms competitive\nbaselines in complex benchmark domains, including real-world web navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have empowered AI agents\ncapable of performing various sequential decision-making tasks. However,\neffectively guiding LLMs to perform well in unfamiliar domains like web\nnavigation, where they lack sufficient knowledge, has proven to be difficult\nwith the demonstration-based in-context learning paradigm. In this paper, we\nintroduce a novel framework, called AutoGuide, which addresses this limitation\nby automatically generating context-aware guidelines from offline experiences.\nImportantly, each context-aware guideline is expressed in concise natural\nlanguage and follows a conditional structure, clearly describing the context\nwhere it is applicable. As a result, our guidelines facilitate the provision of\nrelevant knowledge for the agent's current decision-making process, overcoming\nthe limitations of the conventional demonstration-based learning paradigm. Our\nevaluation demonstrates that AutoGuide significantly outperforms competitive\nbaselines in complex benchmark domains, including real-world web navigation."
                },
                "authors": [
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Sungryull Sohn"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Kyunghoon Bae"
                    },
                    {
                        "name": "Honglak Lee"
                    }
                ],
                "author_detail": {
                    "name": "Honglak Lee"
                },
                "author": "Honglak Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05315v2",
                "updated": "2024-12-03T07:25:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    25,
                    55,
                    1,
                    338,
                    0
                ],
                "published": "2024-01-10T18:33:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    18,
                    33,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "Multi-resolution filters via linear projection for large spatio-temporal\n  datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-resolution filters via linear projection for large spatio-temporal\n  datasets"
                },
                "summary": "Advances in compact sensing devices mounted on satellites have facilitated\nthe collection of large spatio-temporal datasets with coordinates. Since such\ndatasets are often incomplete and noisy, it is useful to create the prediction\nsurface of a spatial field. To this end, we consider an online filtering\ninference by using the Kalman filter based on linear Gaussian state-space\nmodels. However, the Kalman filter is impractically time-consuming when the\nnumber of locations in spatio-temporal datasets is large. To address this\nproblem, we propose a multi-resolution filter via linear projection (MRF-lp), a\nfast computation method for online filtering inference. In the MRF-lp, by\ncarrying out a multi-resolution approximation via linear projection (MRA-lp),\nthe forecast covariance matrix can be approximated while capturing both the\nlarge- and small-scale spatial variations. As a result of this approximation,\nour proposed MRF-lp preserves a block-sparse structure of some matrices\nappearing in the MRF-lp through time, which leads to the scalability of this\nalgorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and\nnon-Gaussian case. Simulation studies and real data analysis for total\nprecipitable water vapor demonstrate that our proposed approach performs well\ncompared with the related methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in compact sensing devices mounted on satellites have facilitated\nthe collection of large spatio-temporal datasets with coordinates. Since such\ndatasets are often incomplete and noisy, it is useful to create the prediction\nsurface of a spatial field. To this end, we consider an online filtering\ninference by using the Kalman filter based on linear Gaussian state-space\nmodels. However, the Kalman filter is impractically time-consuming when the\nnumber of locations in spatio-temporal datasets is large. To address this\nproblem, we propose a multi-resolution filter via linear projection (MRF-lp), a\nfast computation method for online filtering inference. In the MRF-lp, by\ncarrying out a multi-resolution approximation via linear projection (MRA-lp),\nthe forecast covariance matrix can be approximated while capturing both the\nlarge- and small-scale spatial variations. As a result of this approximation,\nour proposed MRF-lp preserves a block-sparse structure of some matrices\nappearing in the MRF-lp through time, which leads to the scalability of this\nalgorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and\nnon-Gaussian case. Simulation studies and real data analysis for total\nprecipitable water vapor demonstrate that our proposed approach performs well\ncompared with the related methods."
                },
                "authors": [
                    {
                        "name": "Toshihiro Hirano"
                    },
                    {
                        "name": "Tsunehiro Ishihara"
                    }
                ],
                "author_detail": {
                    "name": "Tsunehiro Ishihara"
                },
                "author": "Tsunehiro Ishihara",
                "arxiv_comment": "48 pages, 10 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02220v1",
                "updated": "2024-12-03T07:25:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    25,
                    30,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T07:25:30Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    25,
                    30,
                    1,
                    338,
                    0
                ],
                "title": "Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models\n  by Recycling Pre-Tuned LoRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models\n  by Recycling Pre-Tuned LoRAs"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot\nadaptability without requiring fine-tuning, positioning them ideal for\ndata-limited and real-time applications. However, this adaptability has not yet\nbeen replicated in current Visual Foundation Models (VFMs), which require\nexplicit fine-tuning with sufficient tuning data. Besides, the\npretraining-finetuning paradigm has led to the surge of numerous task-specific\nmodular components, such as Low-Rank Adaptation (LoRA). For the first time, we\nexplore the potential of reusing diverse pre-tuned LoRAs without accessing\ntheir original training data, to achieve tuning-free few-shot adaptation in\nVFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned\nLoRAs with a meta-learning objective, using surrogate data generated inversely\nfrom pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is\nempowered to solve new few-shot tasks in a single forward pass, akin to the\nin-context learning of LLMs. Additionally, we incorporate a double-efficient\nmechanism tailored to our framework, significantly accelerating the\nmeta-training process while maintaining or even improving performance.\nExtensive experiments across various few-shot classification benchmarks across\nboth in- and cross-domain scenarios demonstrate the superiority of our\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot\nadaptability without requiring fine-tuning, positioning them ideal for\ndata-limited and real-time applications. However, this adaptability has not yet\nbeen replicated in current Visual Foundation Models (VFMs), which require\nexplicit fine-tuning with sufficient tuning data. Besides, the\npretraining-finetuning paradigm has led to the surge of numerous task-specific\nmodular components, such as Low-Rank Adaptation (LoRA). For the first time, we\nexplore the potential of reusing diverse pre-tuned LoRAs without accessing\ntheir original training data, to achieve tuning-free few-shot adaptation in\nVFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned\nLoRAs with a meta-learning objective, using surrogate data generated inversely\nfrom pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is\nempowered to solve new few-shot tasks in a single forward pass, akin to the\nin-context learning of LLMs. Additionally, we incorporate a double-efficient\nmechanism tailored to our framework, significantly accelerating the\nmeta-training process while maintaining or even improving performance.\nExtensive experiments across various few-shot classification benchmarks across\nboth in- and cross-domain scenarios demonstrate the superiority of our\nframework."
                },
                "authors": [
                    {
                        "name": "Zixuan Hu"
                    },
                    {
                        "name": "Yongxian Wei"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17355v3",
                "updated": "2024-12-03T06:53:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    53,
                    58,
                    1,
                    338,
                    0
                ],
                "published": "2024-08-30T15:39:34Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    39,
                    34,
                    4,
                    243,
                    0
                ],
                "title": "Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling"
                },
                "summary": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its reported effects on the learned policy are\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity in\nstochastic environments. To address this tradeoff, we propose Bidirectional\nDecoding (BID), a test-time inference algorithm that bridges action chunking\nwith closed-loop operations. BID samples multiple predictions at each time step\nand searches for the optimal one based on two criteria: (i) backward coherence,\nwhich favors samples that align with previous decisions; (ii) forward contrast,\nwhich seeks samples of high likelihood for future plans. By coupling decisions\nwithin and across action chunks, BID promotes consistency over time while\nmaintaining reactivity to unexpected changes. Experimental results show that\nBID boosts the performance of two state-of-the-art generative policies across\nseven simulation benchmarks and two real-world tasks. Code and videos are\navailable at https://bid-robot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its reported effects on the learned policy are\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity in\nstochastic environments. To address this tradeoff, we propose Bidirectional\nDecoding (BID), a test-time inference algorithm that bridges action chunking\nwith closed-loop operations. BID samples multiple predictions at each time step\nand searches for the optimal one based on two criteria: (i) backward coherence,\nwhich favors samples that align with previous decisions; (ii) forward contrast,\nwhich seeks samples of high likelihood for future plans. By coupling decisions\nwithin and across action chunks, BID promotes consistency over time while\nmaintaining reactivity to unexpected changes. Experimental results show that\nBID boosts the performance of two state-of-the-art generative policies across\nseven simulation benchmarks and two real-world tasks. Code and videos are\navailable at https://bid-robot.github.io."
                },
                "authors": [
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Jubayer Ibn Hamid"
                    },
                    {
                        "name": "Annie Xie"
                    },
                    {
                        "name": "Yoonho Lee"
                    },
                    {
                        "name": "Maximilian Du"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Project website: https://bid-robot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10946v3",
                "updated": "2024-12-03T06:52:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    52,
                    34,
                    1,
                    338,
                    0
                ],
                "published": "2024-02-09T04:02:43Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    4,
                    2,
                    43,
                    4,
                    40,
                    0
                ],
                "title": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mengzhou Chen"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "NeurIPS 2024; Code is at https://github.com/Scarelette/CultureLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v1",
                "updated": "2024-12-03T06:47:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unifed Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unifed Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11748v3",
                "updated": "2024-12-03T06:23:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    23,
                    38,
                    1,
                    338,
                    0
                ],
                "published": "2024-02-19T00:21:13Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    0,
                    21,
                    13,
                    0,
                    50,
                    0
                ],
                "title": "Low-power SNN-based audio source localisation using a Hilbert Transform\n  spike encoding scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-power SNN-based audio source localisation using a Hilbert Transform\n  spike encoding scheme"
                },
                "summary": "Sound source localisation is used in many consumer devices, to isolate audio\nfrom individual speakers and reject noise. Localization is frequently\naccomplished by ``beamforming'', which combines phase-shifted audio streams to\nincrease power from chosen source directions, under a known microphone array\ngeometry. Dense band-pass filters are often needed to obtain narrowband signal\ncomponents from wideband audio. These approaches achieve high accuracy, but\nnarrowband beamforming is computationally demanding, and not ideal for\nlow-power IoT devices. We demonstrate a novel method for sound source\nlocalisation on arbitrary microphone arrays, designed for efficient\nimplementation in ultra-low-power spiking neural networks (SNNs). We use a\nHilbert transform to avoid dense band-pass filters, and introduce a new\nevent-based encoding method that captures the phase of the complex analytic\nsignal. Our approach achieves state-of-the-art accuracy for SNN methods,\ncomparable with traditional non-SNN super-resolution beamforming. We deploy our\nmethod to low-power SNN inference hardware, with much lower power consumption\nthan super-resolution methods. We demonstrate that signal processing approaches\nco-designed with spiking neural network implementations can achieve much\nimproved power efficiency. Our new Hilbert-transform-based method for\nbeamforming can also improve the efficiency of traditional DSP-based signal\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound source localisation is used in many consumer devices, to isolate audio\nfrom individual speakers and reject noise. Localization is frequently\naccomplished by ``beamforming'', which combines phase-shifted audio streams to\nincrease power from chosen source directions, under a known microphone array\ngeometry. Dense band-pass filters are often needed to obtain narrowband signal\ncomponents from wideband audio. These approaches achieve high accuracy, but\nnarrowband beamforming is computationally demanding, and not ideal for\nlow-power IoT devices. We demonstrate a novel method for sound source\nlocalisation on arbitrary microphone arrays, designed for efficient\nimplementation in ultra-low-power spiking neural networks (SNNs). We use a\nHilbert transform to avoid dense band-pass filters, and introduce a new\nevent-based encoding method that captures the phase of the complex analytic\nsignal. Our approach achieves state-of-the-art accuracy for SNN methods,\ncomparable with traditional non-SNN super-resolution beamforming. We deploy our\nmethod to low-power SNN inference hardware, with much lower power consumption\nthan super-resolution methods. We demonstrate that signal processing approaches\nco-designed with spiking neural network implementations can achieve much\nimproved power efficiency. Our new Hilbert-transform-based method for\nbeamforming can also improve the efficiency of traditional DSP-based signal\nprocessing."
                },
                "authors": [
                    {
                        "name": "Saeid Haghighatshoar"
                    },
                    {
                        "name": "Dylan R Muir"
                    }
                ],
                "author_detail": {
                    "name": "Dylan R Muir"
                },
                "author": "Dylan R Muir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02197v1",
                "updated": "2024-12-03T06:23:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    23,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T06:23:19Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    23,
                    19,
                    1,
                    338,
                    0
                ],
                "title": "Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature\n  Extraction and Interaction with Low-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature\n  Extraction and Interaction with Low-Resolution Images"
                },
                "summary": "In real-world applications of image recognition tasks, such as human pose\nestimation, cameras often capture objects, like human bodies, at low\nresolutions. This scenario poses a challenge in extracting and leveraging\nmulti-scale features, which is often essential for precise inference. To\naddress this challenge, we propose a new attention mechanism, named cascaded\nmulti-scale attention (CMSA), tailored for use in CNN-ViT hybrid architectures,\nto handle low-resolution inputs effectively. The design of CMSA enables the\nextraction and seamless integration of features across various scales without\nnecessitating the downsampling of the input image or feature maps. This is\nachieved through a novel combination of grouped multi-head self-attention\nmechanisms with window-based local attention and cascaded fusion of multi-scale\nfeatures over different scales. This architecture allows for the effective\nhandling of features across different scales, enhancing the model's ability to\nperform tasks such as human pose estimation, head pose estimation, and more\nwith low-resolution images. Our experimental results show that the proposed\nmethod outperforms existing state-of-the-art methods in these areas with fewer\nparameters, showcasing its potential for broad application in real-world\nscenarios where capturing high-resolution images is not feasible. Code is\navailable at https://github.com/xyongLu/CMSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world applications of image recognition tasks, such as human pose\nestimation, cameras often capture objects, like human bodies, at low\nresolutions. This scenario poses a challenge in extracting and leveraging\nmulti-scale features, which is often essential for precise inference. To\naddress this challenge, we propose a new attention mechanism, named cascaded\nmulti-scale attention (CMSA), tailored for use in CNN-ViT hybrid architectures,\nto handle low-resolution inputs effectively. The design of CMSA enables the\nextraction and seamless integration of features across various scales without\nnecessitating the downsampling of the input image or feature maps. This is\nachieved through a novel combination of grouped multi-head self-attention\nmechanisms with window-based local attention and cascaded fusion of multi-scale\nfeatures over different scales. This architecture allows for the effective\nhandling of features across different scales, enhancing the model's ability to\nperform tasks such as human pose estimation, head pose estimation, and more\nwith low-resolution images. Our experimental results show that the proposed\nmethod outperforms existing state-of-the-art methods in these areas with fewer\nparameters, showcasing its potential for broad application in real-world\nscenarios where capturing high-resolution images is not feasible. Code is\navailable at https://github.com/xyongLu/CMSA."
                },
                "authors": [
                    {
                        "name": "Xiangyong Lu"
                    },
                    {
                        "name": "Masanori Suganuma"
                    },
                    {
                        "name": "Takayuki Okatani"
                    }
                ],
                "author_detail": {
                    "name": "Takayuki Okatani"
                },
                "author": "Takayuki Okatani",
                "arxiv_comment": "9 pages, 4 figures, 5 tables. The paper is under consideration at\n  Computer Vision and Image Understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02193v1",
                "updated": "2024-12-03T06:15:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    15,
                    4,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T06:15:04Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    15,
                    4,
                    1,
                    338,
                    0
                ],
                "title": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models"
                },
                "summary": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned\non language instruction. Large language models (LLMs) struggle with generating\nphysically plausible 3D scenes and adherence to input instructions,\nparticularly in cluttered scenes. We introduce LayoutVLM, a framework and scene\nlayout representation that exploits the semantic knowledge of Vision-Language\nModels (VLMs) and supports differentiable optimization to ensure physical\nplausibility. LayoutVLM employs VLMs to generate two mutually reinforcing\nrepresentations from visually marked images, and a self-consistent decoding\nprocess to improve VLMs spatial planning. Our experiments show that LayoutVLM\naddresses the limitations of existing LLM and constraint-based approaches,\nproducing physically plausible 3D layouts better aligned with the semantic\nintent of input language instructions. We also demonstrate that fine-tuning\nVLMs with the proposed scene layout representation extracted from existing\nscene datasets can improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned\non language instruction. Large language models (LLMs) struggle with generating\nphysically plausible 3D scenes and adherence to input instructions,\nparticularly in cluttered scenes. We introduce LayoutVLM, a framework and scene\nlayout representation that exploits the semantic knowledge of Vision-Language\nModels (VLMs) and supports differentiable optimization to ensure physical\nplausibility. LayoutVLM employs VLMs to generate two mutually reinforcing\nrepresentations from visually marked images, and a self-consistent decoding\nprocess to improve VLMs spatial planning. Our experiments show that LayoutVLM\naddresses the limitations of existing LLM and constraint-based approaches,\nproducing physically plausible 3D layouts better aligned with the semantic\nintent of input language instructions. We also demonstrate that fine-tuning\nVLMs with the proposed scene layout representation extracted from existing\nscene datasets can improve performance."
                },
                "authors": [
                    {
                        "name": "Fan-Yun Sun"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Siyi Gu"
                    },
                    {
                        "name": "Dylan Lim"
                    },
                    {
                        "name": "Goutam Bhat"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Nick Haber"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "project website: https://ai.stanford.edu/~sunfanyun/layoutvlm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15876v3",
                "updated": "2024-12-03T05:59:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    59,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-21T10:57:45Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    10,
                    57,
                    45,
                    0,
                    295,
                    0
                ],
                "title": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL"
                },
                "summary": "Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. FlickerFusion\nstochastically drops out parts of the observation space, emulating being\nin-domain when inferenced OOD. The results show that FlickerFusion not only\nachieves superior inference rewards but also uniquely reduces uncertainty\nvis-\\`a-vis the backbone, compared to existing methods. Benchmarks,\nimplementations, and model weights are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. FlickerFusion\nstochastically drops out parts of the observation space, emulating being\nin-domain when inferenced OOD. The results show that FlickerFusion not only\nachieves superior inference rewards but also uniquely reduces uncertainty\nvis-\\`a-vis the backbone, compared to existing methods. Benchmarks,\nimplementations, and model weights are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Wonbeen Oh"
                    },
                    {
                        "name": "Siyeol Kim"
                    },
                    {
                        "name": "Suhin Shin"
                    },
                    {
                        "name": "Hyeongjin Kim"
                    },
                    {
                        "name": "Jaein Jang"
                    },
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "NeurIPS '24 Open-World Agents Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02186v1",
                "updated": "2024-12-03T05:54:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    54,
                    43,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T05:54:43Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    54,
                    43,
                    1,
                    338,
                    0
                ],
                "title": "VideoICL: Confidence-based Iterative In-context Learning for\n  Out-of-Distribution Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoICL: Confidence-based Iterative In-context Learning for\n  Out-of-Distribution Video Understanding"
                },
                "summary": "Recent advancements in video large multimodal models (LMMs) have\nsignificantly improved their video understanding and reasoning capabilities.\nHowever, their performance drops on out-of-distribution (OOD) tasks that are\nunderrepresented in training data. Traditional methods like fine-tuning on OOD\ndatasets are impractical due to high computational costs. While In-context\nlearning (ICL) with demonstration examples has shown promising generalization\nperformance in language tasks and image-language tasks without fine-tuning,\napplying ICL to video-language tasks faces challenges due to the limited\ncontext length in Video LMMs, as videos require longer token lengths. To\naddress these issues, we propose VideoICL, a novel video in-context learning\nframework for OOD tasks that introduces a similarity-based relevant example\nselection strategy and a confidence-based iterative inference approach. This\nallows to select the most relevant examples and rank them based on similarity,\nto be used for inference. If the generated response has low confidence, our\nframework selects new examples and performs inference again, iteratively\nrefining the results until a high-confidence response is obtained. This\napproach improves OOD video understanding performance by extending effective\ncontext length without incurring high costs. The experimental results on\nmultiple benchmarks demonstrate significant performance gains, especially in\ndomain-specific scenarios, laying the groundwork for broader video\ncomprehension applications. Code will be released at\nhttps://github.com/KangsanKim07/VideoICL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video large multimodal models (LMMs) have\nsignificantly improved their video understanding and reasoning capabilities.\nHowever, their performance drops on out-of-distribution (OOD) tasks that are\nunderrepresented in training data. Traditional methods like fine-tuning on OOD\ndatasets are impractical due to high computational costs. While In-context\nlearning (ICL) with demonstration examples has shown promising generalization\nperformance in language tasks and image-language tasks without fine-tuning,\napplying ICL to video-language tasks faces challenges due to the limited\ncontext length in Video LMMs, as videos require longer token lengths. To\naddress these issues, we propose VideoICL, a novel video in-context learning\nframework for OOD tasks that introduces a similarity-based relevant example\nselection strategy and a confidence-based iterative inference approach. This\nallows to select the most relevant examples and rank them based on similarity,\nto be used for inference. If the generated response has low confidence, our\nframework selects new examples and performs inference again, iteratively\nrefining the results until a high-confidence response is obtained. This\napproach improves OOD video understanding performance by extending effective\ncontext length without incurring high costs. The experimental results on\nmultiple benchmarks demonstrate significant performance gains, especially in\ndomain-specific scenarios, laying the groundwork for broader video\ncomprehension applications. Code will be released at\nhttps://github.com/KangsanKim07/VideoICL"
                },
                "authors": [
                    {
                        "name": "Kangsan Kim"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Woongyeong Yeo"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02183v1",
                "updated": "2024-12-03T05:44:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    44,
                    50,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T05:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    44,
                    50,
                    1,
                    338,
                    0
                ],
                "title": "Endogenous Interference in Randomized Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogenous Interference in Randomized Experiments"
                },
                "summary": "This paper investigates the identification and inference of treatment effects\nin randomized controlled trials with social interactions. Two key network\nfeatures characterize the setting and introduce endogeneity: (1) latent\nvariables may affect both network formation and outcomes, and (2) the\nintervention may alter network structure, mediating treatment effects. I make\nthree contributions. First, I define parameters within a post-treatment network\nframework, distinguishing direct effects of treatment from indirect effects\nmediated through changes in network structure. I provide a causal\ninterpretation of the coefficients in a linear outcome model. For estimation\nand inference, I focus on a specific form of peer effects, represented by the\nfraction of treated friends. Second, in the absence of endogeneity, I establish\nthe consistency and asymptotic normality of ordinary least squares estimators.\nThird, if endogeneity is present, I propose addressing it through shift-share\ninstrumental variables, demonstrating the consistency and asymptotic normality\nof instrumental variable estimators in relatively sparse networks. For denser\nnetworks, I propose a denoised estimator based on eigendecomposition to restore\nconsistency. Finally, I revisit Prina (2015) as an empirical illustration,\ndemonstrating that treatment can influence outcomes both directly and through\nnetwork structure changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the identification and inference of treatment effects\nin randomized controlled trials with social interactions. Two key network\nfeatures characterize the setting and introduce endogeneity: (1) latent\nvariables may affect both network formation and outcomes, and (2) the\nintervention may alter network structure, mediating treatment effects. I make\nthree contributions. First, I define parameters within a post-treatment network\nframework, distinguishing direct effects of treatment from indirect effects\nmediated through changes in network structure. I provide a causal\ninterpretation of the coefficients in a linear outcome model. For estimation\nand inference, I focus on a specific form of peer effects, represented by the\nfraction of treated friends. Second, in the absence of endogeneity, I establish\nthe consistency and asymptotic normality of ordinary least squares estimators.\nThird, if endogeneity is present, I propose addressing it through shift-share\ninstrumental variables, demonstrating the consistency and asymptotic normality\nof instrumental variable estimators in relatively sparse networks. For denser\nnetworks, I propose a denoised estimator based on eigendecomposition to restore\nconsistency. Finally, I revisit Prina (2015) as an empirical illustration,\ndemonstrating that treatment can influence outcomes both directly and through\nnetwork structure changes."
                },
                "authors": [
                    {
                        "name": "Mengsi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mengsi Gao"
                },
                "author": "Mengsi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02182v1",
                "updated": "2024-12-03T05:42:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    42,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T05:42:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    42,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "Searching for local associations while controlling the false discovery\n  rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for local associations while controlling the false discovery\n  rate"
                },
                "summary": "We introduce local conditional hypotheses that express how the relation\nbetween explanatory variables and outcomes changes across different contexts,\ndescribed by covariates. By expanding upon the model-X knockoff filter, we show\nhow to adaptively discover these local associations, all while controlling the\nfalse discovery rate. Our enhanced inferences can help explain sample\nheterogeneity and uncover interactions, making better use of the capabilities\noffered by modern machine learning models. Specifically, our method is able to\nleverage any model for the identification of data-driven hypotheses pertaining\nto different contexts. Then, it rigorously test these hypotheses without\nsuccumbing to selection bias. Importantly, our approach is efficient and does\nnot require sample splitting. We demonstrate the effectiveness of our method\nthrough numerical experiments and by studying the genetic architecture of\nWaist-Hip-Ratio across different sexes in the UKBiobank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce local conditional hypotheses that express how the relation\nbetween explanatory variables and outcomes changes across different contexts,\ndescribed by covariates. By expanding upon the model-X knockoff filter, we show\nhow to adaptively discover these local associations, all while controlling the\nfalse discovery rate. Our enhanced inferences can help explain sample\nheterogeneity and uncover interactions, making better use of the capabilities\noffered by modern machine learning models. Specifically, our method is able to\nleverage any model for the identification of data-driven hypotheses pertaining\nto different contexts. Then, it rigorously test these hypotheses without\nsuccumbing to selection bias. Importantly, our approach is efficient and does\nnot require sample splitting. We demonstrate the effectiveness of our method\nthrough numerical experiments and by studying the genetic architecture of\nWaist-Hip-Ratio across different sexes in the UKBiobank."
                },
                "authors": [
                    {
                        "name": "Paula Gablenz"
                    },
                    {
                        "name": "Matteo Sesia"
                    },
                    {
                        "name": "Tianshu Sun"
                    },
                    {
                        "name": "Chiara Sabatti"
                    }
                ],
                "author_detail": {
                    "name": "Chiara Sabatti"
                },
                "author": "Chiara Sabatti",
                "arxiv_comment": "19 pages (55 pages including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.03970v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.03970v4",
                "updated": "2024-12-03T05:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    41,
                    12,
                    1,
                    338,
                    0
                ],
                "published": "2023-08-08T01:01:37Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    1,
                    1,
                    37,
                    1,
                    220,
                    0
                ],
                "title": "Optimal Clustering with Dependent Costs in Bayesian Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Clustering with Dependent Costs in Bayesian Networks"
                },
                "summary": "Clustering of nodes in Bayesian Networks (BNs) and related graphical models\nsuch as Dynamic BNs (DBNs) has been demonstrated to enhance computational\nefficiency and improve model learning. Typically, it involves the partitioning\nof the underlying Directed Acyclic Graph (DAG) into cliques, or optimising for\nsome cost or criteria. Computational cost is important since BN and DBN\ninference, such as estimating marginal distributions given evidence or updating\nmodel parameters, is NP-hard. The challenge is exacerbated by cost dependency,\nwhere inference outcomes and hence clustering cost depends on both nodes within\na cluster and the mapping of clusters that are connected by at least one arc.\nWe propose an algorithm called Dependent Cluster MAPping (DCMAP) which is shown\nanalytically, given an arbitrarily defined, positive cost function, to find all\noptimal cluster mappings, and do so with no more iterations than an equally\ninformed algorithm. DCMAP is demonstrated on a complex systems seagrass DBN,\nwhich has 25 nodes per time-slice, and captures biological, ecological and\nenvironmental dynamics and their interactions to predict the impact of dredging\nstressors on resilience and their cumulative effects over time. The algorithm\nis employed to find clusters to optimise the computational efficiency of\ninferring marginal distributions given evidence. For the 25 (one time-slice)\nand 50-node (two time-slices) DBN, the search space size was $9.91\\times10^9$\nand $1.51\\times10^{21}$ possible cluster mappings, respectively, but the first\noptimal solution was found at iteration number 856 (95\\% CI 852,866), and 1569\n(1566,1581) with a cost that was 4\\% and 0.2\\% of the naive heuristic cost,\nrespectively. Through optimal clustering, DCMAP opens up opportunities for\nfurther research beyond improving computational efficiency, such as using\nclustering to minimise entropy in BN learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering of nodes in Bayesian Networks (BNs) and related graphical models\nsuch as Dynamic BNs (DBNs) has been demonstrated to enhance computational\nefficiency and improve model learning. Typically, it involves the partitioning\nof the underlying Directed Acyclic Graph (DAG) into cliques, or optimising for\nsome cost or criteria. Computational cost is important since BN and DBN\ninference, such as estimating marginal distributions given evidence or updating\nmodel parameters, is NP-hard. The challenge is exacerbated by cost dependency,\nwhere inference outcomes and hence clustering cost depends on both nodes within\na cluster and the mapping of clusters that are connected by at least one arc.\nWe propose an algorithm called Dependent Cluster MAPping (DCMAP) which is shown\nanalytically, given an arbitrarily defined, positive cost function, to find all\noptimal cluster mappings, and do so with no more iterations than an equally\ninformed algorithm. DCMAP is demonstrated on a complex systems seagrass DBN,\nwhich has 25 nodes per time-slice, and captures biological, ecological and\nenvironmental dynamics and their interactions to predict the impact of dredging\nstressors on resilience and their cumulative effects over time. The algorithm\nis employed to find clusters to optimise the computational efficiency of\ninferring marginal distributions given evidence. For the 25 (one time-slice)\nand 50-node (two time-slices) DBN, the search space size was $9.91\\times10^9$\nand $1.51\\times10^{21}$ possible cluster mappings, respectively, but the first\noptimal solution was found at iteration number 856 (95\\% CI 852,866), and 1569\n(1566,1581) with a cost that was 4\\% and 0.2\\% of the naive heuristic cost,\nrespectively. Through optimal clustering, DCMAP opens up opportunities for\nfurther research beyond improving computational efficiency, such as using\nclustering to minimise entropy in BN learning."
                },
                "authors": [
                    {
                        "name": "Paul Pao-Yen Wu"
                    },
                    {
                        "name": "Fabrizio Ruggeri"
                    },
                    {
                        "name": "Kerrie Mengersen"
                    }
                ],
                "author_detail": {
                    "name": "Kerrie Mengersen"
                },
                "author": "Kerrie Mengersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.03970v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.03970v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12820v3",
                "updated": "2024-12-03T05:15:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    15,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-03-19T15:21:00Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    15,
                    21,
                    0,
                    1,
                    79,
                    0
                ],
                "title": "A Physics-embedded Deep Learning Framework for Cloth Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physics-embedded Deep Learning Framework for Cloth Simulation"
                },
                "summary": "Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination."
                },
                "authors": [
                    {
                        "name": "Zhiwei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Zhao"
                },
                "author": "Zhiwei Zhao",
                "arxiv_comment": "updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.3.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02173v1",
                "updated": "2024-12-03T05:05:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    5,
                    13,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T05:05:13Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    5,
                    13,
                    1,
                    338,
                    0
                ],
                "title": "Keeping Experts in the Loop: Expert-Guided Optimization for Clinical\n  Data Classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping Experts in the Loop: Expert-Guided Optimization for Clinical\n  Data Classification using Large Language Models"
                },
                "summary": "Since the emergence of Large Language Models (LLMs), the challenge of\neffectively leveraging their potential in healthcare has taken center stage. A\ncritical barrier to using LLMs for extracting insights from unstructured\nclinical notes lies in the prompt engineering process. Despite its pivotal role\nin determining task performance, a clear framework for prompt optimization\nremains absent. Current methods to address this gap take either a manual prompt\nrefinement approach, where domain experts collaborate with prompt engineers to\ncreate an optimal prompt, which is time-intensive and difficult to scale, or\nthrough employing automatic prompt optimizing approaches, where the value of\nthe input of domain experts is not fully realized. To address this, we propose\nStructEase, a novel framework that bridges the gap between automation and the\ninput of human expertise in prompt engineering. A core innovation of the\nframework is SamplEase, an iterative sampling algorithm that identifies\nhigh-value cases where expert feedback drives significant performance\nimprovements. This approach minimizes expert intervention, to effectively\nenhance classification outcomes. This targeted approach reduces labeling\nredundancy, mitigates human error, and enhances classification outcomes. We\nevaluated the performance of StructEase using a dataset of de-identified\nclinical narratives from the US National Electronic Injury Surveillance System\n(NEISS), demonstrating significant gains in classification performance compared\nto current methods. Our findings underscore the value of expert integration in\nLLM workflows, achieving notable improvements in F1 score while maintaining\nminimal expert effort. By combining transparency, flexibility, and scalability,\nStructEase sets the foundation for a framework to integrate expert input into\nLLM workflows in healthcare and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the emergence of Large Language Models (LLMs), the challenge of\neffectively leveraging their potential in healthcare has taken center stage. A\ncritical barrier to using LLMs for extracting insights from unstructured\nclinical notes lies in the prompt engineering process. Despite its pivotal role\nin determining task performance, a clear framework for prompt optimization\nremains absent. Current methods to address this gap take either a manual prompt\nrefinement approach, where domain experts collaborate with prompt engineers to\ncreate an optimal prompt, which is time-intensive and difficult to scale, or\nthrough employing automatic prompt optimizing approaches, where the value of\nthe input of domain experts is not fully realized. To address this, we propose\nStructEase, a novel framework that bridges the gap between automation and the\ninput of human expertise in prompt engineering. A core innovation of the\nframework is SamplEase, an iterative sampling algorithm that identifies\nhigh-value cases where expert feedback drives significant performance\nimprovements. This approach minimizes expert intervention, to effectively\nenhance classification outcomes. This targeted approach reduces labeling\nredundancy, mitigates human error, and enhances classification outcomes. We\nevaluated the performance of StructEase using a dataset of de-identified\nclinical narratives from the US National Electronic Injury Surveillance System\n(NEISS), demonstrating significant gains in classification performance compared\nto current methods. Our findings underscore the value of expert integration in\nLLM workflows, achieving notable improvements in F1 score while maintaining\nminimal expert effort. By combining transparency, flexibility, and scalability,\nStructEase sets the foundation for a framework to integrate expert input into\nLLM workflows in healthcare and beyond."
                },
                "authors": [
                    {
                        "name": "Nader Karayanni"
                    },
                    {
                        "name": "Aya Awwad"
                    },
                    {
                        "name": "Chein-Lien Hsiao"
                    },
                    {
                        "name": "Surish P Shanmugam"
                    }
                ],
                "author_detail": {
                    "name": "Surish P Shanmugam"
                },
                "author": "Surish P Shanmugam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v2",
                "updated": "2024-12-03T05:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    0,
                    18,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Lee"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13706v2",
                "updated": "2024-12-03T04:57:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    57,
                    32,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-19T16:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    58,
                    32,
                    2,
                    171,
                    0
                ],
                "title": "Developing Story: Case Studies of Generative AI's Use in Journalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Story: Case Studies of Generative AI's Use in Journalism"
                },
                "summary": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context."
                },
                "authors": [
                    {
                        "name": "Natalie Grace Brigham"
                    },
                    {
                        "name": "Chongjiu Gao"
                    },
                    {
                        "name": "Tadayoshi Kohno"
                    },
                    {
                        "name": "Franziska Roesner"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v2",
                "updated": "2024-12-03T04:51:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    51,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qichen Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00218v2",
                "updated": "2024-12-03T04:38:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    38,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-29T19:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "NüshuRescue: Revitalization of the endangered Nüshu Language with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NüshuRescue: Revitalization of the endangered Nüshu Language with AI"
                },
                "summary": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by N\\\"ushu, a rare script historically used by Yao\nwomen in China for self-expression within a patriarchal society. To address\nthis challenge, we introduce N\\\"ushuRescue, an AI-driven framework designed to\ntrain large language models (LLMs) on endangered languages with minimal data.\nN\\\"ushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence N\\\"ushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\\\"ushu\nand only 35 short examples from NCGold, N\\\"ushuRescue achieved 48.69\\%\ntranslation accuracy on 50 withheld sentences and generated NCSilver, a set of\n98 newly translated modern Chinese sentences of varying lengths. A sample of\nboth NCGold and NCSilver is included in the Supplementary Materials.\nAdditionally, we developed FastText-based and Seq2Seq models to further support\nresearch on N\\\"ushu. N\\\"ushuRescue provides a versatile and scalable tool for\nthe revitalization of endangered languages, minimizing the need for extensive\nhuman input.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by N\\\"ushu, a rare script historically used by Yao\nwomen in China for self-expression within a patriarchal society. To address\nthis challenge, we introduce N\\\"ushuRescue, an AI-driven framework designed to\ntrain large language models (LLMs) on endangered languages with minimal data.\nN\\\"ushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence N\\\"ushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\\\"ushu\nand only 35 short examples from NCGold, N\\\"ushuRescue achieved 48.69\\%\ntranslation accuracy on 50 withheld sentences and generated NCSilver, a set of\n98 newly translated modern Chinese sentences of varying lengths. A sample of\nboth NCGold and NCSilver is included in the Supplementary Materials.\nAdditionally, we developed FastText-based and Seq2Seq models to further support\nresearch on N\\\"ushu. N\\\"ushuRescue provides a versatile and scalable tool for\nthe revitalization of endangered languages, minimizing the need for extensive\nhuman input."
                },
                "authors": [
                    {
                        "name": "Ivory Yang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01269v2",
                "updated": "2024-12-03T04:37:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    37,
                    3,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search"
                },
                "summary": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Baijun Ji"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02159v1",
                "updated": "2024-12-03T04:34:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    58,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:34:58Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    58,
                    1,
                    338,
                    0
                ],
                "title": "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods\n  and a New Transcript-Classifier Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods\n  and a New Transcript-Classifier Approach"
                },
                "summary": "Defending large language models against jailbreaks so that they never engage\nin a broadly-defined set of forbidden behaviors is an open problem. In this\npaper, we investigate the difficulty of jailbreak-defense when we only want to\nforbid a narrowly-defined set of behaviors. As a case study, we focus on\npreventing an LLM from helping a user make a bomb. We find that popular\ndefenses such as safety training, adversarial training, and input/output\nclassifiers are unable to fully solve this problem. In pursuit of a better\nsolution, we develop a transcript-classifier defense which outperforms the\nbaseline defenses we test. However, our classifier defense still fails in some\ncircumstances, which highlights the difficulty of jailbreak-defense even in a\nnarrow domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending large language models against jailbreaks so that they never engage\nin a broadly-defined set of forbidden behaviors is an open problem. In this\npaper, we investigate the difficulty of jailbreak-defense when we only want to\nforbid a narrowly-defined set of behaviors. As a case study, we focus on\npreventing an LLM from helping a user make a bomb. We find that popular\ndefenses such as safety training, adversarial training, and input/output\nclassifiers are unable to fully solve this problem. In pursuit of a better\nsolution, we develop a transcript-classifier defense which outperforms the\nbaseline defenses we test. However, our classifier defense still fails in some\ncircumstances, which highlights the difficulty of jailbreak-defense even in a\nnarrow domain."
                },
                "authors": [
                    {
                        "name": "Tony T. Wang"
                    },
                    {
                        "name": "John Hughes"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Rylan Schaeffer"
                    },
                    {
                        "name": "Rajashree Agrawal"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Jesse Mu"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Ethan Perez"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Perez"
                },
                "author": "Ethan Perez",
                "arxiv_comment": "Accepted to the AdvML-Frontiers and SoLaR workshops at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12361v3",
                "updated": "2024-12-03T04:34:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-16T08:24:09Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance"
                },
                "summary": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Shenzhi Yang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Guirong Chen"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02155v1",
                "updated": "2024-12-03T04:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events"
                },
                "summary": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called \\textbf{CausalMob}, to analyze the causal effects of\npublic events. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called \\textbf{CausalMob}, to analyze the causal effects of\npublic events. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Xiaojie Yang"
                    },
                    {
                        "name": "Hangli Ge"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Zipei Fan"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ryosuke Shibasaki"
                    },
                    {
                        "name": "Noboru Koshizuka"
                    }
                ],
                "author_detail": {
                    "name": "Noboru Koshizuka"
                },
                "author": "Noboru Koshizuka",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18142v2",
                "updated": "2024-12-03T04:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    19,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-22T13:03:28Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    3,
                    28,
                    1,
                    296,
                    0
                ],
                "title": "Analyzing Nobel Prize Literature with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Nobel Prize Literature with Large Language Models"
                },
                "summary": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Cen Lu"
                    },
                    {
                        "name": "Jiaxin Tai"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Jinlin Yang"
                    },
                    {
                        "name": "Qixin Liu"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Longjun Ma"
                    },
                    {
                        "name": "Dajiang Zhu"
                    },
                    {
                        "name": "Yudan Ren"
                    },
                    {
                        "name": "Bao Ge"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ning Qiang"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00648v2",
                "updated": "2024-12-03T04:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    14,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T02:55:08Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    55,
                    8,
                    6,
                    336,
                    0
                ],
                "title": "DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated\n  LLMs with Refined Rotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated\n  LLMs with Refined Rotation"
                },
                "summary": "Rotating the activation and weight matrices to reduce the influence of\noutliers in large language models (LLMs) has recently attracted significant\nattention, particularly in the context of model quantization. Prior studies\nhave shown that in low-precision quantization scenarios, such as 4-bit weights\nand 4-bit activations (W4A4), randomized Hadamard transforms can achieve\nsignificantly higher accuracy than randomized orthogonal transforms. Notably,\nthe reason behind this phenomena remains unknown. In this paper, we find that\nthese transformations show substantial improvement in eliminating outliers for\ncommon tokens and achieve similar quantization error. The primary reason for\nthe accuracy difference lies in the fact that randomized Hadamard transforms\ncan slightly reduce the quantization error for tokens with massive activations\nwhile randomized orthogonal transforms increase the quantization error. Due to\nthe extreme rarity of these tokens and their critical impact on model accuracy,\nwe consider this a long-tail optimization problem, and therefore construct a\nsimple yet effective method: a weighted loss function. Additionally, we propose\nan optimization strategy for the rotation matrix that involves alternating\noptimization of quantization parameters while employing orthogonal Procrustes\ntransforms to refine the rotation matrix. This makes the distribution of the\nrotated activation values more conducive to quantization, especially for tokens\nwith massive activations. Our method enhances the Rotated LLMs by achieving\ndual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive\nexperiments demonstrate the effectiveness and efficiency of DFRot. By tuning\nthe rotation matrix using just a single sample, DFRot achieves a perplexity\nimprovement of 0.25 and 0.21 on W4A4KV4 and W4A4KV16, respectively, for\nLLaMA3-8B, a model known for its quantization challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotating the activation and weight matrices to reduce the influence of\noutliers in large language models (LLMs) has recently attracted significant\nattention, particularly in the context of model quantization. Prior studies\nhave shown that in low-precision quantization scenarios, such as 4-bit weights\nand 4-bit activations (W4A4), randomized Hadamard transforms can achieve\nsignificantly higher accuracy than randomized orthogonal transforms. Notably,\nthe reason behind this phenomena remains unknown. In this paper, we find that\nthese transformations show substantial improvement in eliminating outliers for\ncommon tokens and achieve similar quantization error. The primary reason for\nthe accuracy difference lies in the fact that randomized Hadamard transforms\ncan slightly reduce the quantization error for tokens with massive activations\nwhile randomized orthogonal transforms increase the quantization error. Due to\nthe extreme rarity of these tokens and their critical impact on model accuracy,\nwe consider this a long-tail optimization problem, and therefore construct a\nsimple yet effective method: a weighted loss function. Additionally, we propose\nan optimization strategy for the rotation matrix that involves alternating\noptimization of quantization parameters while employing orthogonal Procrustes\ntransforms to refine the rotation matrix. This makes the distribution of the\nrotated activation values more conducive to quantization, especially for tokens\nwith massive activations. Our method enhances the Rotated LLMs by achieving\ndual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive\nexperiments demonstrate the effectiveness and efficiency of DFRot. By tuning\nthe rotation matrix using just a single sample, DFRot achieves a perplexity\nimprovement of 0.25 and 0.21 on W4A4KV4 and W4A4KV16, respectively, for\nLLaMA3-8B, a model known for its quantization challenges."
                },
                "authors": [
                    {
                        "name": "Jingyang Xiang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "24 pages, 38 figures, source code\n  \\url{https://github.com/JingyangXiang/DFRot}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01004v2",
                "updated": "2024-12-03T04:13:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    13,
                    14,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T23:41:42Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    41,
                    42,
                    6,
                    336,
                    0
                ],
                "title": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA"
                },
                "summary": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge."
                },
                "authors": [
                    {
                        "name": "Haodong Lu"
                    },
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Jason Xue"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02149v1",
                "updated": "2024-12-03T04:09:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    9,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:09:36Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    9,
                    36,
                    1,
                    338,
                    0
                ],
                "title": "Leveraging Large Language Models for Comparative Literature\n  Summarization with Reflective Incremental Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Comparative Literature\n  Summarization with Reflective Incremental Mechanisms"
                },
                "summary": "In this paper, we introduce ChatCite, a novel method leveraging large\nlanguage models (LLMs) for generating comparative literature summaries. The\nability to summarize research papers with a focus on key comparisons between\nstudies is an essential task in academic research. Existing summarization\nmodels, while effective at generating concise summaries, fail to provide deep\ncomparative insights. ChatCite addresses this limitation by incorporating a\nmulti-step reasoning mechanism that extracts critical elements from papers,\nincrementally builds a comparative summary, and refines the output through a\nreflective memory process. We evaluate ChatCite on a custom dataset,\nCompLit-LongContext, consisting of 1000 research papers with annotated\ncomparative summaries. Experimental results show that ChatCite outperforms\nseveral baseline methods, including GPT-4, BART, T5, and CoT, across various\nautomatic evaluation metrics such as ROUGE and the newly proposed G-Score.\nHuman evaluation further confirms that ChatCite generates more coherent,\ninsightful, and fluent summaries compared to these baseline models. Our method\nprovides a significant advancement in automatic literature review generation,\noffering researchers a powerful tool for efficiently comparing and synthesizing\nscientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ChatCite, a novel method leveraging large\nlanguage models (LLMs) for generating comparative literature summaries. The\nability to summarize research papers with a focus on key comparisons between\nstudies is an essential task in academic research. Existing summarization\nmodels, while effective at generating concise summaries, fail to provide deep\ncomparative insights. ChatCite addresses this limitation by incorporating a\nmulti-step reasoning mechanism that extracts critical elements from papers,\nincrementally builds a comparative summary, and refines the output through a\nreflective memory process. We evaluate ChatCite on a custom dataset,\nCompLit-LongContext, consisting of 1000 research papers with annotated\ncomparative summaries. Experimental results show that ChatCite outperforms\nseveral baseline methods, including GPT-4, BART, T5, and CoT, across various\nautomatic evaluation metrics such as ROUGE and the newly proposed G-Score.\nHuman evaluation further confirms that ChatCite generates more coherent,\ninsightful, and fluent summaries compared to these baseline models. Our method\nprovides a significant advancement in automatic literature review generation,\noffering researchers a powerful tool for efficiently comparing and synthesizing\nscientific research."
                },
                "authors": [
                    {
                        "name": "Fernando Gabriela Garcia"
                    },
                    {
                        "name": "Spencer Burns"
                    },
                    {
                        "name": "Harrison Fuller"
                    }
                ],
                "author_detail": {
                    "name": "Harrison Fuller"
                },
                "author": "Harrison Fuller",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00759v2",
                "updated": "2024-12-03T04:00:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    0,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T10:32:47Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    10,
                    32,
                    47,
                    6,
                    336,
                    0
                ],
                "title": "DyMO: Training-Free Diffusion Model Alignment with Dynamic\n  Multi-Objective Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyMO: Training-Free Diffusion Model Alignment with Dynamic\n  Multi-Objective Scheduling"
                },
                "summary": "Text-to-image diffusion model alignment is critical for improving the\nalignment between the generated images and human preferences. While\ntraining-based methods are constrained by high computational costs and dataset\nrequirements, training-free alignment methods remain underexplored and are\noften limited by inaccurate guidance. We propose a plug-and-play training-free\nalignment method, DyMO, for aligning the generated images and human preferences\nduring inference. Apart from text-aware human preference scores, we introduce a\nsemantic alignment objective for enhancing the semantic alignment in the early\nstages of diffusion, relying on the fact that the attention maps are effective\nreflections of the semantics in noisy images. We propose dynamic scheduling of\nmultiple objectives and intermediate recurrent steps to reflect the\nrequirements at different steps. Experiments with diverse pre-trained diffusion\nmodels and metrics demonstrate the effectiveness and robustness of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion model alignment is critical for improving the\nalignment between the generated images and human preferences. While\ntraining-based methods are constrained by high computational costs and dataset\nrequirements, training-free alignment methods remain underexplored and are\noften limited by inaccurate guidance. We propose a plug-and-play training-free\nalignment method, DyMO, for aligning the generated images and human preferences\nduring inference. Apart from text-aware human preference scores, we introduce a\nsemantic alignment objective for enhancing the semantic alignment in the early\nstages of diffusion, relying on the fact that the attention maps are effective\nreflections of the semantics in noisy images. We propose dynamic scheduling of\nmultiple objectives and intermediate recurrent steps to reflect the\nrequirements at different steps. Experiments with diverse pre-trained diffusion\nmodels and metrics demonstrate the effectiveness and robustness of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18938v2",
                "updated": "2024-12-03T03:56:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    56,
                    52,
                    1,
                    338,
                    0
                ],
                "published": "2024-09-27T17:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding"
                },
                "summary": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding."
                },
                "authors": [
                    {
                        "name": "Heqing Zou"
                    },
                    {
                        "name": "Tianze Luo"
                    },
                    {
                        "name": "Guiyang Xie"
                    },
                    {
                        "name": "Victor"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Guangcong Wang"
                    },
                    {
                        "name": "Junyang Chen"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Hansheng Zhang"
                    },
                    {
                        "name": "Huaijian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huaijian Zhang"
                },
                "arxiv_affiliation": "Xiao Jie",
                "author": "Huaijian Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02130v1",
                "updated": "2024-12-03T03:36:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    36,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:36:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    36,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "A privacy-preserving distributed credible evidence fusion algorithm for\n  collective decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A privacy-preserving distributed credible evidence fusion algorithm for\n  collective decision-making"
                },
                "summary": "The theory of evidence reasoning has been applied to collective\ndecision-making in recent years. However, existing distributed evidence fusion\nmethods lead to participants' preference leakage and fusion failures as they\ndirectly exchange raw evidence and do not assess evidence credibility like\ncentralized credible evidence fusion (CCEF) does. To do so, a\nprivacy-preserving distributed credible evidence fusion method with three-level\nconsensus (PCEF) is proposed in this paper. In evidence difference measure\n(EDM) neighbor consensus, an evidence-free equivalent expression of EDM among\nneighbored agents is derived with the shared dot product protocol for pignistic\nprobability and the identical judgment of two events with maximal subjective\nprobabilities, so that evidence privacy is guaranteed due to such irreversible\nevidence transformation. In EDM network consensus, the non-neighbored EDMs are\ninferred and neighbored EDMs reach uniformity via interaction between linear\naverage consensus (LAC) and low-rank matrix completion with rank adaptation to\nguarantee EDM consensus convergence and no solution of inferring raw evidence\nin numerical iteration style. In fusion network consensus, a privacy-preserving\nLAC with a self-cancelling differential privacy term is proposed, where each\nagent adds its randomness to the sharing content and step-by-step cancels such\nrandomness in consensus iterations. Besides, the sufficient condition of the\nconvergence to the CCEF is explored, and it is proven that raw evidence is\nimpossibly inferred in such an iterative consensus. The simulations show that\nPCEF is close to CCEF both in credibility and fusion results and obtains higher\ndecision accuracy with less time-comsuming than existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The theory of evidence reasoning has been applied to collective\ndecision-making in recent years. However, existing distributed evidence fusion\nmethods lead to participants' preference leakage and fusion failures as they\ndirectly exchange raw evidence and do not assess evidence credibility like\ncentralized credible evidence fusion (CCEF) does. To do so, a\nprivacy-preserving distributed credible evidence fusion method with three-level\nconsensus (PCEF) is proposed in this paper. In evidence difference measure\n(EDM) neighbor consensus, an evidence-free equivalent expression of EDM among\nneighbored agents is derived with the shared dot product protocol for pignistic\nprobability and the identical judgment of two events with maximal subjective\nprobabilities, so that evidence privacy is guaranteed due to such irreversible\nevidence transformation. In EDM network consensus, the non-neighbored EDMs are\ninferred and neighbored EDMs reach uniformity via interaction between linear\naverage consensus (LAC) and low-rank matrix completion with rank adaptation to\nguarantee EDM consensus convergence and no solution of inferring raw evidence\nin numerical iteration style. In fusion network consensus, a privacy-preserving\nLAC with a self-cancelling differential privacy term is proposed, where each\nagent adds its randomness to the sharing content and step-by-step cancels such\nrandomness in consensus iterations. Besides, the sufficient condition of the\nconvergence to the CCEF is explored, and it is proven that raw evidence is\nimpossibly inferred in such an iterative consensus. The simulations show that\nPCEF is close to CCEF both in credibility and fusion results and obtains higher\ndecision accuracy with less time-comsuming than existing methods."
                },
                "authors": [
                    {
                        "name": "Chaoxiong Ma"
                    },
                    {
                        "name": "Yan Liang"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Huixia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huixia Zhang"
                },
                "author": "Huixia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01197v2",
                "updated": "2024-12-03T03:16:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    16,
                    54,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T06:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    59,
                    52,
                    0,
                    337,
                    0
                ],
                "title": "InstantSwap: Fast Customized Concept Swapping across Sharp Shape\n  Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstantSwap: Fast Customized Concept Swapping across Sharp Shape\n  Differences"
                },
                "summary": "Recent advances in Customized Concept Swapping (CCS) enable a text-to-image\nmodel to swap a concept in the source image with a customized target concept.\nHowever, the existing methods still face the challenges of inconsistency and\ninefficiency. They struggle to maintain consistency in both the foreground and\nbackground during concept swapping, especially when the shape difference is\nlarge between objects. Additionally, they either require time-consuming\ntraining processes or involve redundant calculations during inference. To\ntackle these issues, we introduce InstantSwap, a new CCS method that aims to\nhandle sharp shape disparity at speed. Specifically, we first extract the bbox\nof the object in the source image automatically based on attention map analysis\nand leverage the bbox to achieve both foreground and background consistency.\nFor background consistency, we remove the gradient outside the bbox during the\nswapping process so that the background is free from being modified. For\nforeground consistency, we employ a cross-attention mechanism to inject\nsemantic information into both source and target concepts inside the box. This\nhelps learn semantic-enhanced representations that encourage the swapping\nprocess to focus on the foreground objects. To improve swapping speed, we avoid\ncomputing gradients at each timestep but instead calculate them periodically to\nreduce the number of forward passes, which improves efficiency a lot with a\nlittle sacrifice on performance. Finally, we establish a benchmark dataset to\nfacilitate comprehensive evaluation. Extensive evaluations demonstrate the\nsuperiority and versatility of InstantSwap. Project Page:\nhttps://instantswap.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Customized Concept Swapping (CCS) enable a text-to-image\nmodel to swap a concept in the source image with a customized target concept.\nHowever, the existing methods still face the challenges of inconsistency and\ninefficiency. They struggle to maintain consistency in both the foreground and\nbackground during concept swapping, especially when the shape difference is\nlarge between objects. Additionally, they either require time-consuming\ntraining processes or involve redundant calculations during inference. To\ntackle these issues, we introduce InstantSwap, a new CCS method that aims to\nhandle sharp shape disparity at speed. Specifically, we first extract the bbox\nof the object in the source image automatically based on attention map analysis\nand leverage the bbox to achieve both foreground and background consistency.\nFor background consistency, we remove the gradient outside the bbox during the\nswapping process so that the background is free from being modified. For\nforeground consistency, we employ a cross-attention mechanism to inject\nsemantic information into both source and target concepts inside the box. This\nhelps learn semantic-enhanced representations that encourage the swapping\nprocess to focus on the foreground objects. To improve swapping speed, we avoid\ncomputing gradients at each timestep but instead calculate them periodically to\nreduce the number of forward passes, which improves efficiency a lot with a\nlittle sacrifice on performance. Finally, we establish a benchmark dataset to\nfacilitate comprehensive evaluation. Extensive evaluations demonstrate the\nsuperiority and versatility of InstantSwap. Project Page:\nhttps://instantswap.github.io/"
                },
                "authors": [
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Longxiang Tang"
                    },
                    {
                        "name": "Chengyu Fang"
                    },
                    {
                        "name": "Chubin Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Xiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Li"
                },
                "author": "Xiu Li",
                "arxiv_comment": "Project Page: https://instantswap.github.io/. Github Page:\n  https://github.com/chenyangzhu1/InstantSwap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v3",
                "updated": "2024-12-03T03:16:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    16,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02114v1",
                "updated": "2024-12-03T03:10:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:10:19Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    19,
                    1,
                    338,
                    0
                ],
                "title": "OmniCreator: Self-Supervised Unified Generation with Universal Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCreator: Self-Supervised Unified Generation with Universal Editing"
                },
                "summary": "We introduce OmniCreator, a novel framework that can conduct text-prompted\nunified (image+video) generation as well as editing all in one place.\nOmniCreator acquires generative and universal editing capabilities in a\nself-supervised manner, taking original text-video pairs as conditions while\nutilizing the same video as a denoising target to learn the semantic\ncorrespondence between video and text. During inference, when presented with a\ntext prompt and a video, OmniCreator is capable of generating a target that is\nfaithful to both, achieving a universal editing effect that is unconstrained as\nopposed to existing editing work that primarily focuses on certain editing\ntypes or relies on additional controls (e.g., structural conditions, attention\nfeatures, or DDIM inversion). On the other hand, when presented with a text\nprompt only, OmniCreator becomes generative, producing high-quality video as a\nresult of the semantic correspondence learned. Importantly, we found that the\nsame capabilities extend to images as is, making OmniCreator a truly unified\nframework. Further, due to the lack of existing generative video editing\nbenchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the\nperformance of generative video editing models comprehensively. Extensive\nexperiments demonstrate that OmniCreator exhibits substantial superiority over\nall other models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OmniCreator, a novel framework that can conduct text-prompted\nunified (image+video) generation as well as editing all in one place.\nOmniCreator acquires generative and universal editing capabilities in a\nself-supervised manner, taking original text-video pairs as conditions while\nutilizing the same video as a denoising target to learn the semantic\ncorrespondence between video and text. During inference, when presented with a\ntext prompt and a video, OmniCreator is capable of generating a target that is\nfaithful to both, achieving a universal editing effect that is unconstrained as\nopposed to existing editing work that primarily focuses on certain editing\ntypes or relies on additional controls (e.g., structural conditions, attention\nfeatures, or DDIM inversion). On the other hand, when presented with a text\nprompt only, OmniCreator becomes generative, producing high-quality video as a\nresult of the semantic correspondence learned. Importantly, we found that the\nsame capabilities extend to images as is, making OmniCreator a truly unified\nframework. Further, due to the lack of existing generative video editing\nbenchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the\nperformance of generative video editing models comprehensively. Extensive\nexperiments demonstrate that OmniCreator exhibits substantial superiority over\nall other models."
                },
                "authors": [
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Lan Wang"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ser-Nam Lim"
                },
                "author": "Ser-Nam Lim",
                "arxiv_comment": "Project: https://haroldchen19.github.io/OmniCreator-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.02689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02689v1",
                "updated": "2024-12-03T18:58:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    58,
                    11,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:58:11Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    58,
                    11,
                    1,
                    338,
                    0
                ],
                "title": "Preliminary Investigation into Data Scaling Laws for Imitation\n  Learning-Based End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preliminary Investigation into Data Scaling Laws for Imitation\n  Learning-Based End-to-End Autonomous Driving"
                },
                "summary": "The end-to-end autonomous driving paradigm has recently attracted lots of\nattention due to its scalability. However, existing methods are constrained by\nthe limited scale of real-world data, which hinders a comprehensive exploration\nof the scaling laws associated with end-to-end autonomous driving. To address\nthis issue, we collected substantial data from various driving scenarios and\nbehaviors and conducted an extensive study on the scaling laws of existing\nimitation learning-based end-to-end autonomous driving paradigms. Specifically,\napproximately 4 million demonstrations from 23 different scenario types were\ngathered, amounting to over 30,000 hours of driving demonstrations. We\nperformed open-loop evaluations and closed-loop simulation evaluations in 1,400\ndiverse driving demonstrations (1,300 for open-loop and 100 for closed-loop)\nunder stringent assessment conditions. Through experimental analysis, we\ndiscovered that (1) the performance of the driving model exhibits a power-law\nrelationship with the amount of training data; (2) a small increase in the\nquantity of long-tailed data can significantly improve the performance for the\ncorresponding scenarios; (3) appropriate scaling of data enables the model to\nachieve combinatorial generalization in novel scenes and actions. Our results\nhighlight the critical role of data scaling in improving the generalizability\nof models across diverse autonomous driving scenarios, assuring safe deployment\nin the real world. Project repository:\nhttps://github.com/ucaszyp/Driving-Scaling-Law",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The end-to-end autonomous driving paradigm has recently attracted lots of\nattention due to its scalability. However, existing methods are constrained by\nthe limited scale of real-world data, which hinders a comprehensive exploration\nof the scaling laws associated with end-to-end autonomous driving. To address\nthis issue, we collected substantial data from various driving scenarios and\nbehaviors and conducted an extensive study on the scaling laws of existing\nimitation learning-based end-to-end autonomous driving paradigms. Specifically,\napproximately 4 million demonstrations from 23 different scenario types were\ngathered, amounting to over 30,000 hours of driving demonstrations. We\nperformed open-loop evaluations and closed-loop simulation evaluations in 1,400\ndiverse driving demonstrations (1,300 for open-loop and 100 for closed-loop)\nunder stringent assessment conditions. Through experimental analysis, we\ndiscovered that (1) the performance of the driving model exhibits a power-law\nrelationship with the amount of training data; (2) a small increase in the\nquantity of long-tailed data can significantly improve the performance for the\ncorresponding scenarios; (3) appropriate scaling of data enables the model to\nachieve combinatorial generalization in novel scenes and actions. Our results\nhighlight the critical role of data scaling in improving the generalizability\nof models across diverse autonomous driving scenarios, assuring safe deployment\nin the real world. Project repository:\nhttps://github.com/ucaszyp/Driving-Scaling-Law"
                },
                "authors": [
                    {
                        "name": "Yupeng Zheng"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Teng Zhang"
                    },
                    {
                        "name": "Ben Lu"
                    },
                    {
                        "name": "Xiaochuang Huo"
                    },
                    {
                        "name": "Chao Han"
                    },
                    {
                        "name": "Yixian Li"
                    },
                    {
                        "name": "Mengjie Yu"
                    },
                    {
                        "name": "Bu Jin"
                    },
                    {
                        "name": "Pengxuan Yang"
                    },
                    {
                        "name": "Yuhang Zheng"
                    },
                    {
                        "name": "Haifeng Yuan"
                    },
                    {
                        "name": "Ke Jiang"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Xianpeng Lang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02685v1",
                "updated": "2024-12-03T18:56:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    7,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:56:07Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    56,
                    7,
                    1,
                    338,
                    0
                ],
                "title": "T-REG: Preference Optimization with Token-Level Reward Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-REG: Preference Optimization with Token-Level Reward Regularization"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Tao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Tao Meng"
                },
                "author": "Tao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14052v2",
                "updated": "2024-12-03T18:48:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    48,
                    0,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-17T21:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    47,
                    11,
                    3,
                    291,
                    0
                ],
                "title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs"
                },
                "summary": "Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management."
                },
                "authors": [
                    {
                        "name": "Alireza Rezazadeh"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    }
                ],
                "author_detail": {
                    "name": "Yujia Bao"
                },
                "author": "Yujia Bao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02674v1",
                "updated": "2024-12-03T18:47:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models"
                },
                "summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Udaya Ghai"
                    }
                ],
                "author_detail": {
                    "name": "Udaya Ghai"
                },
                "author": "Udaya Ghai",
                "arxiv_comment": "41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02655v1",
                "updated": "2024-12-03T18:29:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    29,
                    37,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:29:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    29,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation\n  with Instructional Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation\n  with Instructional Inputs"
                },
                "summary": "Autonomous navigation guided by natural language instructions is essential\nfor improving human-robot interaction and enabling complex operations in\ndynamic environments. While large language models (LLMs) are not inherently\ndesigned for planning, they can significantly enhance planning efficiency by\nproviding guidance and informing constraints to ensure safety. This paper\nintroduces a planning framework that integrates LLMs with 2D occupancy grid\nmaps and natural language commands to improve spatial reasoning and task\nexecution in resource-limited settings. By decomposing high-level commands and\nreal-time environmental data, the system generates structured navigation plans\nfor pick-and-place tasks, including obstacle avoidance, goal prioritization,\nand adaptive behaviors. The framework dynamically recalculates paths to address\nenvironmental changes and aligns with implicit social norms for seamless\nhuman-robot interaction. Our results demonstrates the potential of LLMs to\ndesign context-aware system to enhance navigation efficiency and safety in\nindustrial and dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation guided by natural language instructions is essential\nfor improving human-robot interaction and enabling complex operations in\ndynamic environments. While large language models (LLMs) are not inherently\ndesigned for planning, they can significantly enhance planning efficiency by\nproviding guidance and informing constraints to ensure safety. This paper\nintroduces a planning framework that integrates LLMs with 2D occupancy grid\nmaps and natural language commands to improve spatial reasoning and task\nexecution in resource-limited settings. By decomposing high-level commands and\nreal-time environmental data, the system generates structured navigation plans\nfor pick-and-place tasks, including obstacle avoidance, goal prioritization,\nand adaptive behaviors. The framework dynamically recalculates paths to address\nenvironmental changes and aligns with implicit social norms for seamless\nhuman-robot interaction. Our results demonstrates the potential of LLMs to\ndesign context-aware system to enhance navigation efficiency and safety in\nindustrial and dynamic environments."
                },
                "authors": [
                    {
                        "name": "Pranav Doma"
                    },
                    {
                        "name": "Aliasghar Arab"
                    },
                    {
                        "name": "Xuesu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xuesu Xiao"
                },
                "author": "Xuesu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02638v1",
                "updated": "2024-12-03T18:10:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    10,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T18:10:31Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    10,
                    31,
                    1,
                    338,
                    0
                ],
                "title": "QA-TOOLBOX: Conversational Question-Answering for process task guidance\n  in manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-TOOLBOX: Conversational Question-Answering for process task guidance\n  in manufacturing"
                },
                "summary": "In this work we explore utilizing LLMs for data augmentation for\nmanufacturing task guidance system. The dataset consists of representative\nsamples of interactions with technicians working in an advanced manufacturing\nsetting. The purpose of this work to explore the task, data augmentation for\nthe supported tasks and evaluating the performance of the existing LLMs. We\nobserve that that task is complex requiring understanding from procedure\nspecification documents, actions and objects sequenced temporally. The dataset\nconsists of 200,000+ question/answer pairs that refer to the spec document and\nare grounded in narrations and/or video demonstrations. We compared the\nperformance of several popular open-sourced LLMs by developing a baseline using\neach LLM and then compared the responses in a reference-free setting using\nLLM-as-a-judge and compared the ratings with crowd-workers whilst validating\nthe ratings with experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we explore utilizing LLMs for data augmentation for\nmanufacturing task guidance system. The dataset consists of representative\nsamples of interactions with technicians working in an advanced manufacturing\nsetting. The purpose of this work to explore the task, data augmentation for\nthe supported tasks and evaluating the performance of the existing LLMs. We\nobserve that that task is complex requiring understanding from procedure\nspecification documents, actions and objects sequenced temporally. The dataset\nconsists of 200,000+ question/answer pairs that refer to the spec document and\nare grounded in narrations and/or video demonstrations. We compared the\nperformance of several popular open-sourced LLMs by developing a baseline using\neach LLM and then compared the responses in a reference-free setting using\nLLM-as-a-judge and compared the ratings with crowd-workers whilst validating\nthe ratings with experts."
                },
                "authors": [
                    {
                        "name": "Ramesh Manuvinakurike"
                    },
                    {
                        "name": "Elizabeth Watkins"
                    },
                    {
                        "name": "Celal Savur"
                    },
                    {
                        "name": "Anthony Rhodes"
                    },
                    {
                        "name": "Sovan Biswas"
                    },
                    {
                        "name": "Gesem Gudino Mejia"
                    },
                    {
                        "name": "Richard Beckwith"
                    },
                    {
                        "name": "Saurav Sahay"
                    },
                    {
                        "name": "Giuseppe Raffa"
                    },
                    {
                        "name": "Lama Nachman"
                    }
                ],
                "author_detail": {
                    "name": "Lama Nachman"
                },
                "author": "Lama Nachman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02627v1",
                "updated": "2024-12-03T17:56:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    56,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:56:23Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    56,
                    23,
                    1,
                    338,
                    0
                ],
                "title": "Continual Learning of Personalized Generative Face Models with\n  Experience Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning of Personalized Generative Face Models with\n  Experience Replay"
                },
                "summary": "We introduce a novel continual learning problem: how to sequentially update\nthe weights of a personalized 2D and 3D generative face model as new batches of\nphotos in different appearances, styles, poses, and lighting are captured\nregularly. We observe that naive sequential fine-tuning of the model leads to\ncatastrophic forgetting of past representations of the individual's face. We\nthen demonstrate that a simple random sampling-based experience replay method\nis effective at mitigating catastrophic forgetting when a relatively large\nnumber of images can be stored and replayed. However, for long-term deployment\nof these models with relatively smaller storage, this simple random\nsampling-based replay technique also forgets past representations. Thus, we\nintroduce a novel experience replay algorithm that combines random sampling\nwith StyleGAN's latent space to represent the buffer as an optimal convex hull.\nWe observe that our proposed convex hull-based experience replay is more\neffective in preventing forgetting than a random sampling baseline and the\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel continual learning problem: how to sequentially update\nthe weights of a personalized 2D and 3D generative face model as new batches of\nphotos in different appearances, styles, poses, and lighting are captured\nregularly. We observe that naive sequential fine-tuning of the model leads to\ncatastrophic forgetting of past representations of the individual's face. We\nthen demonstrate that a simple random sampling-based experience replay method\nis effective at mitigating catastrophic forgetting when a relatively large\nnumber of images can be stored and replayed. However, for long-term deployment\nof these models with relatively smaller storage, this simple random\nsampling-based replay technique also forgets past representations. Thus, we\nintroduce a novel experience replay algorithm that combines random sampling\nwith StyleGAN's latent space to represent the buffer as an optimal convex hull.\nWe observe that our proposed convex hull-based experience replay is more\neffective in preventing forgetting than a random sampling baseline and the\nlower bound."
                },
                "authors": [
                    {
                        "name": "Annie N. Wang"
                    },
                    {
                        "name": "Luchao Qi"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "arxiv_comment": "Accepted to WACV 2025. Project page (incl. supplementary materials):\n  https://anniedde.github.io/personalizedcontinuallearning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v1",
                "updated": "2024-12-03T17:54:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_journal_ref": "Varun, Y., Madhavan, R., Addepalli, S., Suggala, A., Shanmugam,\n  K., & Jain, P. Time-Reversal Provides Unsupervised Feedback to LLMs. In The\n  Thirty-Eighth Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00608v2",
                "updated": "2024-12-03T17:49:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    49,
                    2,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-30T23:11:44Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    23,
                    11,
                    44,
                    5,
                    335,
                    0
                ],
                "title": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation"
                },
                "summary": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadeq Abolhasani"
                    },
                    {
                        "name": "Rong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Rong Pan"
                },
                "author": "Rong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02611v1",
                "updated": "2024-12-03T17:41:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    41,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:41:23Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    41,
                    23,
                    1,
                    338,
                    0
                ],
                "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?"
                },
                "summary": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Kaixiong Gong"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Bohao Li"
                    },
                    {
                        "name": "Yibing Wang"
                    },
                    {
                        "name": "Mofan Cheng"
                    },
                    {
                        "name": "Shijia Yang"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Yutong Bai"
                    },
                    {
                        "name": "Zhuoran Yang"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Project page: https://av-odyssey.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v2",
                "updated": "2024-12-03T17:38:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    38,
                    54,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02605v1",
                "updated": "2024-12-03T17:34:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    34,
                    50,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:34:50Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    34,
                    50,
                    1,
                    338,
                    0
                ],
                "title": "Interpretable Company Similarity with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Company Similarity with Sparse Autoencoders"
                },
                "summary": "Determining company similarity is a vital task in finance, underpinning\nhedging, risk management, portfolio diversification, and more. Practitioners\noften rely on sector and industry classifications to gauge similarity, such as\nSIC-codes and GICS-codes, the former being used by the U.S. Securities and\nExchange Commission (SEC), and the latter widely used by the investment\ncommunity. Clustering embeddings of company descriptions has been proposed as a\npotential technique for determining company similarity, but the lack of\ninterpretability in token embeddings poses a significant barrier to adoption in\nhigh-stakes contexts. Sparse Autoencoders have shown promise in enhancing the\ninterpretability of Large Language Models by decomposing LLM activations into\ninterpretable features. In this paper, we explore the use of SAE features in\nmeasuring company similarity and benchmark them against (1) SIC codes and (2)\nMajor Group codes. We conclude that SAE features can reproduce and even surpass\nsector classifications in quantifying fundamental characteristics of companies,\nevaluated by the correlation of monthly returns, a proxy for similarity, and\nPnL from cointegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining company similarity is a vital task in finance, underpinning\nhedging, risk management, portfolio diversification, and more. Practitioners\noften rely on sector and industry classifications to gauge similarity, such as\nSIC-codes and GICS-codes, the former being used by the U.S. Securities and\nExchange Commission (SEC), and the latter widely used by the investment\ncommunity. Clustering embeddings of company descriptions has been proposed as a\npotential technique for determining company similarity, but the lack of\ninterpretability in token embeddings poses a significant barrier to adoption in\nhigh-stakes contexts. Sparse Autoencoders have shown promise in enhancing the\ninterpretability of Large Language Models by decomposing LLM activations into\ninterpretable features. In this paper, we explore the use of SAE features in\nmeasuring company similarity and benchmark them against (1) SIC codes and (2)\nMajor Group codes. We conclude that SAE features can reproduce and even surpass\nsector classifications in quantifying fundamental characteristics of companies,\nevaluated by the correlation of monthly returns, a proxy for similarity, and\nPnL from cointegration."
                },
                "authors": [
                    {
                        "name": "Marco Molinari"
                    },
                    {
                        "name": "Vladimir Tregubiak"
                    },
                    {
                        "name": "Victor Shao"
                    },
                    {
                        "name": "Abhimanyu Pandey"
                    },
                    {
                        "name": "Mateusz Mikolajczak"
                    },
                    {
                        "name": "Sebastião Kuznetsov Ryder Torres Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Sebastião Kuznetsov Ryder Torres Pereira"
                },
                "author": "Sebastião Kuznetsov Ryder Torres Pereira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02602v1",
                "updated": "2024-12-03T17:32:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    32,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:32:47Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    32,
                    47,
                    1,
                    338,
                    0
                ],
                "title": "CEGI: Measuring the trade-off between efficiency and carbon emissions\n  for SLMs and VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEGI: Measuring the trade-off between efficiency and carbon emissions\n  for SLMs and VLMs"
                },
                "summary": "This paper analyzes the performance of Small Language Models (SLMs) and\nVision Language Models (VLMs) and evaluates the trade-off between model\nperformance and carbon emissions across 4 essential tasks: Image Captioning,\nVisual Question Answering (VQA), Dialogue Summarization and Text-to-SQL\nconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of\nparameters, quantization level and fine-tuning parameters are evaluated. The\nmodel variant's performance and carbon emissions are calculated. To quantify\nthe trade-off between model performance and carbon emissions, we introduce a\nnovel metric called CEGI (Carbon Efficient Gain Index). This metric represents\nthe carbon emission per unit percentage gain per million trainable parameters .\nThis metric provides a normalized measure to compare model's efficiency in\nterms of performance improvement relative to their environmental cost. The\nexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve\nperformance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal\ngains in accuracy from larger models do not justify the substantial increase in\ncarbon emissions. Leveraging lower-bit quantization levels, the proposed metric\nfurther enhances energy efficiency without compromising performance. This study\nhighlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for\nenvironmentally-friendly AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper analyzes the performance of Small Language Models (SLMs) and\nVision Language Models (VLMs) and evaluates the trade-off between model\nperformance and carbon emissions across 4 essential tasks: Image Captioning,\nVisual Question Answering (VQA), Dialogue Summarization and Text-to-SQL\nconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture\nfamily are chosen and variants based on model size in terms of the number of\nparameters, quantization level and fine-tuning parameters are evaluated. The\nmodel variant's performance and carbon emissions are calculated. To quantify\nthe trade-off between model performance and carbon emissions, we introduce a\nnovel metric called CEGI (Carbon Efficient Gain Index). This metric represents\nthe carbon emission per unit percentage gain per million trainable parameters .\nThis metric provides a normalized measure to compare model's efficiency in\nterms of performance improvement relative to their environmental cost. The\nexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve\nperformance levels comparable to Large Language Models (LLMs) while producing\nsignificantly less carbon emissions. Our findings suggest that the marginal\ngains in accuracy from larger models do not justify the substantial increase in\ncarbon emissions. Leveraging lower-bit quantization levels, the proposed metric\nfurther enhances energy efficiency without compromising performance. This study\nhighlights balancing high performance and environmental sustainability. It\noffers a valuable metric for selecting models suitable for\nenvironmentally-friendly AI development."
                },
                "authors": [
                    {
                        "name": "Abhas Kumar"
                    },
                    {
                        "name": "Kapil Pathak"
                    },
                    {
                        "name": "Rajesh Kavuru"
                    },
                    {
                        "name": "Prabhakar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Prabhakar Srinivasan"
                },
                "author": "Prabhakar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02594v1",
                "updated": "2024-12-03T17:26:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    26,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:26:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    26,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "PrefixLLM: LLM-aided Prefix Circuit Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixLLM: LLM-aided Prefix Circuit Design"
                },
                "summary": "Prefix circuits are fundamental components in digital adders, widely used in\ndigital systems due to their efficiency in calculating carry signals.\nSynthesizing prefix circuits with minimized area and delay is crucial for\nenhancing the performance of modern computing systems. Recently, large language\nmodels (LLMs) have demonstrated a surprising ability to perform text generation\ntasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.\nPrefixLLM transforms the prefix circuit synthesis task into a structured text\ngeneration problem, termed the Structured Prefix Circuit Representation (SPCR),\nand introduces an iterative framework to automatically and accurately generate\nvalid SPCRs. We further present a design space exploration (DSE) framework that\nuses LLMs to iteratively search for area and delay optimized prefix circuits.\nCompared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the\nsame delay constraint. This work highlights the use of LLMs in the synthesis of\narithmetic circuits, which can be transformed into the structured text\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix circuits are fundamental components in digital adders, widely used in\ndigital systems due to their efficiency in calculating carry signals.\nSynthesizing prefix circuits with minimized area and delay is crucial for\nenhancing the performance of modern computing systems. Recently, large language\nmodels (LLMs) have demonstrated a surprising ability to perform text generation\ntasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.\nPrefixLLM transforms the prefix circuit synthesis task into a structured text\ngeneration problem, termed the Structured Prefix Circuit Representation (SPCR),\nand introduces an iterative framework to automatically and accurately generate\nvalid SPCRs. We further present a design space exploration (DSE) framework that\nuses LLMs to iteratively search for area and delay optimized prefix circuits.\nCompared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the\nsame delay constraint. This work highlights the use of LLMs in the synthesis of\narithmetic circuits, which can be transformed into the structured text\ngeneration."
                },
                "authors": [
                    {
                        "name": "Weihua Xiao"
                    },
                    {
                        "name": "Venkata Sai Charan Putrevu"
                    },
                    {
                        "name": "Raghu Vamshi Hemadri"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02592v1",
                "updated": "2024-12-03T17:23:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    23,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:23:47Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    23,
                    47,
                    1,
                    338,
                    0
                ],
                "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench"
                },
                "authors": [
                    {
                        "name": "Junyuan Zhang"
                    },
                    {
                        "name": "Qintong Zhang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Linke Ouyang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Ka-Ho Chow"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02588v1",
                "updated": "2024-12-03T17:17:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    17,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T17:17:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    17,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Explainable CTR Prediction via LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable CTR Prediction via LLM Reasoning"
                },
                "summary": "Recommendation Systems have become integral to modern user experiences, but\nlack transparency in their decision-making processes. Existing explainable\nrecommendation methods are hindered by reliance on a post-hoc paradigm, wherein\nexplanation generators are trained independently of the underlying recommender\nmodels. This paradigm necessitates substantial human effort in data\nconstruction and raises concerns about explanation reliability. In this paper,\nwe present ExpCTR, a novel framework that integrates large language model based\nexplanation generation directly into the CTR prediction process. Inspired by\nrecent advances in reinforcement learning, we employ two carefully designed\nreward mechanisms, LC alignment, which ensures explanations reflect user\nintentions, and IC alignment, which maintains consistency with traditional\nID-based CTR models. Our approach incorporates an efficient training paradigm\nwith LoRA and a three-stage iterative process. ExpCTR circumvents the need for\nextensive explanation datasets while fostering synergy between CTR prediction\nand explanation generation. Experimental results demonstrate that ExpCTR\nsignificantly enhances both recommendation accuracy and interpretability across\nthree real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation Systems have become integral to modern user experiences, but\nlack transparency in their decision-making processes. Existing explainable\nrecommendation methods are hindered by reliance on a post-hoc paradigm, wherein\nexplanation generators are trained independently of the underlying recommender\nmodels. This paradigm necessitates substantial human effort in data\nconstruction and raises concerns about explanation reliability. In this paper,\nwe present ExpCTR, a novel framework that integrates large language model based\nexplanation generation directly into the CTR prediction process. Inspired by\nrecent advances in reinforcement learning, we employ two carefully designed\nreward mechanisms, LC alignment, which ensures explanations reflect user\nintentions, and IC alignment, which maintains consistency with traditional\nID-based CTR models. Our approach incorporates an efficient training paradigm\nwith LoRA and a three-stage iterative process. ExpCTR circumvents the need for\nextensive explanation datasets while fostering synergy between CTR prediction\nand explanation generation. Experimental results demonstrate that ExpCTR\nsignificantly enhances both recommendation accuracy and interpretability across\nthree real-world datasets."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Chong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chong Chen"
                },
                "author": "Chong Chen",
                "arxiv_comment": "WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v2",
                "updated": "2024-12-03T16:55:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    55,
                    24,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02563v1",
                "updated": "2024-12-03T16:52:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:52:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "Semantic Tokens in Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Tokens in Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered\nsignificant attention for their ability to improve truth grounding and\ncoherence in natural language processing tasks. However, the reliability of RAG\nsystems in producing accurate answers diminishes as the volume of data they\naccess increases. Even with smaller datasets, these systems occasionally fail\nto address simple queries. This issue arises from their dependence on\nstate-of-the-art large language models (LLMs), which can introduce uncertainty\ninto the system's outputs. In this work, I propose a novel Comparative RAG\nsystem that introduces an evaluator module to bridge the gap between\nprobabilistic RAG systems and deterministically verifiable responses. The\nevaluator compares external recommendations with the retrieved document chunks,\nadding a decision-making layer that enhances the system's reliability. This\napproach ensures that the chunks retrieved are both semantically relevant and\nlogically consistent with deterministic insights, thereby improving the\naccuracy and overall efficiency of RAG systems. This framework paves the way\nfor more reliable and scalable question-answering applications in domains\nrequiring high precision and verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) architectures have recently garnered\nsignificant attention for their ability to improve truth grounding and\ncoherence in natural language processing tasks. However, the reliability of RAG\nsystems in producing accurate answers diminishes as the volume of data they\naccess increases. Even with smaller datasets, these systems occasionally fail\nto address simple queries. This issue arises from their dependence on\nstate-of-the-art large language models (LLMs), which can introduce uncertainty\ninto the system's outputs. In this work, I propose a novel Comparative RAG\nsystem that introduces an evaluator module to bridge the gap between\nprobabilistic RAG systems and deterministically verifiable responses. The\nevaluator compares external recommendations with the retrieved document chunks,\nadding a decision-making layer that enhances the system's reliability. This\napproach ensures that the chunks retrieved are both semantically relevant and\nlogically consistent with deterministic insights, thereby improving the\naccuracy and overall efficiency of RAG systems. This framework paves the way\nfor more reliable and scalable question-answering applications in domains\nrequiring high precision and verifiability."
                },
                "authors": [
                    {
                        "name": "Joel Suro"
                    }
                ],
                "author_detail": {
                    "name": "Joel Suro"
                },
                "author": "Joel Suro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02549v1",
                "updated": "2024-12-03T16:43:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    43,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:43:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    43,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "Patent-CR: A Dataset for Patent Claim Revision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent-CR: A Dataset for Patent Claim Revision"
                },
                "summary": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Pascal A Scherz"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_comment": "15 pages, 6 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v2",
                "updated": "2024-12-03T16:37:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    37,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Wenyi Zhao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02538v1",
                "updated": "2024-12-03T16:32:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    32,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    32,
                    19,
                    1,
                    338,
                    0
                ],
                "title": "On the Privacy, Security, and Trustworthy for Distributed Wireless Large\n  AI Model (WLAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Privacy, Security, and Trustworthy for Distributed Wireless Large\n  AI Model (WLAM)"
                },
                "summary": "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, the detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM is provided in the\naspect of electromagnetic signal processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, the detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM is provided in the\naspect of electromagnetic signal processing."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Zhijin Qin"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02525v1",
                "updated": "2024-12-03T16:18:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T16:18:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "LLMForecaster: Improving Seasonal Event Forecasts with Unstructured\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMForecaster: Improving Seasonal Event Forecasts with Unstructured\n  Textual Data"
                },
                "summary": "Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Chuck Arvin"
                    },
                    {
                        "name": "Dmitry Efimov"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Dominique Perrault-Joncas"
                    },
                    {
                        "name": "Shankar Ramasubramanian"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    },
                    {
                        "name": "Malcolm Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Malcolm Wolff"
                },
                "author": "Malcolm Wolff",
                "arxiv_comment": "Presented at NeurIPS Time Series in the Age of Large Models (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16019v2",
                "updated": "2024-12-03T16:18:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    18,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-04-24T17:51:36Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    17,
                    51,
                    36,
                    2,
                    115,
                    0
                ],
                "title": "The PRISM Alignment Dataset: What Participatory, Representative and\n  Individualised Human Feedback Reveals About the Subjective and Multicultural\n  Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PRISM Alignment Dataset: What Participatory, Representative and\n  Individualised Human Feedback Reveals About the Subjective and Multicultural\n  Alignment of Large Language Models"
                },
                "summary": "Human feedback is central to the alignment of Large Language Models (LLMs).\nHowever, open questions remain about methods (how), domains (where), people\n(who) and objectives (to what end) of feedback processes. To navigate these\nquestions, we introduce PRISM, a dataset that maps the sociodemographics and\nstated preferences of 1,500 diverse participants from 75 countries, to their\ncontextual preferences and fine-grained feedback in 8,011 live conversations\nwith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic\nparticipation in feedback; (ii) census-representative samples for two countries\n(UK, US); and (iii) individualised ratings that link to detailed participant\nprofiles, permitting personalisation and attribution of sample artefacts. We\ntarget subjective and multicultural perspectives on value-laden and\ncontroversial issues, where we expect interpersonal and cross-cultural\ndisagreement. We use PRISM in three case studies to demonstrate the need for\ncareful consideration of which humans provide what alignment data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback is central to the alignment of Large Language Models (LLMs).\nHowever, open questions remain about methods (how), domains (where), people\n(who) and objectives (to what end) of feedback processes. To navigate these\nquestions, we introduce PRISM, a dataset that maps the sociodemographics and\nstated preferences of 1,500 diverse participants from 75 countries, to their\ncontextual preferences and fine-grained feedback in 8,011 live conversations\nwith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic\nparticipation in feedback; (ii) census-representative samples for two countries\n(UK, US); and (iii) individualised ratings that link to detailed participant\nprofiles, permitting personalisation and attribution of sample artefacts. We\ntarget subjective and multicultural perspectives on value-laden and\ncontroversial issues, where we expect interpersonal and cross-cultural\ndisagreement. We use PRISM in three case studies to demonstrate the need for\ncareful consideration of which humans provide what alignment data."
                },
                "authors": [
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Alexander Whitefield"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Andrew Bean"
                    },
                    {
                        "name": "Katerina Margatina"
                    },
                    {
                        "name": "Juan Ciro"
                    },
                    {
                        "name": "Rafael Mosquera"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Scott A. Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A. Hale"
                },
                "author": "Scott A. Hale",
                "arxiv_journal_ref": "The Thirty-eight Conference on Neural Information Processing\n  Systems Datasets and Benchmarks Track (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12317v2",
                "updated": "2024-12-03T15:56:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    15,
                    56,
                    26,
                    1,
                    338,
                    0
                ],
                "published": "2024-02-19T17:37:28Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    17,
                    37,
                    28,
                    0,
                    50,
                    0
                ],
                "title": "EVOR: Evolving Retrieval for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOR: Evolving Retrieval for Code Generation"
                },
                "summary": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io."
                },
                "authors": [
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yuhang Lai"
                    },
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Boao Shi"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Yu"
                },
                "author": "Tao Yu",
                "arxiv_comment": "Retrieval-augmented code generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10182v2",
                "updated": "2024-12-03T15:35:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    15,
                    35,
                    24,
                    1,
                    338,
                    0
                ],
                "published": "2024-03-15T10:38:48Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    10,
                    38,
                    48,
                    4,
                    75,
                    0
                ],
                "title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification"
                },
                "summary": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage."
                },
                "authors": [
                    {
                        "name": "Arthur Thuy"
                    },
                    {
                        "name": "Dries F. Benoit"
                    }
                ],
                "author_detail": {
                    "name": "Dries F. Benoit"
                },
                "author": "Dries F. Benoit",
                "arxiv_comment": "Submitted to Annals of Operations Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v4",
                "updated": "2024-12-03T14:31:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    31,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning."
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v2",
                "updated": "2024-12-03T14:17:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    17,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02467v1",
                "updated": "2024-12-03T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators"
                },
                "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose \\ours, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose \\ours, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."
                },
                "authors": [
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; G.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02454v1",
                "updated": "2024-12-03T13:43:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    43,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T13:43:36Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    13,
                    43,
                    36,
                    1,
                    338,
                    0
                ],
                "title": "Gracefully Filtering Backdoor Samples for Generative Large Language\n  Models without Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gracefully Filtering Backdoor Samples for Generative Large Language\n  Models without Retraining"
                },
                "summary": "Backdoor attacks remain significant security threats to generative large\nlanguage models (LLMs). Since generative LLMs output sequences of\nhigh-dimensional token logits instead of low-dimensional classification logits,\nmost existing backdoor defense methods designed for discriminative models like\nBERT are ineffective for generative LLMs. Inspired by the observed differences\nin learning behavior between backdoor and clean mapping in the frequency space,\nwe transform gradients of each training sample, directly influencing parameter\nupdates, into the frequency space. Our findings reveal a distinct separation\nbetween the gradients of backdoor and clean samples in the frequency space.\nBased on this phenomenon, we propose Gradient Clustering in the Frequency Space\nfor Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients\nin the frequency space to effectively identify backdoor samples without\nrequiring retraining LLMs. Experimental results show that GraCeFul outperforms\nbaselines significantly. Notably, GraCeFul exhibits remarkable computational\nefficiency, achieving nearly 100% recall and F1 scores in identifying backdoor\nsamples, reducing the average success rate of various backdoor attacks to 0%\nwith negligible drops in clean accuracy across multiple free-style question\nanswering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna.\nThe codes are publicly available at https://github.com/ZrW00/GraceFul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks remain significant security threats to generative large\nlanguage models (LLMs). Since generative LLMs output sequences of\nhigh-dimensional token logits instead of low-dimensional classification logits,\nmost existing backdoor defense methods designed for discriminative models like\nBERT are ineffective for generative LLMs. Inspired by the observed differences\nin learning behavior between backdoor and clean mapping in the frequency space,\nwe transform gradients of each training sample, directly influencing parameter\nupdates, into the frequency space. Our findings reveal a distinct separation\nbetween the gradients of backdoor and clean samples in the frequency space.\nBased on this phenomenon, we propose Gradient Clustering in the Frequency Space\nfor Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients\nin the frequency space to effectively identify backdoor samples without\nrequiring retraining LLMs. Experimental results show that GraCeFul outperforms\nbaselines significantly. Notably, GraCeFul exhibits remarkable computational\nefficiency, achieving nearly 100% recall and F1 scores in identifying backdoor\nsamples, reducing the average success rate of various backdoor attacks to 0%\nwith negligible drops in clean accuracy across multiple free-style question\nanswering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna.\nThe codes are publicly available at https://github.com/ZrW00/GraceFul."
                },
                "authors": [
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Lingyong Fang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02427v1",
                "updated": "2024-12-03T12:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    46,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T12:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    46,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "GerPS-Compare: Comparing NER methods for legal norm analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GerPS-Compare: Comparing NER methods for legal norm analysis"
                },
                "summary": "We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems."
                },
                "authors": [
                    {
                        "name": "Sarah T. Bachinger"
                    },
                    {
                        "name": "Christoph Unger"
                    },
                    {
                        "name": "Robin Erd"
                    },
                    {
                        "name": "Leila Feddoul"
                    },
                    {
                        "name": "Clara Lachenmaier"
                    },
                    {
                        "name": "Sina Zarrieß"
                    },
                    {
                        "name": "Birgitta König-Ries"
                    }
                ],
                "author_detail": {
                    "name": "Birgitta König-Ries"
                },
                "author": "Birgitta König-Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16200v2",
                "updated": "2024-12-03T12:39:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    39,
                    11,
                    1,
                    338,
                    0
                ],
                "published": "2024-08-29T01:42:38Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    42,
                    38,
                    3,
                    242,
                    0
                ],
                "title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View"
                },
                "summary": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git."
                },
                "authors": [
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Quanli Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Liyong Zhang"
                    },
                    {
                        "name": "Xiaoguang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Zhao"
                },
                "author": "Xiaoguang Zhao",
                "arxiv_comment": "11 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02410v1",
                "updated": "2024-12-03T12:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    5,
                    56,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T12:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    5,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "A Multi-Agent Framework for Extensible Structured Text Generation in\n  PLCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Framework for Extensible Structured Text Generation in\n  PLCs"
                },
                "summary": "Programmable Logic Controllers (PLCs) are microcomputers essential for\nautomating factory operations. Structured Text (ST), a high-level language\nadhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to\nexpress logic succinctly and to seamlessly integrate with other languages\nwithin the same standard. However, vendors develop their own customized\nversions of ST, and the lack of comprehensive and standardized documentation\nfor the full semantics of ST has contributed to inconsistencies in how the\nlanguage is implemented. Consequently, the steep learning curve associated with\nST, combined with ever-evolving industrial requirements, presents significant\nchallenges for developers. In response to these issues, we present AutoPLC, an\nLLM-based approach designed to automate the generation of vendor-specific ST\ncode. To facilitate effective code generation, we first built a comprehensive\nknowledge base, including Rq2ST Case Library (requirements and corresponding\nimplementations) and Instruction libraries. Then we developed a retrieval\nmodule to incorporate the domain-specific knowledge by identifying pertinent\ncases and instructions, guiding the LLM to generate code that meets the\nrequirements. In order to verify and improve the quality of the generated code,\nwe designed an adaptable code checker. If errors are detected, we initiate an\niterative self-improvement process to instruct the LLM to revise the generated\ncode. We evaluate AutoPLC's performance against seven state-of-the-art\nbaselines using three benchmarks, one for open-source basic ST and two for\ncommercial Structured Control Language (SCL) from Siemens. The results show\nthat our approach consistently achieves superior performance across all\nbenchmarks. Ablation study emphasizes the significance of our modules. Further\nmanual analysis confirm the practical utility of the ST code generated by\nAutoPLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmable Logic Controllers (PLCs) are microcomputers essential for\nautomating factory operations. Structured Text (ST), a high-level language\nadhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to\nexpress logic succinctly and to seamlessly integrate with other languages\nwithin the same standard. However, vendors develop their own customized\nversions of ST, and the lack of comprehensive and standardized documentation\nfor the full semantics of ST has contributed to inconsistencies in how the\nlanguage is implemented. Consequently, the steep learning curve associated with\nST, combined with ever-evolving industrial requirements, presents significant\nchallenges for developers. In response to these issues, we present AutoPLC, an\nLLM-based approach designed to automate the generation of vendor-specific ST\ncode. To facilitate effective code generation, we first built a comprehensive\nknowledge base, including Rq2ST Case Library (requirements and corresponding\nimplementations) and Instruction libraries. Then we developed a retrieval\nmodule to incorporate the domain-specific knowledge by identifying pertinent\ncases and instructions, guiding the LLM to generate code that meets the\nrequirements. In order to verify and improve the quality of the generated code,\nwe designed an adaptable code checker. If errors are detected, we initiate an\niterative self-improvement process to instruct the LLM to revise the generated\ncode. We evaluate AutoPLC's performance against seven state-of-the-art\nbaselines using three benchmarks, one for open-source basic ST and two for\ncommercial Structured Control Language (SCL) from Siemens. The results show\nthat our approach consistently achieves superior performance across all\nbenchmarks. Ablation study emphasizes the significance of our modules. Further\nmanual analysis confirm the practical utility of the ST code generated by\nAutoPLC."
                },
                "authors": [
                    {
                        "name": "Donghao Yang"
                    },
                    {
                        "name": "Aolang Wu"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Yuming Ren"
                    },
                    {
                        "name": "Jiaji Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jiaji Tian"
                },
                "author": "Jiaji Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02372v1",
                "updated": "2024-12-03T10:58:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    58,
                    34,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:58:34Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    58,
                    34,
                    1,
                    338,
                    0
                ],
                "title": "HERO: Hint-Based Efficient and Reliable Query Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERO: Hint-Based Efficient and Reliable Query Optimizer"
                },
                "summary": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Sergey Iazov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Iazov"
                },
                "author": "Sergey Iazov",
                "arxiv_comment": "Submitted to VLDB 2025; 13 pages; 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02368v1",
                "updated": "2024-12-03T10:52:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:52:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    52,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?"
                },
                "summary": "Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts."
                },
                "authors": [
                    {
                        "name": "Leixin Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Yinjie Cheng"
                    },
                    {
                        "name": "Weihe Zhai"
                    },
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Christoph Leiter"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Fahimeh Moafian"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20463v2",
                "updated": "2024-12-03T10:20:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    20,
                    53,
                    1,
                    338,
                    0
                ],
                "published": "2024-07-29T23:42:12Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    23,
                    42,
                    12,
                    0,
                    211,
                    0
                ],
                "title": "5G NR Positioning with OpenAirInterface: Tools and Methodologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G NR Positioning with OpenAirInterface: Tools and Methodologies"
                },
                "summary": "The fifth-generation new radio (5G NR) technology is expected to provide\nprecise and reliable positioning capabilities along with high data rates. The\nThird Generation Partnership Project (3GPP) has started introducing positioning\ntechniques from Release-16 based on time, angle, and signal strength using\nreference signals. However, validating these techniques with experimental\nprototypes is crucial before successful real-world deployment. This work\nprovides useful tools and implementation details that are required in\nperforming 5G positioning experiments with OpenAirInterface (OAI). As an\nexample use case, we present an round trip time (RTT) estimation test-bed based\non OAI and discusses the real-word experiment and measurement process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fifth-generation new radio (5G NR) technology is expected to provide\nprecise and reliable positioning capabilities along with high data rates. The\nThird Generation Partnership Project (3GPP) has started introducing positioning\ntechniques from Release-16 based on time, angle, and signal strength using\nreference signals. However, validating these techniques with experimental\nprototypes is crucial before successful real-world deployment. This work\nprovides useful tools and implementation details that are required in\nperforming 5G positioning experiments with OpenAirInterface (OAI). As an\nexample use case, we present an round trip time (RTT) estimation test-bed based\non OAI and discusses the real-word experiment and measurement process."
                },
                "authors": [
                    {
                        "name": "Rakesh Mundlamuri"
                    },
                    {
                        "name": "Rajeev Gangula"
                    },
                    {
                        "name": "Florian Kaltenberger"
                    },
                    {
                        "name": "Raymond Knopp"
                    }
                ],
                "author_detail": {
                    "name": "Raymond Knopp"
                },
                "author": "Raymond Knopp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02344v1",
                "updated": "2024-12-03T10:04:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    4,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T10:04:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    4,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices"
                },
                "summary": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications"
                },
                "authors": [
                    {
                        "name": "Seul-Ki Yeom"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "13 Pages, 8 Tables, 7 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04710v2",
                "updated": "2024-12-03T09:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    51,
                    23,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-07T07:38:33Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    7,
                    38,
                    33,
                    4,
                    159,
                    0
                ],
                "title": "Morescient GAI for Software Engineering (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morescient GAI for Software Engineering (Extended Version)"
                },
                "summary": "The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science."
                },
                "authors": [
                    {
                        "name": "Marcus Kessel"
                    },
                    {
                        "name": "Colin Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Colin Atkinson"
                },
                "author": "Colin Atkinson",
                "arxiv_comment": "To appear in ACM Transactions on Software Engineering and\n  Methodology, Special Issue \"2030 Roadmap Software Engineering\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; I.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19035v2",
                "updated": "2024-12-03T09:50:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    50,
                    3,
                    1,
                    338,
                    0
                ],
                "published": "2024-05-29T12:23:29Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    23,
                    29,
                    2,
                    150,
                    0
                ],
                "title": "A Good Foundation is Worth Many Labels: Label-Efficient Panoptic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Good Foundation is Worth Many Labels: Label-Efficient Panoptic\n  Segmentation"
                },
                "summary": "A key challenge for the widespread application of learning-based models for\nrobotic perception is to significantly reduce the required amount of annotated\ntraining data while achieving accurate predictions. This is essential not only\nto decrease operating costs but also to speed up deployment time. In this work,\nwe address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by\nexploiting the groundwork paved by visual foundation models. We leverage\ndescriptive image features from such a model to train two lightweight network\nheads for semantic segmentation and object boundary detection, using very few\nannotated training samples. We then merge their predictions via a novel fusion\nmodule that yields panoptic maps based on normalized cut. To further enhance\nthe performance, we utilize self-training on unlabeled images selected by a\nfeature-driven similarity scheme. We underline the relevance of our approach by\nemploying PASTEL to important robot perception use cases from autonomous\ndriving and agricultural robotics. In extensive experiments, we demonstrate\nthat PASTEL significantly outperforms previous methods for label-efficient\nsegmentation even when using fewer annotations. The code of our work is\npublicly available at http://pastel.cs.uni-freiburg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge for the widespread application of learning-based models for\nrobotic perception is to significantly reduce the required amount of annotated\ntraining data while achieving accurate predictions. This is essential not only\nto decrease operating costs but also to speed up deployment time. In this work,\nwe address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by\nexploiting the groundwork paved by visual foundation models. We leverage\ndescriptive image features from such a model to train two lightweight network\nheads for semantic segmentation and object boundary detection, using very few\nannotated training samples. We then merge their predictions via a novel fusion\nmodule that yields panoptic maps based on normalized cut. To further enhance\nthe performance, we utilize self-training on unlabeled images selected by a\nfeature-driven similarity scheme. We underline the relevance of our approach by\nemploying PASTEL to important robot perception use cases from autonomous\ndriving and agricultural robotics. In extensive experiments, we demonstrate\nthat PASTEL significantly outperforms previous methods for label-efficient\nsegmentation even when using fewer annotations. The code of our work is\npublicly available at http://pastel.cs.uni-freiburg.de."
                },
                "authors": [
                    {
                        "name": "Niclas Vödisch"
                    },
                    {
                        "name": "Kürsat Petek"
                    },
                    {
                        "name": "Markus Käppeler"
                    },
                    {
                        "name": "Abhinav Valada"
                    },
                    {
                        "name": "Wolfram Burgard"
                    }
                ],
                "author_detail": {
                    "name": "Wolfram Burgard"
                },
                "author": "Wolfram Burgard",
                "arxiv_doi": "10.1109/LRA.2024.3505779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3505779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.19035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 10, no. 1, pp. 216-223,\n  January 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10583v2",
                "updated": "2024-12-03T09:43:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    43,
                    55,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-15T21:09:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    21,
                    9,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Personalization of Code Readability Evaluation Based on LLM Using\n  Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization of Code Readability Evaluation Based on LLM Using\n  Collaborative Filtering"
                },
                "summary": "Code readability is an important indicator of software maintenance as it can\nsignificantly impact maintenance efforts. Recently, LLM (large language models)\nhave been utilized for code readability evaluation. However, readability\nevaluation differs among developers, so personalization of the evaluation by\nLLM is needed. This study proposes a method which calibrates the evaluation,\nusing collaborative filtering. Our preliminary analysis suggested that the\nmethod effectively enhances the accuracy of the readability evaluation using\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code readability is an important indicator of software maintenance as it can\nsignificantly impact maintenance efforts. Recently, LLM (large language models)\nhave been utilized for code readability evaluation. However, readability\nevaluation differs among developers, so personalization of the evaluation by\nLLM is needed. This study proposes a method which calibrates the evaluation,\nusing collaborative filtering. Our preliminary analysis suggested that the\nmethod effectively enhances the accuracy of the readability evaluation using\nLLMs."
                },
                "authors": [
                    {
                        "name": "Buntaro Hiraki"
                    },
                    {
                        "name": "Kensei Hamamoto"
                    },
                    {
                        "name": "Ami Kimura"
                    },
                    {
                        "name": "Masateru Tsunoda"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Kwabena Ebo Bennin"
                    },
                    {
                        "name": "Akito Monden"
                    },
                    {
                        "name": "Keitaro Nakasai"
                    }
                ],
                "author_detail": {
                    "name": "Keitaro Nakasai"
                },
                "author": "Keitaro Nakasai",
                "arxiv_comment": "2 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v2",
                "updated": "2024-12-03T09:25:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    25,
                    11,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understanding time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understanding time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02289v1",
                "updated": "2024-12-03T09:06:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    6,
                    57,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T09:06:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    6,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "Learn More by Using Less: Distributed Learning with Energy-Constrained\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn More by Using Less: Distributed Learning with Energy-Constrained\n  Devices"
                },
                "summary": "Federated Learning (FL) has emerged as a solution for distributed model\ntraining across decentralized, privacy-preserving devices, but the different\nenergy capacities of participating devices (system heterogeneity) constrain\nreal-world implementations. These energy limitations not only reduce model\naccuracy but also increase dropout rates, impacting on convergence in practical\nFL deployments. In this work, we propose LeanFed, an energy-aware FL framework\ndesigned to optimize client selection and training workloads on\nbattery-constrained devices. LeanFed leverages adaptive data usage by\ndynamically adjusting the fraction of local data each device utilizes during\ntraining, thereby maximizing device participation across communication rounds\nwhile ensuring they do not run out of battery during the process. We rigorously\nevaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets,\nsimulating various levels of data heterogeneity and device participation rates.\nResults show that LeanFed consistently enhances model accuracy and stability,\nparticularly in settings with high data heterogeneity and limited battery life,\nby mitigating client dropout and extending device availability. This approach\ndemonstrates the potential of energy-efficient, privacy-preserving FL in\nreal-world, large-scale applications, setting a foundation for robust and\nsustainable pervasive AI on resource-constrained networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has emerged as a solution for distributed model\ntraining across decentralized, privacy-preserving devices, but the different\nenergy capacities of participating devices (system heterogeneity) constrain\nreal-world implementations. These energy limitations not only reduce model\naccuracy but also increase dropout rates, impacting on convergence in practical\nFL deployments. In this work, we propose LeanFed, an energy-aware FL framework\ndesigned to optimize client selection and training workloads on\nbattery-constrained devices. LeanFed leverages adaptive data usage by\ndynamically adjusting the fraction of local data each device utilizes during\ntraining, thereby maximizing device participation across communication rounds\nwhile ensuring they do not run out of battery during the process. We rigorously\nevaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets,\nsimulating various levels of data heterogeneity and device participation rates.\nResults show that LeanFed consistently enhances model accuracy and stability,\nparticularly in settings with high data heterogeneity and limited battery life,\nby mitigating client dropout and extending device availability. This approach\ndemonstrates the potential of energy-efficient, privacy-preserving FL in\nreal-world, large-scale applications, setting a foundation for robust and\nsustainable pervasive AI on resource-constrained networks."
                },
                "authors": [
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Cristian J. Vaca-Rubio"
                    },
                    {
                        "name": "Luis Blanco"
                    }
                ],
                "author_detail": {
                    "name": "Luis Blanco"
                },
                "author": "Luis Blanco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19146v2",
                "updated": "2024-12-03T09:06:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    6,
                    33,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-28T13:45:42Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    45,
                    42,
                    3,
                    333,
                    0
                ],
                "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir adoption is limited by high computational costs during inference. While\nincreasing parameter counts enhances accuracy, it also widens the gap between\nstate-of-the-art capabilities and practical deployability. We present Puzzle, a\nframework to accelerate LLM inference on specific hardware while preserving\ntheir capabilities. Through an innovative application of neural architecture\nsearch (NAS) at an unprecedented scale, Puzzle systematically optimizes models\nwith tens of billions of parameters under hardware constraints. Our approach\nutilizes blockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We demonstrate the real-world impact of our framework through\nLlama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model\nderived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4%\nof the original model's capabilities. Nemotron-51B currently stands as the most\naccurate language model capable of inference on a single GPU with large batch\nsizes. Remarkably, this transformation required just 45B training tokens,\ncompared to over 15T tokens used for the 70B model it was derived from. This\nestablishes a new paradigm where powerful models can be optimized for efficient\ndeployment with only negligible compromise of their capabilities, demonstrating\nthat inference performance, not parameter count alone, should guide model\nselection. With the release of Nemotron-51B and the presentation of the Puzzle\nframework, we provide practitioners immediate access to state-of-the-art\nlanguage modeling capabilities at significantly reduced computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir adoption is limited by high computational costs during inference. While\nincreasing parameter counts enhances accuracy, it also widens the gap between\nstate-of-the-art capabilities and practical deployability. We present Puzzle, a\nframework to accelerate LLM inference on specific hardware while preserving\ntheir capabilities. Through an innovative application of neural architecture\nsearch (NAS) at an unprecedented scale, Puzzle systematically optimizes models\nwith tens of billions of parameters under hardware constraints. Our approach\nutilizes blockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We demonstrate the real-world impact of our framework through\nLlama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model\nderived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4%\nof the original model's capabilities. Nemotron-51B currently stands as the most\naccurate language model capable of inference on a single GPU with large batch\nsizes. Remarkably, this transformation required just 45B training tokens,\ncompared to over 15T tokens used for the 70B model it was derived from. This\nestablishes a new paradigm where powerful models can be optimized for efficient\ndeployment with only negligible compromise of their capabilities, demonstrating\nthat inference performance, not parameter count alone, should guide model\nselection. With the release of Nemotron-51B and the presentation of the Puzzle\nframework, we provide practitioners immediate access to state-of-the-art\nlanguage modeling capabilities at significantly reduced computational costs."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ran Rubin"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05754v2",
                "updated": "2024-12-03T09:00:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    0,
                    41,
                    1,
                    338,
                    0
                ],
                "published": "2024-07-08T09:05:37Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    5,
                    37,
                    0,
                    190,
                    0
                ],
                "title": "Reconfigurable Intelligent Surfaces in Upper Mid-Band 6G Networks: Gain\n  or Pain?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces in Upper Mid-Band 6G Networks: Gain\n  or Pain?"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as one of the most\nstudied topics in recent years, hailed as a transformative technology with the\npotential to revolutionize future wireless systems. While RISs are recognized\nfor their ability to enhance spectral efficiency, coverage, and the reliability\nof wireless channels, several challenges remain. Notably, convincing and\nprofitable use cases must be developed before widespread commercial deployment\ncan be realized. The first sixth-generation (6G) networks will most likely\nutilize upper mid-band frequencies (i.e., 7-24\\,GHz). This is regarded as the\n\\textit{golden band} since it combines good coverage, much new spectrum, and\nenables many antennas in compact form factors. There has been much prior work\non channel modeling, coexistence, and possible implementation scenarios for\nthese bands. There are significant frequency-specific challenges related to RIS\ndeployment, use cases, number of required elements, channel estimation, and\ncontrol. These are previously unaddressed for the upper mid-band. In this\npaper, we aim to bridge this gap by exploring various use cases, including\nRIS-assisted fixed wireless access (FWA), enhanced capacity in mobile\ncommunications, and increased reliability at the cell edge. We identify the\nconditions under which RIS can provide major benefits and optimal strategies\nfor deploying RIS to enhance the performance of 6G upper mid-band communication\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have emerged as one of the most\nstudied topics in recent years, hailed as a transformative technology with the\npotential to revolutionize future wireless systems. While RISs are recognized\nfor their ability to enhance spectral efficiency, coverage, and the reliability\nof wireless channels, several challenges remain. Notably, convincing and\nprofitable use cases must be developed before widespread commercial deployment\ncan be realized. The first sixth-generation (6G) networks will most likely\nutilize upper mid-band frequencies (i.e., 7-24\\,GHz). This is regarded as the\n\\textit{golden band} since it combines good coverage, much new spectrum, and\nenables many antennas in compact form factors. There has been much prior work\non channel modeling, coexistence, and possible implementation scenarios for\nthese bands. There are significant frequency-specific challenges related to RIS\ndeployment, use cases, number of required elements, channel estimation, and\ncontrol. These are previously unaddressed for the upper mid-band. In this\npaper, we aim to bridge this gap by exploring various use cases, including\nRIS-assisted fixed wireless access (FWA), enhanced capacity in mobile\ncommunications, and increased reliability at the cell edge. We identify the\nconditions under which RIS can provide major benefits and optimal strategies\nfor deploying RIS to enhance the performance of 6G upper mid-band communication\nsystems."
                },
                "authors": [
                    {
                        "name": "Ferdi Kara"
                    },
                    {
                        "name": "Özlem Tuğfe Demir"
                    },
                    {
                        "name": "Emil Björnson"
                    }
                ],
                "author_detail": {
                    "name": "Emil Björnson"
                },
                "author": "Emil Björnson",
                "arxiv_comment": "submitted to IEEE Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02279v1",
                "updated": "2024-12-03T08:54:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    54,
                    17,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:54:17Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    54,
                    17,
                    1,
                    338,
                    0
                ],
                "title": "A Comprehensive Evaluation of Large Language Models on Aspect-Based\n  Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Large Language Models on Aspect-Based\n  Sentiment Analysis"
                },
                "summary": "Recently, Large Language Models (LLMs) have garnered increasing attention in\nthe field of natural language processing, revolutionizing numerous downstream\ntasks with powerful reasoning and generation abilities. For example, In-Context\nLearning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box\nLLMs to execute downstream tasks by analogy learning without any fine-tuning.\nBesides, in a fine-tuning-dependent paradigm where substantial training data\nexists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,\nenable LLMs to achieve excellent performance comparable to full fine-tuning.\n  However, these fascinating techniques employed by LLMs have not been fully\nexploited in the ABSA field. Previous works probe LLMs in ABSA by merely using\nrandomly selected input-output pairs as demonstrations in ICL, resulting in an\nincomplete and superficial evaluation. In this paper, we shed light on a\ncomprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8\nABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation\nto unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''\nFor the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using\ninstruction-based multi-task learning. For the fine-tuning-free paradigm, we\npropose 3 demonstration selection strategies to stimulate the few-shot\nabilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a\nnew state-of-the-art performance compared to fine-tuned Small Language Models\n(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the\nfine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still\nshowcase impressive potential and even compete with fine-tuned SLMs on some\nABSA subtasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have garnered increasing attention in\nthe field of natural language processing, revolutionizing numerous downstream\ntasks with powerful reasoning and generation abilities. For example, In-Context\nLearning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box\nLLMs to execute downstream tasks by analogy learning without any fine-tuning.\nBesides, in a fine-tuning-dependent paradigm where substantial training data\nexists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,\nenable LLMs to achieve excellent performance comparable to full fine-tuning.\n  However, these fascinating techniques employed by LLMs have not been fully\nexploited in the ABSA field. Previous works probe LLMs in ABSA by merely using\nrandomly selected input-output pairs as demonstrations in ICL, resulting in an\nincomplete and superficial evaluation. In this paper, we shed light on a\ncomprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8\nABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation\nto unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''\nFor the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using\ninstruction-based multi-task learning. For the fine-tuning-free paradigm, we\npropose 3 demonstration selection strategies to stimulate the few-shot\nabilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a\nnew state-of-the-art performance compared to fine-tuned Small Language Models\n(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the\nfine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still\nshowcase impressive potential and even compete with fine-tuned SLMs on some\nABSA subtasks."
                },
                "authors": [
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Dandan Song"
                    },
                    {
                        "name": "Yuhang Tian"
                    },
                    {
                        "name": "Zhijing Wu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Shuhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuhao Zhang"
                },
                "author": "Shuhao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02271v1",
                "updated": "2024-12-03T08:41:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    41,
                    13,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:41:13Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    41,
                    13,
                    1,
                    338,
                    0
                ],
                "title": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News\n  Headlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News\n  Headlines"
                },
                "summary": "In this paper, we introduce the MediaSpin dataset aiming to help in the\ndevelopment of models that can detect different forms of media bias present in\nnews headlines, developed through human-supervised and -validated Large\nLanguage Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs\nof news headlines and annotations with explanations of the 13 distinct types of\nmedia bias categories assigned. We demonstrate the usefulness of our dataset\nfor automated bias detection in news edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the MediaSpin dataset aiming to help in the\ndevelopment of models that can detect different forms of media bias present in\nnews headlines, developed through human-supervised and -validated Large\nLanguage Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs\nof news headlines and annotations with explanations of the 13 distinct types of\nmedia bias categories assigned. We demonstrate the usefulness of our dataset\nfor automated bias detection in news edits."
                },
                "authors": [
                    {
                        "name": "Preetika Verma"
                    },
                    {
                        "name": "Kokil Jaidka"
                    }
                ],
                "author_detail": {
                    "name": "Kokil Jaidka"
                },
                "author": "Kokil Jaidka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02263v1",
                "updated": "2024-12-03T08:35:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:35:51Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    35,
                    51,
                    1,
                    338,
                    0
                ],
                "title": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting Large Language Models with Blockchain: Advancing the\n  Evolution of Smart Contracts from Automation to Intelligence"
                },
                "summary": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain smart contracts have catalyzed the development of decentralized\napplications across various domains, including decentralized finance. However,\ndue to constraints in computational resources and the prevalence of data silos,\ncurrent smart contracts face significant challenges in fully leveraging the\npowerful capabilities of Large Language Models (LLMs) for tasks such as\nintelligent analysis and reasoning. To address this gap, this paper proposes\nand implements a universal framework for integrating LLMs with blockchain data,\n{\\sysname}, effectively overcoming the interoperability barriers between\nblockchain and LLMs. By combining semantic relatedness with truth discovery\nmethods, we introduce an innovative data aggregation approach, {\\funcname},\nwhich significantly enhances the accuracy and trustworthiness of data generated\nby LLMs. To validate the framework's effectiveness, we construct a dataset\nconsisting of three types of questions, capturing Q\\&A interactions between 10\noracle nodes and 5 LLM models. Experimental results demonstrate that, even with\n40\\% malicious nodes, the proposed solution improves data accuracy by an\naverage of 17.74\\% compared to the optimal baseline. This research not only\nprovides an innovative solution for the intelligent enhancement of smart\ncontracts but also highlights the potential for deep integration between LLMs\nand blockchain technology, paving the way for more intelligent and complex\napplications of smart contracts in the future."
                },
                "authors": [
                    {
                        "name": "Youquan Xian"
                    },
                    {
                        "name": "Xueying Zeng"
                    },
                    {
                        "name": "Duancheng Xuan"
                    },
                    {
                        "name": "Danping Yang"
                    },
                    {
                        "name": "Chunpei Li"
                    },
                    {
                        "name": "Peng Fan"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02258v1",
                "updated": "2024-12-03T08:33:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    33,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:33:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    33,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "Lessons learned from establishing a rooftop photovoltaic system\n  crowdsourced by students and employees at Aarhus University",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons learned from establishing a rooftop photovoltaic system\n  crowdsourced by students and employees at Aarhus University"
                },
                "summary": "Energy communities are promoted in the European legislation as a strategy to\nenable citizen participation in the energy transition. Solar photovoltaic (PV)\nsystems, due to their distributed nature, present an opportunity to create such\ncommunities. At Aarhus University (Denmark), we have established an energy\ncommunity consisting of a 98-kW rooftop solar PV installation, crowdsourced by\nstudents and employees of the university. The participants can buy one or\nseveral shares of the installation (which is divided into 900 shares), the\nelectricity is consumed by the university, and the shareowners receive some\neconomic compensation every year. The road to establishing this energy\ncommunity has been rough, and we have gathered many learnings. In this\nmanuscript, we present the 10 largest challenges which might arise when setting\nup a university energy community and our particular approach to facing them.\nSharing these learnings might pave the way for those willing to establish their\nown energy community. We also include policy recommendations at a European,\nnational, and municipality levels to facilitate the deployment of energy\ncommunities",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy communities are promoted in the European legislation as a strategy to\nenable citizen participation in the energy transition. Solar photovoltaic (PV)\nsystems, due to their distributed nature, present an opportunity to create such\ncommunities. At Aarhus University (Denmark), we have established an energy\ncommunity consisting of a 98-kW rooftop solar PV installation, crowdsourced by\nstudents and employees of the university. The participants can buy one or\nseveral shares of the installation (which is divided into 900 shares), the\nelectricity is consumed by the university, and the shareowners receive some\neconomic compensation every year. The road to establishing this energy\ncommunity has been rough, and we have gathered many learnings. In this\nmanuscript, we present the 10 largest challenges which might arise when setting\nup a university energy community and our particular approach to facing them.\nSharing these learnings might pave the way for those willing to establish their\nown energy community. We also include policy recommendations at a European,\nnational, and municipality levels to facilitate the deployment of energy\ncommunities"
                },
                "authors": [
                    {
                        "name": "Marta Victoria"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Gorm B. Andresen"
                    },
                    {
                        "name": "Parisa Rahdan"
                    },
                    {
                        "name": "Ebbe K. Gøtske"
                    }
                ],
                "author_detail": {
                    "name": "Ebbe K. Gøtske"
                },
                "author": "Ebbe K. Gøtske",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14026v3",
                "updated": "2024-12-03T08:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    3,
                    25,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-20T06:46:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    46,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations"
                },
                "summary": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/"
                },
                "authors": [
                    {
                        "name": "Xisen Jin"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "10 pages; preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15316v3",
                "updated": "2024-12-03T07:57:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    57,
                    33,
                    1,
                    338,
                    0
                ],
                "published": "2023-11-26T14:35:23Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    14,
                    35,
                    23,
                    6,
                    330,
                    0
                ],
                "title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference"
                },
                "summary": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses."
                },
                "authors": [
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Huan Liu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02228v1",
                "updated": "2024-12-03T07:51:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    51,
                    14,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T07:51:14Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    51,
                    14,
                    1,
                    338,
                    0
                ],
                "title": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition"
                },
                "summary": "Despite the recent success of two-stage prototypical networks in few-shot\nnamed entity recognition (NER), challenges such as over/under-detected false\nspans in the span detection stage and unaligned entity prototypes in the type\nclassification stage persist. Additionally, LLMs have not proven to be\neffective few-shot information extractors in general. In this paper, we propose\nan approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to\naddress these issues. We introduce a boundary-aware contrastive learning\nstrategy to enhance the LLM's ability to perceive entity boundaries for\ngeneralized entity spans. Additionally, we utilize LoRAHub to align information\nfrom the target domain to the source domain, thereby enhancing adaptive\ncross-domain classification capabilities. Extensive experiments across various\nbenchmarks demonstrate that our framework outperforms prior methods, validating\nits effectiveness. In particular, the proposed strategies demonstrate\neffectiveness across a range of LLM architectures. The code and data are\nreleased on https://github.com/UESTC-GQJ/BANER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of two-stage prototypical networks in few-shot\nnamed entity recognition (NER), challenges such as over/under-detected false\nspans in the span detection stage and unaligned entity prototypes in the type\nclassification stage persist. Additionally, LLMs have not proven to be\neffective few-shot information extractors in general. In this paper, we propose\nan approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to\naddress these issues. We introduce a boundary-aware contrastive learning\nstrategy to enhance the LLM's ability to perceive entity boundaries for\ngeneralized entity spans. Additionally, we utilize LoRAHub to align information\nfrom the target domain to the source domain, thereby enhancing adaptive\ncross-domain classification capabilities. Extensive experiments across various\nbenchmarks demonstrate that our framework outperforms prior methods, validating\nits effectiveness. In particular, the proposed strategies demonstrate\neffectiveness across a range of LLM architectures. The code and data are\nreleased on https://github.com/UESTC-GQJ/BANER."
                },
                "authors": [
                    {
                        "name": "Quanjiang Guo"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ling Tian"
                    },
                    {
                        "name": "Zhao Kang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Sijie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sijie Wang"
                },
                "author": "Sijie Wang",
                "arxiv_comment": "Appear on COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06638v2",
                "updated": "2024-12-03T07:40:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    40,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-11T00:18:54Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    0,
                    18,
                    54,
                    0,
                    316,
                    0
                ],
                "title": "Model Editing for LLMs4Code: How Far are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Editing for LLMs4Code: How Far are We?"
                },
                "summary": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Weimin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhang"
                },
                "author": "Weimin Zhang",
                "arxiv_comment": "Accepted by ICSE2025. The code is available at:\n  https://github.com/xpq-tech/code-llmedit.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08978v2",
                "updated": "2024-12-03T07:36:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    36,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-03-13T22:06:03Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    22,
                    6,
                    3,
                    2,
                    73,
                    0
                ],
                "title": "AutoGuide: Automated Generation and Selection of Context-Aware\n  Guidelines for Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoGuide: Automated Generation and Selection of Context-Aware\n  Guidelines for Large Language Model Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have empowered AI agents\ncapable of performing various sequential decision-making tasks. However,\neffectively guiding LLMs to perform well in unfamiliar domains like web\nnavigation, where they lack sufficient knowledge, has proven to be difficult\nwith the demonstration-based in-context learning paradigm. In this paper, we\nintroduce a novel framework, called AutoGuide, which addresses this limitation\nby automatically generating context-aware guidelines from offline experiences.\nImportantly, each context-aware guideline is expressed in concise natural\nlanguage and follows a conditional structure, clearly describing the context\nwhere it is applicable. As a result, our guidelines facilitate the provision of\nrelevant knowledge for the agent's current decision-making process, overcoming\nthe limitations of the conventional demonstration-based learning paradigm. Our\nevaluation demonstrates that AutoGuide significantly outperforms competitive\nbaselines in complex benchmark domains, including real-world web navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have empowered AI agents\ncapable of performing various sequential decision-making tasks. However,\neffectively guiding LLMs to perform well in unfamiliar domains like web\nnavigation, where they lack sufficient knowledge, has proven to be difficult\nwith the demonstration-based in-context learning paradigm. In this paper, we\nintroduce a novel framework, called AutoGuide, which addresses this limitation\nby automatically generating context-aware guidelines from offline experiences.\nImportantly, each context-aware guideline is expressed in concise natural\nlanguage and follows a conditional structure, clearly describing the context\nwhere it is applicable. As a result, our guidelines facilitate the provision of\nrelevant knowledge for the agent's current decision-making process, overcoming\nthe limitations of the conventional demonstration-based learning paradigm. Our\nevaluation demonstrates that AutoGuide significantly outperforms competitive\nbaselines in complex benchmark domains, including real-world web navigation."
                },
                "authors": [
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Sungryull Sohn"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Kyunghoon Bae"
                    },
                    {
                        "name": "Honglak Lee"
                    }
                ],
                "author_detail": {
                    "name": "Honglak Lee"
                },
                "author": "Honglak Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17501v2",
                "updated": "2024-12-03T07:30:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    30,
                    1,
                    1,
                    338,
                    0
                ],
                "published": "2024-09-26T03:13:29Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    13,
                    29,
                    3,
                    270,
                    0
                ],
                "title": "Development of a novel bunch oscillation recorder with RFSoC technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a novel bunch oscillation recorder with RFSoC technology"
                },
                "summary": "The SuperKEKB accelerator is designed to achieve unprecedented luminosity\nlevels, but this goal is currently hindered by Sudden Beam Loss (SBL) events.\nThese events not only obstruct luminosity improvement but also pose a\nsignificant risk to accelerator components, the Belle II detectors, and the\nsuperconducting focusing system, potentially leading to severe damage and\nquenching of the superconducting system. To address this critical challenge, we\nhave developed a novel Bunch Oscillation Recorder (BOR) based on RFSoC\ntechnology. The BOR has demonstrated high precision with a position resolution\nof 0.03 mm, making it a powerful tool for real-time beam monitoring. In its\ninitial deployment, the BOR successfully recorded multiple SBL events,\nproviding valuable data for further analysis. By strategically positioning BORs\nat the suspected points of SBL origin, we aim to directly identify sources of\nbeam instability. We anticipate that this portable, high-speed BOR monitor will\nplay a crucial role in resolving the SBL issue, ultimately helping achieve\nSuperKEKB's luminosity targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SuperKEKB accelerator is designed to achieve unprecedented luminosity\nlevels, but this goal is currently hindered by Sudden Beam Loss (SBL) events.\nThese events not only obstruct luminosity improvement but also pose a\nsignificant risk to accelerator components, the Belle II detectors, and the\nsuperconducting focusing system, potentially leading to severe damage and\nquenching of the superconducting system. To address this critical challenge, we\nhave developed a novel Bunch Oscillation Recorder (BOR) based on RFSoC\ntechnology. The BOR has demonstrated high precision with a position resolution\nof 0.03 mm, making it a powerful tool for real-time beam monitoring. In its\ninitial deployment, the BOR successfully recorded multiple SBL events,\nproviding valuable data for further analysis. By strategically positioning BORs\nat the suspected points of SBL origin, we aim to directly identify sources of\nbeam instability. We anticipate that this portable, high-speed BOR monitor will\nplay a crucial role in resolving the SBL issue, ultimately helping achieve\nSuperKEKB's luminosity targets."
                },
                "authors": [
                    {
                        "name": "Riku Nomaru"
                    },
                    {
                        "name": "Gaku Mitsuka"
                    },
                    {
                        "name": "Larry Ruckman"
                    },
                    {
                        "name": "Ryan Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Herbst"
                },
                "author": "Ryan Herbst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02220v1",
                "updated": "2024-12-03T07:25:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    25,
                    30,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T07:25:30Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    25,
                    30,
                    1,
                    338,
                    0
                ],
                "title": "Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models\n  by Recycling Pre-Tuned LoRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models\n  by Recycling Pre-Tuned LoRAs"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot\nadaptability without requiring fine-tuning, positioning them ideal for\ndata-limited and real-time applications. However, this adaptability has not yet\nbeen replicated in current Visual Foundation Models (VFMs), which require\nexplicit fine-tuning with sufficient tuning data. Besides, the\npretraining-finetuning paradigm has led to the surge of numerous task-specific\nmodular components, such as Low-Rank Adaptation (LoRA). For the first time, we\nexplore the potential of reusing diverse pre-tuned LoRAs without accessing\ntheir original training data, to achieve tuning-free few-shot adaptation in\nVFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned\nLoRAs with a meta-learning objective, using surrogate data generated inversely\nfrom pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is\nempowered to solve new few-shot tasks in a single forward pass, akin to the\nin-context learning of LLMs. Additionally, we incorporate a double-efficient\nmechanism tailored to our framework, significantly accelerating the\nmeta-training process while maintaining or even improving performance.\nExtensive experiments across various few-shot classification benchmarks across\nboth in- and cross-domain scenarios demonstrate the superiority of our\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot\nadaptability without requiring fine-tuning, positioning them ideal for\ndata-limited and real-time applications. However, this adaptability has not yet\nbeen replicated in current Visual Foundation Models (VFMs), which require\nexplicit fine-tuning with sufficient tuning data. Besides, the\npretraining-finetuning paradigm has led to the surge of numerous task-specific\nmodular components, such as Low-Rank Adaptation (LoRA). For the first time, we\nexplore the potential of reusing diverse pre-tuned LoRAs without accessing\ntheir original training data, to achieve tuning-free few-shot adaptation in\nVFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned\nLoRAs with a meta-learning objective, using surrogate data generated inversely\nfrom pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is\nempowered to solve new few-shot tasks in a single forward pass, akin to the\nin-context learning of LLMs. Additionally, we incorporate a double-efficient\nmechanism tailored to our framework, significantly accelerating the\nmeta-training process while maintaining or even improving performance.\nExtensive experiments across various few-shot classification benchmarks across\nboth in- and cross-domain scenarios demonstrate the superiority of our\nframework."
                },
                "authors": [
                    {
                        "name": "Zixuan Hu"
                    },
                    {
                        "name": "Yongxian Wei"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02215v1",
                "updated": "2024-12-03T07:11:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    11,
                    21,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T07:11:21Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    7,
                    11,
                    21,
                    1,
                    338,
                    0
                ],
                "title": "Recovering implicit physics model under real-world constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering implicit physics model under real-world constraints"
                },
                "summary": "Recovering a physics-driven model, i.e. a governing set of equations of the\nunderlying dynamical systems, from the real-world data has been of recent\ninterest. Most existing methods either operate on simulation data with\nunrealistically high sampling rates or require explicit measurements of all\nsystem variables, which is not amenable in real-world deployments. Moreover,\nthey assume the timestamps of external perturbations to the physical system are\nknown a priori, without uncertainty, implicitly discounting any sensor\ntime-synchronization or human reporting errors. In this paper, we propose a\nnovel liquid time constant neural network (LTC-NN) based architecture to\nrecover underlying model of physical dynamics from real-world data. The\nautomatic differentiation property of LTC-NN nodes overcomes problems\nassociated with low sampling rates, the input dependent time constant in the\nforward pass of the hidden layer of LTC-NN nodes creates a massive search space\nof implicit physical dynamics, the physics model solver based data\nreconstruction loss guides the search for the correct set of implicit dynamics,\nand the use of the dropout regularization in the dense layer ensures extraction\nof the sparsest model. Further, to account for the perturbation timing error,\nwe utilize dense layer nodes to search through input shifts that results in the\nlowest reconstruction loss. Experiments on four benchmark dynamical systems,\nthree with simulation data and one with the real-world data show that the\nLTC-NN architecture is more accurate in recovering implicit physics model\ncoefficients than the state-of-the-art sparse model recovery approaches. We\nalso introduce four additional case studies (total eight) on real-life medical\nexamples in simulation and with real-world clinical data to show effectiveness\nof our approach in recovering underlying model in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering a physics-driven model, i.e. a governing set of equations of the\nunderlying dynamical systems, from the real-world data has been of recent\ninterest. Most existing methods either operate on simulation data with\nunrealistically high sampling rates or require explicit measurements of all\nsystem variables, which is not amenable in real-world deployments. Moreover,\nthey assume the timestamps of external perturbations to the physical system are\nknown a priori, without uncertainty, implicitly discounting any sensor\ntime-synchronization or human reporting errors. In this paper, we propose a\nnovel liquid time constant neural network (LTC-NN) based architecture to\nrecover underlying model of physical dynamics from real-world data. The\nautomatic differentiation property of LTC-NN nodes overcomes problems\nassociated with low sampling rates, the input dependent time constant in the\nforward pass of the hidden layer of LTC-NN nodes creates a massive search space\nof implicit physical dynamics, the physics model solver based data\nreconstruction loss guides the search for the correct set of implicit dynamics,\nand the use of the dropout regularization in the dense layer ensures extraction\nof the sparsest model. Further, to account for the perturbation timing error,\nwe utilize dense layer nodes to search through input shifts that results in the\nlowest reconstruction loss. Experiments on four benchmark dynamical systems,\nthree with simulation data and one with the real-world data show that the\nLTC-NN architecture is more accurate in recovering implicit physics model\ncoefficients than the state-of-the-art sparse model recovery approaches. We\nalso introduce four additional case studies (total eight) on real-life medical\nexamples in simulation and with real-world clinical data to show effectiveness\nof our approach in recovering underlying model in practice."
                },
                "authors": [
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Sandeep K. S. Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep K. S. Gupta"
                },
                "author": "Sandeep K. S. Gupta",
                "arxiv_doi": "10.3233/FAIA240556",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240556",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper is published in ECAI 2024,\n  https://ebooks.iospress.nl/volumearticle/69651",
                "arxiv_journal_ref": "27 th European conference on Artificial Intelligence 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10946v3",
                "updated": "2024-12-03T06:52:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    52,
                    34,
                    1,
                    338,
                    0
                ],
                "published": "2024-02-09T04:02:43Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    4,
                    2,
                    43,
                    4,
                    40,
                    0
                ],
                "title": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mengzhou Chen"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "NeurIPS 2024; Code is at https://github.com/Scarelette/CultureLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v1",
                "updated": "2024-12-03T06:47:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unifed Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unifed Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02193v1",
                "updated": "2024-12-03T06:15:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    15,
                    4,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T06:15:04Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    15,
                    4,
                    1,
                    338,
                    0
                ],
                "title": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models"
                },
                "summary": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned\non language instruction. Large language models (LLMs) struggle with generating\nphysically plausible 3D scenes and adherence to input instructions,\nparticularly in cluttered scenes. We introduce LayoutVLM, a framework and scene\nlayout representation that exploits the semantic knowledge of Vision-Language\nModels (VLMs) and supports differentiable optimization to ensure physical\nplausibility. LayoutVLM employs VLMs to generate two mutually reinforcing\nrepresentations from visually marked images, and a self-consistent decoding\nprocess to improve VLMs spatial planning. Our experiments show that LayoutVLM\naddresses the limitations of existing LLM and constraint-based approaches,\nproducing physically plausible 3D layouts better aligned with the semantic\nintent of input language instructions. We also demonstrate that fine-tuning\nVLMs with the proposed scene layout representation extracted from existing\nscene datasets can improve performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned\non language instruction. Large language models (LLMs) struggle with generating\nphysically plausible 3D scenes and adherence to input instructions,\nparticularly in cluttered scenes. We introduce LayoutVLM, a framework and scene\nlayout representation that exploits the semantic knowledge of Vision-Language\nModels (VLMs) and supports differentiable optimization to ensure physical\nplausibility. LayoutVLM employs VLMs to generate two mutually reinforcing\nrepresentations from visually marked images, and a self-consistent decoding\nprocess to improve VLMs spatial planning. Our experiments show that LayoutVLM\naddresses the limitations of existing LLM and constraint-based approaches,\nproducing physically plausible 3D layouts better aligned with the semantic\nintent of input language instructions. We also demonstrate that fine-tuning\nVLMs with the proposed scene layout representation extracted from existing\nscene datasets can improve performance."
                },
                "authors": [
                    {
                        "name": "Fan-Yun Sun"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Siyi Gu"
                    },
                    {
                        "name": "Dylan Lim"
                    },
                    {
                        "name": "Goutam Bhat"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Nick Haber"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "project website: https://ai.stanford.edu/~sunfanyun/layoutvlm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02173v1",
                "updated": "2024-12-03T05:05:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    5,
                    13,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T05:05:13Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    5,
                    13,
                    1,
                    338,
                    0
                ],
                "title": "Keeping Experts in the Loop: Expert-Guided Optimization for Clinical\n  Data Classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping Experts in the Loop: Expert-Guided Optimization for Clinical\n  Data Classification using Large Language Models"
                },
                "summary": "Since the emergence of Large Language Models (LLMs), the challenge of\neffectively leveraging their potential in healthcare has taken center stage. A\ncritical barrier to using LLMs for extracting insights from unstructured\nclinical notes lies in the prompt engineering process. Despite its pivotal role\nin determining task performance, a clear framework for prompt optimization\nremains absent. Current methods to address this gap take either a manual prompt\nrefinement approach, where domain experts collaborate with prompt engineers to\ncreate an optimal prompt, which is time-intensive and difficult to scale, or\nthrough employing automatic prompt optimizing approaches, where the value of\nthe input of domain experts is not fully realized. To address this, we propose\nStructEase, a novel framework that bridges the gap between automation and the\ninput of human expertise in prompt engineering. A core innovation of the\nframework is SamplEase, an iterative sampling algorithm that identifies\nhigh-value cases where expert feedback drives significant performance\nimprovements. This approach minimizes expert intervention, to effectively\nenhance classification outcomes. This targeted approach reduces labeling\nredundancy, mitigates human error, and enhances classification outcomes. We\nevaluated the performance of StructEase using a dataset of de-identified\nclinical narratives from the US National Electronic Injury Surveillance System\n(NEISS), demonstrating significant gains in classification performance compared\nto current methods. Our findings underscore the value of expert integration in\nLLM workflows, achieving notable improvements in F1 score while maintaining\nminimal expert effort. By combining transparency, flexibility, and scalability,\nStructEase sets the foundation for a framework to integrate expert input into\nLLM workflows in healthcare and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the emergence of Large Language Models (LLMs), the challenge of\neffectively leveraging their potential in healthcare has taken center stage. A\ncritical barrier to using LLMs for extracting insights from unstructured\nclinical notes lies in the prompt engineering process. Despite its pivotal role\nin determining task performance, a clear framework for prompt optimization\nremains absent. Current methods to address this gap take either a manual prompt\nrefinement approach, where domain experts collaborate with prompt engineers to\ncreate an optimal prompt, which is time-intensive and difficult to scale, or\nthrough employing automatic prompt optimizing approaches, where the value of\nthe input of domain experts is not fully realized. To address this, we propose\nStructEase, a novel framework that bridges the gap between automation and the\ninput of human expertise in prompt engineering. A core innovation of the\nframework is SamplEase, an iterative sampling algorithm that identifies\nhigh-value cases where expert feedback drives significant performance\nimprovements. This approach minimizes expert intervention, to effectively\nenhance classification outcomes. This targeted approach reduces labeling\nredundancy, mitigates human error, and enhances classification outcomes. We\nevaluated the performance of StructEase using a dataset of de-identified\nclinical narratives from the US National Electronic Injury Surveillance System\n(NEISS), demonstrating significant gains in classification performance compared\nto current methods. Our findings underscore the value of expert integration in\nLLM workflows, achieving notable improvements in F1 score while maintaining\nminimal expert effort. By combining transparency, flexibility, and scalability,\nStructEase sets the foundation for a framework to integrate expert input into\nLLM workflows in healthcare and beyond."
                },
                "authors": [
                    {
                        "name": "Nader Karayanni"
                    },
                    {
                        "name": "Aya Awwad"
                    },
                    {
                        "name": "Chein-Lien Hsiao"
                    },
                    {
                        "name": "Surish P Shanmugam"
                    }
                ],
                "author_detail": {
                    "name": "Surish P Shanmugam"
                },
                "author": "Surish P Shanmugam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v2",
                "updated": "2024-12-03T05:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    5,
                    0,
                    18,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Lee"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13706v2",
                "updated": "2024-12-03T04:57:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    57,
                    32,
                    1,
                    338,
                    0
                ],
                "published": "2024-06-19T16:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    58,
                    32,
                    2,
                    171,
                    0
                ],
                "title": "Developing Story: Case Studies of Generative AI's Use in Journalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Story: Case Studies of Generative AI's Use in Journalism"
                },
                "summary": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context."
                },
                "authors": [
                    {
                        "name": "Natalie Grace Brigham"
                    },
                    {
                        "name": "Chongjiu Gao"
                    },
                    {
                        "name": "Tadayoshi Kohno"
                    },
                    {
                        "name": "Franziska Roesner"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v2",
                "updated": "2024-12-03T04:51:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    51,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qichen Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00218v2",
                "updated": "2024-12-03T04:38:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    38,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-29T19:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "NüshuRescue: Revitalization of the endangered Nüshu Language with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NüshuRescue: Revitalization of the endangered Nüshu Language with AI"
                },
                "summary": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by N\\\"ushu, a rare script historically used by Yao\nwomen in China for self-expression within a patriarchal society. To address\nthis challenge, we introduce N\\\"ushuRescue, an AI-driven framework designed to\ntrain large language models (LLMs) on endangered languages with minimal data.\nN\\\"ushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence N\\\"ushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\\\"ushu\nand only 35 short examples from NCGold, N\\\"ushuRescue achieved 48.69\\%\ntranslation accuracy on 50 withheld sentences and generated NCSilver, a set of\n98 newly translated modern Chinese sentences of varying lengths. A sample of\nboth NCGold and NCSilver is included in the Supplementary Materials.\nAdditionally, we developed FastText-based and Seq2Seq models to further support\nresearch on N\\\"ushu. N\\\"ushuRescue provides a versatile and scalable tool for\nthe revitalization of endangered languages, minimizing the need for extensive\nhuman input.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by N\\\"ushu, a rare script historically used by Yao\nwomen in China for self-expression within a patriarchal society. To address\nthis challenge, we introduce N\\\"ushuRescue, an AI-driven framework designed to\ntrain large language models (LLMs) on endangered languages with minimal data.\nN\\\"ushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence N\\\"ushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\\\"ushu\nand only 35 short examples from NCGold, N\\\"ushuRescue achieved 48.69\\%\ntranslation accuracy on 50 withheld sentences and generated NCSilver, a set of\n98 newly translated modern Chinese sentences of varying lengths. A sample of\nboth NCGold and NCSilver is included in the Supplementary Materials.\nAdditionally, we developed FastText-based and Seq2Seq models to further support\nresearch on N\\\"ushu. N\\\"ushuRescue provides a versatile and scalable tool for\nthe revitalization of endangered languages, minimizing the need for extensive\nhuman input."
                },
                "authors": [
                    {
                        "name": "Ivory Yang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01269v2",
                "updated": "2024-12-03T04:37:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    37,
                    3,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search"
                },
                "summary": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Baijun Ji"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02159v1",
                "updated": "2024-12-03T04:34:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    58,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:34:58Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    58,
                    1,
                    338,
                    0
                ],
                "title": "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods\n  and a New Transcript-Classifier Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods\n  and a New Transcript-Classifier Approach"
                },
                "summary": "Defending large language models against jailbreaks so that they never engage\nin a broadly-defined set of forbidden behaviors is an open problem. In this\npaper, we investigate the difficulty of jailbreak-defense when we only want to\nforbid a narrowly-defined set of behaviors. As a case study, we focus on\npreventing an LLM from helping a user make a bomb. We find that popular\ndefenses such as safety training, adversarial training, and input/output\nclassifiers are unable to fully solve this problem. In pursuit of a better\nsolution, we develop a transcript-classifier defense which outperforms the\nbaseline defenses we test. However, our classifier defense still fails in some\ncircumstances, which highlights the difficulty of jailbreak-defense even in a\nnarrow domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending large language models against jailbreaks so that they never engage\nin a broadly-defined set of forbidden behaviors is an open problem. In this\npaper, we investigate the difficulty of jailbreak-defense when we only want to\nforbid a narrowly-defined set of behaviors. As a case study, we focus on\npreventing an LLM from helping a user make a bomb. We find that popular\ndefenses such as safety training, adversarial training, and input/output\nclassifiers are unable to fully solve this problem. In pursuit of a better\nsolution, we develop a transcript-classifier defense which outperforms the\nbaseline defenses we test. However, our classifier defense still fails in some\ncircumstances, which highlights the difficulty of jailbreak-defense even in a\nnarrow domain."
                },
                "authors": [
                    {
                        "name": "Tony T. Wang"
                    },
                    {
                        "name": "John Hughes"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Rylan Schaeffer"
                    },
                    {
                        "name": "Rajashree Agrawal"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Jesse Mu"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Ethan Perez"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Perez"
                },
                "author": "Ethan Perez",
                "arxiv_comment": "Accepted to the AdvML-Frontiers and SoLaR workshops at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12361v3",
                "updated": "2024-12-03T04:34:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    34,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-16T08:24:09Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance"
                },
                "summary": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Shenzhi Yang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Guirong Chen"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02155v1",
                "updated": "2024-12-03T04:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events"
                },
                "summary": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called \\textbf{CausalMob}, to analyze the causal effects of\npublic events. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called \\textbf{CausalMob}, to analyze the causal effects of\npublic events. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Xiaojie Yang"
                    },
                    {
                        "name": "Hangli Ge"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Zipei Fan"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ryosuke Shibasaki"
                    },
                    {
                        "name": "Noboru Koshizuka"
                    }
                ],
                "author_detail": {
                    "name": "Noboru Koshizuka"
                },
                "author": "Noboru Koshizuka",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18142v2",
                "updated": "2024-12-03T04:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    19,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-22T13:03:28Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    3,
                    28,
                    1,
                    296,
                    0
                ],
                "title": "Analyzing Nobel Prize Literature with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Nobel Prize Literature with Large Language Models"
                },
                "summary": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Cen Lu"
                    },
                    {
                        "name": "Jiaxin Tai"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Jinlin Yang"
                    },
                    {
                        "name": "Qixin Liu"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Longjun Ma"
                    },
                    {
                        "name": "Dajiang Zhu"
                    },
                    {
                        "name": "Yudan Ren"
                    },
                    {
                        "name": "Bao Ge"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ning Qiang"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00648v2",
                "updated": "2024-12-03T04:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    14,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T02:55:08Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    55,
                    8,
                    6,
                    336,
                    0
                ],
                "title": "DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated\n  LLMs with Refined Rotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated\n  LLMs with Refined Rotation"
                },
                "summary": "Rotating the activation and weight matrices to reduce the influence of\noutliers in large language models (LLMs) has recently attracted significant\nattention, particularly in the context of model quantization. Prior studies\nhave shown that in low-precision quantization scenarios, such as 4-bit weights\nand 4-bit activations (W4A4), randomized Hadamard transforms can achieve\nsignificantly higher accuracy than randomized orthogonal transforms. Notably,\nthe reason behind this phenomena remains unknown. In this paper, we find that\nthese transformations show substantial improvement in eliminating outliers for\ncommon tokens and achieve similar quantization error. The primary reason for\nthe accuracy difference lies in the fact that randomized Hadamard transforms\ncan slightly reduce the quantization error for tokens with massive activations\nwhile randomized orthogonal transforms increase the quantization error. Due to\nthe extreme rarity of these tokens and their critical impact on model accuracy,\nwe consider this a long-tail optimization problem, and therefore construct a\nsimple yet effective method: a weighted loss function. Additionally, we propose\nan optimization strategy for the rotation matrix that involves alternating\noptimization of quantization parameters while employing orthogonal Procrustes\ntransforms to refine the rotation matrix. This makes the distribution of the\nrotated activation values more conducive to quantization, especially for tokens\nwith massive activations. Our method enhances the Rotated LLMs by achieving\ndual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive\nexperiments demonstrate the effectiveness and efficiency of DFRot. By tuning\nthe rotation matrix using just a single sample, DFRot achieves a perplexity\nimprovement of 0.25 and 0.21 on W4A4KV4 and W4A4KV16, respectively, for\nLLaMA3-8B, a model known for its quantization challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotating the activation and weight matrices to reduce the influence of\noutliers in large language models (LLMs) has recently attracted significant\nattention, particularly in the context of model quantization. Prior studies\nhave shown that in low-precision quantization scenarios, such as 4-bit weights\nand 4-bit activations (W4A4), randomized Hadamard transforms can achieve\nsignificantly higher accuracy than randomized orthogonal transforms. Notably,\nthe reason behind this phenomena remains unknown. In this paper, we find that\nthese transformations show substantial improvement in eliminating outliers for\ncommon tokens and achieve similar quantization error. The primary reason for\nthe accuracy difference lies in the fact that randomized Hadamard transforms\ncan slightly reduce the quantization error for tokens with massive activations\nwhile randomized orthogonal transforms increase the quantization error. Due to\nthe extreme rarity of these tokens and their critical impact on model accuracy,\nwe consider this a long-tail optimization problem, and therefore construct a\nsimple yet effective method: a weighted loss function. Additionally, we propose\nan optimization strategy for the rotation matrix that involves alternating\noptimization of quantization parameters while employing orthogonal Procrustes\ntransforms to refine the rotation matrix. This makes the distribution of the\nrotated activation values more conducive to quantization, especially for tokens\nwith massive activations. Our method enhances the Rotated LLMs by achieving\ndual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive\nexperiments demonstrate the effectiveness and efficiency of DFRot. By tuning\nthe rotation matrix using just a single sample, DFRot achieves a perplexity\nimprovement of 0.25 and 0.21 on W4A4KV4 and W4A4KV16, respectively, for\nLLaMA3-8B, a model known for its quantization challenges."
                },
                "authors": [
                    {
                        "name": "Jingyang Xiang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "24 pages, 38 figures, source code\n  \\url{https://github.com/JingyangXiang/DFRot}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17106v3",
                "updated": "2024-12-03T04:14:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    14,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-26T04:49:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    49,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution"
                },
                "summary": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR."
                },
                "authors": [
                    {
                        "name": "Libo Zhu"
                    },
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "https://github.com/libozhu03/PassionSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01004v2",
                "updated": "2024-12-03T04:13:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    13,
                    14,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T23:41:42Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    41,
                    42,
                    6,
                    336,
                    0
                ],
                "title": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA"
                },
                "summary": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge."
                },
                "authors": [
                    {
                        "name": "Haodong Lu"
                    },
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Jason Xue"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02149v1",
                "updated": "2024-12-03T04:09:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    9,
                    36,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T04:09:36Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    9,
                    36,
                    1,
                    338,
                    0
                ],
                "title": "Leveraging Large Language Models for Comparative Literature\n  Summarization with Reflective Incremental Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Comparative Literature\n  Summarization with Reflective Incremental Mechanisms"
                },
                "summary": "In this paper, we introduce ChatCite, a novel method leveraging large\nlanguage models (LLMs) for generating comparative literature summaries. The\nability to summarize research papers with a focus on key comparisons between\nstudies is an essential task in academic research. Existing summarization\nmodels, while effective at generating concise summaries, fail to provide deep\ncomparative insights. ChatCite addresses this limitation by incorporating a\nmulti-step reasoning mechanism that extracts critical elements from papers,\nincrementally builds a comparative summary, and refines the output through a\nreflective memory process. We evaluate ChatCite on a custom dataset,\nCompLit-LongContext, consisting of 1000 research papers with annotated\ncomparative summaries. Experimental results show that ChatCite outperforms\nseveral baseline methods, including GPT-4, BART, T5, and CoT, across various\nautomatic evaluation metrics such as ROUGE and the newly proposed G-Score.\nHuman evaluation further confirms that ChatCite generates more coherent,\ninsightful, and fluent summaries compared to these baseline models. Our method\nprovides a significant advancement in automatic literature review generation,\noffering researchers a powerful tool for efficiently comparing and synthesizing\nscientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ChatCite, a novel method leveraging large\nlanguage models (LLMs) for generating comparative literature summaries. The\nability to summarize research papers with a focus on key comparisons between\nstudies is an essential task in academic research. Existing summarization\nmodels, while effective at generating concise summaries, fail to provide deep\ncomparative insights. ChatCite addresses this limitation by incorporating a\nmulti-step reasoning mechanism that extracts critical elements from papers,\nincrementally builds a comparative summary, and refines the output through a\nreflective memory process. We evaluate ChatCite on a custom dataset,\nCompLit-LongContext, consisting of 1000 research papers with annotated\ncomparative summaries. Experimental results show that ChatCite outperforms\nseveral baseline methods, including GPT-4, BART, T5, and CoT, across various\nautomatic evaluation metrics such as ROUGE and the newly proposed G-Score.\nHuman evaluation further confirms that ChatCite generates more coherent,\ninsightful, and fluent summaries compared to these baseline models. Our method\nprovides a significant advancement in automatic literature review generation,\noffering researchers a powerful tool for efficiently comparing and synthesizing\nscientific research."
                },
                "authors": [
                    {
                        "name": "Fernando Gabriela Garcia"
                    },
                    {
                        "name": "Spencer Burns"
                    },
                    {
                        "name": "Harrison Fuller"
                    }
                ],
                "author_detail": {
                    "name": "Harrison Fuller"
                },
                "author": "Harrison Fuller",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18938v2",
                "updated": "2024-12-03T03:56:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    56,
                    52,
                    1,
                    338,
                    0
                ],
                "published": "2024-09-27T17:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding"
                },
                "summary": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding."
                },
                "authors": [
                    {
                        "name": "Heqing Zou"
                    },
                    {
                        "name": "Tianze Luo"
                    },
                    {
                        "name": "Guiyang Xie"
                    },
                    {
                        "name": "Victor"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Guangcong Wang"
                    },
                    {
                        "name": "Junyang Chen"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Hansheng Zhang"
                    },
                    {
                        "name": "Huaijian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huaijian Zhang"
                },
                "arxiv_affiliation": "Xiao Jie",
                "author": "Huaijian Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v3",
                "updated": "2024-12-03T03:16:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    16,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02113v1",
                "updated": "2024-12-03T03:10:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    12,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Trust & Safety of LLMs and LLMs in Trust & Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust & Safety of LLMs and LLMs in Trust & Safety"
                },
                "summary": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Dan Chon"
                    }
                ],
                "author_detail": {
                    "name": "Dan Chon"
                },
                "author": "Dan Chon",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02110v1",
                "updated": "2024-12-03T03:08:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    8,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:08:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    8,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Retrofitting XoM for Stripped Binaries without Embedded Data Relocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrofitting XoM for Stripped Binaries without Embedded Data Relocation"
                },
                "summary": "In this paper, we present PXoM, a practical technique to seamlessly retrofit\nXoM into stripped binaries on the x86-64 platform. As handling the mixture of\ncode and data is a well-known challenge for XoM, most existing methods require\nthe strict separation of code and data areas via either compile-time\ntransformation or binary patching, so that the unreadable permission can be\nsafely enforced at the granularity of memory pages. In contrast to previous\napproaches, we provide a fine-grained memory permission control mechanism to\nrestrict the read permission of code while allowing legitimate data reads\nwithin code pages. This novelty enables PXoM to harden stripped binaries but\nwithout resorting to error-prone embedded data relocation. We leverage Intel's\nhardware feature, Memory Protection Keys, to offer an efficient fine-grained\npermission control. We measure PXoM's performance with both micro- and\nmacro-benchmarks, and it only introduces negligible runtime overhead. Our\nsecurity evaluation shows that PXoM leaves adversaries with little wiggle room\nto harvest all of the required gadgets, suggesting PXoM is practical for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present PXoM, a practical technique to seamlessly retrofit\nXoM into stripped binaries on the x86-64 platform. As handling the mixture of\ncode and data is a well-known challenge for XoM, most existing methods require\nthe strict separation of code and data areas via either compile-time\ntransformation or binary patching, so that the unreadable permission can be\nsafely enforced at the granularity of memory pages. In contrast to previous\napproaches, we provide a fine-grained memory permission control mechanism to\nrestrict the read permission of code while allowing legitimate data reads\nwithin code pages. This novelty enables PXoM to harden stripped binaries but\nwithout resorting to error-prone embedded data relocation. We leverage Intel's\nhardware feature, Memory Protection Keys, to offer an efficient fine-grained\npermission control. We measure PXoM's performance with both micro- and\nmacro-benchmarks, and it only introduces negligible runtime overhead. Our\nsecurity evaluation shows that PXoM leaves adversaries with little wiggle room\nto harvest all of the required gadgets, suggesting PXoM is practical for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chenke Luo"
                    },
                    {
                        "name": "Jiang Ming"
                    },
                    {
                        "name": "Mengfei Xie"
                    },
                    {
                        "name": "Guojun Peng"
                    },
                    {
                        "name": "Jianming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jianming Fu"
                },
                "author": "Jianming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01078v2",
                "updated": "2024-12-03T02:59:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    2,
                    59,
                    43,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T03:31:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    31,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with\n  Over 60,000 Hours of Synthetic Speech Dialogue Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with\n  Over 60,000 Hours of Synthetic Speech Dialogue Data"
                },
                "summary": "The GPT-4o represents a significant milestone in enabling real-time\ninteraction with large language models (LLMs) through speech, its remarkable\nlow latency and high fluency not only capture attention but also stimulate\nresearch interest in the field. This real-time speech interaction is\nparticularly valuable in scenarios requiring rapid feedback and immediate\nresponses, dramatically enhancing user experience. However, there is a notable\nlack of research focused on real-time large speech language models,\nparticularly for Chinese. In this work, we present KE-Omni, a seamless large\nspeech language model built upon Ke-SpeechChat, a large-scale high-quality\nsynthetic speech interaction dataset consisting of 7 million Chinese and\nEnglish conversations, featuring 42,002 speakers, and totaling over 60,000\nhours, This contributes significantly to the advancement of research and\ndevelopment in this field. The demos can be accessed at\n\\url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GPT-4o represents a significant milestone in enabling real-time\ninteraction with large language models (LLMs) through speech, its remarkable\nlow latency and high fluency not only capture attention but also stimulate\nresearch interest in the field. This real-time speech interaction is\nparticularly valuable in scenarios requiring rapid feedback and immediate\nresponses, dramatically enhancing user experience. However, there is a notable\nlack of research focused on real-time large speech language models,\nparticularly for Chinese. In this work, we present KE-Omni, a seamless large\nspeech language model built upon Ke-SpeechChat, a large-scale high-quality\nsynthetic speech interaction dataset consisting of 7 million Chinese and\nEnglish conversations, featuring 42,002 speakers, and totaling over 60,000\nhours, This contributes significantly to the advancement of research and\ndevelopment in this field. The demos can be accessed at\n\\url{https://huggingface.co/spaces/KE-Team/KE-Omni}."
                },
                "authors": [
                    {
                        "name": "Shuaijiang Zhao"
                    },
                    {
                        "name": "Tingwei Guo"
                    },
                    {
                        "name": "Bajian Xiang"
                    },
                    {
                        "name": "Tongtang Wan"
                    },
                    {
                        "name": "Qiang Niu"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "arxiv_comment": "KE-Omni, Ke-SpeechChat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02104v1",
                "updated": "2024-12-03T02:54:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    2,
                    54,
                    31,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T02:54:31Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    2,
                    54,
                    31,
                    1,
                    338,
                    0
                ],
                "title": "Explainable and Interpretable Multimodal Large Language Models: A\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable and Interpretable Multimodal Large Language Models: A\n  Comprehensive Survey"
                },
                "summary": "The rapid development of Artificial Intelligence (AI) has revolutionized\nnumerous fields, with large language models (LLMs) and computer vision (CV)\nsystems driving advancements in natural language understanding and visual\nprocessing, respectively. The convergence of these technologies has catalyzed\nthe rise of multimodal AI, enabling richer, cross-modal understanding that\nspans text, vision, audio, and video modalities. Multimodal large language\nmodels (MLLMs), in particular, have emerged as a powerful framework,\ndemonstrating impressive capabilities in tasks like image-text generation,\nvisual question answering, and cross-modal retrieval. Despite these\nadvancements, the complexity and scale of MLLMs introduce significant\nchallenges in interpretability and explainability, essential for establishing\ntransparency, trustworthiness, and reliability in high-stakes applications.\nThis paper provides a comprehensive survey on the interpretability and\nexplainability of MLLMs, proposing a novel framework that categorizes existing\nresearch across three perspectives: (I) Data, (II) Model, (III) Training \\&\nInference. We systematically analyze interpretability from token-level to\nembedding-level representations, assess approaches related to both architecture\nanalysis and design, and explore training and inference strategies that enhance\ntransparency. By comparing various methodologies, we identify their strengths\nand limitations and propose future research directions to address unresolved\nchallenges in multimodal explainability. This survey offers a foundational\nresource for advancing interpretability and transparency in MLLMs, guiding\nresearchers and practitioners toward developing more accountable and robust\nmultimodal AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Artificial Intelligence (AI) has revolutionized\nnumerous fields, with large language models (LLMs) and computer vision (CV)\nsystems driving advancements in natural language understanding and visual\nprocessing, respectively. The convergence of these technologies has catalyzed\nthe rise of multimodal AI, enabling richer, cross-modal understanding that\nspans text, vision, audio, and video modalities. Multimodal large language\nmodels (MLLMs), in particular, have emerged as a powerful framework,\ndemonstrating impressive capabilities in tasks like image-text generation,\nvisual question answering, and cross-modal retrieval. Despite these\nadvancements, the complexity and scale of MLLMs introduce significant\nchallenges in interpretability and explainability, essential for establishing\ntransparency, trustworthiness, and reliability in high-stakes applications.\nThis paper provides a comprehensive survey on the interpretability and\nexplainability of MLLMs, proposing a novel framework that categorizes existing\nresearch across three perspectives: (I) Data, (II) Model, (III) Training \\&\nInference. We systematically analyze interpretability from token-level to\nembedding-level representations, assess approaches related to both architecture\nanalysis and design, and explore training and inference strategies that enhance\ntransparency. By comparing various methodologies, we identify their strengths\nand limitations and propose future research directions to address unresolved\nchallenges in multimodal explainability. This survey offers a foundational\nresource for advancing interpretability and transparency in MLLMs, guiding\nresearchers and practitioners toward developing more accountable and robust\nmultimodal AI systems."
                },
                "authors": [
                    {
                        "name": "Yunkai Dang"
                    },
                    {
                        "name": "Kaichen Huang"
                    },
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Sirui Huang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Mengxi Gao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18908v2",
                "updated": "2024-12-03T02:50:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    2,
                    50,
                    50,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-28T04:50:13Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    4,
                    50,
                    13,
                    3,
                    333,
                    0
                ],
                "title": "DuetML: Human-LLM Collaborative Machine Learning Framework for\n  Non-Expert Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetML: Human-LLM Collaborative Machine Learning Framework for\n  Non-Expert Users"
                },
                "summary": "Machine learning (ML) models have significantly impacted various domains in\nour everyday lives. While large language models (LLMs) offer intuitive\ninterfaces and versatility, task-specific ML models remain valuable for their\nefficiency and focused performance in specialized tasks. However, developing\nthese models requires technical expertise, making it particularly challenging\nfor non-expert users to customize them for their unique needs. Although\ninteractive machine learning (IML) aims to democratize ML development through\nuser-friendly interfaces, users struggle to translate their requirements into\nappropriate ML tasks. We propose human-LLM collaborative ML as a new paradigm\nbridging human-driven IML and machine-driven LLM approaches. To realize this\nvision, we introduce DuetML, a framework that integrates multimodal LLMs\n(MLLMs) as interactive agents collaborating with users throughout the ML\nprocess. Our system carefully balances MLLM capabilities with user agency by\nimplementing both reactive and proactive interactions between users and MLLM\nagents. Through a comparative user study, we demonstrate that DuetML enables\nnon-expert users to define training data that better aligns with target tasks\nwithout increasing cognitive load, while offering opportunities for deeper\nengagement with ML task formulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models have significantly impacted various domains in\nour everyday lives. While large language models (LLMs) offer intuitive\ninterfaces and versatility, task-specific ML models remain valuable for their\nefficiency and focused performance in specialized tasks. However, developing\nthese models requires technical expertise, making it particularly challenging\nfor non-expert users to customize them for their unique needs. Although\ninteractive machine learning (IML) aims to democratize ML development through\nuser-friendly interfaces, users struggle to translate their requirements into\nappropriate ML tasks. We propose human-LLM collaborative ML as a new paradigm\nbridging human-driven IML and machine-driven LLM approaches. To realize this\nvision, we introduce DuetML, a framework that integrates multimodal LLMs\n(MLLMs) as interactive agents collaborating with users throughout the ML\nprocess. Our system carefully balances MLLM capabilities with user agency by\nimplementing both reactive and proactive interactions between users and MLLM\nagents. Through a comparative user study, we demonstrate that DuetML enables\nnon-expert users to define training data that better aligns with target tasks\nwithout increasing cognitive load, while offering opportunities for deeper\nengagement with ML task formulation."
                },
                "authors": [
                    {
                        "name": "Wataru Kawabe"
                    },
                    {
                        "name": "Yusuke Sugano"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Sugano"
                },
                "author": "Yusuke Sugano",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02081v1",
                "updated": "2024-12-03T01:53:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    53,
                    6,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T01:53:06Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    53,
                    6,
                    1,
                    338,
                    0
                ],
                "title": "Let's Think Var-by-Var: Large Language Models Enable Ad Hoc\n  Probabilistic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Think Var-by-Var: Large Language Models Enable Ad Hoc\n  Probabilistic Reasoning"
                },
                "summary": "A hallmark of intelligence is the ability to flesh out underspecified\nsituations using \"common sense.\" We propose to extract that common sense from\nlarge language models (LLMs), in a form that can feed into probabilistic\ninference. We focus our investigation on $\\textit{guesstimation}$ questions\nsuch as \"How much are Airbnb listings in Newark, NJ?\" Formulating a sensible\nanswer without access to data requires drawing on, and integrating, bits of\ncommon knowledge about how $\\texttt{Price}$ and $\\texttt{Location}$ may relate\nto other variables, such as $\\texttt{Property Type}$. Our framework answers\nsuch a question by synthesizing an $\\textit{ad hoc}$ probabilistic model. First\nwe prompt an LLM to propose a set of random variables relevant to the question,\nfollowed by moment constraints on their joint distribution. We then optimize\nthe joint distribution $p$ within a log-linear family to maximize the overall\nconstraint satisfaction. Our experiments show that LLMs can successfully be\nprompted to propose reasonable variables, and while the proposed numerical\nconstraints can be noisy, jointly optimizing for their satisfaction reconciles\nthem. When evaluated on probabilistic questions derived from three real-world\ntabular datasets, we find that our framework performs comparably to a direct\nprompting baseline in terms of total variation distance from the dataset\ndistribution, and is similarly robust to noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hallmark of intelligence is the ability to flesh out underspecified\nsituations using \"common sense.\" We propose to extract that common sense from\nlarge language models (LLMs), in a form that can feed into probabilistic\ninference. We focus our investigation on $\\textit{guesstimation}$ questions\nsuch as \"How much are Airbnb listings in Newark, NJ?\" Formulating a sensible\nanswer without access to data requires drawing on, and integrating, bits of\ncommon knowledge about how $\\texttt{Price}$ and $\\texttt{Location}$ may relate\nto other variables, such as $\\texttt{Property Type}$. Our framework answers\nsuch a question by synthesizing an $\\textit{ad hoc}$ probabilistic model. First\nwe prompt an LLM to propose a set of random variables relevant to the question,\nfollowed by moment constraints on their joint distribution. We then optimize\nthe joint distribution $p$ within a log-linear family to maximize the overall\nconstraint satisfaction. Our experiments show that LLMs can successfully be\nprompted to propose reasonable variables, and while the proposed numerical\nconstraints can be noisy, jointly optimizing for their satisfaction reconciles\nthem. When evaluated on probabilistic questions derived from three real-world\ntabular datasets, we find that our framework performs comparably to a direct\nprompting baseline in terms of total variation distance from the dataset\ndistribution, and is similarly robust to noise."
                },
                "authors": [
                    {
                        "name": "Shepard Xia"
                    },
                    {
                        "name": "Brian Lu"
                    },
                    {
                        "name": "Jason Eisner"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eisner"
                },
                "author": "Jason Eisner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01032v2",
                "updated": "2024-12-03T01:34:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    34,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-01T19:49:19Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    19,
                    49,
                    19,
                    1,
                    275,
                    0
                ],
                "title": "Teaching Cloud Infrastructure and Scalable Application Deployment in an\n  Undergraduate Computer Science Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Cloud Infrastructure and Scalable Application Deployment in an\n  Undergraduate Computer Science Program"
                },
                "summary": "Making successful use of cloud computing requires nuanced approaches to both\nsystem design and deployment methodology, involving reasoning about the\nelasticity, cost, and security models of cloud services. Building cloud-native\napplications without a firm understanding of the fundamentals of cloud\nengineering can leave students susceptible to cost and security pitfalls. Yet,\ncloud computing is not commonly taught at the undergraduate level. To address\nthis gap, we designed an undergraduate-level course that frames cloud\ninfrastructure deployment as a software engineering practice. Our course\nfeatured a number of hands-on assignments that gave students experience with\nmodern, best-practice concepts and tools including infrastructure-as-code\n(IaC). We describe the design of the course, our experience teaching its\ninitial offering, and provide our reflections on what worked well and potential\nareas for improvement. Our course material is available at\nhttps://infracourse.cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making successful use of cloud computing requires nuanced approaches to both\nsystem design and deployment methodology, involving reasoning about the\nelasticity, cost, and security models of cloud services. Building cloud-native\napplications without a firm understanding of the fundamentals of cloud\nengineering can leave students susceptible to cost and security pitfalls. Yet,\ncloud computing is not commonly taught at the undergraduate level. To address\nthis gap, we designed an undergraduate-level course that frames cloud\ninfrastructure deployment as a software engineering practice. Our course\nfeatured a number of hands-on assignments that gave students experience with\nmodern, best-practice concepts and tools including infrastructure-as-code\n(IaC). We describe the design of the course, our experience teaching its\ninitial offering, and provide our reflections on what worked well and potential\nareas for improvement. Our course material is available at\nhttps://infracourse.cloud."
                },
                "authors": [
                    {
                        "name": "Aditya Saligrama"
                    },
                    {
                        "name": "Cody Ho"
                    },
                    {
                        "name": "Benjamin Tripp"
                    },
                    {
                        "name": "Michael Abbott"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "To appear in SIGCSE TS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02072v1",
                "updated": "2024-12-03T01:25:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    25,
                    55,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T01:25:55Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    25,
                    55,
                    1,
                    338,
                    0
                ],
                "title": "Performance Comparison of Deep Learning Techniques in Naira\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Comparison of Deep Learning Techniques in Naira\n  Classification"
                },
                "summary": "The Naira is Nigeria's official currency in daily transactions. This study\npresents the deployment and evaluation of Deep Learning (DL) models to classify\nCurrency Notes (Naira) by denomination. Using a diverse dataset of 1,808 images\nof Naira notes captured under different conditions, trained the models\nemploying different architectures and got the highest accuracy with\nMobileNetV2, the model achieved a high accuracy rate of in training of 90.75%\nand validation accuracy of 87.04% in classification tasks and demonstrated\nsubstantial performance across various scenarios. This model holds significant\npotential for practical applications, including automated cash handling\nsystems, sorting systems, and assistive technology for the visually impaired.\nThe results demonstrate how the model could boost the Nigerian economy's\nsecurity and efficiency of financial transactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Naira is Nigeria's official currency in daily transactions. This study\npresents the deployment and evaluation of Deep Learning (DL) models to classify\nCurrency Notes (Naira) by denomination. Using a diverse dataset of 1,808 images\nof Naira notes captured under different conditions, trained the models\nemploying different architectures and got the highest accuracy with\nMobileNetV2, the model achieved a high accuracy rate of in training of 90.75%\nand validation accuracy of 87.04% in classification tasks and demonstrated\nsubstantial performance across various scenarios. This model holds significant\npotential for practical applications, including automated cash handling\nsystems, sorting systems, and assistive technology for the visually impaired.\nThe results demonstrate how the model could boost the Nigerian economy's\nsecurity and efficiency of financial transactions."
                },
                "authors": [
                    {
                        "name": "Ismail Ismail Tijjani"
                    },
                    {
                        "name": "Ahmad Abubakar Mustapha"
                    },
                    {
                        "name": "Isma'il Tijjani Idris"
                    }
                ],
                "author_detail": {
                    "name": "Isma'il Tijjani Idris"
                },
                "author": "Isma'il Tijjani Idris",
                "arxiv_doi": "10.14445/23497157/IJRES-V11I5P113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14445/23497157/IJRES-V11I5P113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07870v3",
                "updated": "2024-12-03T01:04:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    1,
                    4,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-12T15:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    15,
                    26,
                    17,
                    1,
                    317,
                    0
                ],
                "title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders"
                },
                "summary": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Jaya Krishna Mandivarapu"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Krishna Mandivarapu"
                },
                "author": "Jaya Krishna Mandivarapu",
                "arxiv_journal_ref": "EMNLP CustomNLP4U 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02065v1",
                "updated": "2024-12-03T00:59:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    59,
                    56,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T00:59:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    59,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "Leveraging Large Language Models to Democratize Access to Costly\n  Financial Datasets for Academic Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Democratize Access to Costly\n  Financial Datasets for Academic Research"
                },
                "summary": "Unequal access to costly datasets essential for empirical research has long\nhindered researchers from disadvantaged institutions, limiting their ability to\ncontribute to their fields and advance their careers. Recent breakthroughs in\nLarge Language Models (LLMs) have the potential to democratize data access by\nautomating data collection from unstructured sources. We develop and evaluate a\nnovel methodology using GPT-4o-mini within a Retrieval-Augmented Generation\n(RAG) framework to collect data from corporate disclosures. Our approach\nachieves human-level accuracy in collecting CEO pay ratios from approximately\n10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000\n10-K filings, with LLM processing times of 9 and 40 minutes respectively, each\nat a cost under $10. This stands in stark contrast to the hundreds of hours\nneeded for manual collection or the thousands of dollars required for\ncommercial database subscriptions. To foster a more inclusive research\ncommunity by empowering researchers with limited resources to explore new\navenues of inquiry, we share our methodology and the resulting datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unequal access to costly datasets essential for empirical research has long\nhindered researchers from disadvantaged institutions, limiting their ability to\ncontribute to their fields and advance their careers. Recent breakthroughs in\nLarge Language Models (LLMs) have the potential to democratize data access by\nautomating data collection from unstructured sources. We develop and evaluate a\nnovel methodology using GPT-4o-mini within a Retrieval-Augmented Generation\n(RAG) framework to collect data from corporate disclosures. Our approach\nachieves human-level accuracy in collecting CEO pay ratios from approximately\n10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000\n10-K filings, with LLM processing times of 9 and 40 minutes respectively, each\nat a cost under $10. This stands in stark contrast to the hundreds of hours\nneeded for manual collection or the thousands of dollars required for\ncommercial database subscriptions. To foster a more inclusive research\ncommunity by empowering researchers with limited resources to explore new\navenues of inquiry, we share our methodology and the resulting datasets."
                },
                "authors": [
                    {
                        "name": "Julian Junyan Wang"
                    },
                    {
                        "name": "Victor Xiaoqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Victor Xiaoqi Wang"
                },
                "author": "Victor Xiaoqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02054v1",
                "updated": "2024-12-03T00:26:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    26,
                    4,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T00:26:04Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    26,
                    4,
                    1,
                    338,
                    0
                ],
                "title": "Redundant Queries in DETR-Based 3D Detection Methods: Unnecessary and\n  Prunable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redundant Queries in DETR-Based 3D Detection Methods: Unnecessary and\n  Prunable"
                },
                "summary": "Query-based models are extensively used in 3D object detection tasks, with a\nwide range of pre-trained checkpoints readily available online. However,\ndespite their popularity, these models often require an excessive number of\nobject queries, far surpassing the actual number of objects to detect. The\nredundant queries result in unnecessary computational and memory costs. In this\npaper, we find that not all queries contribute equally -- a significant portion\nof queries have a much smaller impact compared to others. Based on this\nobservation, we propose an embarrassingly simple approach called \\bd{G}radually\n\\bd{P}runing \\bd{Q}ueries (GPQ), which prunes queries incrementally based on\ntheir classification scores. It is straightforward to implement in any\nquery-based method, as it can be seamlessly integrated as a fine-tuning step\nusing an existing checkpoint after training. With GPQ, users can easily\ngenerate multiple models with fewer queries, starting from a checkpoint with an\nexcessive number of queries. Experiments on various advanced 3D detectors show\nthat GPQ effectively reduces redundant queries while maintaining performance.\nUsing our method, model inference on desktop GPUs can be accelerated by up to\n1.31x. Moreover, after deployment on edge devices, it achieves up to a 67.86\\%\nreduction in FLOPs and a 76.38\\% decrease in inference time. The code will be\navailable at \\url{https://github.com/iseri27/Gpq}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based models are extensively used in 3D object detection tasks, with a\nwide range of pre-trained checkpoints readily available online. However,\ndespite their popularity, these models often require an excessive number of\nobject queries, far surpassing the actual number of objects to detect. The\nredundant queries result in unnecessary computational and memory costs. In this\npaper, we find that not all queries contribute equally -- a significant portion\nof queries have a much smaller impact compared to others. Based on this\nobservation, we propose an embarrassingly simple approach called \\bd{G}radually\n\\bd{P}runing \\bd{Q}ueries (GPQ), which prunes queries incrementally based on\ntheir classification scores. It is straightforward to implement in any\nquery-based method, as it can be seamlessly integrated as a fine-tuning step\nusing an existing checkpoint after training. With GPQ, users can easily\ngenerate multiple models with fewer queries, starting from a checkpoint with an\nexcessive number of queries. Experiments on various advanced 3D detectors show\nthat GPQ effectively reduces redundant queries while maintaining performance.\nUsing our method, model inference on desktop GPUs can be accelerated by up to\n1.31x. Moreover, after deployment on edge devices, it achieves up to a 67.86\\%\nreduction in FLOPs and a 76.38\\% decrease in inference time. The code will be\navailable at \\url{https://github.com/iseri27/Gpq}."
                },
                "authors": [
                    {
                        "name": "Lizhen Xu"
                    },
                    {
                        "name": "Shanmin Pang"
                    },
                    {
                        "name": "Wenzhao Qiu"
                    },
                    {
                        "name": "Zehao Wu"
                    },
                    {
                        "name": "Xiuxiu Bai"
                    },
                    {
                        "name": "Kuizhi Mei"
                    },
                    {
                        "name": "Jianru Xue"
                    }
                ],
                "author_detail": {
                    "name": "Jianru Xue"
                },
                "author": "Jianru Xue",
                "arxiv_comment": "13 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02047v1",
                "updated": "2024-12-03T00:07:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    7,
                    46,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T00:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    7,
                    46,
                    1,
                    338,
                    0
                ],
                "title": "Simplifying HPC resource selection: A tool for optimizing execution time\n  and cost on Azure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying HPC resource selection: A tool for optimizing execution time\n  and cost on Azure"
                },
                "summary": "Azure Cloud offers a wide range of resources for running HPC workloads,\nrequiring users to configure their deployment by selecting VM types, number of\nVMs, and processes per VM. Suboptimal decisions may lead to longer execution\ntimes or additional costs for the user. We are developing an open-source tool\nto assist users in making these decisions by considering application input\nparameters, as they influence resource consumption. The tool automates the\ntime-consuming process of setting up the cloud environment, executing the\nbenchmarking runs, handling output, and providing users with resource selection\nrecommendations as high level insights on run times and costs across different\nVM types and number of VMs. In this work, we present initial results and\ninsights on reducing the number of cloud executions needed to provide such\nguidance, leveraging data analytics and optimization techniques with two\nwell-known HPC applications: OpenFOAM and LAMMPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Azure Cloud offers a wide range of resources for running HPC workloads,\nrequiring users to configure their deployment by selecting VM types, number of\nVMs, and processes per VM. Suboptimal decisions may lead to longer execution\ntimes or additional costs for the user. We are developing an open-source tool\nto assist users in making these decisions by considering application input\nparameters, as they influence resource consumption. The tool automates the\ntime-consuming process of setting up the cloud environment, executing the\nbenchmarking runs, handling output, and providing users with resource selection\nrecommendations as high level insights on run times and costs across different\nVM types and number of VMs. In this work, we present initial results and\ninsights on reducing the number of cloud executions needed to provide such\nguidance, leveraging data analytics and optimization techniques with two\nwell-known HPC applications: OpenFOAM and LAMMPS."
                },
                "authors": [
                    {
                        "name": "Marco A. S. Netto"
                    },
                    {
                        "name": "Wolfgang De Savador"
                    },
                    {
                        "name": "Davide Vanzo"
                    }
                ],
                "author_detail": {
                    "name": "Davide Vanzo"
                },
                "author": "Davide Vanzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02043v1",
                "updated": "2024-12-03T00:01:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    1,
                    48,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T00:01:48Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    0,
                    1,
                    48,
                    1,
                    338,
                    0
                ],
                "title": "Future of Information Retrieval Research in the Age of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future of Information Retrieval Research in the Age of Generative AI"
                },
                "summary": "In the fast-evolving field of information retrieval (IR), the integration of\ngenerative AI technologies such as large language models (LLMs) is transforming\nhow users search for and interact with information. Recognizing this paradigm\nshift at the intersection of IR and generative AI (IR-GenAI), a visioning\nworkshop supported by the Computing Community Consortium (CCC) was held in July\n2024 to discuss the future of IR in the age of generative AI. This workshop\nconvened 44 experts in information retrieval, natural language processing,\nhuman-computer interaction, and artificial intelligence from academia,\nindustry, and government to explore how generative AI can enhance IR and vice\nversa, and to identify the major challenges and opportunities in this rapidly\nadvancing field.\n  This report contains a summary of discussions as potentially important\nresearch topics and contains a list of recommendations for academics, industry\npractitioners, institutions, evaluation campaigns, and funding agencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-evolving field of information retrieval (IR), the integration of\ngenerative AI technologies such as large language models (LLMs) is transforming\nhow users search for and interact with information. Recognizing this paradigm\nshift at the intersection of IR and generative AI (IR-GenAI), a visioning\nworkshop supported by the Computing Community Consortium (CCC) was held in July\n2024 to discuss the future of IR in the age of generative AI. This workshop\nconvened 44 experts in information retrieval, natural language processing,\nhuman-computer interaction, and artificial intelligence from academia,\nindustry, and government to explore how generative AI can enhance IR and vice\nversa, and to identify the major challenges and opportunities in this rapidly\nadvancing field.\n  This report contains a summary of discussions as potentially important\nresearch topics and contains a list of recommendations for academics, industry\npractitioners, institutions, evaluation campaigns, and funding agencies."
                },
                "authors": [
                    {
                        "name": "James Allan"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Daniel P. Lopresti"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02035v1",
                "updated": "2024-12-02T23:31:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    23,
                    31,
                    52,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T23:31:52Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    23,
                    31,
                    52,
                    0,
                    337,
                    0
                ],
                "title": "LLMs4Life: Large Language Models for Ontology Learning in Life Sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4Life: Large Language Models for Ontology Learning in Life Sciences"
                },
                "summary": "Ontology learning in complex domains, such as life sciences, poses\nsignificant challenges for current Large Language Models (LLMs). Existing LLMs\nstruggle to generate ontologies with multiple hierarchical levels, rich\ninterconnections, and comprehensive class coverage due to constraints on the\nnumber of tokens they can generate and inadequate domain adaptation. To address\nthese issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs\nwith advanced prompt engineering techniques and ontology reuse to enhance the\ngenerated ontologies' domain-specific reasoning and structural depth. Our work\nevaluates the capabilities of LLMs in ontology learning in the context of\nhighly specialized and complex domains such as life science domains. To assess\nthe logical consistency, completeness, and scalability of the generated\nontologies, we use the AquaDiva ontology developed and used in the\ncollaborative research center AquaDiva as a case study. Our evaluation shows\nthe viability of LLMs for ontology learning in specialized domains, providing\nsolutions to longstanding limitations in model performance and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology learning in complex domains, such as life sciences, poses\nsignificant challenges for current Large Language Models (LLMs). Existing LLMs\nstruggle to generate ontologies with multiple hierarchical levels, rich\ninterconnections, and comprehensive class coverage due to constraints on the\nnumber of tokens they can generate and inadequate domain adaptation. To address\nthese issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs\nwith advanced prompt engineering techniques and ontology reuse to enhance the\ngenerated ontologies' domain-specific reasoning and structural depth. Our work\nevaluates the capabilities of LLMs in ontology learning in the context of\nhighly specialized and complex domains such as life science domains. To assess\nthe logical consistency, completeness, and scalability of the generated\nontologies, we use the AquaDiva ontology developed and used in the\ncollaborative research center AquaDiva as a case study. Our evaluation shows\nthe viability of LLMs for ontology learning in specialized domains, providing\nsolutions to longstanding limitations in model performance and scalability."
                },
                "authors": [
                    {
                        "name": "Nadeen Fathallah"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Alsayed Algergawy"
                    }
                ],
                "author_detail": {
                    "name": "Alsayed Algergawy"
                },
                "author": "Alsayed Algergawy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02030v1",
                "updated": "2024-12-02T23:20:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    23,
                    20,
                    35,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T23:20:35Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    23,
                    20,
                    35,
                    0,
                    337,
                    0
                ],
                "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic\n  Adversarial Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic\n  Adversarial Training"
                },
                "summary": "We introduce NitroFusion, a fundamentally different approach to single-step\ndiffusion that achieves high-quality generation through a dynamic adversarial\nframework. While one-step methods offer dramatic speed advantages, they\ntypically suffer from quality degradation compared to their multi-step\ncounterparts. Just as a panel of art critics provides comprehensive feedback by\nspecializing in different aspects like composition, color, and technique, our\napproach maintains a large pool of specialized discriminator heads that\ncollectively guide the generation process. Each discriminator group develops\nexpertise in specific quality aspects at different noise levels, providing\ndiverse feedback that enables high-fidelity one-step generation. Our framework\ncombines: (i) a dynamic discriminator pool with specialized discriminator\ngroups to improve generation quality, (ii) strategic refresh mechanisms to\nprevent discriminator overfitting, and (iii) global-local discriminator heads\nfor multi-scale quality assessment, and unconditional/conditional training for\nbalanced generation. Additionally, our framework uniquely supports flexible\ndeployment through bottom-up refinement, allowing users to dynamically choose\nbetween 1-4 denoising steps with the same model for direct quality-speed\ntrade-offs. Through comprehensive experiments, we demonstrate that NitroFusion\nsignificantly outperforms existing single-step methods across multiple\nevaluation metrics, particularly excelling in preserving fine details and\nglobal consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NitroFusion, a fundamentally different approach to single-step\ndiffusion that achieves high-quality generation through a dynamic adversarial\nframework. While one-step methods offer dramatic speed advantages, they\ntypically suffer from quality degradation compared to their multi-step\ncounterparts. Just as a panel of art critics provides comprehensive feedback by\nspecializing in different aspects like composition, color, and technique, our\napproach maintains a large pool of specialized discriminator heads that\ncollectively guide the generation process. Each discriminator group develops\nexpertise in specific quality aspects at different noise levels, providing\ndiverse feedback that enables high-fidelity one-step generation. Our framework\ncombines: (i) a dynamic discriminator pool with specialized discriminator\ngroups to improve generation quality, (ii) strategic refresh mechanisms to\nprevent discriminator overfitting, and (iii) global-local discriminator heads\nfor multi-scale quality assessment, and unconditional/conditional training for\nbalanced generation. Additionally, our framework uniquely supports flexible\ndeployment through bottom-up refinement, allowing users to dynamically choose\nbetween 1-4 denoising steps with the same model for direct quality-speed\ntrade-offs. Through comprehensive experiments, we demonstrate that NitroFusion\nsignificantly outperforms existing single-step methods across multiple\nevaluation metrics, particularly excelling in preserving fine details and\nglobal consistency."
                },
                "authors": [
                    {
                        "name": "Dar-Yen Chen"
                    },
                    {
                        "name": "Hmrishav Bandyopadhyay"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Yi-Zhe Song"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhe Song"
                },
                "author": "Yi-Zhe Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13104v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13104v3",
                "updated": "2024-12-02T22:14:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    22,
                    14,
                    44,
                    0,
                    337,
                    0
                ],
                "published": "2023-10-19T19:01:27Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    19,
                    1,
                    27,
                    3,
                    292,
                    0
                ],
                "title": "Making Differential Privacy Easier to Use for Data Controllers using a\n  Privacy Risk Indicator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Differential Privacy Easier to Use for Data Controllers using a\n  Privacy Risk Indicator"
                },
                "summary": "Differential privacy (DP) enables private data analysis but is difficult to\nuse in practice. In a typical DP deployment, data controllers manage\nindividuals' sensitive data and are responsible for answering data analysts'\nqueries while protecting individuals' privacy; they do so by choosing\n$\\epsilon$, the privacy loss budget, which controls how much noise to add to\nthe query output. However, it is challenging for data controllers to choose\n$\\epsilon$ because of the difficulty of interpreting the privacy implications\nof such a choice on the individuals they wish to protect.\n  To address this challenge, we first derive a privacy risk indicator (PRI)\ndirectly from the definition of ex-post per-instance privacy loss in the DP\nliterature. The PRI indicates the impact of choosing $\\epsilon$ on individuals'\nprivacy. We then leverage the PRI to design an algorithm to choose $\\epsilon$\nand release query output based on data controllers' privacy preferences. We\ndesign a modification of the algorithm that allows releasing both the query\noutput and $\\epsilon$ while satisfying differential privacy, and we propose a\nsolution that bounds the total privacy loss when using the algorithm to answer\nmultiple queries without requiring controllers to set the total privacy loss\nbudget. We demonstrate our contributions through an IRB-approved user study and\nexperimental evaluations that show the PRI is useful for helping controllers\nchoose $\\epsilon$ and our algorithms are efficient. Overall, our work\ncontributes to making DP easier to use for controllers by lowering adoption\nbarriers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) enables private data analysis but is difficult to\nuse in practice. In a typical DP deployment, data controllers manage\nindividuals' sensitive data and are responsible for answering data analysts'\nqueries while protecting individuals' privacy; they do so by choosing\n$\\epsilon$, the privacy loss budget, which controls how much noise to add to\nthe query output. However, it is challenging for data controllers to choose\n$\\epsilon$ because of the difficulty of interpreting the privacy implications\nof such a choice on the individuals they wish to protect.\n  To address this challenge, we first derive a privacy risk indicator (PRI)\ndirectly from the definition of ex-post per-instance privacy loss in the DP\nliterature. The PRI indicates the impact of choosing $\\epsilon$ on individuals'\nprivacy. We then leverage the PRI to design an algorithm to choose $\\epsilon$\nand release query output based on data controllers' privacy preferences. We\ndesign a modification of the algorithm that allows releasing both the query\noutput and $\\epsilon$ while satisfying differential privacy, and we propose a\nsolution that bounds the total privacy loss when using the algorithm to answer\nmultiple queries without requiring controllers to set the total privacy loss\nbudget. We demonstrate our contributions through an IRB-approved user study and\nexperimental evaluations that show the PRI is useful for helping controllers\nchoose $\\epsilon$ and our algorithms are efficient. Overall, our work\ncontributes to making DP easier to use for controllers by lowering adoption\nbarriers."
                },
                "authors": [
                    {
                        "name": "Zhiru Zhu"
                    },
                    {
                        "name": "Raul Castro Fernandez"
                    }
                ],
                "author_detail": {
                    "name": "Raul Castro Fernandez"
                },
                "author": "Raul Castro Fernandez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13104v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13104v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01999v1",
                "updated": "2024-12-02T22:07:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    22,
                    7,
                    23,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T22:07:23Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    22,
                    7,
                    23,
                    0,
                    337,
                    0
                ],
                "title": "AVA: Fault-tolerant Reconfigurable Geo-Replication on Heterogeneous\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVA: Fault-tolerant Reconfigurable Geo-Replication on Heterogeneous\n  Clusters"
                },
                "summary": "Fault-tolerant replicated database systems consume less energy than the\ncompute-intensive proof-of-work blockchain. Thus, they are promising\ntechnologies for the building blocks that assemble global financial\ninfrastructure. To facilitate global scaling, clustered replication protocols\nare essential in orchestrating nodes into clusters based on proximity. However,\nthe existing approaches often assume a homogeneous and fixed model in which the\nnumber of nodes across clusters is the same and fixed, and often limited to a\nfail-stop fault model. This paper presents heterogeneous and reconfigurable\nclustered replication for the general environment with arbitrary failures. In\nparticular, we present AVA, a fault-tolerant reconfigurable geo-replication\nthat allows dynamic membership: replicas are allowed to join and leave\nclusters. We formally state and prove the safety and liveness properties of the\nprotocol. Furthermore, our replication protocol is consensus-agnostic, meaning\neach cluster can utilize any local replication mechanism. In our comprehensive\nevaluation, we instantiate our replication with both HotStuff and BFT-SMaRt.\nExperiments on geo-distributed deployments on Google Cloud demonstrates that\nmembers of clusters can be reconfigured without considerably affecting\ntransaction processing, and that heterogeneity of clusters may significantly\nimprove throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault-tolerant replicated database systems consume less energy than the\ncompute-intensive proof-of-work blockchain. Thus, they are promising\ntechnologies for the building blocks that assemble global financial\ninfrastructure. To facilitate global scaling, clustered replication protocols\nare essential in orchestrating nodes into clusters based on proximity. However,\nthe existing approaches often assume a homogeneous and fixed model in which the\nnumber of nodes across clusters is the same and fixed, and often limited to a\nfail-stop fault model. This paper presents heterogeneous and reconfigurable\nclustered replication for the general environment with arbitrary failures. In\nparticular, we present AVA, a fault-tolerant reconfigurable geo-replication\nthat allows dynamic membership: replicas are allowed to join and leave\nclusters. We formally state and prove the safety and liveness properties of the\nprotocol. Furthermore, our replication protocol is consensus-agnostic, meaning\neach cluster can utilize any local replication mechanism. In our comprehensive\nevaluation, we instantiate our replication with both HotStuff and BFT-SMaRt.\nExperiments on geo-distributed deployments on Google Cloud demonstrates that\nmembers of clusters can be reconfigured without considerably affecting\ntransaction processing, and that heterogeneity of clusters may significantly\nimprove throughput."
                },
                "authors": [
                    {
                        "name": "Tejas Mane"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Mohammad Sadoghi"
                    },
                    {
                        "name": "Mohsen Lesani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Lesani"
                },
                "author": "Mohsen Lesani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10794v3",
                "updated": "2024-12-02T21:48:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    48,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-16T03:38:48Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    3,
                    38,
                    48,
                    6,
                    168,
                    0
                ],
                "title": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space\n  Analysis"
                },
                "summary": "Large language models (LLMs) are susceptible to a type of attack known as\njailbreaking, which misleads LLMs to output harmful contents. Although there\nare diverse jailbreak attack strategies, there is no unified understanding on\nwhy some methods succeed and others fail. This paper explores the behavior of\nharmful and harmless prompts in the LLM's representation space to investigate\nthe intrinsic properties of successful jailbreak attacks. We hypothesize that\nsuccessful attacks share some similar properties: They are effective in moving\nthe representation of the harmful prompt towards the direction to the harmless\nprompts. We leverage hidden representations into the objective of existing\njailbreak attacks to move the attacks along the acceptance direction, and\nconduct experiments to validate the above hypothesis using the proposed\nobjective. We hope this study provides new insights into understanding how LLMs\nunderstand harmfulness information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to a type of attack known as\njailbreaking, which misleads LLMs to output harmful contents. Although there\nare diverse jailbreak attack strategies, there is no unified understanding on\nwhy some methods succeed and others fail. This paper explores the behavior of\nharmful and harmless prompts in the LLM's representation space to investigate\nthe intrinsic properties of successful jailbreak attacks. We hypothesize that\nsuccessful attacks share some similar properties: They are effective in moving\nthe representation of the harmful prompt towards the direction to the harmless\nprompts. We leverage hidden representations into the objective of existing\njailbreak attacks to move the attacks along the acceptance direction, and\nconduct experiments to validate the above hypothesis using the proposed\nobjective. We hope this study provides new insights into understanding how LLMs\nunderstand harmfulness information."
                },
                "authors": [
                    {
                        "name": "Yuping Lin"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Makoto Yamada"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "Accepted by EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13060v2",
                "updated": "2024-12-02T21:42:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    42,
                    36,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-16T21:40:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    21,
                    40,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "AERO: Softmax-Only LLMs for Efficient Private Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AERO: Softmax-Only LLMs for Efficient Private Inference"
                },
                "summary": "The pervasiveness of proprietary language models has raised privacy concerns\nfor users' sensitive data, emphasizing the need for private inference (PI),\nwhere inference is performed directly on encrypted inputs. However, current PI\nmethods face prohibitively higher communication and latency overheads,\nprimarily due to nonlinear operations. In this paper, we present a\ncomprehensive analysis to understand the role of nonlinearities in\ntransformer-based decoder-only language models. We introduce AERO, a four-step\narchitectural optimization framework that refines the existing LLM architecture\nfor efficient PI by systematically removing nonlinearities such as LayerNorm\nand GELU and reducing FLOPs counts. For the first time, we propose a\nSoftmax-only architecture with significantly fewer FLOPs tailored for efficient\nPI. Furthermore, we devise a novel entropy regularization technique to improve\nthe performance of Softmax-only models. AERO achieves up to 4.23$\\times$\ncommunication and 1.94$\\times$ latency reduction. We validate the effectiveness\nof AERO by benchmarking it against the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasiveness of proprietary language models has raised privacy concerns\nfor users' sensitive data, emphasizing the need for private inference (PI),\nwhere inference is performed directly on encrypted inputs. However, current PI\nmethods face prohibitively higher communication and latency overheads,\nprimarily due to nonlinear operations. In this paper, we present a\ncomprehensive analysis to understand the role of nonlinearities in\ntransformer-based decoder-only language models. We introduce AERO, a four-step\narchitectural optimization framework that refines the existing LLM architecture\nfor efficient PI by systematically removing nonlinearities such as LayerNorm\nand GELU and reducing FLOPs counts. For the first time, we propose a\nSoftmax-only architecture with significantly fewer FLOPs tailored for efficient\nPI. Furthermore, we devise a novel entropy regularization technique to improve\nthe performance of Softmax-only models. AERO achieves up to 4.23$\\times$\ncommunication and 1.94$\\times$ latency reduction. We validate the effectiveness\nof AERO by benchmarking it against the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Nandan Kumar Jha"
                    },
                    {
                        "name": "Brandon Reagen"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Reagen"
                },
                "author": "Brandon Reagen",
                "arxiv_comment": "40 pages, 21 figures, and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16886v2",
                "updated": "2024-12-02T21:35:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    35,
                    55,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-07T22:15:15Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    22,
                    15,
                    15,
                    2,
                    38,
                    0
                ],
                "title": "Using text embedding models as text classifiers with medical data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using text embedding models as text classifiers with medical data"
                },
                "summary": "The advent of Large Language Models (LLMs) is promising and LLMs have been\napplied to numerous fields. However, it is not trivial to implement LLMs in the\nmedical field, due to the high standards for precision and accuracy. Currently,\nthe diagnosis of medical ailments must be done by hand, as it is costly to\nbuild a sufficiently broad LLM that can diagnose a wide range of diseases.\nHere, we explore the use of vector databases and embedding models as a means of\nencoding and classifying text with medical text data without the need to train\na new model altogether. We used various LLMs to generate the medical data, then\nencoded the data with a text embedding model and stored it in a vector\ndatabase. We hypothesized that higher embedding dimensions coupled with\ndescriptive data in the vector database would lead to better classifications\nand designed a robustness test to test our hypothesis. By using vector\ndatabases and text embedding models to classify a clinician's notes on a\npatient presenting with a certain ailment, we showed that these tools can be\nsuccessful at classifying medical text data. We found that a higher embedding\ndimension did indeed yield better results, however, querying with simple data\nin the database was optimal for performance. We have shown in this study the\napplicability of text embedding models and vector databases on a small scale,\nand our work lays the groundwork for applying these tools on a larger scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) is promising and LLMs have been\napplied to numerous fields. However, it is not trivial to implement LLMs in the\nmedical field, due to the high standards for precision and accuracy. Currently,\nthe diagnosis of medical ailments must be done by hand, as it is costly to\nbuild a sufficiently broad LLM that can diagnose a wide range of diseases.\nHere, we explore the use of vector databases and embedding models as a means of\nencoding and classifying text with medical text data without the need to train\na new model altogether. We used various LLMs to generate the medical data, then\nencoded the data with a text embedding model and stored it in a vector\ndatabase. We hypothesized that higher embedding dimensions coupled with\ndescriptive data in the vector database would lead to better classifications\nand designed a robustness test to test our hypothesis. By using vector\ndatabases and text embedding models to classify a clinician's notes on a\npatient presenting with a certain ailment, we showed that these tools can be\nsuccessful at classifying medical text data. We found that a higher embedding\ndimension did indeed yield better results, however, querying with simple data\nin the database was optimal for performance. We have shown in this study the\napplicability of text embedding models and vector databases on a small scale,\nand our work lays the groundwork for applying these tools on a larger scale."
                },
                "authors": [
                    {
                        "name": "Rishabh Goel"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Goel"
                },
                "author": "Rishabh Goel",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01978v1",
                "updated": "2024-12-02T21:14:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    14,
                    55,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T21:14:55Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    14,
                    55,
                    0,
                    337,
                    0
                ],
                "title": "Human-centred test and evaluation of military AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-centred test and evaluation of military AI"
                },
                "summary": "The REAIM 2024 Blueprint for Action states that AI applications in the\nmilitary domain should be ethical and human-centric and that humans must remain\nresponsible and accountable for their use and effects. Developing rigorous test\nand evaluation, verification and validation (TEVV) frameworks will contribute\nto robust oversight mechanisms. TEVV in the development and deployment of AI\nsystems needs to involve human users throughout the lifecycle. Traditional\nhuman-centred test and evaluation methods from human factors need to be adapted\nfor deployed AI systems that require ongoing monitoring and evaluation. The\nlanguage around AI-enabled systems should be shifted to inclusion of the\nhuman(s) as a component of the system. Standards and requirements supporting\nthis adjusted definition are needed, as are metrics and means to evaluate them.\nThe need for dialogue between technologists and policymakers on human-centred\nTEVV will be evergreen, but dialogue needs to be initiated with an objective in\nmind for it to be productive. Development of TEVV throughout system lifecycle\nis critical to support this evolution including the issue of human scalability\nand impact on scale of achievable testing. Communication between technical and\nnon technical communities must be improved to ensure operators and\npolicy-makers understand risk assumed by system use and to better inform\nresearch and development. Test and evaluation in support of responsible AI\ndeployment must include the effect of the human to reflect operationally\nrealised system performance. Means of communicating the results of TEVV to\nthose using and making decisions regarding the use of AI based systems will be\nkey in informing risk based decisions regarding use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The REAIM 2024 Blueprint for Action states that AI applications in the\nmilitary domain should be ethical and human-centric and that humans must remain\nresponsible and accountable for their use and effects. Developing rigorous test\nand evaluation, verification and validation (TEVV) frameworks will contribute\nto robust oversight mechanisms. TEVV in the development and deployment of AI\nsystems needs to involve human users throughout the lifecycle. Traditional\nhuman-centred test and evaluation methods from human factors need to be adapted\nfor deployed AI systems that require ongoing monitoring and evaluation. The\nlanguage around AI-enabled systems should be shifted to inclusion of the\nhuman(s) as a component of the system. Standards and requirements supporting\nthis adjusted definition are needed, as are metrics and means to evaluate them.\nThe need for dialogue between technologists and policymakers on human-centred\nTEVV will be evergreen, but dialogue needs to be initiated with an objective in\nmind for it to be productive. Development of TEVV throughout system lifecycle\nis critical to support this evolution including the issue of human scalability\nand impact on scale of achievable testing. Communication between technical and\nnon technical communities must be improved to ensure operators and\npolicy-makers understand risk assumed by system use and to better inform\nresearch and development. Test and evaluation in support of responsible AI\ndeployment must include the effect of the human to reflect operationally\nrealised system performance. Means of communicating the results of TEVV to\nthose using and making decisions regarding the use of AI based systems will be\nkey in informing risk based decisions regarding use."
                },
                "authors": [
                    {
                        "name": "David Helmer"
                    },
                    {
                        "name": "Michael Boardman"
                    },
                    {
                        "name": "S. Kate Conroy"
                    },
                    {
                        "name": "Adam J. Hepworth"
                    },
                    {
                        "name": "Manoj Harjani"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Harjani"
                },
                "author": "Manoj Harjani",
                "arxiv_comment": "11 pages, summary report from 'Human-centred test and evaluation of\n  military AI' panel at Responsible AI in the Military Domain 2024, Seoul\n  Korea, 9-10 September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18915v2",
                "updated": "2024-12-02T21:08:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    8,
                    0,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-28T05:12:17Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    5,
                    12,
                    17,
                    3,
                    333,
                    0
                ],
                "title": "MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for\n  Tabular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for\n  Tabular Applications"
                },
                "summary": "Mathematical reasoning capabilities are increasing with tool-augmented\nlanguage agents, but methods often rely either on closed-source or large\nmodels, external data, or extensive prompt engineering. This work introduces\nMATATA, a novel cost-effective method to train LLM agents for tabular data\nproblems through reasoning, planning, and tool use. With a progressive\nself-improvement paradigm and an iterative weak supervision, it empowers\n3.8B/8B Small Language Models (SLMs), particularly suited for local hosting and\nsensitive business contexts where data privacy is crucial. By employing a\nflexible and reusable tools across different datasets, it achieves robust\nperformance with effective scalability across shared tasks. Experiments show\nthat MATATA reaches state-of-the-art performances on FinQA and TAT-QA among\nreasoning frameworks based on open-source models. Moreover, MATATA models\ncompete with GPT-4 based frameworks on TabMWP, while being SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning capabilities are increasing with tool-augmented\nlanguage agents, but methods often rely either on closed-source or large\nmodels, external data, or extensive prompt engineering. This work introduces\nMATATA, a novel cost-effective method to train LLM agents for tabular data\nproblems through reasoning, planning, and tool use. With a progressive\nself-improvement paradigm and an iterative weak supervision, it empowers\n3.8B/8B Small Language Models (SLMs), particularly suited for local hosting and\nsensitive business contexts where data privacy is crucial. By employing a\nflexible and reusable tools across different datasets, it achieves robust\nperformance with effective scalability across shared tasks. Experiments show\nthat MATATA reaches state-of-the-art performances on FinQA and TAT-QA among\nreasoning frameworks based on open-source models. Moreover, MATATA models\ncompete with GPT-4 based frameworks on TabMWP, while being SLMs."
                },
                "authors": [
                    {
                        "name": "Vishnou Vinayagame"
                    },
                    {
                        "name": "Gregory Senay"
                    },
                    {
                        "name": "Luis Martí"
                    }
                ],
                "author_detail": {
                    "name": "Luis Martí"
                },
                "author": "Luis Martí",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02741v2",
                "updated": "2024-12-02T21:06:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    21,
                    6,
                    29,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-03T17:54:56Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    56,
                    3,
                    277,
                    0
                ],
                "title": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization"
                },
                "summary": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems. We release our code\nat \\url{https://github.com/amazon-science/SigExt}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems. We release our code\nat \\url{https://github.com/amazon-science/SigExt}"
                },
                "authors": [
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Mohammed Asad Karim"
                    },
                    {
                        "name": "Saket Dingliwal"
                    },
                    {
                        "name": "Aparna Elangovan"
                    }
                ],
                "author_detail": {
                    "name": "Aparna Elangovan"
                },
                "author": "Aparna Elangovan",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. Code available at\n  https://github.com/amazon-science/SigExt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13446v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13446v3",
                "updated": "2024-12-02T20:55:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    55,
                    15,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-21T00:44:04Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    0,
                    44,
                    4,
                    2,
                    52,
                    0
                ],
                "title": "Large Language Models for Data Annotation and Synthesis: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Data Annotation and Synthesis: A Survey"
                },
                "summary": "Data annotation and synthesis generally refers to the labeling or generating\nof raw data with relevant information, which could be used for improving the\nefficacy of machine learning models. The process, however, is labor-intensive\nand costly. The emergence of advanced Large Language Models (LLMs), exemplified\nby GPT-4, presents an unprecedented opportunity to automate the complicated\nprocess of data annotation and synthesis. While existing surveys have\nextensively covered LLM architecture, training, and general applications, we\nuniquely focus on their specific utility for data annotation. This survey\ncontributes to three core aspects: LLM-Based Annotation Generation,\nLLM-Generated Annotations Assessment, and LLM-Generated Annotations\nUtilization. Furthermore, this survey includes an in-depth taxonomy of data\ntypes that LLMs can annotate, a comprehensive review of learning strategies for\nmodels utilizing LLM-generated annotations, and a detailed discussion of the\nprimary challenges and limitations associated with using LLMs for data\nannotation and synthesis. Serving as a key guide, this survey aims to assist\nresearchers and practitioners in exploring the potential of the latest LLMs for\ndata annotation, thereby fostering future advancements in this critical field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data annotation and synthesis generally refers to the labeling or generating\nof raw data with relevant information, which could be used for improving the\nefficacy of machine learning models. The process, however, is labor-intensive\nand costly. The emergence of advanced Large Language Models (LLMs), exemplified\nby GPT-4, presents an unprecedented opportunity to automate the complicated\nprocess of data annotation and synthesis. While existing surveys have\nextensively covered LLM architecture, training, and general applications, we\nuniquely focus on their specific utility for data annotation. This survey\ncontributes to three core aspects: LLM-Based Annotation Generation,\nLLM-Generated Annotations Assessment, and LLM-Generated Annotations\nUtilization. Furthermore, this survey includes an in-depth taxonomy of data\ntypes that LLMs can annotate, a comprehensive review of learning strategies for\nmodels utilizing LLM-generated annotations, and a detailed discussion of the\nprimary challenges and limitations associated with using LLMs for data\nannotation and synthesis. Serving as a key guide, this survey aims to assist\nresearchers and practitioners in exploring the potential of the latest LLMs for\ndata annotation, thereby fostering future advancements in this critical field."
                },
                "authors": [
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Mansooreh Karami"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Accepted to EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13446v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13446v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01957v1",
                "updated": "2024-12-02T20:36:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    36,
                    41,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:36:41Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    36,
                    41,
                    0,
                    337,
                    0
                ],
                "title": "Usage Governance Advisor: from Intent to AI Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usage Governance Advisor: from Intent to AI Governance"
                },
                "summary": "Evaluating the safety of AI Systems is a pressing concern for organizations\ndeploying them. In addition to the societal damage done by the lack of fairness\nof those systems, deployers are concerned about the legal repercussions and the\nreputational damage incurred by the use of models that are unsafe. Safety\ncovers both what a model does; e.g., can it be used to reveal personal\ninformation from its training set, and how a model was built; e.g., was it only\ntrained on licensed data sets. Determining the safety of an AI system requires\ngathering information from a wide set of heterogeneous sources including safety\nbenchmarks and technical documentation for the set of models used in that\nsystem. In addition, responsible use is encouraged through mechanisms that\nadvise and help the user to take mitigating actions where safety risks are\ndetected. We present Usage Governance Advisor which creates semi-structured\ngovernance information, identifies and prioritizes risks according to the\nintended use case, recommends appropriate benchmarks and risk assessments and\nimportantly proposes mitigation strategies and actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety of AI Systems is a pressing concern for organizations\ndeploying them. In addition to the societal damage done by the lack of fairness\nof those systems, deployers are concerned about the legal repercussions and the\nreputational damage incurred by the use of models that are unsafe. Safety\ncovers both what a model does; e.g., can it be used to reveal personal\ninformation from its training set, and how a model was built; e.g., was it only\ntrained on licensed data sets. Determining the safety of an AI system requires\ngathering information from a wide set of heterogeneous sources including safety\nbenchmarks and technical documentation for the set of models used in that\nsystem. In addition, responsible use is encouraged through mechanisms that\nadvise and help the user to take mitigating actions where safety risks are\ndetected. We present Usage Governance Advisor which creates semi-structured\ngovernance information, identifies and prioritizes risks according to the\nintended use case, recommends appropriate benchmarks and risk assessments and\nimportantly proposes mitigation strategies and actions."
                },
                "authors": [
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Sean Rooney"
                    },
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Luis Garces-Erice"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    },
                    {
                        "name": "Frank Bagehorn"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Christopher Giblin"
                    },
                    {
                        "name": "Mira L. Wolf-Bauwens"
                    },
                    {
                        "name": "Ioana Giurgiu"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Peter Urbanetz"
                    }
                ],
                "author_detail": {
                    "name": "Peter Urbanetz"
                },
                "author": "Peter Urbanetz",
                "arxiv_comment": "9 pages, 8 figures, AAAI workshop submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]