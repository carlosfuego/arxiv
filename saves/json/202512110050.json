[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.17238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17238v2",
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "updated": "2025-12-09T17:34:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.08829v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08829v1",
                "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"
                },
                "updated": "2025-12-09T17:18:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08829v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "16 pages, 8 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Shaoyu Chen"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.01646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01646v2",
                "title": "StarDist: A Code Generator for Distributed Graph Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarDist: A Code Generator for Distributed Graph Algorithms"
                },
                "updated": "2025-12-09T15:15:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    15,
                    19,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    18,
                    32,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Barenya Kumar Nandy"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2512.08626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08626v1",
                "title": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR"
                },
                "updated": "2025-12-09T14:10:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:10:41Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Yuqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Zhou"
                },
                "author": "Yuqi Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.08571v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08571v1",
                "title": "Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization"
                },
                "updated": "2025-12-09T13:09:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    9,
                    22,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08571v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties."
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:09:22Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    9,
                    22,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph"
                },
                "authors": [
                    {
                        "name": "Phani Motamarri"
                    },
                    {
                        "name": "Gourab Panigrahi"
                    }
                ],
                "author_detail": {
                    "name": "Gourab Panigrahi"
                },
                "author": "Gourab Panigrahi"
            },
            {
                "id": "http://arxiv.org/abs/2512.07155v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07155v2",
                "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics"
                },
                "updated": "2025-12-09T07:35:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    35,
                    43,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07155v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T04:39:12Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    4,
                    39,
                    12,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dahyeon Kye"
                    },
                    {
                        "name": "Jeahun Sung"
                    },
                    {
                        "name": "Mingyu Jeon"
                    },
                    {
                        "name": "Jihyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jihyong Oh"
                },
                "author": "Jihyong Oh"
            },
            {
                "id": "http://arxiv.org/abs/2512.07993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07993v1",
                "title": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models"
                },
                "updated": "2025-12-08T19:32:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    32,
                    6,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T19:32:06Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    32,
                    6,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiayi Tian"
                    },
                    {
                        "name": "Seyedarmin Azizi"
                    },
                    {
                        "name": "Yequan Zhao"
                    },
                    {
                        "name": "Erfan Baghaei Potraghloo"
                    },
                    {
                        "name": "Sean McPherson"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Massoud Pedram"
                    },
                    {
                        "name": "Souvik Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Kundu"
                },
                "author": "Souvik Kundu"
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08143v2",
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores"
                },
                "updated": "2025-12-08T19:02:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    2,
                    12,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08143v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme"
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16653v2",
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference"
                },
                "updated": "2025-12-08T13:48:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    13,
                    48,
                    55,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16653v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v2",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-08T11:44:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    11,
                    44,
                    59,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,23 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.07312v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07312v1",
                "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management"
                },
                "updated": "2025-12-08T08:56:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    8,
                    56,
                    10,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07312v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T08:56:10Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    8,
                    56,
                    10,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Zhongchun Zhou"
                    },
                    {
                        "name": "Chengtao Lai"
                    },
                    {
                        "name": "Yuhang Gu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.07173v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07173v1",
                "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration"
                },
                "updated": "2025-12-08T05:15:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    5,
                    15,
                    41,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07173v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T05:15:41Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    5,
                    15,
                    41,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "8 pages, 3 figures. Preprint under review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Gaurav Sarkar"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Souvik Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Kundu"
                },
                "author": "Souvik Kundu"
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.09442v3",
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"
                },
                "updated": "2025-12-08T02:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    2,
                    23,
                    36,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.09442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.09442v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "arxiv_comment": "This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin"
            },
            {
                "id": "http://arxiv.org/abs/2512.07090v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07090v1",
                "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging KV Similarity for Online Structured Pruning in LLMs"
                },
                "updated": "2025-12-08T01:56:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    1,
                    56,
                    27,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07090v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T01:56:27Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    1,
                    56,
                    27,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jungmin Lee"
                    },
                    {
                        "name": "Gwangeun Byeon"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Seokin Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seokin Hong"
                },
                "author": "Seokin Hong"
            },
            {
                "id": "http://arxiv.org/abs/2512.02924v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02924v2",
                "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference"
                },
                "updated": "2025-12-08T00:15:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    0,
                    15,
                    33,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02924v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T16:45:25Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    45,
                    25,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Liangmin Wu"
                    },
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Zhiyuan Cheng"
                    },
                    {
                        "name": "Yicheng Qian"
                    },
                    {
                        "name": "Lingyue Zhu"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Luoyi Liang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Han Yang"
                    }
                ],
                "author_detail": {
                    "name": "Han Yang"
                },
                "author": "Han Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01266v2",
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion Controls"
                },
                "updated": "2025-12-08T00:05:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    0,
                    5,
                    23,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01266v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.06865v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06865v1",
                "title": "Spatial Retrieval Augmented Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Retrieval Augmented Autonomous Driving"
                },
                "updated": "2025-12-07T14:40:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    40,
                    49,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06865v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T14:40:49Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    40,
                    49,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaosong Jia"
                    },
                    {
                        "name": "Chenhe Zhang"
                    },
                    {
                        "name": "Yule Jiang"
                    },
                    {
                        "name": "Songbur Wong"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10963v2",
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "updated": "2025-12-07T14:21:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    21,
                    14,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10963v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09261v2",
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling"
                },
                "updated": "2025-12-07T12:44:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    12,
                    44,
                    2,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09261v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\\mathcal{O}(n^4)$ to $\\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\\times$ memory reduction and a $1.57\\times$ speedup on Infinity-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\\mathcal{O}(n^4)$ to $\\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\\times$ memory reduction and a $1.57\\times$ speedup on Infinity-8B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.06727v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06727v1",
                "title": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models"
                },
                "updated": "2025-12-07T08:40:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    8,
                    40,
                    52,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06727v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T08:40:52Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    8,
                    40,
                    52,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sourjya Roy"
                    },
                    {
                        "name": "Shrihari Sridharan"
                    },
                    {
                        "name": "Surya Selvam"
                    },
                    {
                        "name": "Anand Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Anand Raghunathan"
                },
                "author": "Anand Raghunathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.06664v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06664v1",
                "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving"
                },
                "updated": "2025-12-07T05:28:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    5,
                    28,
                    40,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06664v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T05:28:40Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    5,
                    28,
                    40,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "9 pages",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Wei-Bin Kou"
                    },
                    {
                        "name": "Guangxu Zhu"
                    },
                    {
                        "name": "Jingreng Lei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    },
                    {
                        "name": "Jianping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wang"
                },
                "author": "Jianping Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.06523v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06523v1",
                "title": "Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm"
                },
                "updated": "2025-12-06T18:21:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    18,
                    21,
                    21,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06523v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T18:21:21Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    18,
                    21,
                    21,
                    5,
                    340,
                    0
                ],
                "arxiv_comment": "59 pages and 11 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Daniel Goldsmith"
                    },
                    {
                        "name": "Xing Liang"
                    },
                    {
                        "name": "Dimitrios Makris"
                    },
                    {
                        "name": "Hongwei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wu"
                },
                "author": "Hongwei Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.06468v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06468v1",
                "title": "Convolution operators preserving the set of totally positive sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolution operators preserving the set of totally positive sequences"
                },
                "updated": "2025-12-06T15:13:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    15,
                    13,
                    2,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06468v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A real sequence $(a_k)_{k=0}^\\infty$ is called {\\it totally positive} if all minors of the infinite Toeplitz matrix $ \\left\\| a_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $a_k=0$ for $k<0$). In this paper, which continues our earlier work \\cite{kv}, we investigate the set of real sequences $(b_k)_{k=0}^\\infty$ with the property that for every totally positive sequence $(a_k)_{k=0}^\\infty,$ the sequense of termwise products $(a_k b_k)_{k=0}^\\infty$ is also totally positive. In particular, we show that for every totally positive sequence $(a_k)_{k=0}^\\infty$ the sequence $\\left(a_k a^{-k (k-1)}\\right)_{k=0}^\\infty$ is totally positive whenever $a^2\\geq 3{.}503.$ We also propose several open problems concerning convolution operators that preserve total positivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A real sequence $(a_k)_{k=0}^\\infty$ is called {\\it totally positive} if all minors of the infinite Toeplitz matrix $ \\left\\| a_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $a_k=0$ for $k<0$). In this paper, which continues our earlier work \\cite{kv}, we investigate the set of real sequences $(b_k)_{k=0}^\\infty$ with the property that for every totally positive sequence $(a_k)_{k=0}^\\infty,$ the sequense of termwise products $(a_k b_k)_{k=0}^\\infty$ is also totally positive. In particular, we show that for every totally positive sequence $(a_k)_{k=0}^\\infty$ the sequence $\\left(a_k a^{-k (k-1)}\\right)_{k=0}^\\infty$ is totally positive whenever $a^2\\geq 3{.}503.$ We also propose several open problems concerning convolution operators that preserve total positivity."
                },
                "tags": [
                    {
                        "term": "math.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T15:13:02Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    15,
                    13,
                    2,
                    5,
                    340,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.CV"
                },
                "authors": [
                    {
                        "name": "Olga Katkova"
                    },
                    {
                        "name": "Anna Vishnyakova"
                    }
                ],
                "author_detail": {
                    "name": "Anna Vishnyakova"
                },
                "author": "Anna Vishnyakova"
            },
            {
                "id": "http://arxiv.org/abs/2512.06443v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06443v1",
                "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices"
                },
                "updated": "2025-12-06T14:14:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    14,
                    14,
                    1,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06443v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T14:14:01Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    14,
                    14,
                    1,
                    5,
                    340,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Chengyu Yin"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu"
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.02493v2",
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization"
                },
                "updated": "2025-12-06T08:55:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    8,
                    55,
                    13,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.02493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.02493v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05916v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05916v1",
                "title": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity"
                },
                "updated": "2025-12-05T17:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05916v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Damien Lesens"
                    },
                    {
                        "name": "Beheshteh T. Rakhshan"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau"
            },
            {
                "id": "http://arxiv.org/abs/2512.01678v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01678v3",
                "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphling: Fast, Fused, and Flexible GNN Training at Scale"
                },
                "updated": "2025-12-05T16:07:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    7,
                    38,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01678v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01678v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:45:03Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    45,
                    3,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Anubhab"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2512.04677v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04677v2",
                "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
                },
                "updated": "2025-12-05T06:32:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    32,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04677v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T11:11:24Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    11,
                    11,
                    24,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Hailong Guo"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Steven Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoi"
                },
                "author": "Steven Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.09665v2",
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "updated": "2025-12-05T04:52:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    4,
                    52,
                    54,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.09665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.09665v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2511.21958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21958v2",
                "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"
                },
                "updated": "2025-12-05T02:13:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    2,
                    13,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T22:34:26Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    22,
                    34,
                    26,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "12 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Bintang Dwi Marthen"
                    },
                    {
                        "name": "Sarath Balivada"
                    },
                    {
                        "name": "Vamsi Sudhakar Bojji"
                    },
                    {
                        "name": "Eric Knauft"
                    },
                    {
                        "name": "Jitender Rohilla"
                    },
                    {
                        "name": "Jiaqi Zuo"
                    },
                    {
                        "name": "Quanxing Liu"
                    },
                    {
                        "name": "Maxime Austruy"
                    },
                    {
                        "name": "Wenguang Wang"
                    },
                    {
                        "name": "Juncheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Yang"
                },
                "author": "Juncheng Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.14748v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14748v2",
                "title": "Cloud-Native Vector Search: A Comprehensive Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Native Vector Search: A Comprehensive Performance Analysis"
                },
                "updated": "2025-12-05T00:04:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    0,
                    4,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14748v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T18:50:15Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    18,
                    50,
                    15,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Silu Huang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Yuanjin Lin"
                    },
                    {
                        "name": "Ke Wu"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Jianjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Chen"
                },
                "author": "Jianjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.05081v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05081v1",
                "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression"
                },
                "updated": "2025-12-04T18:46:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05081v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T18:46:44Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jung Yi"
                    },
                    {
                        "name": "Wooseok Jang"
                    },
                    {
                        "name": "Paul Hyunbin Cho"
                    },
                    {
                        "name": "Jisu Nam"
                    },
                    {
                        "name": "Heeji Yoon"
                    },
                    {
                        "name": "Seungryong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seungryong Kim"
                },
                "author": "Seungryong Kim"
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10875v2",
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "updated": "2025-12-04T17:57:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    17,
                    57,
                    10,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10875v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.04939v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04939v1",
                "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging"
                },
                "updated": "2025-12-04T16:07:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04939v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T16:07:02Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhijian Shu"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Ben Li"
                    },
                    {
                        "name": "Zhiyuan Pu"
                    },
                    {
                        "name": "Weize Li"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Xiaoyang Guo"
                    },
                    {
                        "name": "Xiao-Xiao Long"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Xiao Long"
                },
                "author": "Xiao-Xiao Long"
            },
            {
                "id": "http://arxiv.org/abs/2512.04857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04857v1",
                "title": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens"
                },
                "updated": "2025-12-04T14:41:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T14:41:21Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Chanfan Gan"
                    },
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.03397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03397v2",
                "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing"
                },
                "updated": "2025-12-04T12:53:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    12,
                    53,
                    34,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T03:07:08Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    3,
                    7,
                    8,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Seungwon Choi"
                    },
                    {
                        "name": "Dong-Gyu Park"
                    },
                    {
                        "name": "Seo-Yeon Hwang"
                    },
                    {
                        "name": "Tae-Wan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Wan Kim"
                },
                "author": "Tae-Wan Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.22421v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22421v2",
                "title": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing"
                },
                "updated": "2025-12-04T07:11:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    7,
                    11,
                    48,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22421v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T12:58:25Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    12,
                    58,
                    25,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Weijia Jia"
                    },
                    {
                        "name": "Wei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhao"
                },
                "author": "Wei Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.04515v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04515v1",
                "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLCD: Egocentric Video Generation with Long Context Diffusion"
                },
                "updated": "2025-12-04T06:53:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04515v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T06:53:01Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Liuzhou Zhang"
                    },
                    {
                        "name": "Jiarui Ye"
                    },
                    {
                        "name": "Yuanlei Wang"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Mingju Cao"
                    },
                    {
                        "name": "Wanke Xia"
                    },
                    {
                        "name": "Bowen Zeng"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v2",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-12-04T06:19:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    19,
                    39,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.04226v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04226v1",
                "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection"
                },
                "updated": "2025-12-03T19:46:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04226v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T19:46:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Ryan Swann"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Xiaohu Guo"
                    },
                    {
                        "name": "Bryant Nelson"
                    },
                    {
                        "name": "Lixun Zhang"
                    },
                    {
                        "name": "Alex Brown"
                    },
                    {
                        "name": "Yen Ong"
                    },
                    {
                        "name": "Ali Yazdani"
                    },
                    {
                        "name": "Sean Siddens"
                    },
                    {
                        "name": "Ganesh Dasika"
                    },
                    {
                        "name": "Alex Underwood"
                    }
                ],
                "author_detail": {
                    "name": "Alex Underwood"
                },
                "author": "Alex Underwood"
            },
            {
                "id": "http://arxiv.org/abs/2512.04040v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04040v1",
                "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELIC: Interactive Video World Model with Long-Horizon Memory"
                },
                "updated": "2025-12-03T18:29:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04040v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:29:20Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yicong Hong"
                    },
                    {
                        "name": "Yiqun Mei"
                    },
                    {
                        "name": "Chongjian Ge"
                    },
                    {
                        "name": "Yiran Xu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sai Bi"
                    },
                    {
                        "name": "Yannick Hold-Geoffroy"
                    },
                    {
                        "name": "Mike Roberts"
                    },
                    {
                        "name": "Matthew Fisher"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Hao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tan"
                },
                "author": "Hao Tan"
            },
            {
                "id": "http://arxiv.org/abs/2512.04033v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04033v1",
                "title": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension"
                },
                "updated": "2025-12-03T18:17:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04033v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 m-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85m showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 m-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85m showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:17:12Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Julian Gervassi-Saga"
                    },
                    {
                        "name": "Martha R. McCartney"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "David Malcolm McComas"
                    },
                    {
                        "name": "David J. Smith"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal"
            },
            {
                "id": "http://arxiv.org/abs/2512.04025v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04025v1",
                "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation"
                },
                "updated": "2025-12-03T18:02:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04025v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:02:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "Tech report",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Youping Gu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03972v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03972v1",
                "title": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis"
                },
                "updated": "2025-12-03T17:05:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03972v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T17:05:58Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Hassan Arafat"
                    },
                    {
                        "name": "David Bremner"
                    },
                    {
                        "name": "Kenneth B. Kent"
                    },
                    {
                        "name": "Julian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Julian Wang"
                },
                "author": "Julian Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03927v1",
                "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference"
                },
                "updated": "2025-12-03T16:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T16:27:16Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Liujianfu Wang"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Yuchen Pan"
                    },
                    {
                        "name": "Soung Chang Liew"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Kexin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kexin Chen"
                },
                "author": "Kexin Chen"
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.05235v2",
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving"
                },
                "updated": "2025-12-03T16:21:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    21,
                    24,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.05235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.05235v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris"
            },
            {
                "id": "http://arxiv.org/abs/2512.03870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03870v1",
                "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
                },
                "updated": "2025-12-03T15:22:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T15:22:00Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Xinmiao Zhang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yunlong Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.03608v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03608v1",
                "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing"
                },
                "updated": "2025-12-03T09:41:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03608v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T09:41:03Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lishuo Deng"
                    },
                    {
                        "name": "Shaojie Xu"
                    },
                    {
                        "name": "Jinwu Chen"
                    },
                    {
                        "name": "Changwei Yan"
                    },
                    {
                        "name": "Jiajie Wang"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Weiwei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Shan"
                },
                "author": "Weiwei Shan"
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.08351v2",
                "title": "Fletch: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fletch: File-System Metadata Caching in Programmable Switches"
                },
                "updated": "2025-12-03T09:23:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    23,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.08351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.08351v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "author": "Patrick P. C. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.02513v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02513v2",
                "title": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network"
                },
                "updated": "2025-12-03T08:13:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    8,
                    13,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02513v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T08:13:38Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    8,
                    13,
                    38,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "accepted at IEEE Globecom Workshop 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Carlo Fischione"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Fischione"
                },
                "author": "Carlo Fischione"
            },
            {
                "id": "http://arxiv.org/abs/2512.03324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03324v1",
                "title": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"
                },
                "updated": "2025-12-03T00:20:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T00:20:35Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Simran Lamba"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2512.03007v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03007v1",
                "title": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules"
                },
                "updated": "2025-12-02T18:32:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03007v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T18:32:11Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Shrestha Biswas"
                    },
                    {
                        "name": "Sebastian Eppelt"
                    },
                    {
                        "name": "Christian Buchberger"
                    },
                    {
                        "name": "Xing-Yan Chen"
                    },
                    {
                        "name": "Andreas Schindewolf"
                    },
                    {
                        "name": "Michael Hani"
                    },
                    {
                        "name": "Erwin Biebl"
                    },
                    {
                        "name": "Immanuel Bloch"
                    },
                    {
                        "name": "Xin-Yu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xin-Yu Luo"
                },
                "author": "Xin-Yu Luo"
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.04216v2",
                "title": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm"
                },
                "updated": "2025-12-02T07:05:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    7,
                    5,
                    10,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.04216v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1361-6595/ae259e",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "arxiv_journal_ref": "Plasma Sources Sci. Technol. (2025)",
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Liyang Zhang"
                    },
                    {
                        "name": "Xiaobing Zou"
                    },
                    {
                        "name": "Haiyun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Haiyun Luo"
                },
                "author": "Haiyun Luo",
                "arxiv_doi": "10.1088/1361-6595/ae259e"
            },
            {
                "id": "http://arxiv.org/abs/2512.02444v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02444v1",
                "title": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning"
                },
                "updated": "2025-12-02T06:05:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02444v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T06:05:48Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Sainyam Galhotra"
                    }
                ],
                "author_detail": {
                    "name": "Sainyam Galhotra"
                },
                "author": "Sainyam Galhotra"
            },
            {
                "id": "http://arxiv.org/abs/2512.02337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02337v1",
                "title": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification"
                },
                "updated": "2025-12-02T02:15:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T02:15:33Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Chaoyi Hu"
                    },
                    {
                        "name": "Junjie Peng"
                    },
                    {
                        "name": "Kun Xia"
                    }
                ],
                "author_detail": {
                    "name": "Kun Xia"
                },
                "author": "Kun Xia"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v2",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-12-02T01:24:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    1,
                    24,
                    46,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Zirui Neil Zhao"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.19602v3",
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation"
                },
                "updated": "2025-12-02T00:43:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    0,
                    43,
                    12,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.19602v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.19602v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "arxiv_comment": "23 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura"
            },
            {
                "id": "http://arxiv.org/abs/2512.02281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02281v1",
                "title": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving"
                },
                "updated": "2025-12-01T23:53:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T23:53:42Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian"
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05332v2",
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding"
                },
                "updated": "2025-12-01T22:47:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    22,
                    47,
                    17,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05332v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025, Project page: https://videomarathon.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum"
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19686v3",
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers"
                },
                "updated": "2025-12-01T21:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    21,
                    56,
                    32,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19686v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "Revised to around 9 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan"
            },
            {
                "id": "http://arxiv.org/abs/2512.02189v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02189v1",
                "title": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis"
                },
                "updated": "2025-12-01T20:31:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02189v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T20:31:10Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran"
            },
            {
                "id": "http://arxiv.org/abs/2512.01953v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01953v1",
                "title": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference"
                },
                "updated": "2025-12-01T18:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01953v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T18:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sai Gokhale"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Elliott Delaye"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Delaye"
                },
                "author": "Elliott Delaye"
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.04844v2",
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments"
                },
                "updated": "2025-12-01T17:42:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    42,
                    16,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.04844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.04844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-Garca"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander"
            },
            {
                "id": "http://arxiv.org/abs/2512.01915v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01915v1",
                "title": "A Low-Cost Reliable Racetrack Cache Based on Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-Cost Reliable Racetrack Cache Based on Data Compression"
                },
                "updated": "2025-12-01T17:32:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01915v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead."
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T17:32:25Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Fateme Shokouhinia"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v2",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-01T12:51:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    12,
                    51,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.01541v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01541v1",
                "title": "RoMe: Row Granularity Access Memory System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoMe: Row Granularity Access Memory System for Large Language Models"
                },
                "updated": "2025-12-01T11:14:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01541v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:14:31Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "15 pages, 14 figures, accepted at HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hwayong Nam"
                    },
                    {
                        "name": "Seungmin Baek"
                    },
                    {
                        "name": "Jumin Kim"
                    },
                    {
                        "name": "Michael Jaemin Kim"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.01540v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01540v1",
                "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention"
                },
                "updated": "2025-12-01T11:12:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01540v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:12:37Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zipeng Wang"
                    },
                    {
                        "name": "Dan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Xu"
                },
                "author": "Dan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01357v1",
                "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity"
                },
                "updated": "2025-12-01T07:10:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T07:10:34Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wenbin Zhu"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Zili Shao"
                    },
                    {
                        "name": "Hongjun Dai"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "arxiv_affiliation": "Indiana University Bloomington",
                "author": "Feng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.14712v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14712v2",
                "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation"
                },
                "updated": "2025-12-01T06:11:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    6,
                    11,
                    56,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14712v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T17:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    56,
                    4,
                    1,
                    322,
                    0
                ],
                "arxiv_comment": "23 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunfeng Wu"
                    },
                    {
                        "name": "Jiayi Song"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Zihao He"
                    },
                    {
                        "name": "Songhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Songhua Liu"
                },
                "author": "Songhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01278v1",
                "title": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding"
                },
                "updated": "2025-12-01T04:50:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T04:50:55Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2304.10805v3",
                "title": "EPLKG: Efficient Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPLKG: Efficient Prompt Learning with Knowledge Graph"
                },
                "updated": "2025-11-30T14:24:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    24,
                    30,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2304.10805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2304.10805v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.00903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00903v1",
                "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead"
                },
                "updated": "2025-11-30T14:10:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T14:10:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaojun Ni"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Zheng Zhu"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Tianrun Chen"
                    },
                    {
                        "name": "Guosheng Zhao"
                    },
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Zhehao Dong"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Yun Ye"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Guan Huang"
                    },
                    {
                        "name": "Wenjun Mei"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Mei"
                },
                "author": "Wenjun Mei"
            },
            {
                "id": "http://arxiv.org/abs/2512.00891v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00891v1",
                "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression"
                },
                "updated": "2025-11-30T13:44:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00891v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T13:44:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Code is avaliable at \\url{https://github.com/lern-to-write/STC}",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Xinying Lin"
                    },
                    {
                        "name": "Boxue Yang"
                    },
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Tailai Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.00722v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00722v1",
                "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs"
                },
                "updated": "2025-11-30T04:32:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00722v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:32:43Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Hanzhen Wang"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Jiancai Ye"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai"
            },
            {
                "id": "http://arxiv.org/abs/2512.00719v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00719v1",
                "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving"
                },
                "updated": "2025-11-30T04:15:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00719v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:15:34Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zane Cao"
                    },
                    {
                        "name": "Yongchao He"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao He"
                },
                "author": "Yongchao He"
            },
            {
                "id": "http://arxiv.org/abs/2512.00635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00635v1",
                "title": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA"
                },
                "updated": "2025-11-29T21:12:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T21:12:22Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "This extended abstract is archived for educational purposes as an example for different PhD forum competitions. Total page is 3",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Archisman Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Archisman Ghosh"
                },
                "author": "Archisman Ghosh"
            },
            {
                "id": "http://arxiv.org/abs/2512.00504v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00504v1",
                "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-KV: Decoding-Time KV Cache Eviction with Global Attention"
                },
                "updated": "2025-11-29T14:21:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00504v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T14:21:33Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mengqi Liao"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Zekai Shen"
                    },
                    {
                        "name": "Xiaowei Mao"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Huaiyu Wan"
                    }
                ],
                "author_detail": {
                    "name": "Huaiyu Wan"
                },
                "author": "Huaiyu Wan"
            },
            {
                "id": "http://arxiv.org/abs/2512.00300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00300v1",
                "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion"
                },
                "updated": "2025-11-29T03:47:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T03:47:14Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "14 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Haozhi Cao"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Tianxin Hu"
                    },
                    {
                        "name": "Weixiang Guo"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Lihua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Xie"
                },
                "author": "Lihua Xie"
            },
            {
                "id": "http://arxiv.org/abs/2512.05134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05134v1",
                "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models"
                },
                "updated": "2025-11-29T02:34:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T02:34:23Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "8 pages main, 8 pages appendix, 16 figures, 5 tables. Code: https://github.com/zihaowu25/InvarDiff",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Wu"
                },
                "author": "Zihao Wu"
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10104v2",
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "updated": "2025-11-28T21:55:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    21,
                    55,
                    41,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10104v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense"
            },
            {
                "id": "http://arxiv.org/abs/2511.23070v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23070v1",
                "title": "Buffer replay enhances the robustness of multimodal learning under missing-modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Buffer replay enhances the robustness of multimodal learning under missing-modality"
                },
                "updated": "2025-11-28T10:55:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23070v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T10:55:31Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongye Zhu"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Yanwen Ba"
                    },
                    {
                        "name": "Jingye Xue"
                    },
                    {
                        "name": "Shigeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shigeng Zhang"
                },
                "author": "Shigeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.23011v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23011v1",
                "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation"
                },
                "updated": "2025-11-28T09:22:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23011v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T09:22:37Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Accepted by HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Sunfeng Gao"
                    },
                    {
                        "name": "Yibo Tang"
                    },
                    {
                        "name": "Junhui Luo"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Dezun Dong"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Mingche Lai"
                    }
                ],
                "author_detail": {
                    "name": "Mingche Lai"
                },
                "author": "Mingche Lai"
            },
            {
                "id": "http://arxiv.org/abs/2511.22973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22973v1",
                "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation"
                },
                "updated": "2025-11-28T08:25:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T08:25:59Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Shuning Chang"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22889v1",
                "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference"
                },
                "updated": "2025-11-28T05:36:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:36:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Fang Li"
                },
                "author": "Fang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.22880v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22880v1",
                "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"
                },
                "updated": "2025-11-28T05:04:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22880v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:04:02Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shashwat Jaiswal"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Alind Khare"
                    },
                    {
                        "name": "Chloi Alverti"
                    },
                    {
                        "name": "Renee St Amant"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas"
            },
            {
                "id": "http://arxiv.org/abs/2511.22857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22857v1",
                "title": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera"
                },
                "updated": "2025-11-28T03:24:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T03:24:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Peihan Tu"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta"
            },
            {
                "id": "http://arxiv.org/abs/2511.22681v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22681v1",
                "title": "CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights"
                },
                "updated": "2025-11-27T18:30:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    18,
                    30,
                    19,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22681v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T18:30:19Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    18,
                    30,
                    19,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mohaiminul Al Nahian"
                    },
                    {
                        "name": "Abeer Matar A. Almalky"
                    },
                    {
                        "name": "Gamana Aragonda"
                    },
                    {
                        "name": "Ranyang Zhou"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    },
                    {
                        "name": "Dmitry Ponomarev"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Shaahin Angizi"
                    },
                    {
                        "name": "Adnan Siraj Rakin"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Siraj Rakin"
                },
                "arxiv_affiliation": "SUNY Binghamton",
                "author": "Adnan Siraj Rakin"
            },
            {
                "id": "http://arxiv.org/abs/2511.22551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22551v1",
                "title": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison"
                },
                "updated": "2025-11-27T15:39:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    39,
                    45,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T15:39:45Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    39,
                    45,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asad"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asad"
                },
                "author": "Hossein Asad"
            },
            {
                "id": "http://arxiv.org/abs/2511.22533v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22533v1",
                "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"
                },
                "updated": "2025-11-27T15:13:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    13,
                    32,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22533v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%)."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T15:13:32Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    15,
                    13,
                    32,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mengyu Yang"
                    },
                    {
                        "name": "Yanming Yang"
                    },
                    {
                        "name": "Chenyi Xu"
                    },
                    {
                        "name": "Chenxi Song"
                    },
                    {
                        "name": "Yufan Zuo"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Ruibo Li"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22483v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22483v1",
                "title": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges"
                },
                "updated": "2025-11-27T14:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    17,
                    43,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22483v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T14:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    17,
                    43,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "ASP-DAC 2026 Special Session",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiqiang Que"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan"
            },
            {
                "id": "http://arxiv.org/abs/2511.22481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22481v1",
                "title": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency"
                },
                "updated": "2025-11-27T14:13:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    13,
                    47,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer)."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T14:13:47Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    14,
                    13,
                    47,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Project page: [this https URL](https://gitee.com/omniai/omniinfer)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yunxiang Yao"
                    },
                    {
                        "name": "Wenwei Kuang"
                    },
                    {
                        "name": "Runze Mao"
                    },
                    {
                        "name": "Zhenhao Sun"
                    },
                    {
                        "name": "Zhuang Tao"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Dengyu Li"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Zhili Wang"
                    },
                    {
                        "name": "Kai Cui"
                    },
                    {
                        "name": "Congzhi Cai"
                    },
                    {
                        "name": "Longwen Lan"
                    },
                    {
                        "name": "Ken Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ken Zhang"
                },
                "author": "Ken Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.00112v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00112v1",
                "title": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache"
                },
                "updated": "2025-11-27T13:55:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    55,
                    50,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00112v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.\n  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T13:55:50Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    55,
                    50,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh"
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18546v2",
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval"
                },
                "updated": "2025-11-27T13:54:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    13,
                    54,
                    42,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18546v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.22333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22333v1",
                "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"
                },
                "updated": "2025-11-27T11:10:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:10:30Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS'26",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jinjun Yi"
                    },
                    {
                        "name": "Zhixin Zhao"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Laiping Zhao"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Wenxin Li"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16607v3",
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search"
                },
                "updated": "2025-11-27T09:37:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    9,
                    37,
                    42,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16607v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Zhao Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Jin"
                },
                "author": "Zhao Jin"
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09936v2",
                "title": "KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference"
                },
                "updated": "2025-11-27T07:44:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    7,
                    44,
                    35,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09936v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22118v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22118v1",
                "title": "Statistical Independence Aware Caching for LLM Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Independence Aware Caching for LLM Workflows"
                },
                "updated": "2025-11-27T05:16:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    27,
                    5,
                    16,
                    28,
                    3,
                    331,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22118v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T05:16:28Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    5,
                    16,
                    28,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yihan Dai"
                    },
                    {
                        "name": "Dimitrios Stamatios Bouras"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev"
            },
            {
                "id": "http://arxiv.org/abs/2511.21612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21612v1",
                "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases"
                },
                "updated": "2025-11-26T17:36:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    17,
                    36,
                    15,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T17:36:15Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    17,
                    36,
                    15,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shahir Abdullah"
                    },
                    {
                        "name": "Syed Rohit Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Syed Rohit Zaman"
                },
                "author": "Syed Rohit Zaman"
            },
            {
                "id": "http://arxiv.org/abs/2511.21535v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21535v1",
                "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation"
                },
                "updated": "2025-11-26T16:01:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    1,
                    32,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21535v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T16:01:32Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    16,
                    1,
                    32,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Morteza Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Sadeghi"
                },
                "author": "Morteza Sadeghi"
            },
            {
                "id": "http://arxiv.org/abs/2511.21408v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21408v1",
                "title": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Depth and Timescale Transformers: Learning Where and When to Compute"
                },
                "updated": "2025-11-26T14:00:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    26,
                    14,
                    0,
                    18,
                    2,
                    330,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21408v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T14:00:18Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    14,
                    0,
                    18,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Frederico Wieser"
                    },
                    {
                        "name": "Martin Benfeghoul"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    }
                ],
                "author_detail": {
                    "name": "Zafeirios Fountas"
                },
                "author": "Zafeirios Fountas"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.08932v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08932v1",
                "title": "Hot Jupiters are Inflated Primarily by Shallow Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hot Jupiters are Inflated Primarily by Shallow Heating"
                },
                "updated": "2025-12-09T18:59:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    59,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08932v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The unexpectedly large radii of transiting hot Jupiters have led to many proposals for the physical mechanisms responsible for heating their interiors. While it has been shown that hot Jupiters reinflate as their host stars brighten due to heating deep in planetary interiors, young hot Jupiters also exhibit signs of delayed cooling possibly related to heating closer to their surfaces. To investigate this tension, we enhance our previously published hot Jupiter thermal evolution model by adding a parameter that allows for both deep heating and delayed cooling. We fit our thermal evolution models to a homogeneous, physically self-consistent catalog of accurate and precise hot Jupiter system properties in a hierarchical Bayesian framework. We find that hot Jupiters' interior cooling rates are reduced on average by 95\\%--98\\% compared to simpler anomalous heating models. The most plausible explanation for this inference is substantial shallow heating just below their radiative--convective boundaries that enables reinflation with much less deep heating. Shallow heating by Ohmic dissipation and/or temperature advection are therefore important components of accurate models of hot Jupiter atmospheres, especially in circulation models. If hot Jupiters are inflated primarily by shallow heating as we propose, then we predict that their observed phase curve offsets should increase with temperature in the range $T_{\\text{eq}}~\\lesssim1500~\\text{K}$, peak in the range $1500~\\text{K}~\\lesssim~T_{\\text{eq}}~\\lesssim~1800~\\text{K}$, and decrease in the range $T_{\\text{eq}}~\\gtrsim~1800~\\text{K}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unexpectedly large radii of transiting hot Jupiters have led to many proposals for the physical mechanisms responsible for heating their interiors. While it has been shown that hot Jupiters reinflate as their host stars brighten due to heating deep in planetary interiors, young hot Jupiters also exhibit signs of delayed cooling possibly related to heating closer to their surfaces. To investigate this tension, we enhance our previously published hot Jupiter thermal evolution model by adding a parameter that allows for both deep heating and delayed cooling. We fit our thermal evolution models to a homogeneous, physically self-consistent catalog of accurate and precise hot Jupiter system properties in a hierarchical Bayesian framework. We find that hot Jupiters' interior cooling rates are reduced on average by 95\\%--98\\% compared to simpler anomalous heating models. The most plausible explanation for this inference is substantial shallow heating just below their radiative--convective boundaries that enables reinflation with much less deep heating. Shallow heating by Ohmic dissipation and/or temperature advection are therefore important components of accurate models of hot Jupiter atmospheres, especially in circulation models. If hot Jupiters are inflated primarily by shallow heating as we propose, then we predict that their observed phase curve offsets should increase with temperature in the range $T_{\\text{eq}}~\\lesssim1500~\\text{K}$, peak in the range $1500~\\text{K}~\\lesssim~T_{\\text{eq}}~\\lesssim~1800~\\text{K}$, and decrease in the range $T_{\\text{eq}}~\\gtrsim~1800~\\text{K}$."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:59:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    59,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "17 Pages, 4 figures, 2 tables. Submitted to AAS Journals - comments welcome!",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Stephen P. Schmidt"
                    },
                    {
                        "name": "Daniel P. Thorngren"
                    },
                    {
                        "name": "Kevin C. Schlaufman"
                    }
                ],
                "author_detail": {
                    "name": "Kevin C. Schlaufman"
                },
                "author": "Kevin C. Schlaufman"
            },
            {
                "id": "http://arxiv.org/abs/2512.08924v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08924v1",
                "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time"
                },
                "updated": "2025-12-09T18:57:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    57,
                    21,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08924v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:57:21Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    57,
                    21,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Project Page: https://d4rt-paper.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Guillaume Le Moing"
                    },
                    {
                        "name": "Skanda Koppula"
                    },
                    {
                        "name": "Ignacio Rocco"
                    },
                    {
                        "name": "Liliane Momeni"
                    },
                    {
                        "name": "Junyu Xie"
                    },
                    {
                        "name": "Shuyang Sun"
                    },
                    {
                        "name": "Rahul Sukthankar"
                    },
                    {
                        "name": "Jolle K Barral"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Andrew Zisserman"
                    },
                    {
                        "name": "Junlin Zhang"
                    },
                    {
                        "name": "Mehdi SM Sajjadi"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi SM Sajjadi"
                },
                "author": "Mehdi SM Sajjadi"
            },
            {
                "id": "http://arxiv.org/abs/2512.08920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08920v1",
                "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer"
                },
                "updated": "2025-12-09T18:56:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    56,
                    30,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:56:30Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    56,
                    30,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Project website: https://jessicayin.github.io/osmo_tactile_glove/",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Jessica Yin"
                    },
                    {
                        "name": "Haozhi Qi"
                    },
                    {
                        "name": "Youngsun Wi"
                    },
                    {
                        "name": "Sayantan Kundu"
                    },
                    {
                        "name": "Mike Lambeta"
                    },
                    {
                        "name": "William Yang"
                    },
                    {
                        "name": "Changhao Wang"
                    },
                    {
                        "name": "Tingfan Wu"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Tess Hellebrekers"
                    }
                ],
                "author_detail": {
                    "name": "Tess Hellebrekers"
                },
                "author": "Tess Hellebrekers"
            },
            {
                "id": "http://arxiv.org/abs/2512.08918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08918v1",
                "title": "Improved Pseudorandom Codes from Permuted Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Pseudorandom Codes from Permuted Puzzles"
                },
                "updated": "2025-12-09T18:53:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    53,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.\n  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.\n  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.\n  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.\n  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:53:42Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    53,
                    42,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Miranda Christ"
                    },
                    {
                        "name": "Noah Golowich"
                    },
                    {
                        "name": "Sam Gunn"
                    },
                    {
                        "name": "Ankur Moitra"
                    },
                    {
                        "name": "Daniel Wichs"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Wichs"
                },
                "author": "Daniel Wichs"
            },
            {
                "id": "http://arxiv.org/abs/2504.15268v14",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.15268v14",
                "title": "Beyond Correlation: Positive Definite Dependence Measures for Robust Inference, Flexible Scenarios, and Causal Modeling for Financial Portfolios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Correlation: Positive Definite Dependence Measures for Robust Inference, Flexible Scenarios, and Causal Modeling for Financial Portfolios"
                },
                "updated": "2025-12-09T18:52:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    52,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.15268v14",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.15268v14",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We live in a multivariate world, and effective construction, allocation, forecasting, and risk analysis of financial portfolios simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid and useable inferences about their estimates for essential purposes, such as hypothesis testing, dynamic monitoring, realistic and granular (reverse) scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distribution of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC remains valid under marginal asset distributions with notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. It provides p-values and confidence intervals at both the matrix level and the pairwise cell level, for both one and two-sample tests, with analytic consistency across levels. It provides these results even when selected cells in the matrix are frozen, thus enabling flexible, granular, and realistic scenarios and stress tests. Finally, when applied to directional dependence measures, NAbC enables accurate DAG recovery in causal modeling. NAbC stands alone in providing all of these capabilities, and should prove to be a very useful means by which we can better manage financial portfolios in our multivariate world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a multivariate world, and effective construction, allocation, forecasting, and risk analysis of financial portfolios simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid and useable inferences about their estimates for essential purposes, such as hypothesis testing, dynamic monitoring, realistic and granular (reverse) scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distribution of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC remains valid under marginal asset distributions with notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. It provides p-values and confidence intervals at both the matrix level and the pairwise cell level, for both one and two-sample tests, with analytic consistency across levels. It provides these results even when selected cells in the matrix are frozen, thus enabling flexible, granular, and realistic scenarios and stress tests. Finally, when applied to directional dependence measures, NAbC enables accurate DAG recovery in causal modeling. NAbC stands alone in providing all of these capabilities, and should prove to be a very useful means by which we can better manage financial portfolios in our multivariate world."
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-21T17:52:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM"
                },
                "authors": [
                    {
                        "name": "JD Opdyke"
                    }
                ],
                "author_detail": {
                    "name": "JD Opdyke"
                },
                "author": "JD Opdyke"
            },
            {
                "id": "http://arxiv.org/abs/2512.07823v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07823v2",
                "title": "DESI Strong Lens Foundry V: A Sample of HST-Observed Strong Lenses Modeled with GIGA-Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESI Strong Lens Foundry V: A Sample of HST-Observed Strong Lenses Modeled with GIGA-Lens"
                },
                "updated": "2025-12-09T18:50:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    50,
                    1,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07823v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present six galaxy-scale strong lenses with HST imaging modeled using GIGA-Lens. This is Paper V of the DESI Strong Lens Foundry series. These systems were discovered in the DESI Legacy Imaging Surveys using ML/AI methods and confirmed with DESI, Keck/NIRES, and VLT/MUSE spectroscopy. They span $z_d = 0.39 - 1.1$ and $z_s = 1.4 - 3.3$. This is the first HST strong lens sample modeled with full forward modeling -- all lens and source parameters sampled simultaneously in a single inference -- with explicit convergence validation using both $\\widehat{R}$ and effective sample size (ESS) for each system. All inferred parameters satisfy $\\widehat{R} < 1.1$ and ${\\rm ESS} \\gtrsim 10,000$, demonstrating that GIGA-Lens achieves statistically robust inference even for some of the most complex galaxy-scale lenses known. These results pave the way for scaling to much larger, high-resolution strong lens samples from HST, Euclid, JWST, and Roman. Convergence-validated modeling will be critical for key science goals, including constraining the mass-density profile of galaxies, detecting low-mass dark matter (sub)halos, and delivering precise and accurate cosmological constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present six galaxy-scale strong lenses with HST imaging modeled using GIGA-Lens. This is Paper V of the DESI Strong Lens Foundry series. These systems were discovered in the DESI Legacy Imaging Surveys using ML/AI methods and confirmed with DESI, Keck/NIRES, and VLT/MUSE spectroscopy. They span $z_d = 0.39 - 1.1$ and $z_s = 1.4 - 3.3$. This is the first HST strong lens sample modeled with full forward modeling -- all lens and source parameters sampled simultaneously in a single inference -- with explicit convergence validation using both $\\widehat{R}$ and effective sample size (ESS) for each system. All inferred parameters satisfy $\\widehat{R} < 1.1$ and ${\\rm ESS} \\gtrsim 10,000$, demonstrating that GIGA-Lens achieves statistically robust inference even for some of the most complex galaxy-scale lenses known. These results pave the way for scaling to much larger, high-resolution strong lens samples from HST, Euclid, JWST, and Roman. Convergence-validated modeling will be critical for key science goals, including constraining the mass-density profile of galaxies, detecting low-mass dark matter (sub)halos, and delivering precise and accurate cosmological constraints."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T18:54:28Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    18,
                    54,
                    28,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "40 pages, 26 figures, and 19 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Xiaosheng Huang"
                    },
                    {
                        "name": "David Alvarez-Garcia"
                    },
                    {
                        "name": "Monica Ubeda"
                    },
                    {
                        "name": "Vikram Bhamre"
                    },
                    {
                        "name": "Sean Xu"
                    },
                    {
                        "name": "S. Baltasar"
                    },
                    {
                        "name": "N. Ratier-Werbin"
                    },
                    {
                        "name": "F. Urcelay"
                    },
                    {
                        "name": "S. Agarwal"
                    },
                    {
                        "name": "A. Cikota"
                    },
                    {
                        "name": "Y. Hsu"
                    },
                    {
                        "name": "E. Lin"
                    },
                    {
                        "name": "D. J. Schlegel"
                    },
                    {
                        "name": "E. Silver"
                    },
                    {
                        "name": "C. J. Storfer"
                    },
                    {
                        "name": "M. Tamargo-Arizmendi"
                    }
                ],
                "author_detail": {
                    "name": "M. Tamargo-Arizmendi"
                },
                "author": "M. Tamargo-Arizmendi"
            },
            {
                "id": "http://arxiv.org/abs/2511.13666v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13666v2",
                "title": "A model-independent assessment of the late-time dark energy density evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model-independent assessment of the late-time dark energy density evolution"
                },
                "updated": "2025-12-09T18:49:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    49,
                    37,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13666v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $$-Cold Dark Matter ($$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\\% C.L., with the largest discrepancy coming from $$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $$-Cold Dark Matter ($$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\\% C.L., with the largest discrepancy coming from $$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:21:05Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    5,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "13 pages, 6 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Rayff de Souza"
                    },
                    {
                        "name": "Agripino Sousa-Neto"
                    },
                    {
                        "name": "Javier E. Gonzlez"
                    },
                    {
                        "name": "Jailson Alcaniz"
                    }
                ],
                "author_detail": {
                    "name": "Jailson Alcaniz"
                },
                "author": "Jailson Alcaniz"
            },
            {
                "id": "http://arxiv.org/abs/2512.08911v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08911v1",
                "title": "A Bayesian Approach Study of Hybrid Neutron Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach Study of Hybrid Neutron Stars"
                },
                "updated": "2025-12-09T18:47:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    47,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08911v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we explore how astronomical observations (specifically measurements of masses, radii, and tidal deformabilities) can constrain the presence of quark matter inside neutron stars, namely the phase transition from nuclear matter to deconfined quark matter. Our approach employs Bayesian analysis to study this phenomenon. Hadronic matter is modeled using the relativistic mean-field (RMF) approximation, for which we have selected two parameter sets: \\(NL3^{*}\\), representing hadronic matter with nucleons only, and $EL3$ with nucleons only and $EL3Y$, which includes hyperons. On the other hand deconfined quark matter is modeled using the vector-MIT bag model. For our purpose, the phase transition is implemented using the Maxwell construction. Bayesian inference is performed by tuning three parameters: the bag constant (i.e. $B^{1/4}$), the vector coupling constant \\(\\left(G_{v}\\right)\\), and the Dirac sea contribution ($b_{4}$). We found that a phase transition could exist at densities below \\(2.0\\,n_{0}\\) for both the $EL3- EL3Y $ and $NL3^{*}$ parametrizations. As a consequence, our results also indicate that a hybrid neutron star could have a large quark core that comprises more than \\(80\\%\\) of its size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore how astronomical observations (specifically measurements of masses, radii, and tidal deformabilities) can constrain the presence of quark matter inside neutron stars, namely the phase transition from nuclear matter to deconfined quark matter. Our approach employs Bayesian analysis to study this phenomenon. Hadronic matter is modeled using the relativistic mean-field (RMF) approximation, for which we have selected two parameter sets: \\(NL3^{*}\\), representing hadronic matter with nucleons only, and $EL3$ with nucleons only and $EL3Y$, which includes hyperons. On the other hand deconfined quark matter is modeled using the vector-MIT bag model. For our purpose, the phase transition is implemented using the Maxwell construction. Bayesian inference is performed by tuning three parameters: the bag constant (i.e. $B^{1/4}$), the vector coupling constant \\(\\left(G_{v}\\right)\\), and the Dirac sea contribution ($b_{4}$). We found that a phase transition could exist at densities below \\(2.0\\,n_{0}\\) for both the $EL3- EL3Y $ and $NL3^{*}$ parametrizations. As a consequence, our results also indicate that a hybrid neutron star could have a large quark core that comprises more than \\(80\\%\\) of its size."
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:47:02Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    47,
                    2,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "19 pages and 24 figures",
                "arxiv_primary_category": {
                    "term": "nucl-th"
                },
                "authors": [
                    {
                        "name": "Fbio Kpp"
                    },
                    {
                        "name": "Csar H. Lenzi"
                    },
                    {
                        "name": "Csar V. Flores"
                    },
                    {
                        "name": "and Dbora P. Menezes"
                    }
                ],
                "author_detail": {
                    "name": "and Dbora P. Menezes"
                },
                "author": "and Dbora P. Menezes"
            },
            {
                "id": "http://arxiv.org/abs/2512.05306v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05306v2",
                "title": "Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)"
                },
                "updated": "2025-12-09T18:44:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    44,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05306v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T22:58:32Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    22,
                    58,
                    32,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "20 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Y. Sungtaek Ju"
                    }
                ],
                "author_detail": {
                    "name": "Y. Sungtaek Ju"
                },
                "author": "Y. Sungtaek Ju"
            },
            {
                "id": "http://arxiv.org/abs/2512.08898v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08898v1",
                "title": "Self-lensing of moving gravitational-wave sources can break the microlensing crossing timescale degeneracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-lensing of moving gravitational-wave sources can break the microlensing crossing timescale degeneracy"
                },
                "updated": "2025-12-09T18:38:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    38,
                    47,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08898v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When a moving gravitational-wave (GW) source travels behind a massive astrophysical object, its signal is gravitationally lensed, showing a waveform distortion similar to a Paczyski curve. We present a first study of the lensing signature of a massive black hole (MBH) on a frequency-dependent GW signal from a moving binary merger. For both light and GW sources in a Keplerian circular orbit around a MBH lens, the self-lensing geometry breaks the microlensing degeneracy in the Einstein radius crossing timescale $t_{\\rm E}$. The duration of the curve ($2 t_{\\rm E}$) becomes independent on the MBH mass $M_{\\rm MBH}$, and provides a direct measure of the distance $d_{\\rm LS}$ to the MBH. However, $M_{\\rm MBH}$ remains unknown. We show that, in GW signals, the redshifted mass $M_{{\\rm MBH},z}$ can additionally be obtained from the interference pattern, by measuring the modulation period $T$, the GW frequency $f$, and $t_{\\rm E}$: $M_{{\\rm MBH},z}\\simeq 2.5\\times 10^6\\,M_\\odot\\,(t_{\\rm E}/[100\\,{\\rm s}])\\,(f\\,T)^{-1}$. If this lensing signature is not considered, it may be confused with other waveform distortions, especially in the modeling of overlapping signals in next generation ground-based GW detectors. The observation of one of these curves and its associated parameters may help (1) constrain the orbital distance $d_{\\rm LS}$ of sources, especially around low mass MBHs at the centers of star clusters and galaxies, (2) additionally estimate the mass $M_{{\\rm MBH},z}$ of these MBHs, and (3) infer the orbital inclination of the binary. Simultaneously obtaining $d_{\\rm LS}$ and $M_{{\\rm MBH},z}$ through self-lensing can help constrain the astrophysical environments where GW signals come from.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a moving gravitational-wave (GW) source travels behind a massive astrophysical object, its signal is gravitationally lensed, showing a waveform distortion similar to a Paczyski curve. We present a first study of the lensing signature of a massive black hole (MBH) on a frequency-dependent GW signal from a moving binary merger. For both light and GW sources in a Keplerian circular orbit around a MBH lens, the self-lensing geometry breaks the microlensing degeneracy in the Einstein radius crossing timescale $t_{\\rm E}$. The duration of the curve ($2 t_{\\rm E}$) becomes independent on the MBH mass $M_{\\rm MBH}$, and provides a direct measure of the distance $d_{\\rm LS}$ to the MBH. However, $M_{\\rm MBH}$ remains unknown. We show that, in GW signals, the redshifted mass $M_{{\\rm MBH},z}$ can additionally be obtained from the interference pattern, by measuring the modulation period $T$, the GW frequency $f$, and $t_{\\rm E}$: $M_{{\\rm MBH},z}\\simeq 2.5\\times 10^6\\,M_\\odot\\,(t_{\\rm E}/[100\\,{\\rm s}])\\,(f\\,T)^{-1}$. If this lensing signature is not considered, it may be confused with other waveform distortions, especially in the modeling of overlapping signals in next generation ground-based GW detectors. The observation of one of these curves and its associated parameters may help (1) constrain the orbital distance $d_{\\rm LS}$ of sources, especially around low mass MBHs at the centers of star clusters and galaxies, (2) additionally estimate the mass $M_{{\\rm MBH},z}$ of these MBHs, and (3) infer the orbital inclination of the binary. Simultaneously obtaining $d_{\\rm LS}$ and $M_{{\\rm MBH},z}$ through self-lensing can help constrain the astrophysical environments where GW signals come from."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:38:47Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    38,
                    47,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures. Comments welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Helena Ubach"
                    }
                ],
                "author_detail": {
                    "name": "Helena Ubach"
                },
                "author": "Helena Ubach"
            },
            {
                "id": "http://arxiv.org/abs/2511.21667v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21667v3",
                "title": "Escaping the Verifier: Learning to Reason via Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escaping the Verifier: Learning to Reason via Demonstrations"
                },
                "updated": "2025-12-09T18:37:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    37,
                    56,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21667v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T18:42:52Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    18,
                    42,
                    52,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Locke Cai"
                    },
                    {
                        "name": "Ivan Provilkov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Provilkov"
                },
                "author": "Ivan Provilkov"
            },
            {
                "id": "http://arxiv.org/abs/2512.08895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08895v1",
                "title": "Unsupervised Learning of Density Estimates with Topological Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Learning of Density Estimates with Topological Optimization"
                },
                "updated": "2025-12-09T18:35:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    35,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:35:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    35,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Suina Tanweer"
                    },
                    {
                        "name": "Firas A. Khasawneh"
                    }
                ],
                "author_detail": {
                    "name": "Firas A. Khasawneh"
                },
                "author": "Firas A. Khasawneh"
            },
            {
                "id": "http://arxiv.org/abs/2509.13316v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.13316v3",
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?"
                },
                "updated": "2025-12-09T18:35:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    35,
                    28,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.13316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.13316v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "arxiv_comment": "40 pages, 6 figures. Updated and added content",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace"
            },
            {
                "id": "http://arxiv.org/abs/2512.08894v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08894v1",
                "title": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training"
                },
                "updated": "2025-12-09T18:33:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08894v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:33:48Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    48,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jakub Krajewski"
                    },
                    {
                        "name": "Amitis Shidani"
                    },
                    {
                        "name": "Dan Busbridge"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Jason Ramapuram"
                    }
                ],
                "author_detail": {
                    "name": "Jason Ramapuram"
                },
                "author": "Jason Ramapuram"
            },
            {
                "id": "http://arxiv.org/abs/2512.08892v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08892v1",
                "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders"
                },
                "updated": "2025-12-09T18:33:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    22,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08892v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:33:22Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    22,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Zhenghao He"
                    },
                    {
                        "name": "Bohan Liu"
                    },
                    {
                        "name": "Sanchit Sinha"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05033v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05033v2",
                "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation"
                },
                "updated": "2025-12-09T18:32:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    32,
                    43,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05033v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T17:50:53Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    17,
                    50,
                    53,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2512.08889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08889v1",
                "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers"
                },
                "updated": "2025-12-09T18:30:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    30,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:30:23Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    30,
                    23,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Project webpage: https://glab-caltech.github.io/valor/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Damiano Marsili"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gkioxari"
                },
                "author": "Georgia Gkioxari"
            },
            {
                "id": "http://arxiv.org/abs/2504.02800v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.02800v3",
                "title": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents"
                },
                "updated": "2025-12-09T18:29:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    29,
                    25,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.02800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.02800v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ICDEW67478.2025.00027",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. Large Language Models (LLMs) offer stronger semantic understanding and reasoning than traditional deep learning, but their use in high-stakes clinical settings is limited by hallucinations and the lack of persistent memory. However, existing literature has not sufficiently investigated how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pretrained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. Large Language Models (LLMs) offer stronger semantic understanding and reasoning than traditional deep learning, but their use in high-stakes clinical settings is limited by hallucinations and the lack of persistent memory. However, existing literature has not sufficiently investigated how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pretrained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-03T17:43:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    14,
                    3,
                    93,
                    0
                ],
                "arxiv_comment": "20 pages, 10 figures. This is an extension of ICDEW 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhuohan Ge"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Xinyi Zhu"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Mingtao Zhang"
                    },
                    {
                        "name": "Shihao Qi"
                    },
                    {
                        "name": "Yuming Xu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_doi": "10.1109/ICDEW67478.2025.00027"
            },
            {
                "id": "http://arxiv.org/abs/2504.08937v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.08937v4",
                "title": "Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion"
                },
                "updated": "2025-12-09T18:19:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    19,
                    43,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.08937v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.08937v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-11T19:33:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    19,
                    33,
                    6,
                    4,
                    101,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Minjie Deng"
                    },
                    {
                        "name": "Yan Wei"
                    },
                    {
                        "name": "An Wu"
                    },
                    {
                        "name": "Yuncan Ouyang"
                    },
                    {
                        "name": "Hao Zhai"
                    },
                    {
                        "name": "Qianyao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Qianyao Peng"
                },
                "author": "Qianyao Peng"
            },
            {
                "id": "http://arxiv.org/abs/2512.08884v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08884v1",
                "title": "AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike"
                },
                "updated": "2025-12-09T18:19:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    19,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08884v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:19:42Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    19,
                    42,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Yiwei Wu"
                    },
                    {
                        "name": "Leah Ajmani"
                    },
                    {
                        "name": "Nathan TeBlunthuis"
                    },
                    {
                        "name": "Hanlin Li"
                    }
                ],
                "author_detail": {
                    "name": "Hanlin Li"
                },
                "author": "Hanlin Li"
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v7",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04652v7",
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-12-09T18:12:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    12,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04652v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04652v7",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges. Our code is available at https://github.com/OpenMLRL/CoMLRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges. Our code is available at https://github.com/OpenMLRL/CoMLRL."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato"
            },
            {
                "id": "http://arxiv.org/abs/2508.06457v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.06457v2",
                "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"
                },
                "updated": "2025-12-09T18:09:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    9,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.06457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.06457v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T17:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for Information Security. 19 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "Proceedings of Machine Learning Research 299, 2025 Conference on Applied Machine Learning for Information Security",
                "authors": [
                    {
                        "name": "Sanket Badhe"
                    }
                ],
                "author_detail": {
                    "name": "Sanket Badhe"
                },
                "author": "Sanket Badhe"
            },
            {
                "id": "http://arxiv.org/abs/2512.08875v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08875v1",
                "title": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation"
                },
                "updated": "2025-12-09T18:06:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    6,
                    31,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08875v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:06:31Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    6,
                    31,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Joshua Ward"
                    },
                    {
                        "name": "Bochao Gu"
                    },
                    {
                        "name": "Chi-Hua Wang"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.08870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08870v1",
                "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents"
                },
                "updated": "2025-12-09T18:04:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    4,
                    41,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:04:41Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    4,
                    41,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Qizhen Lan"
                    },
                    {
                        "name": "Yuchao Qiu"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu"
            },
            {
                "id": "http://arxiv.org/abs/2412.02767v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.02767v4",
                "title": "Endogenous Heteroskedasticity in Linear Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogenous Heteroskedasticity in Linear Models"
                },
                "updated": "2025-12-09T18:03:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    3,
                    50,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.02767v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.02767v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Linear regressions with endogeneity are widely used to estimate causal effects. This paper studies a framework that involves two common practical issues: endogeneity of the regressors and heteroskedasticity that depends on endogenous regressors, i.e., endogenous heteroskedasticity. To address the inconsistency of the two-stage least squares estimator in this scenario, and recover the causal parameters of interest, we develop a framework for practical estimation and inference based on the control function approach allowing for discrete and continuous regressors. In particular, we suggest a simple two-step estimation procedure. We establish the limiting properties of the estimator, namely, consistency and asymptotic normality. In addition, we develop practical valid inference methods by proposing an estimator for the asymptotic variance-covariance matrix, and formally establishing its consistency. Monte Carlo simulations provide evidence on the finite-sample performance of the proposed methods and evaluate different implementation strategies. We revisit an empirical application on job training to illustrate the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear regressions with endogeneity are widely used to estimate causal effects. This paper studies a framework that involves two common practical issues: endogeneity of the regressors and heteroskedasticity that depends on endogenous regressors, i.e., endogenous heteroskedasticity. To address the inconsistency of the two-stage least squares estimator in this scenario, and recover the causal parameters of interest, we develop a framework for practical estimation and inference based on the control function approach allowing for discrete and continuous regressors. In particular, we suggest a simple two-step estimation procedure. We establish the limiting properties of the estimator, namely, consistency and asymptotic normality. In addition, we develop practical valid inference methods by proposing an estimator for the asymptotic variance-covariance matrix, and formally establishing its consistency. Monte Carlo simulations provide evidence on the finite-sample performance of the proposed methods and evaluate different implementation strategies. We revisit an empirical application on job training to illustrate the methods."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-03T19:09:48Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    19,
                    9,
                    48,
                    1,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Javier Alejo"
                    },
                    {
                        "name": "Antonio F. Galvao"
                    },
                    {
                        "name": "Julian Martinez-Iriarte"
                    },
                    {
                        "name": "Gabriel Montes-Rojas"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Montes-Rojas"
                },
                "author": "Gabriel Montes-Rojas"
            },
            {
                "id": "http://arxiv.org/abs/2512.08867v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08867v1",
                "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA"
                },
                "updated": "2025-12-09T17:58:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    58,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08867v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:58:36Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    58,
                    36,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Lianghong Guo"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.08864v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08864v1",
                "title": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse"
                },
                "updated": "2025-12-09T17:54:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    54,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08864v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:54:17Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    54,
                    17,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matthew Smith"
                    },
                    {
                        "name": "Jakub Kry"
                    },
                    {
                        "name": "Simon Campos"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Sevan Hayrapet"
                    },
                    {
                        "name": "Fred Heiding"
                    },
                    {
                        "name": "Omer Nevo"
                    },
                    {
                        "name": "Adam Swanda"
                    },
                    {
                        "name": "Jair Aguirre"
                    },
                    {
                        "name": "Asher Brass Gershovich"
                    },
                    {
                        "name": "Eric Clay"
                    },
                    {
                        "name": "Ryan Fetterman"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Marc Juarez"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Henry Papadatos"
                    }
                ],
                "author_detail": {
                    "name": "Henry Papadatos"
                },
                "author": "Henry Papadatos"
            },
            {
                "id": "http://arxiv.org/abs/2512.08862v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08862v1",
                "title": "Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety"
                },
                "updated": "2025-12-09T17:53:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    53,
                    19,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08862v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:53:19Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    53,
                    19,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mohamed Elmahallawy"
                    },
                    {
                        "name": "Sanjay Madria"
                    },
                    {
                        "name": "Samuel Frimpong"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Frimpong"
                },
                "author": "Samuel Frimpong"
            },
            {
                "id": "http://arxiv.org/abs/2407.20432v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.20432v2",
                "title": "Neural Surrogate HMC: On Using Neural Likelihoods for Hamiltonian Monte Carlo in Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Surrogate HMC: On Using Neural Likelihoods for Hamiltonian Monte Carlo in Simulation-Based Inference"
                },
                "updated": "2025-12-09T17:48:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    48,
                    44,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.20432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.20432v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian inference methods such as Markov Chain Monte Carlo (MCMC) typically require repeated computations of the likelihood function, but in some scenarios this is infeasible and alternative methods are needed. Simulation-based inference (SBI) methods address this problem by using machine learning to amortize computations. In this work, we highlight a particular synergy between the SBI method of neural likelihood estimation and the classic MCMC method of Hamiltonian Monte Carlo. We show that approximating the likelihood function with a neural network model can provide three distinct advantages: (1) amortizing the computations for MCMC; (2) providing gradients for Hamiltonian Monte Carlo, and (3) smoothing over noisy simulations resulting from numerical instabilities. We provide practical guidelines for defining a prior, sampling a training set, and evaluating convergence. The method is demonstrated in an application modeling the heliospheric transport of galactic cosmic rays, where it enables efficient inference of latent parameters in the Parker equation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference methods such as Markov Chain Monte Carlo (MCMC) typically require repeated computations of the likelihood function, but in some scenarios this is infeasible and alternative methods are needed. Simulation-based inference (SBI) methods address this problem by using machine learning to amortize computations. In this work, we highlight a particular synergy between the SBI method of neural likelihood estimation and the classic MCMC method of Hamiltonian Monte Carlo. We show that approximating the likelihood function with a neural network model can provide three distinct advantages: (1) amortizing the computations for MCMC; (2) providing gradients for Hamiltonian Monte Carlo, and (3) smoothing over noisy simulations resulting from numerical instabilities. We provide practical guidelines for defining a prior, sampling a training set, and evaluating convergence. The method is demonstrated in an application modeling the heliospheric transport of galactic cosmic rays, where it enables efficient inference of latent parameters in the Parker equation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-29T21:54:57Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    21,
                    54,
                    57,
                    0,
                    211,
                    0
                ],
                "arxiv_comment": "19 pages, 14 figures, in review for publication in JGR: Machine Learning and Computation",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Linnea M Wolniewicz"
                    },
                    {
                        "name": "Peter Sadowski"
                    },
                    {
                        "name": "Claudio Corti"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Corti"
                },
                "author": "Claudio Corti"
            },
            {
                "id": "http://arxiv.org/abs/2512.08847v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08847v1",
                "title": "Partially Bayes p-values for large scale inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Bayes p-values for large scale inference"
                },
                "updated": "2025-12-09T17:40:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    40,
                    34,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08847v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We seek to conduct statistical inference for a large collection of primary parameters, each with its own nuisance parameters. Our approach is partially Bayesian, in that we treat the primary parameters as fixed while we model the nuisance parameters as random and drawn from an unknown distribution which we endow with a nonparametric prior. We compute partially Bayes p-values by conditioning on nuisance parameter statistics, that is, statistics that are ancillary for the primary parameters and informative about the nuisance parameters. The proposed p-values have a Bayesian interpretation as tail areas computed with respect to the posterior distribution of the nuisance parameters. Similarly to the conditional predictive p-values of Bayarri and Berger, the partially Bayes p-values avoid double use of the data (unlike posterior predictive p-values). A key ingredient of our approach is that we model nuisance parameters hierarchically across problems; the sharing of information across problems leads to improved calibration. We illustrate the proposed partially Bayes p-values in two applications: the normal means problem with unknown variances and a location-scale model with unknown distribution shape. We model the scales via Dirichlet processes in both examples and the distribution shape via Plya trees in the second. Our proposed partially Bayes p-values increase power and calibration compared to purely frequentist alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We seek to conduct statistical inference for a large collection of primary parameters, each with its own nuisance parameters. Our approach is partially Bayesian, in that we treat the primary parameters as fixed while we model the nuisance parameters as random and drawn from an unknown distribution which we endow with a nonparametric prior. We compute partially Bayes p-values by conditioning on nuisance parameter statistics, that is, statistics that are ancillary for the primary parameters and informative about the nuisance parameters. The proposed p-values have a Bayesian interpretation as tail areas computed with respect to the posterior distribution of the nuisance parameters. Similarly to the conditional predictive p-values of Bayarri and Berger, the partially Bayes p-values avoid double use of the data (unlike posterior predictive p-values). A key ingredient of our approach is that we model nuisance parameters hierarchically across problems; the sharing of information across problems leads to improved calibration. We illustrate the proposed partially Bayes p-values in two applications: the normal means problem with unknown variances and a location-scale model with unknown distribution shape. We model the scales via Dirichlet processes in both examples and the distribution shape via Plya trees in the second. Our proposed partially Bayes p-values increase power and calibration compared to purely frequentist alternatives."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:40:34Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    40,
                    34,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Nikolaos Ignatiadis"
                    },
                    {
                        "name": "Li Ma"
                    }
                ],
                "author_detail": {
                    "name": "Li Ma"
                },
                "author": "Li Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.08844v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08844v1",
                "title": "A Methodology for Quantitative AI Risk Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Quantitative AI Risk Modeling"
                },
                "updated": "2025-12-09T17:34:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08844v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:34:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matt Smith"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Simon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Simon Campos"
                },
                "author": "Simon Campos"
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17238v2",
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "updated": "2025-12-09T17:34:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13436v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13436v5",
                "title": "Shaping the Mantle: The Role of Superheated Core After Giant Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping the Mantle: The Role of Superheated Core After Giant Impacts"
                },
                "updated": "2025-12-09T17:22:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    22,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13436v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13436v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Moon-forming giant impact significantly influenced the initial thermal state of Earth's mantle by generating a global magma ocean, marking the onset of mantle evolution. Recent Smoothed Particle Hydrodynamics (SPH) simulations indicate that such a collision would produce a superheated core, whose cooling would strongly influence subsequent mantle dynamics. Here, we present systematic SPH simulations of diverse giant-impact scenarios and show that the superheated core formed after the impact can trigger secondary mantle melting, thereby governing the final state of the magma ocean. To further quantify this effect, we employ a parameterized mantle-melting model to evaluate the influence of secondary melting on the lower mantle. Our results suggest three possible outcomes: complete mantle melting, the formation of a basal melt layer, or the initiation of an early superplume. Combined with recent two-phase magma-ocean solidification models, we infer that all three scenarios would result in basal melt layers of varying thickness, partially retaining the thermal energy of the superheated core. In the canonical Moon-forming scenario, the superheated core would rapidly transfer heat to Earth's lower mantle, causing secondary mantle melting within approximately 277-5983 years and generating either a basal melt layer or a fully molten mantle. Both outcomes would effectively erase primordial heterogeneities in the lower mantle and impose distinct pathways for its subsequent thermal evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moon-forming giant impact significantly influenced the initial thermal state of Earth's mantle by generating a global magma ocean, marking the onset of mantle evolution. Recent Smoothed Particle Hydrodynamics (SPH) simulations indicate that such a collision would produce a superheated core, whose cooling would strongly influence subsequent mantle dynamics. Here, we present systematic SPH simulations of diverse giant-impact scenarios and show that the superheated core formed after the impact can trigger secondary mantle melting, thereby governing the final state of the magma ocean. To further quantify this effect, we employ a parameterized mantle-melting model to evaluate the influence of secondary melting on the lower mantle. Our results suggest three possible outcomes: complete mantle melting, the formation of a basal melt layer, or the initiation of an early superplume. Combined with recent two-phase magma-ocean solidification models, we infer that all three scenarios would result in basal melt layers of varying thickness, partially retaining the thermal energy of the superheated core. In the canonical Moon-forming scenario, the superheated core would rapidly transfer heat to Earth's lower mantle, causing secondary mantle melting within approximately 277-5983 years and generating either a basal melt layer or a fully molten mantle. Both outcomes would effectively erase primordial heterogeneities in the lower mantle and impose distinct pathways for its subsequent thermal evolution."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:46:32Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    46,
                    32,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "26 pages, 7 figures, 2 tables, and 5 supplementary figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "You Zhou"
                    }
                ],
                "author_detail": {
                    "name": "You Zhou"
                },
                "author": "You Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.08829v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08829v1",
                "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"
                },
                "updated": "2025-12-09T17:18:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08829v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "16 pages, 8 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Shaoyu Chen"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08828v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08828v1",
                "title": "Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference"
                },
                "updated": "2025-12-09T17:18:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    9,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08828v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS)."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:18:09Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    9,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Swaraj Bose"
                    },
                    {
                        "name": "Walter Dempsey"
                    }
                ],
                "author_detail": {
                    "name": "Walter Dempsey"
                },
                "author": "Walter Dempsey"
            },
            {
                "id": "http://arxiv.org/abs/2510.05526v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05526v2",
                "title": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment"
                },
                "updated": "2025-12-09T17:10:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    10,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05526v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T02:32:47Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    32,
                    47,
                    1,
                    280,
                    0
                ],
                "arxiv_comment": "Edited a few incorrect numbers in Tables 2 and 3",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Peiran Yu"
                    },
                    {
                        "name": "Heng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Huang"
                },
                "author": "Heng Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08814v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08814v1",
                "title": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts"
                },
                "updated": "2025-12-09T17:07:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    7,
                    54,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08814v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -> user vector -> labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -> user vector -> labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:07:54Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    7,
                    54,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yifan Lyu"
                    },
                    {
                        "name": "Liang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhang"
                },
                "author": "Liang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.15817v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15817v3",
                "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code"
                },
                "updated": "2025-12-09T17:06:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    6,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15817v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773164",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T19:18:28Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    19,
                    18,
                    28,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Alejandro Velasco"
                    },
                    {
                        "name": "Daniel Rodriguez-Cardenas"
                    },
                    {
                        "name": "Dipin Khati"
                    },
                    {
                        "name": "David N. Palacio"
                    },
                    {
                        "name": "Luftar Rahman Alif"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    }
                ],
                "author_detail": {
                    "name": "Denys Poshyvanyk"
                },
                "author": "Denys Poshyvanyk",
                "arxiv_doi": "10.1145/3744916.3773164"
            },
            {
                "id": "http://arxiv.org/abs/2512.08812v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08812v1",
                "title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation"
                },
                "updated": "2025-12-09T17:05:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    5,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08812v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:05:36Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    5,
                    36,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025). https://www.intellisky.org/workshops/AIMG2025/workshop_AIMG2025.html",
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Anna Jordanous"
                    }
                ],
                "author_detail": {
                    "name": "Anna Jordanous"
                },
                "author": "Anna Jordanous"
            },
            {
                "id": "http://arxiv.org/abs/2512.08810v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08810v1",
                "title": "Multicalibration for LLM-based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multicalibration for LLM-based Code Generation"
                },
                "updated": "2025-12-09T17:04:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    4,
                    1,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08810v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:04:01Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    4,
                    1,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted at AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Viola Campos"
                    },
                    {
                        "name": "Robin Kuschnereit"
                    },
                    {
                        "name": "Adrian Ulges"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Ulges"
                },
                "author": "Adrian Ulges"
            },
            {
                "id": "http://arxiv.org/abs/2512.08809v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08809v1",
                "title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration"
                },
                "updated": "2025-12-09T17:03:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    3,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08809v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:03:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    3,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted at IEEE INFOCOM 2026 (full version)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Weixiang Han"
                    },
                    {
                        "name": "Chengjun Cai"
                    },
                    {
                        "name": "Xingliang Yuan"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08802v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08802v1",
                "title": "Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework"
                },
                "updated": "2025-12-09T16:58:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    58,
                    8,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08802v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.\n  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.\n  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:58:08Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    58,
                    8,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "published in CAMLIS 2025, https://www.camlis.org/",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Sadegh Momeni"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Birkett Huber"
                    },
                    {
                        "name": "Hamza Harkous"
                    },
                    {
                        "name": "Sam Lipton"
                    },
                    {
                        "name": "Benoit Seguin"
                    },
                    {
                        "name": "Yanis Pavlidis"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Pavlidis"
                },
                "author": "Yanis Pavlidis"
            },
            {
                "id": "http://arxiv.org/abs/2512.08798v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08798v1",
                "title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?"
                },
                "updated": "2025-12-09T16:51:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    51,
                    30,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08798v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:51:30Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    51,
                    30,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Rejected from LoG 2025 (submitted August 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jeongwhan Choi"
                    },
                    {
                        "name": "Woosung Kang"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Jongwoo Kim"
                    },
                    {
                        "name": "Noseong Park"
                    }
                ],
                "author_detail": {
                    "name": "Noseong Park"
                },
                "author": "Noseong Park"
            },
            {
                "id": "http://arxiv.org/abs/2512.08786v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08786v1",
                "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs"
                },
                "updated": "2025-12-09T16:39:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    39,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08786v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:39:32Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    39,
                    32,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mahmoud Srewa"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Salma Elmalaki"
                    }
                ],
                "author_detail": {
                    "name": "Salma Elmalaki"
                },
                "author": "Salma Elmalaki"
            },
            {
                "id": "http://arxiv.org/abs/2504.10578v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.10578v2",
                "title": "Smooth sailing or ragged climb? -- Increasing the robustness of power spectrum de-wiggling and ShapeFit parameter compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smooth sailing or ragged climb? -- Increasing the robustness of power spectrum de-wiggling and ShapeFit parameter compression"
                },
                "updated": "2025-12-09T16:23:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    23,
                    29,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.10578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.10578v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1475-7516/2025/11/029",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The baryonic features in the galaxy power spectrum offer tight, time-resolved constraints on the expansion history of the Universe but complicate the measurement of the broadband shape of the power spectrum, which also contains precious cosmological information. In the context of ShapeFit, the broadband information is compressed into a single parameter, the slope of the power spectrum at the pivot scale, $m$, is sensitive to matter-radiation equality and the baryonic suppression. To calculate this parameter, two steps are necessary: 1) smoothing the power spectrum to remove the baryonic oscillations and 2) calculating the derivative of the power spectrum ratio at the pivot scale. In this work we compare thirteen methods designed to separate the broadband and oscillating components and examine their performance. The systematic uncertainty between different de-wiggling procedures is at most $2$%, depending on the scale. For the obtained slope, we show that the de-wiggling procedures impart large 50% differences, but as long as the theory and data pipelines are consistent, this is of no concern for cosmological inference given the precision of existing and ongoing surveys. However, it still motivates the search for more robust ways of extracting the slope. We show that post-processing the power spectrum ratio before taking the derivative makes the slope values far more robust. We further investigate eleven ways of extracting the slope and highlight the two most successful ones. We derive a systematic uncertainty on the slope $m$ of $_{m,\\mathrm{syst}} = 0.023 |m| + 0.001$ by studying the behavior of the slopes in different cosmologies and the impact in cosmological inference. In cosmologies with a feature in the matter-power spectrum, such as in the early dark energy cosmologies, this systematic uncertainty estimate does not necessarily hold, and further investigation is required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The baryonic features in the galaxy power spectrum offer tight, time-resolved constraints on the expansion history of the Universe but complicate the measurement of the broadband shape of the power spectrum, which also contains precious cosmological information. In the context of ShapeFit, the broadband information is compressed into a single parameter, the slope of the power spectrum at the pivot scale, $m$, is sensitive to matter-radiation equality and the baryonic suppression. To calculate this parameter, two steps are necessary: 1) smoothing the power spectrum to remove the baryonic oscillations and 2) calculating the derivative of the power spectrum ratio at the pivot scale. In this work we compare thirteen methods designed to separate the broadband and oscillating components and examine their performance. The systematic uncertainty between different de-wiggling procedures is at most $2$%, depending on the scale. For the obtained slope, we show that the de-wiggling procedures impart large 50% differences, but as long as the theory and data pipelines are consistent, this is of no concern for cosmological inference given the precision of existing and ongoing surveys. However, it still motivates the search for more robust ways of extracting the slope. We show that post-processing the power spectrum ratio before taking the derivative makes the slope values far more robust. We further investigate eleven ways of extracting the slope and highlight the two most successful ones. We derive a systematic uncertainty on the slope $m$ of $_{m,\\mathrm{syst}} = 0.023 |m| + 0.001$ by studying the behavior of the slopes in different cosmologies and the impact in cosmological inference. In cosmologies with a feature in the matter-power spectrum, such as in the early dark energy cosmologies, this systematic uncertainty estimate does not necessarily hold, and further investigation is required."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    18,
                    0,
                    1,
                    0,
                    104,
                    0
                ],
                "arxiv_comment": "Changes to match the published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "arxiv_journal_ref": "Journal of Cosmology and Astroparticle Physics, 2025(11), 029",
                "authors": [
                    {
                        "name": "Katayoon Ghaemi"
                    },
                    {
                        "name": "Nils Schneberg"
                    },
                    {
                        "name": "Licia Verde"
                    }
                ],
                "author_detail": {
                    "name": "Licia Verde"
                },
                "author": "Licia Verde",
                "arxiv_doi": "10.1088/1475-7516/2025/11/029"
            },
            {
                "id": "http://arxiv.org/abs/2512.08769v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08769v1",
                "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows"
                },
                "updated": "2025-12-09T16:23:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    23,
                    5,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08769v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:23:05Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    23,
                    5,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Eranga Bandara"
                    },
                    {
                        "name": "Ross Gore"
                    },
                    {
                        "name": "Peter Foytik"
                    },
                    {
                        "name": "Sachin Shetty"
                    },
                    {
                        "name": "Ravi Mukkamala"
                    },
                    {
                        "name": "Abdul Rahman"
                    },
                    {
                        "name": "Xueping Liang"
                    },
                    {
                        "name": "Safdar H. Bouk"
                    },
                    {
                        "name": "Amin Hass"
                    },
                    {
                        "name": "Sachini Rajapakse"
                    },
                    {
                        "name": "Ng Wee Keong"
                    },
                    {
                        "name": "Kasun De Zoysa"
                    },
                    {
                        "name": "Aruna Withanage"
                    },
                    {
                        "name": "Nilaan Loganathan"
                    }
                ],
                "author_detail": {
                    "name": "Nilaan Loganathan"
                },
                "author": "Nilaan Loganathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.08764v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08764v1",
                "title": "Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?"
                },
                "updated": "2025-12-09T16:13:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    13,
                    27,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08764v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/978-3-032-09037-9",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:13:27Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    13,
                    27,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "8 pages, 5 tables",
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "arxiv_journal_ref": "Advances in Soft Computing. MICAI 2025. Lecture Notes in Computer Science, vol 16221. Springer",
                "authors": [
                    {
                        "name": "Nicolas Reche"
                    },
                    {
                        "name": "Elvys Linhares-Pontes"
                    },
                    {
                        "name": "Juan-Manuel Torres-Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Juan-Manuel Torres-Moreno"
                },
                "author": "Juan-Manuel Torres-Moreno",
                "arxiv_doi": "10.1007/978-3-032-09037-9"
            },
            {
                "id": "http://arxiv.org/abs/2511.10240v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10240v2",
                "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs"
                },
                "updated": "2025-12-09T16:01:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    1,
                    50,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10240v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:14:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minbae Park"
                    },
                    {
                        "name": "Hyemin Yang"
                    },
                    {
                        "name": "Jeonghyun Kim"
                    },
                    {
                        "name": "Kunsoo Park"
                    },
                    {
                        "name": "Hyunjoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjoon Kim"
                },
                "author": "Hyunjoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.03254v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03254v3",
                "title": "Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection"
                },
                "updated": "2025-12-09T15:50:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    50,
                    15,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03254v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T21:56:08Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    21,
                    56,
                    8,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Philippe A. Boileau"
                    },
                    {
                        "name": "Hani Zaki"
                    },
                    {
                        "name": "Gabriele Lileikyte"
                    },
                    {
                        "name": "Niklas Nielsen"
                    },
                    {
                        "name": "Patrick R. Lawler"
                    },
                    {
                        "name": "Mireille E. Schnitzer"
                    }
                ],
                "author_detail": {
                    "name": "Mireille E. Schnitzer"
                },
                "author": "Mireille E. Schnitzer"
            },
            {
                "id": "http://arxiv.org/abs/2512.08737v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08737v1",
                "title": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy"
                },
                "updated": "2025-12-09T15:47:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    47,
                    16,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08737v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:47:16Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    47,
                    16,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Submitted to AAMAS 2026",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Botao 'Amber' Hu"
                    },
                    {
                        "name": "Bangdao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bangdao Chen"
                },
                "author": "Bangdao Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.08735v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08735v1",
                "title": "Stationary Point Constrained Inference via Diffeomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stationary Point Constrained Inference via Diffeomorphisms"
                },
                "updated": "2025-12-09T15:46:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    46,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08735v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:46:48Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    46,
                    48,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Michael Price"
                    },
                    {
                        "name": "Debdeep Pati"
                    },
                    {
                        "name": "Ning Ning"
                    }
                ],
                "author_detail": {
                    "name": "Ning Ning"
                },
                "author": "Ning Ning"
            },
            {
                "id": "http://arxiv.org/abs/2506.10083v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.10083v2",
                "title": "Bayesian luminosity function estimation in multi-depth datasets with selection effects: A case study for $3<z<5$ Lyman $$ emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian luminosity function estimation in multi-depth datasets with selection effects: A case study for $3<z<5$ Lyman $$ emitters"
                },
                "updated": "2025-12-09T15:44:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    44,
                    9,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.10083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.10083v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1051/0004-6361/202555898",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We present a hierarchical Bayesian framework designed to infer the luminosity function of any class of object by jointly modelling data from multiple surveys with varying depth, completeness, and sky coverage. Our method explicitly accounts for selection effects and measurement uncertainties (e.g. in luminosity) and can be generalized to any extensive quantity, such as mass. We validated the model using mock catalogues; from this we determined that deep data reaching $\\gtrsim 1.5$ dex below a characteristic luminosity ($\\tilde{L}^\\star$) are essential to reducing biases at the faint end ($\\lesssim 0.1$ dex) and that wide-area data help constrain the bright end. As a proof of concept, we considered a combined sample of 1176 Lyman $$ emitters at redshift $3 < z < 5$ drawn from several MUSE surveys, ranging from ultra-deep ($\\gtrsim 90$ hr) and narrow ($\\lesssim 1$ arcmin$^2$) fields to shallow ($\\lesssim 5$ hr) and wide ($\\gtrsim 20$ arcmin$^2$) fields. With this complete sample, we constrain the luminosity function parameters $\\log(^\\star/\\mathrm{Mpc^{-3}}) = -2.86^{+0.15}_{-0.17}$, $\\log(L^\\star/\\mathrm{erg\\,s^{-1}}) = 42.72^{+0.10}_{-0.09}$, and $= -1.81^{+0.09}_{-0.09}$, where the uncertainties represent the $90\\%$ credible intervals. These values are in agreement with the results of studies based on gravitational lensing that reach $\\log(L/\\mathrm{erg\\,s^{-1}}) \\approx 41$, although differences in the faint-end slope underscore how systematic errors are starting to dominate. In contrast, wide-area surveys represent the natural extension needed to constrain the brightest Lyman $$ emitters [$\\log(L/\\mathrm{erg\\,s^{-1}}) \\gtrsim 43$], where statistical uncertainties still dominate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a hierarchical Bayesian framework designed to infer the luminosity function of any class of object by jointly modelling data from multiple surveys with varying depth, completeness, and sky coverage. Our method explicitly accounts for selection effects and measurement uncertainties (e.g. in luminosity) and can be generalized to any extensive quantity, such as mass. We validated the model using mock catalogues; from this we determined that deep data reaching $\\gtrsim 1.5$ dex below a characteristic luminosity ($\\tilde{L}^\\star$) are essential to reducing biases at the faint end ($\\lesssim 0.1$ dex) and that wide-area data help constrain the bright end. As a proof of concept, we considered a combined sample of 1176 Lyman $$ emitters at redshift $3 < z < 5$ drawn from several MUSE surveys, ranging from ultra-deep ($\\gtrsim 90$ hr) and narrow ($\\lesssim 1$ arcmin$^2$) fields to shallow ($\\lesssim 5$ hr) and wide ($\\gtrsim 20$ arcmin$^2$) fields. With this complete sample, we constrain the luminosity function parameters $\\log(^\\star/\\mathrm{Mpc^{-3}}) = -2.86^{+0.15}_{-0.17}$, $\\log(L^\\star/\\mathrm{erg\\,s^{-1}}) = 42.72^{+0.10}_{-0.09}$, and $= -1.81^{+0.09}_{-0.09}$, where the uncertainties represent the $90\\%$ credible intervals. These values are in agreement with the results of studies based on gravitational lensing that reach $\\log(L/\\mathrm{erg\\,s^{-1}}) \\approx 41$, although differences in the faint-end slope underscore how systematic errors are starting to dominate. In contrast, wide-area surveys represent the natural extension needed to constrain the brightest Lyman $$ emitters [$\\log(L/\\mathrm{erg\\,s^{-1}}) \\gtrsim 43$], where statistical uncertainties still dominate."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-11T18:04:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    4,
                    38,
                    2,
                    162,
                    0
                ],
                "arxiv_comment": "11 pages, 3 figures, 3 tables (in the main text). Published in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "arxiv_journal_ref": "A&A 704, A201 (2025)",
                "authors": [
                    {
                        "name": "Davide Tornotti"
                    },
                    {
                        "name": "Matteo Fossati"
                    },
                    {
                        "name": "Michele Fumagalli"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Lorenzo Pizzuti"
                    },
                    {
                        "name": "Fabrizio Arrigoni Battaia"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Arrigoni Battaia"
                },
                "author": "Fabrizio Arrigoni Battaia",
                "arxiv_doi": "10.1051/0004-6361/202555898"
            },
            {
                "id": "http://arxiv.org/abs/2512.08732v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08732v1",
                "title": "Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data"
                },
                "updated": "2025-12-09T15:44:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    44,
                    3,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08732v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:44:03Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    44,
                    3,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Udesh Habaraduwa"
                    },
                    {
                        "name": "Andrei Lixandru"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Lixandru"
                },
                "author": "Andrei Lixandru"
            },
            {
                "id": "http://arxiv.org/abs/2512.08731v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08731v1",
                "title": "LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design"
                },
                "updated": "2025-12-09T15:43:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    43,
                    24,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08731v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:43:24Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    43,
                    24,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Qipan Wang"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Shuangchen Li"
                    },
                    {
                        "name": "Hongzhong Zheng"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08724v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08724v1",
                "title": "Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search"
                },
                "updated": "2025-12-09T15:39:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    39,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08724v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:39:04Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    39,
                    4,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Manos Plitsis"
                    },
                    {
                        "name": "Giorgos Bouritsas"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    },
                    {
                        "name": "Yannis Panagakis"
                    }
                ],
                "author_detail": {
                    "name": "Yannis Panagakis"
                },
                "author": "Yannis Panagakis"
            },
            {
                "id": "http://arxiv.org/abs/2512.08716v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08716v1",
                "title": "Optical polarimetry of the accreting black hole X-ray binary Swift J1727.8$-$1613 over the state transition and radio ejections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical polarimetry of the accreting black hole X-ray binary Swift J1727.8$-$1613 over the state transition and radio ejections"
                },
                "updated": "2025-12-09T15:30:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    30,
                    50,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08716v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present the first optical ($BVR$) polarimetric observations of Swift J1727.8$-$1613 during its 2023--2024 outburst. Observations were performed during the X-ray hard-to-soft state transition, the soft state and the decaying hard state of the source. For the vast majority of nights, we detect statistically significant polarization of ${\\approx}1$\\%, a fraction of which is of interstellar origin. We find a significant change of polarization coinciding in time with discrete radio ejections. The direction of this polarization variation differs from the directions inferred from the X-ray, sub-mm and radio polarization angles, as well as from the resolved jet orientation. After correcting for the interstellar component, we find that the intrinsic polarization degree remained approximately constant at PD $\\approx 0.3$\\% throughout the hard-intermediate state. We explore several possible origins for the polarization and conclude that it is most plausibly produced by scattering within the optically thin accretion disk wind. The intrinsic polarization angle, PA $\\approx-15$, is notably offset from the jet axis, which we interpret as evidence for a misalignment between the black hole spin and the orbital axis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first optical ($BVR$) polarimetric observations of Swift J1727.8$-$1613 during its 2023--2024 outburst. Observations were performed during the X-ray hard-to-soft state transition, the soft state and the decaying hard state of the source. For the vast majority of nights, we detect statistically significant polarization of ${\\approx}1$\\%, a fraction of which is of interstellar origin. We find a significant change of polarization coinciding in time with discrete radio ejections. The direction of this polarization variation differs from the directions inferred from the X-ray, sub-mm and radio polarization angles, as well as from the resolved jet orientation. After correcting for the interstellar component, we find that the intrinsic polarization degree remained approximately constant at PD $\\approx 0.3$\\% throughout the hard-intermediate state. We explore several possible origins for the polarization and conclude that it is most plausibly produced by scattering within the optically thin accretion disk wind. The intrinsic polarization angle, PA $\\approx-15$, is notably offset from the jet axis, which we interpret as evidence for a misalignment between the black hole spin and the orbital axis."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:30:50Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    30,
                    50,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Submitted to Astronomy & Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Anagha P. Nitindala"
                    },
                    {
                        "name": "Alexandra Veledina"
                    },
                    {
                        "name": "Vadim Kravtsov"
                    },
                    {
                        "name": "Andrei V. Berdyugin"
                    },
                    {
                        "name": "Mara Alejandra Daz Teodori"
                    },
                    {
                        "name": "Vilppu Piirola"
                    },
                    {
                        "name": "Takeshi Sakanoi"
                    },
                    {
                        "name": "Masato Kagitani"
                    },
                    {
                        "name": "Svetlana V. Berdyugina"
                    },
                    {
                        "name": "Juri Poutanen"
                    }
                ],
                "author_detail": {
                    "name": "Juri Poutanen"
                },
                "author": "Juri Poutanen"
            },
            {
                "id": "http://arxiv.org/abs/2508.09369v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.09369v2",
                "title": "FedJam: Multimodal Federated Learning Framework for Jamming Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedJam: Multimodal Federated Learning Framework for Jamming Detection"
                },
                "updated": "2025-12-09T15:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    27,
                    16,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.09369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.09369v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Jamming attacks pose a critical threat to wireless networks, yet existing detection methods remain largely unimodal, centralized and resource-intensive, limiting their performance, scalability, and deployment feasibility, respectively. To address these limitations, we present FedJam, a multimodal Federated Learning (FL) framework for on-device jamming detection and classification. FedJam locally fuses spectrograms and cross-layer network Key Performance Indicators (KPIs) using a lightweight dual-encoder architecture with an integrated fusion module and multimodal projection head, that enables privacy-preserving training and inference without transmitting raw data. We prototype and deploy FedJam on a wireless experimental testbed and evaluate it using the first, over-the-air multimodal dataset comprising synchronized samples across benign and three distinct jamming attack types. FedJam outperforms state-of-the-art unimodal baselines by up to 15% in accuracy, while requiring 60% fewer communication rounds to converge, and maintains low resource utilization. Its advantage is especially pronounced in realistic scenarios, where it remains extremely robust under heterogeneous data distributions across devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jamming attacks pose a critical threat to wireless networks, yet existing detection methods remain largely unimodal, centralized and resource-intensive, limiting their performance, scalability, and deployment feasibility, respectively. To address these limitations, we present FedJam, a multimodal Federated Learning (FL) framework for on-device jamming detection and classification. FedJam locally fuses spectrograms and cross-layer network Key Performance Indicators (KPIs) using a lightweight dual-encoder architecture with an integrated fusion module and multimodal projection head, that enables privacy-preserving training and inference without transmitting raw data. We prototype and deploy FedJam on a wireless experimental testbed and evaluate it using the first, over-the-air multimodal dataset comprising synchronized samples across benign and three distinct jamming attack types. FedJam outperforms state-of-the-art unimodal baselines by up to 15% in accuracy, while requiring 60% fewer communication rounds to converge, and maintains low resource utilization. Its advantage is especially pronounced in realistic scenarios, where it remains extremely robust under heterogeneous data distributions across devices."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-12T21:52:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    21,
                    52,
                    0,
                    1,
                    224,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Ioannis Panitsas"
                    },
                    {
                        "name": "Iason Ofeidis"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas"
            },
            {
                "id": "http://arxiv.org/abs/2512.08706v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08706v1",
                "title": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing"
                },
                "updated": "2025-12-09T15:21:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    21,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08706v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:21:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    21,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted for ICSE 2026 Demonstration track",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Leon Kogler"
                    },
                    {
                        "name": "Maximilian Ehrhart"
                    },
                    {
                        "name": "Benedikt Dornauer"
                    },
                    {
                        "name": "Eduard Paul Enoiu"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Paul Enoiu"
                },
                "author": "Eduard Paul Enoiu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08700v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08700v1",
                "title": "Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth"
                },
                "updated": "2025-12-09T15:17:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    17,
                    20,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08700v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:17:20Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    17,
                    20,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted at IEEE Robotics and Automation Letters (RA-L) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kyumin Hwang"
                    },
                    {
                        "name": "Wonhyeok Choi"
                    },
                    {
                        "name": "Kiljoon Han"
                    },
                    {
                        "name": "Wonjoon Choi"
                    },
                    {
                        "name": "Minwoo Choi"
                    },
                    {
                        "name": "Yongcheon Na"
                    },
                    {
                        "name": "Minwoo Park"
                    },
                    {
                        "name": "Sunghoon Im"
                    }
                ],
                "author_detail": {
                    "name": "Sunghoon Im"
                },
                "author": "Sunghoon Im"
            },
            {
                "id": "http://arxiv.org/abs/2512.08692v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08692v1",
                "title": "A magnetar outburst with atypical evolution: the case of Swift J1555.2-5402",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A magnetar outburst with atypical evolution: the case of Swift J1555.2-5402"
                },
                "updated": "2025-12-09T15:10:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    10,
                    56,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08692v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The magnetar Swift J1555.2-5402 was discovered in outburst on 2021 June 3 by the Burst Alert Telescope on board the Swift satellite. Early X-ray follow-up revealed a spin period P~3.86 s, a period derivative Pdot~3e-11 s/s, dozens of short bursts, and an unusually flux decline. We report here on the X-ray monitoring of Swift J1555.2-5402 over the first ~29 months of its outburst with Swift, NICER, NuSTAR, INTEGRAL and Insight-HXMT, as well as radio observations with Parkes soon after the outburst onset. The observed 0.3-10 keV flux remained at levels >~1e-11 erg/cm^2/s for nearly 500 days before dropping by a factor of ~10 from its June 2021 peak towards the end of the monitoring campaign. During this time span, the spectrum was dominated by a single blackbody, with temperature attaining approximately a constant value (~1.2 keV) while the inferred radius shrank from ~1.7 km to ~0.3 km (assuming a source distance of 10 kpc). The long-term spin-down rate (Pdot~3.6e-11 s/s) is only ~15 % higher than that measured in the first 30 days. No periodic or burst-like radio emission was detected, in line with what has been previously reported using different radio facilities. The persistently high temperature, shrinking hotspot, and a prolonged bright flux plateau followed by a fast dimming observed during the outburst evolution pose a challenge for the outburst mechanisms proposed so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The magnetar Swift J1555.2-5402 was discovered in outburst on 2021 June 3 by the Burst Alert Telescope on board the Swift satellite. Early X-ray follow-up revealed a spin period P~3.86 s, a period derivative Pdot~3e-11 s/s, dozens of short bursts, and an unusually flux decline. We report here on the X-ray monitoring of Swift J1555.2-5402 over the first ~29 months of its outburst with Swift, NICER, NuSTAR, INTEGRAL and Insight-HXMT, as well as radio observations with Parkes soon after the outburst onset. The observed 0.3-10 keV flux remained at levels >~1e-11 erg/cm^2/s for nearly 500 days before dropping by a factor of ~10 from its June 2021 peak towards the end of the monitoring campaign. During this time span, the spectrum was dominated by a single blackbody, with temperature attaining approximately a constant value (~1.2 keV) while the inferred radius shrank from ~1.7 km to ~0.3 km (assuming a source distance of 10 kpc). The long-term spin-down rate (Pdot~3.6e-11 s/s) is only ~15 % higher than that measured in the first 30 days. No periodic or burst-like radio emission was detected, in line with what has been previously reported using different radio facilities. The persistently high temperature, shrinking hotspot, and a prolonged bright flux plateau followed by a fast dimming observed during the outburst evolution pose a challenge for the outburst mechanisms proposed so far."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:10:56Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    10,
                    56,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "14 pages including 2 appendices, 5 figures and 3 tables. Accepted on 8 December 2025 for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "A. Borghese"
                    },
                    {
                        "name": "F. Coti Zelati"
                    },
                    {
                        "name": "M. Imbrogno"
                    },
                    {
                        "name": "G. L. Israel"
                    },
                    {
                        "name": "D. De Grandis"
                    },
                    {
                        "name": "D. P. Pacholski"
                    },
                    {
                        "name": "M. Trudu"
                    },
                    {
                        "name": "M. Burgay"
                    },
                    {
                        "name": "S. Mereghetti"
                    },
                    {
                        "name": "N. Rea"
                    },
                    {
                        "name": "P. Esposito"
                    },
                    {
                        "name": "M. Pilia"
                    },
                    {
                        "name": "A. Possenti"
                    },
                    {
                        "name": "R. Turolla"
                    },
                    {
                        "name": "L. Ducci"
                    }
                ],
                "author_detail": {
                    "name": "L. Ducci"
                },
                "author": "L. Ducci"
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02943v6",
                "title": "Hallucination to Consensus: Multi-Agent LLMs for End-to-End Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination to Consensus: Multi-Agent LLMs for End-to-End Test Generation"
                },
                "updated": "2025-12-09T15:07:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    7,
                    34,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02943v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02943v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Unit testing plays a critical role in ensuring software correctness. However, writing unit tests manually is labor-intensive, especially for strongly typed languages like Java, motivating the need for automated approaches. Traditional methods primarily rely on search-based or randomized algorithms to achieve high code coverage and produce regression oracles, which are derived from the program's current behavior rather than its intended functionality. Recent advances in LLMs have enabled oracle generation from natural language descriptions, aligning better with user requirements. However, existing LLM-based methods often require fine-tuning or rely on external tools such as EvoSuite for test prefix generation, making them costly or cumbersome to apply in practice.\n  In this work, we propose CANDOR, a novel prompt engineering-based LLM framework for automated unit test generation in Java. CANDOR orchestrates multiple specialized LLM agents to collaboratively generate complete tests. To mitigate the notorious hallucinations in LLMs and improve oracle correctness, we introduce a novel strategy that engages multiple reasoning LLMs in a panel discussion and generates accurate oracles based on consensus. Additionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a novel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments show that CANDOR is comparable with EvoSuite in generating tests with high code coverage and clearly superior in terms of mutation score. Moreover, our prompt engineering-based approach CANDOR significantly outperforms the SOTA fine-tuning-based oracle generator TOGLL by at least 21.1 percentage points in oracle correctness on both correct and faulty source code. Further ablation studies confirm the critical contributions of key agents in generating high-quality tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However, writing unit tests manually is labor-intensive, especially for strongly typed languages like Java, motivating the need for automated approaches. Traditional methods primarily rely on search-based or randomized algorithms to achieve high code coverage and produce regression oracles, which are derived from the program's current behavior rather than its intended functionality. Recent advances in LLMs have enabled oracle generation from natural language descriptions, aligning better with user requirements. However, existing LLM-based methods often require fine-tuning or rely on external tools such as EvoSuite for test prefix generation, making them costly or cumbersome to apply in practice.\n  In this work, we propose CANDOR, a novel prompt engineering-based LLM framework for automated unit test generation in Java. CANDOR orchestrates multiple specialized LLM agents to collaboratively generate complete tests. To mitigate the notorious hallucinations in LLMs and improve oracle correctness, we introduce a novel strategy that engages multiple reasoning LLMs in a panel discussion and generates accurate oracles based on consensus. Additionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a novel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments show that CANDOR is comparable with EvoSuite in generating tests with high code coverage and clearly superior in terms of mutation score. Moreover, our prompt engineering-based approach CANDOR significantly outperforms the SOTA fine-tuning-based oracle generator TOGLL by at least 21.1 percentage points in oracle correctness on both correct and faulty source code. Further ablation studies confirm the critical contributions of key agents in generating high-quality tests."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu"
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26699v2",
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment"
                },
                "updated": "2025-12-09T15:06:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    6,
                    55,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26699v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median)."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "Accepted at 1st International Workshop on Agentic Engineering (AGENT 2026, colocated with ICSE), pages 1-5",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente"
            },
            {
                "id": "http://arxiv.org/abs/2512.08662v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08662v1",
                "title": "Spectroscopic readout of chiral photonic topology in a single-cavity spin-orbit-coupled Bose-Einstein condensate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectroscopic readout of chiral photonic topology in a single-cavity spin-orbit-coupled Bose-Einstein condensate"
                },
                "updated": "2025-12-09T14:50:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    50,
                    13,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08662v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Topological photonic phases are typically identified through band reconstruction, steady-state transmission, or real-space imaging of edge modes. In this work, we present a framework for spectroscopic readout of chiral photonic topology in a single driven optical cavity containing a spin-orbit-coupled Bose-Einstein condensate. We demonstrate that the cavity transmission power spectral density provides a direct and measurable proxy for a momentum- and frequency-resolved photonic Chern marker, enabling topological characteristics to be inferred from spectral data without the need for bulk-band tomography. In the loss-dominated regime, where cavity decay exceeds atomic dissipation, the power spectral density exhibits Dirac-like gapped hybrid modes with a vanishing Chern marker, indicating a trivial phase. When the dissipation imbalance is reversed, a bright, gap-spanning spectral ridge emerges, co-localized with peaks in both the Chern marker and Berry curvature. The complex spectrum reveals parity-time symmetric coalescences and gain-loss bifurcations, marking exceptional points and enabling chiral, gap-traversing transport. By linking noise spectroscopy to geometric and non-Hermitian topology in a minimal cavity-QED architecture, this work provides a framework for spectroscopic detection of topological order in driven quantum systems. This approach offers a pathway to compact, tunable topological photonics across a broad range of light-matter platforms, providing a method for the study and control of topological phases in hybrid quantum systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological photonic phases are typically identified through band reconstruction, steady-state transmission, or real-space imaging of edge modes. In this work, we present a framework for spectroscopic readout of chiral photonic topology in a single driven optical cavity containing a spin-orbit-coupled Bose-Einstein condensate. We demonstrate that the cavity transmission power spectral density provides a direct and measurable proxy for a momentum- and frequency-resolved photonic Chern marker, enabling topological characteristics to be inferred from spectral data without the need for bulk-band tomography. In the loss-dominated regime, where cavity decay exceeds atomic dissipation, the power spectral density exhibits Dirac-like gapped hybrid modes with a vanishing Chern marker, indicating a trivial phase. When the dissipation imbalance is reversed, a bright, gap-spanning spectral ridge emerges, co-localized with peaks in both the Chern marker and Berry curvature. The complex spectrum reveals parity-time symmetric coalescences and gain-loss bifurcations, marking exceptional points and enabling chiral, gap-traversing transport. By linking noise spectroscopy to geometric and non-Hermitian topology in a minimal cavity-QED architecture, this work provides a framework for spectroscopic detection of topological order in driven quantum systems. This approach offers a pathway to compact, tunable topological photonics across a broad range of light-matter platforms, providing a method for the study and control of topological phases in hybrid quantum systems."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:50:13Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    50,
                    13,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "29 pages, 7 Figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Kashif Ammar Yasir"
                    },
                    {
                        "name": "Gao Xianlong"
                    }
                ],
                "author_detail": {
                    "name": "Gao Xianlong"
                },
                "author": "Gao Xianlong"
            },
            {
                "id": "http://arxiv.org/abs/2511.11599v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11599v2",
                "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection"
                },
                "updated": "2025-12-09T14:42:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    42,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11599v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T09:27:36Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    9,
                    27,
                    36,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Arefeh Kazemi"
                    },
                    {
                        "name": "Hamza Qadeer"
                    },
                    {
                        "name": "Joachim Wagner"
                    },
                    {
                        "name": "Hossein Hosseini"
                    },
                    {
                        "name": "Sri Balaaji Natarajan Kalaivendan"
                    },
                    {
                        "name": "Brian Davis"
                    }
                ],
                "author_detail": {
                    "name": "Brian Davis"
                },
                "author": "Brian Davis"
            },
            {
                "id": "http://arxiv.org/abs/2512.08648v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08648v1",
                "title": "Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank"
                },
                "updated": "2025-12-09T14:39:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    39,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08648v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\\mname} achieves a state-of-the-art FID of \\textbf{2.40} within 400k steps, significantly outperforming comparable methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\\mname} achieves a state-of-the-art FID of \\textbf{2.40} within 400k steps, significantly outperforming comparable methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:39:26Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    39,
                    26,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "19 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Xuanqi Chen"
                    },
                    {
                        "name": "Ning Liao"
                    },
                    {
                        "name": "Haoxiang Zhao"
                    },
                    {
                        "name": "Xiaoxing Wang"
                    },
                    {
                        "name": "Haoru Tan"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Xiaosong Jia"
                    },
                    {
                        "name": "Qi Fan"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan"
            },
            {
                "id": "http://arxiv.org/abs/2512.08647v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08647v1",
                "title": "C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition"
                },
                "updated": "2025-12-09T14:35:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    35,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08647v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:35:35Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    35,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Keito Inoshita"
                    }
                ],
                "author_detail": {
                    "name": "Keito Inoshita"
                },
                "author": "Keito Inoshita"
            },
            {
                "id": "http://arxiv.org/abs/2512.08646v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08646v1",
                "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models"
                },
                "updated": "2025-12-09T14:35:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08646v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:35:26Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    26,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The Python package is available at https://github.com/dess-mannheim/QSTN/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maximilian Kreutner"
                    },
                    {
                        "name": "Jens Rupprecht"
                    },
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier"
            },
            {
                "id": "http://arxiv.org/abs/2512.08645v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08645v1",
                "title": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation"
                },
                "updated": "2025-12-09T14:35:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    12,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08645v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:35:12Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    12,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "19 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Young Kyung Kim"
                    },
                    {
                        "name": "Oded Schlesinger"
                    },
                    {
                        "name": "Yuzhou Zhao"
                    },
                    {
                        "name": "J. Matias Di Martino"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Sapiro"
                },
                "author": "Guillermo Sapiro"
            },
            {
                "id": "http://arxiv.org/abs/2509.25643v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25643v3",
                "title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models"
                },
                "updated": "2025-12-09T14:21:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    21,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25643v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T01:27:46Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    1,
                    27,
                    46,
                    1,
                    273,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Justin Chavarria"
                    },
                    {
                        "name": "Rohan Raizada"
                    },
                    {
                        "name": "Justin White"
                    },
                    {
                        "name": "Eyad Alhetairshi"
                    }
                ],
                "author_detail": {
                    "name": "Eyad Alhetairshi"
                },
                "author": "Eyad Alhetairshi"
            },
            {
                "id": "http://arxiv.org/abs/2506.11180v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.11180v2",
                "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing"
                },
                "updated": "2025-12-09T14:19:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    19,
                    21,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.11180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.11180v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ETFA65518.2025.11205601",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-12T13:02:16Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    2,
                    16,
                    3,
                    163,
                    0
                ],
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Kcher"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    }
                ],
                "author_detail": {
                    "name": "Felix Gehlhoff"
                },
                "author": "Felix Gehlhoff",
                "arxiv_doi": "10.1109/ETFA65518.2025.11205601"
            },
            {
                "id": "http://arxiv.org/abs/2512.08626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08626v1",
                "title": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR"
                },
                "updated": "2025-12-09T14:10:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:10:41Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Yuqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Zhou"
                },
                "author": "Yuqi Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.08617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08617v1",
                "title": "HealthcareNLP: where are we and what is next?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthcareNLP: where are we and what is next?"
                },
                "updated": "2025-12-09T14:01:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    1,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is \"Introductory to CL/NLP topics (HealthcareNLP)\" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is \"Introductory to CL/NLP topics (HealthcareNLP)\" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:01:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    1,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted Tutorial by LREC 2026 https://lrec2026.info/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lifeng Han"
                    },
                    {
                        "name": "Paul Rayson"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Andrew Moore"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic"
            },
            {
                "id": "http://arxiv.org/abs/2512.08609v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08609v1",
                "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models"
                },
                "updated": "2025-12-09T13:54:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    54,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08609v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:54:18Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    54,
                    18,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Chaoxu Mu"
                    }
                ],
                "author_detail": {
                    "name": "Chaoxu Mu"
                },
                "author": "Chaoxu Mu"
            },
            {
                "id": "http://arxiv.org/abs/2411.12950v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.12950v3",
                "title": "NumCoKE: Ordinal-Aware Numerical Reasoning over Knowledge Graphs with Mixture-of-Experts and Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NumCoKE: Ordinal-Aware Numerical Reasoning over Knowledge Graphs with Mixture-of-Experts and Contrastive Learning"
                },
                "updated": "2025-12-09T13:49:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    49,
                    33,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.12950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.12950v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Knowledge graphs (KGs) serve as a vital backbone for a wide range of AI applications, including natural language understanding and recommendation. A promising yet underexplored direction is numerical reasoning over KGs, which involves inferring new facts by leveraging not only symbolic triples but also numerical attribute values (e.g., length, weight). However, existing methods fall short in two key aspects: (1) Incomplete semantic integration: Most models struggle to jointly encode entities, relations, and numerical attributes in a unified representation space, limiting their ability to extract relation-aware semantics from numeric information. (2) Ordinal indistinguishability: Due to subtle differences between close values and sampling imbalance, models often fail to capture fine-grained ordinal relationships (e.g., longer, heavier), especially in the presence of hard negatives. To address these challenges, we propose NumCoKE, a numerical reasoning framework for KGs based on Mixture-of-Experts and Ordinal Contrastive Embedding. To overcome (C1), we introduce a Mixture-of-Experts Knowledge-Aware (MoEKA) encoder that jointly aligns symbolic and numeric components into a shared semantic space, while dynamically routing attribute features to relation-specific experts. To handle (C2), we propose Ordinal Knowledge Contrastive Learning (OKCL), which constructs ordinal-aware positive and negative samples using prior knowledge, enabling the model to better discriminate subtle semantic shifts. Extensive experiments on three public KG benchmarks demonstrate that NumCoKE consistently outperforms competitive baselines across diverse attribute distributions, validating its superiority in both semantic integration and ordinal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) serve as a vital backbone for a wide range of AI applications, including natural language understanding and recommendation. A promising yet underexplored direction is numerical reasoning over KGs, which involves inferring new facts by leveraging not only symbolic triples but also numerical attribute values (e.g., length, weight). However, existing methods fall short in two key aspects: (1) Incomplete semantic integration: Most models struggle to jointly encode entities, relations, and numerical attributes in a unified representation space, limiting their ability to extract relation-aware semantics from numeric information. (2) Ordinal indistinguishability: Due to subtle differences between close values and sampling imbalance, models often fail to capture fine-grained ordinal relationships (e.g., longer, heavier), especially in the presence of hard negatives. To address these challenges, we propose NumCoKE, a numerical reasoning framework for KGs based on Mixture-of-Experts and Ordinal Contrastive Embedding. To overcome (C1), we introduce a Mixture-of-Experts Knowledge-Aware (MoEKA) encoder that jointly aligns symbolic and numeric components into a shared semantic space, while dynamically routing attribute features to relation-specific experts. To handle (C2), we propose Ordinal Knowledge Contrastive Learning (OKCL), which constructs ordinal-aware positive and negative samples using prior knowledge, enabling the model to better discriminate subtle semantic shifts. Extensive experiments on three public KG benchmarks demonstrate that NumCoKE consistently outperforms competitive baselines across diverse attribute distributions, validating its superiority in both semantic integration and ordinal reasoning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-20T00:47:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    3,
                    2,
                    325,
                    0
                ],
                "arxiv_comment": "Update",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Zongsheng Cao"
                    },
                    {
                        "name": "Qiqing Xia"
                    },
                    {
                        "name": "Chenyang Tu"
                    },
                    {
                        "name": "Neng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Neng Gao"
                },
                "author": "Neng Gao"
            },
            {
                "id": "http://arxiv.org/abs/2512.08601v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08601v1",
                "title": "Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis"
                },
                "updated": "2025-12-09T13:40:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    40,
                    8,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08601v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:40:08Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    40,
                    8,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Orit Davidovich"
                    },
                    {
                        "name": "Shimrit Shtern"
                    },
                    {
                        "name": "Segev Wasserkrug"
                    },
                    {
                        "name": "Nimrod Megiddo"
                    }
                ],
                "author_detail": {
                    "name": "Nimrod Megiddo"
                },
                "author": "Nimrod Megiddo"
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v23",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2312.00326v23",
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "updated": "2025-12-09T13:31:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    31,
                    53,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2312.00326v23",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2312.00326v23",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "arxiv_comment": "31 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor"
            },
            {
                "id": "http://arxiv.org/abs/2505.08950v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.08950v2",
                "title": "The Economic Impact of Low- and High-Frequency Temperature Changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Economic Impact of Low- and High-Frequency Temperature Changes"
                },
                "updated": "2025-12-09T13:28:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    28,
                    44,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.08950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.08950v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Temperature variations at different frequencies may have distinct impacts on economic outcomes. We first explore ways to estimate the low- and high-frequency components in a U.S. panel of 48 states. All methods suggest slowly evolving low-frequency components of temperature at the state level, and that they share a common factor which covaries with the low-frequency component of economic activity. While we fail to find a statistically significant impact of low-frequency temperature changes on U.S. growth, an international panel of 50 countries suggests that a 1C increase in the low-frequency component will reduce economic growth by about one percent in the long run. The linear effect of the high-frequency component is not well determined in all panels, but there is evidence of a non-linear effect in the international panel. The findings are corroborated by time series estimation using data at the unit and national levels. Our empirical work pays attention to distortions that may arise from using one-way clustered errors for inference, and to the possible inadequacy of the additive fixed effect specification in controlling for common time effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temperature variations at different frequencies may have distinct impacts on economic outcomes. We first explore ways to estimate the low- and high-frequency components in a U.S. panel of 48 states. All methods suggest slowly evolving low-frequency components of temperature at the state level, and that they share a common factor which covaries with the low-frequency component of economic activity. While we fail to find a statistically significant impact of low-frequency temperature changes on U.S. growth, an international panel of 50 countries suggests that a 1C increase in the low-frequency component will reduce economic growth by about one percent in the long run. The linear effect of the high-frequency component is not well determined in all panels, but there is evidence of a non-linear effect in the international panel. The findings are corroborated by time series estimation using data at the unit and national levels. Our empirical work pays attention to distortions that may arise from using one-way clustered errors for inference, and to the possible inadequacy of the additive fixed effect specification in controlling for common time effects."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-13T20:28:12Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    28,
                    12,
                    1,
                    133,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Nikolay Gospodinov"
                    },
                    {
                        "name": "Ignacio Lopez Gaffney"
                    },
                    {
                        "name": "Serena Ng"
                    }
                ],
                "author_detail": {
                    "name": "Serena Ng"
                },
                "author": "Serena Ng"
            },
            {
                "id": "http://arxiv.org/abs/2412.08179v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.08179v3",
                "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis -- A Case Study for the Semiconductor Sector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis -- A Case Study for the Semiconductor Sector"
                },
                "updated": "2025-12-09T13:27:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    27,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.08179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.08179v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyzes require significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analysis through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a sector-specific financial instruction dataset derived from semiconductor industry documents to guide the LLM adaptation to specialized financial reasoning. Using NVIDIA, AMD, and Broadcom as representative companies, our case study demonstrates that RAG-IT substantially improves a general-purpose open-source LLM and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyzes require significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analysis through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a sector-specific financial instruction dataset derived from semiconductor industry documents to guide the LLM adaptation to specialized financial reasoning. Using NVIDIA, AMD, and Broadcom as representative companies, our case study demonstrates that RAG-IT substantially improves a general-purpose open-source LLM and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting."
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-11T08:09:42Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "arxiv_comment": "We updated title, abstract and added more details in experiment section. We also updated the list of authors",
                "arxiv_primary_category": {
                    "term": "q-fin.ST"
                },
                "authors": [
                    {
                        "name": "Hai-Thien To"
                    },
                    {
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "name": "Van-Duc Le"
                    }
                ],
                "author_detail": {
                    "name": "Van-Duc Le"
                },
                "author": "Van-Duc Le"
            },
            {
                "id": "http://arxiv.org/abs/2401.15378v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2401.15378v6",
                "title": "Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS"
                },
                "updated": "2025-12-09T13:24:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    24,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2401.15378v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2401.15378v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Religious teachings can sometimes be complex and challenging to grasp, but chatbots can serve as effective assistants in this domain. Large Language Model (LLM) based chatbots, powered by Natural Language Processing (NLP), can connect related topics and provide well-supported responses to intricate questions, making them valuable tools for religious education. However, LLMs are prone to hallucinations as they can generate inaccurate or irrelevant information, and these can include sensitive content that could be offensive, inappropriate, or controversial. Addressing such topics without inadvertently promoting hate speech or disrespecting certain beliefs remains a significant challenge. As a solution to these issues, we introduce MufassirQAS, a system that enhances LLM accuracy and transparency using a vector database-driven Retrieval-Augmented Generation (RAG) approach. We built a dataset comprising fundamental books containing Turkish translations and interpretations of Islamic texts. This database is leveraged to answer religious inquiries while ensuring that responses remain reliable and contextually grounded. Our system also presents the relevant dataset sections alongside the LLM-generated answers, reinforcing transparency. We carefully designed system prompts to prevent harmful, offensive, or disrespectful outputs, ensuring that responses align with ethical and respectful discourse. Moreover, MufassirQAS provides supplementary details, such as source page numbers and referenced articles, to enhance credibility. To evaluate its effectiveness, we tested MufassirQAS against ChatGPT with sensitive questions, and our system demonstrated superior performance in maintaining accuracy and reliability. Future work will focus on improving accuracy and refining prompt engineering techniques to further minimize biases and ensure even more reliable responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Religious teachings can sometimes be complex and challenging to grasp, but chatbots can serve as effective assistants in this domain. Large Language Model (LLM) based chatbots, powered by Natural Language Processing (NLP), can connect related topics and provide well-supported responses to intricate questions, making them valuable tools for religious education. However, LLMs are prone to hallucinations as they can generate inaccurate or irrelevant information, and these can include sensitive content that could be offensive, inappropriate, or controversial. Addressing such topics without inadvertently promoting hate speech or disrespecting certain beliefs remains a significant challenge. As a solution to these issues, we introduce MufassirQAS, a system that enhances LLM accuracy and transparency using a vector database-driven Retrieval-Augmented Generation (RAG) approach. We built a dataset comprising fundamental books containing Turkish translations and interpretations of Islamic texts. This database is leveraged to answer religious inquiries while ensuring that responses remain reliable and contextually grounded. Our system also presents the relevant dataset sections alongside the LLM-generated answers, reinforcing transparency. We carefully designed system prompts to prevent harmful, offensive, or disrespectful outputs, ensuring that responses align with ethical and respectful discourse. Moreover, MufassirQAS provides supplementary details, such as source page numbers and referenced articles, to enhance credibility. To evaluate its effectiveness, we tested MufassirQAS against ChatGPT with sensitive questions, and our system demonstrated superior performance in maintaining accuracy and reliability. Future work will focus on improving accuracy and refining prompt engineering techniques to further minimize biases and ensure even more reliable responses."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-01-27T10:50:11Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    10,
                    50,
                    11,
                    5,
                    27,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ahmet Yusuf Alan"
                    },
                    {
                        "name": "Enis Karaarslan"
                    },
                    {
                        "name": "Omer Aydin"
                    }
                ],
                "author_detail": {
                    "name": "Omer Aydin"
                },
                "author": "Omer Aydin"
            },
            {
                "id": "http://arxiv.org/abs/2512.06749v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06749v2",
                "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems"
                },
                "updated": "2025-12-09T13:22:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    22,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06749v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T09:23:48Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    9,
                    23,
                    48,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ming Ma"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Tianming Yang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2508.07499v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.07499v2",
                "title": "Comparing a Compact-Binary Mass-Shell Model with Select Observed Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing a Compact-Binary Mass-Shell Model with Select Observed Gravitational Waves"
                },
                "updated": "2025-12-09T13:19:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    19,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.07499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.07499v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In a recent work, coalescing compact binaries (CCBs) were modeled as a rotating and contracting compact mass shell, providing an alternative effective representation to the state-of-the-art effective-one-body approach. Using a variational methodology, the Laplace-Beltrami formulation of the Ricci tensor was applied to a Kerr metric Ansatz, and the corresponding energy density $T_{00}$ of the CCB mass shell was obtained via the Einstein field equations. At the time of coalescence $t_C$, the resulting surface energy depends on the reduced mass $$, the symmetric mass ratio $$, and the normalized orbital spin velocity of the CCB. In this work, we evaluate the radiated energy predicted by this variational approach for 45 select gravitational wave (GW) events from the O1--O4 runs, and compare these values with those inferred from observational catalogs, either directly or via the total-minus-remnant mass difference. For 38 of the 45 events analyzed, the predicted radiated energies agree with observationally inferred values within the reported uncertainties, with 1:1 ratios spanning from $0.828$ to $0.997$ (mean $0.942$, median $0.955$). Three events exhibit ratios in the range $0.721\\sim0.779$, one event yields a ratio of $0.466$, and for the remaining events the radiated energy is either unconstrained or inaccessible due to undocumented total-minus-remnant mass differences. These results indicate that the analytical approximation captures, for the most part, the leading-order energy scaling of compact binary mergers, while also suggesting clear avenues for further systematic improvement, including incorporating post-Newtonian corrections due to e.g. a non-zero eccentricity or combined tidal deformability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a recent work, coalescing compact binaries (CCBs) were modeled as a rotating and contracting compact mass shell, providing an alternative effective representation to the state-of-the-art effective-one-body approach. Using a variational methodology, the Laplace-Beltrami formulation of the Ricci tensor was applied to a Kerr metric Ansatz, and the corresponding energy density $T_{00}$ of the CCB mass shell was obtained via the Einstein field equations. At the time of coalescence $t_C$, the resulting surface energy depends on the reduced mass $$, the symmetric mass ratio $$, and the normalized orbital spin velocity of the CCB. In this work, we evaluate the radiated energy predicted by this variational approach for 45 select gravitational wave (GW) events from the O1--O4 runs, and compare these values with those inferred from observational catalogs, either directly or via the total-minus-remnant mass difference. For 38 of the 45 events analyzed, the predicted radiated energies agree with observationally inferred values within the reported uncertainties, with 1:1 ratios spanning from $0.828$ to $0.997$ (mean $0.942$, median $0.955$). Three events exhibit ratios in the range $0.721\\sim0.779$, one event yields a ratio of $0.466$, and for the remaining events the radiated energy is either unconstrained or inaccessible due to undocumented total-minus-remnant mass differences. These results indicate that the analytical approximation captures, for the most part, the leading-order energy scaling of compact binary mergers, while also suggesting clear avenues for further systematic improvement, including incorporating post-Newtonian corrections due to e.g. a non-zero eccentricity or combined tidal deformability."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-10T22:12:00Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    22,
                    12,
                    0,
                    6,
                    222,
                    0
                ],
                "arxiv_comment": "(Version 2: IOP) 26 pages, 4 tables",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Noah M. MacKay"
                    }
                ],
                "author_detail": {
                    "name": "Noah M. MacKay"
                },
                "author": "Noah M. MacKay"
            },
            {
                "id": "http://arxiv.org/abs/2512.08580v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08580v1",
                "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning"
                },
                "updated": "2025-12-09T13:19:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    19,
                    37,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08580v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:19:37Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    19,
                    37,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "49 pages, 25 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Peijun Tang"
                    },
                    {
                        "name": "Shangjin Xie"
                    },
                    {
                        "name": "Binyan Sun"
                    },
                    {
                        "name": "Baifu Huang"
                    },
                    {
                        "name": "Kuncheng Luo"
                    },
                    {
                        "name": "Haotian Yang"
                    },
                    {
                        "name": "Weiqi Jin"
                    },
                    {
                        "name": "Jianan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianan Wang"
                },
                "author": "Jianan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08559v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08559v1",
                "title": "Bayesian Extraction of HQET Parameters from Inclusive Semi-Leptonic Decay of the $_{c}^{+}$ Baryon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Extraction of HQET Parameters from Inclusive Semi-Leptonic Decay of the $_{c}^{+}$ Baryon"
                },
                "updated": "2025-12-09T13:00:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    0,
                    47,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08559v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We extract the non-perturbative Heavy Quark Effective Theory (HQET) parameters from the inclusive semi-leptonic decay $_c^+ \\to X e^+ _e$. Unlike charmed mesons produced near threshold, $_c^+$ baryons produced in $e^+e^-$ annihilation exhibit a complex momentum distribution, making the transformation of the electron energy spectrum from the laboratory frame to the $_c^+$ rest frame non-trivial. To address this, we develop a novel Bayesian inference method to reconstruct the electron energy moments in the $_c^+$ rest frame. By performing a global fit of theoretical predictions in the 1S mass scheme to these extracted moments, we determine the HQET parameters $_^2(_c^+)$ and $_D^3(_c^+)$ for the first time using a purely data-driven approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extract the non-perturbative Heavy Quark Effective Theory (HQET) parameters from the inclusive semi-leptonic decay $_c^+ \\to X e^+ _e$. Unlike charmed mesons produced near threshold, $_c^+$ baryons produced in $e^+e^-$ annihilation exhibit a complex momentum distribution, making the transformation of the electron energy spectrum from the laboratory frame to the $_c^+$ rest frame non-trivial. To address this, we develop a novel Bayesian inference method to reconstruct the electron energy moments in the $_c^+$ rest frame. By performing a global fit of theoretical predictions in the 1S mass scheme to these extracted moments, we determine the HQET parameters $_^2(_c^+)$ and $_D^3(_c^+)$ for the first time using a purely data-driven approach."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:00:47Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    0,
                    47,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Kangkang Shao"
                    },
                    {
                        "name": "Dong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xiao"
                },
                "author": "Dong Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2510.03291v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03291v2",
                "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs"
                },
                "updated": "2025-12-09T12:42:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    42,
                    46,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03291v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T13:38:28Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    38,
                    28,
                    0,
                    272,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yizhuo Ding"
                    },
                    {
                        "name": "Wanying Qu"
                    },
                    {
                        "name": "Jiawei Geng"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08543v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08543v1",
                "title": "High-Redshift Galactic Outflows: Orientation Effects, Kinematics, and Metallicity in TNG50 and SERRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Redshift Galactic Outflows: Orientation Effects, Kinematics, and Metallicity in TNG50 and SERRA"
                },
                "updated": "2025-12-09T12:39:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    39,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08543v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context: Recently, JWST/NIRSpec observations have provided the first detections of warm ionised outflows in low-mass galaxies at high redshifts (z>3), revealing an occurrence rate of 25-40% depending on the intensity of the emission lines. This fraction is lower than predicted by simulations, which suggest that fast outflowing gas should be a common feature of all star-forming galaxies in the early Universe. Aims: In order to better understand the discrepancies between simulations and observations, we identify and characterize outflows in high-redshift galaxies using the TNG50 cosmological and SERRA zoom-in simulations. Our study examines how outflow detectability depends on the line of sight, explores the properties of the fast gas, and investigates its relationship with key galactic properties. Methods: We analyse approximately 60000 galaxies from TNG50 and 3000 galaxies from SERRA over the redshift ranges z=3-5 and z=4-5, respectively, spanning stellar masses of Mstar=10^7.5-10^11Msun. Outflows in the immediate vicinity of each galaxy are identified using a Gaussian mixture model algorithm that uses the gas velocity, star-formation-rate, and location as input parameters. We subsequently compare the simulated outflows to those observed in the JWST/JADES NIRSpec survey. Results: Outflow masses in both TNG50 and SERRA broadly reproduce the JWST/JADES measurements within roughly 0.5dex, though simulations tend to predict slightly higher values, suggesting that optical emission lines capture only a fraction of the multiphase outflow. However, simulated outflow velocities are typically an order of magnitude lower than those inferred from observations. TNG50 indicates a clear orientation dependence as outflows in face-on galaxies are approximately 15% more likely to be detected than in edge-on systems, with this difference increasing to nearly 40% for more massive, disc-shaped galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Recently, JWST/NIRSpec observations have provided the first detections of warm ionised outflows in low-mass galaxies at high redshifts (z>3), revealing an occurrence rate of 25-40% depending on the intensity of the emission lines. This fraction is lower than predicted by simulations, which suggest that fast outflowing gas should be a common feature of all star-forming galaxies in the early Universe. Aims: In order to better understand the discrepancies between simulations and observations, we identify and characterize outflows in high-redshift galaxies using the TNG50 cosmological and SERRA zoom-in simulations. Our study examines how outflow detectability depends on the line of sight, explores the properties of the fast gas, and investigates its relationship with key galactic properties. Methods: We analyse approximately 60000 galaxies from TNG50 and 3000 galaxies from SERRA over the redshift ranges z=3-5 and z=4-5, respectively, spanning stellar masses of Mstar=10^7.5-10^11Msun. Outflows in the immediate vicinity of each galaxy are identified using a Gaussian mixture model algorithm that uses the gas velocity, star-formation-rate, and location as input parameters. We subsequently compare the simulated outflows to those observed in the JWST/JADES NIRSpec survey. Results: Outflow masses in both TNG50 and SERRA broadly reproduce the JWST/JADES measurements within roughly 0.5dex, though simulations tend to predict slightly higher values, suggesting that optical emission lines capture only a fraction of the multiphase outflow. However, simulated outflow velocities are typically an order of magnitude lower than those inferred from observations. TNG50 indicates a clear orientation dependence as outflows in face-on galaxies are approximately 15% more likely to be detected than in edge-on systems, with this difference increasing to nearly 40% for more massive, disc-shaped galaxies."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:39:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    39,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "14 pages, 12 figures. Submitted to A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Ivan Kostyuk"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Mahsa Kohandel"
                    },
                    {
                        "name": "Andrea Pallottini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Pallottini"
                },
                "author": "Andrea Pallottini"
            },
            {
                "id": "http://arxiv.org/abs/2512.08537v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08537v1",
                "title": "Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation"
                },
                "updated": "2025-12-09T12:35:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    35,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08537v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\\times$256, TransDiff attains 4.3$\\times$ lossless speedup, and NextStep-1 achieves 3$\\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\\times$256, TransDiff attains 4.3$\\times$ lossless speedup, and NextStep-1 achieves 3$\\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:35:18Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    35,
                    18,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.08536v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08536v1",
                "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans"
                },
                "updated": "2025-12-09T12:34:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    34,
                    54,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08536v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:34:54Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    34,
                    54,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tammy Zhong"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Maurice Pagnucco"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Pagnucco"
                },
                "author": "Maurice Pagnucco"
            },
            {
                "id": "http://arxiv.org/abs/2512.08534v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08534v1",
                "title": "PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation"
                },
                "updated": "2025-12-09T12:31:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    31,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08534v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:31:00Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    31,
                    0,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "14 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhangli Hu"
                    },
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Jiajun Yao"
                    },
                    {
                        "name": "Bingbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Bingbing Ni"
                },
                "author": "Bingbing Ni"
            },
            {
                "id": "http://arxiv.org/abs/2510.16809v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.16809v2",
                "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation"
                },
                "updated": "2025-12-09T12:20:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    20,
                    58,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.16809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.16809v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-19T12:29:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    29,
                    13,
                    6,
                    292,
                    0
                ],
                "arxiv_comment": "Accepted to ICSE 2026 (RECODE workshop)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Amirkia Rafiei Oskooei"
                    },
                    {
                        "name": "Kaan Baturalp Cosdan"
                    },
                    {
                        "name": "Husamettin Isiktas"
                    },
                    {
                        "name": "Mehmet S. Aktas"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet S. Aktas"
                },
                "author": "Mehmet S. Aktas"
            },
            {
                "id": "http://arxiv.org/abs/2511.22334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22334v2",
                "title": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends"
                },
                "updated": "2025-12-09T12:19:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    19,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:11:01Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    11,
                    1,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "8 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Pablo Prieto"
                    },
                    {
                        "name": "Pablo Abad"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Abad"
                },
                "author": "Pablo Abad"
            },
            {
                "id": "http://arxiv.org/abs/2512.08529v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08529v1",
                "title": "MVP: Multiple View Prediction Improves GUI Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVP: Multiple View Prediction Improves GUI Grounding"
                },
                "updated": "2025-12-09T12:19:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    19,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08529v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:19:00Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    19,
                    0,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunzhu Zhang"
                    },
                    {
                        "name": "Zeyu Pan"
                    },
                    {
                        "name": "Zhengwen Zeng"
                    },
                    {
                        "name": "Shuheng Shen"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Linchao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Linchao Zhu"
                },
                "author": "Linchao Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08524v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08524v1",
                "title": "Beyond Real Weights: Hypercomplex Representations for Stable Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Real Weights: Hypercomplex Representations for Stable Quantization"
                },
                "updated": "2025-12-09T12:10:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    10,
                    57,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08524v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:10:57Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    10,
                    57,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted in Winter Conference on Applications of Computer Vision (WACV) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jawad Ibn Ahad"
                    },
                    {
                        "name": "Maisha Rahman"
                    },
                    {
                        "name": "Amrijit Biswas"
                    },
                    {
                        "name": "Muhammad Rafsan Kabir"
                    },
                    {
                        "name": "Robin Krambroeckers"
                    },
                    {
                        "name": "Sifat Momen"
                    },
                    {
                        "name": "Nabeel Mohammed"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman"
            },
            {
                "id": "http://arxiv.org/abs/2512.08521v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08521v1",
                "title": "Time-Averaged Template for Stochastic Gravitational-Wave Background Detection in Space-Based Interferometers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Averaged Template for Stochastic Gravitational-Wave Background Detection in Space-Based Interferometers"
                },
                "updated": "2025-12-09T12:08:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    8,
                    57,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08521v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Stochastic gravitational-wave background (SGWB) poses significant challenges for data analysis and parameter inference in future space-based gravitational-wave missions, such as LISA and Taiji, as it appears as an additional stochastic component along with instrumental noise. Previous studies have developed various approaches to distinguish the SGWB from instrumental noise, often under simplified assumptions such as static or equal-arm configurations. However, in realistic scenarios, time-varying arm-lengths introduce additional complexities that require careful modeling. In this work, we investigate the impact of template construction on SGWB parameter estimation under realistic orbital configurations. Using the simulated SGWB signals and dominant instrumental noise sources, we compare three template strategies: time-averaged template constructed from segmented data, equal-arm template, and a template treating the arm-lengths as a free parameter. Our results show that the time-averaged template yield improves parameter estimation accuracy under time-varying arm-lengths, whereas introducing the effective arm-length as a free parameter increases estimation uncertainty. These findings highlight the importance of realistic template construction for high-precision SGWB analysis in future space-based missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic gravitational-wave background (SGWB) poses significant challenges for data analysis and parameter inference in future space-based gravitational-wave missions, such as LISA and Taiji, as it appears as an additional stochastic component along with instrumental noise. Previous studies have developed various approaches to distinguish the SGWB from instrumental noise, often under simplified assumptions such as static or equal-arm configurations. However, in realistic scenarios, time-varying arm-lengths introduce additional complexities that require careful modeling. In this work, we investigate the impact of template construction on SGWB parameter estimation under realistic orbital configurations. Using the simulated SGWB signals and dominant instrumental noise sources, we compare three template strategies: time-averaged template constructed from segmented data, equal-arm template, and a template treating the arm-lengths as a free parameter. Our results show that the time-averaged template yield improves parameter estimation accuracy under time-varying arm-lengths, whereas introducing the effective arm-length as a free parameter increases estimation uncertainty. These findings highlight the importance of realistic template construction for high-precision SGWB analysis in future space-based missions."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:08:57Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    8,
                    57,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "1+20 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Jing-yi Wu"
                    },
                    {
                        "name": "Yong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Tang"
                },
                "author": "Yong Tang"
            },
            {
                "id": "http://arxiv.org/abs/2509.23823v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23823v2",
                "title": "Control Your Robot: A Unified System for Robot Control and Policy Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Your Robot: A Unified System for Robot Control and Policy Deployment"
                },
                "updated": "2025-12-09T12:08:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    8,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23823v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T11:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    51,
                    29,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/Tian-Nian/control_your_robot",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Tian Nian"
                    },
                    {
                        "name": "Weijie Ke"
                    },
                    {
                        "name": "Shaolong Zhu"
                    },
                    {
                        "name": "Bingshan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bingshan Hu"
                },
                "author": "Bingshan Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13888v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13888v2",
                "title": "Tractable Probabilistic Models for Investment Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tractable Probabilistic Models for Investment Planning"
                },
                "updated": "2025-12-09T11:52:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    52,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13888v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.\n  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the effectiveness of the approach through a representative power system planning case study, illustrating its computational and reliability advantages over traditional scenario-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.\n  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the effectiveness of the approach through a representative power system planning case study, illustrating its computational and reliability advantages over traditional scenario-based models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T20:23:34Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    20,
                    23,
                    34,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nicolas M. Cuadrado A."
                    },
                    {
                        "name": "Mohannad Takrouri"
                    },
                    {
                        "name": "Ji Nmeek"
                    },
                    {
                        "name": "Martin Tak"
                    },
                    {
                        "name": "Jakub Mareek"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Mareek"
                },
                "author": "Jakub Mareek"
            },
            {
                "id": "http://arxiv.org/abs/2512.08503v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08503v1",
                "title": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models"
                },
                "updated": "2025-12-09T11:35:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    35,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08503v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T11:35:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    35,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Che Wang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Wei Yang Bryan Lim"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang Bryan Lim"
                },
                "author": "Wei Yang Bryan Lim"
            },
            {
                "id": "http://arxiv.org/abs/2507.06277v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.06277v3",
                "title": "The Prompt War: How AI Decides on a Military Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt War: How AI Decides on a Military Intervention"
                },
                "updated": "2025-12-09T11:25:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    25,
                    22,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.06277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.06277v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Which factors determine AI's propensity to support military intervention? While the use of AI in high-stakes decision-making is growing exponentially, we still lack systematic analysis of the key drivers embedded in these models. This paper conducts a conjoint experiment in which large language models (LLMs) from leading providers (OpenAI, Anthropic, Google) are asked to decide on military intervention across 128 vignettes, with each vignette run 10 times. This design enables a systematic assessment of AI decision-making in military contexts. The results are remarkably consistent across models: all models place substantial weight on the probability of success and domestic support, prioritizing these factors over civilian casualties, economic shock, or international sanctions. The paper then tests whether LLMs are sensitive to context by introducing different motivations for intervention. The scoring is indeed context-dependent; however, probability of victory remains the most important factor in all scenarios. Finally, the paper evaluates numerical sensitivity and finds that models display some responsiveness to the scale of civilian casualties but no detectable sensitivity to the size of the economic shock.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which factors determine AI's propensity to support military intervention? While the use of AI in high-stakes decision-making is growing exponentially, we still lack systematic analysis of the key drivers embedded in these models. This paper conducts a conjoint experiment in which large language models (LLMs) from leading providers (OpenAI, Anthropic, Google) are asked to decide on military intervention across 128 vignettes, with each vignette run 10 times. This design enables a systematic assessment of AI decision-making in military contexts. The results are remarkably consistent across models: all models place substantial weight on the probability of success and domestic support, prioritizing these factors over civilian casualties, economic shock, or international sanctions. The paper then tests whether LLMs are sensitive to context by introducing different motivations for intervention. The scoring is indeed context-dependent; however, probability of victory remains the most important factor in all scenarios. Finally, the paper evaluates numerical sensitivity and finds that models display some responsiveness to the scale of civilian casualties but no detectable sensitivity to the size of the economic shock."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-08T12:52:08Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    52,
                    8,
                    1,
                    189,
                    0
                ],
                "arxiv_comment": "22 pages, 4 tables, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2507.15771v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.15771v2",
                "title": "Left Leaning Models: How AI Evaluates Economic Policy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left Leaning Models: How AI Evaluates Economic Policy?"
                },
                "updated": "2025-12-09T11:17:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    17,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.15771v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Would artificial intelligence (AI) cut interest rates or adopt conservative monetary policy? Would it deregulate or opt for a more controlled economy? As AI use by economic policymakers, academics, and market participants grows exponentially, it is becoming critical to understand AI preferences over economic policy. However, these preferences are not yet systematically evaluated and remain a black box. This paper makes a conjoint experiment on leading large language models (LLMs) from OpenAI, Anthropic, and Google, asking them to evaluate economic policy under multi-factor constraints. The results are remarkably consistent across models: most LLMs exhibit a strong preference for high growth, low unemployment, and low inequality over traditional macroeconomic concerns such as low inflation and low public debt. Scenario-specific experiments show that LLMs are sensitive to context but still display strong preferences for low unemployment and low inequality even in monetary-policy settings. Numerical sensitivity tests reveal intuitive responses to quantitative changes but also uncover non-linear patterns such as loss aversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would artificial intelligence (AI) cut interest rates or adopt conservative monetary policy? Would it deregulate or opt for a more controlled economy? As AI use by economic policymakers, academics, and market participants grows exponentially, it is becoming critical to understand AI preferences over economic policy. However, these preferences are not yet systematically evaluated and remain a black box. This paper makes a conjoint experiment on leading large language models (LLMs) from OpenAI, Anthropic, and Google, asking them to evaluate economic policy under multi-factor constraints. The results are remarkably consistent across models: most LLMs exhibit a strong preference for high growth, low unemployment, and low inequality over traditional macroeconomic concerns such as low inflation and low public debt. Scenario-specific experiments show that LLMs are sensitive to context but still display strong preferences for low unemployment and low inequality even in monetary-policy settings. Numerical sensitivity tests reveal intuitive responses to quantitative changes but also uncover non-linear patterns such as loss aversion."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-21T16:27:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "arxiv_comment": "16 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2512.08493v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08493v1",
                "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?"
                },
                "updated": "2025-12-09T11:15:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    15,
                    13,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08493v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T11:15:13Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    15,
                    13,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "6 pages, Submitted to ESAAN 2026, Under pier review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Dyna Soumhane Ouchebara"
                    },
                    {
                        "name": "Stphane Dupont"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Dupont"
                },
                "author": "Stphane Dupont"
            },
            {
                "id": "http://arxiv.org/abs/2511.14295v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14295v2",
                "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models"
                },
                "updated": "2025-12-09T11:10:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    10,
                    7,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14295v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T09:47:01Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    9,
                    47,
                    1,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Zbeeb"
                    },
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Sina Mukalled"
                    },
                    {
                        "name": "Nadine Rizk"
                    },
                    {
                        "name": "Fatima Karnib"
                    },
                    {
                        "name": "Issam Lakkis"
                    },
                    {
                        "name": "Ammar Mohanna"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.08918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08918v1",
                "title": "Improved Pseudorandom Codes from Permuted Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Pseudorandom Codes from Permuted Puzzles"
                },
                "updated": "2025-12-09T18:53:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    53,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.\n  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.\n  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.\n  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.\n  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:53:42Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    53,
                    42,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Miranda Christ"
                    },
                    {
                        "name": "Noah Golowich"
                    },
                    {
                        "name": "Sam Gunn"
                    },
                    {
                        "name": "Ankur Moitra"
                    },
                    {
                        "name": "Daniel Wichs"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Wichs"
                },
                "author": "Daniel Wichs"
            },
            {
                "id": "http://arxiv.org/abs/2512.08906v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08906v1",
                "title": "The impact of lunar topography on the 21-cm power spectrum for grid-based arrays : Insights for the Dark-ages EXplorer (DEX)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of lunar topography on the 21-cm power spectrum for grid-based arrays : Insights for the Dark-ages EXplorer (DEX)"
                },
                "updated": "2025-12-09T18:45:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    45,
                    6,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08906v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Dark Ages (DA) provides a crucial window into the physics of the infant Universe, with the 21-cm signal offering the only direct probe for mapping out the three-dimensional distribution of matter at this epoch. To measure this cosmological signal, the Dark-ages EXplorer (DEX) has been proposed as a compact, grid-based radio array on the lunar farside. The minimal design consists of a 32 $\\times$ 32 array of 3-m dipole antennas, operating in the $7 - 50$ MHz band. A practical challenge on the lunar surface is that the antennas may get displaced from their intended positions due to deployment imprecisions and non-coplanarity arising from local surface undulations. We present, for the first time, an end-to-end simulation pipeline, called SPADE-21cm, that integrates a sky model with a DA 21-cm signal model simulated in the lunar frame and incorporating lunar topography data. We study the effects of both lateral (xy) and vertical (z) offsets on the two-dimensional power spectra across the $7 - 12$ MHz and $30 - 35$ MHz spectral windows, with tolerance thresholds derived only for the latter. Our results show that positional offsets bias the power spectrum by $10 - 30$ per cent relative to the expected 21-cm power spectrum during DA. Lateral offsets within $_{xy}/\\lesssim 0.027$ (at 32.5 MHz) keep the fraction of Fourier modes with strong contamination (> 50 per cent of the signal) to less than 1 per cent, whereas vertical height offsets affect a larger fraction. This conclusion holds for the 21-cm window with $k_\\parallel > 0.5$ $h$ cMpc$^{-1}$ over the range of $k_\\perp = 0.003 - 0.009$ $h$ cMpc$^{-1}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Ages (DA) provides a crucial window into the physics of the infant Universe, with the 21-cm signal offering the only direct probe for mapping out the three-dimensional distribution of matter at this epoch. To measure this cosmological signal, the Dark-ages EXplorer (DEX) has been proposed as a compact, grid-based radio array on the lunar farside. The minimal design consists of a 32 $\\times$ 32 array of 3-m dipole antennas, operating in the $7 - 50$ MHz band. A practical challenge on the lunar surface is that the antennas may get displaced from their intended positions due to deployment imprecisions and non-coplanarity arising from local surface undulations. We present, for the first time, an end-to-end simulation pipeline, called SPADE-21cm, that integrates a sky model with a DA 21-cm signal model simulated in the lunar frame and incorporating lunar topography data. We study the effects of both lateral (xy) and vertical (z) offsets on the two-dimensional power spectra across the $7 - 12$ MHz and $30 - 35$ MHz spectral windows, with tolerance thresholds derived only for the latter. Our results show that positional offsets bias the power spectrum by $10 - 30$ per cent relative to the expected 21-cm power spectrum during DA. Lateral offsets within $_{xy}/\\lesssim 0.027$ (at 32.5 MHz) keep the fraction of Fourier modes with strong contamination (> 50 per cent of the signal) to less than 1 per cent, whereas vertical height offsets affect a larger fraction. This conclusion holds for the 21-cm window with $k_\\parallel > 0.5$ $h$ cMpc$^{-1}$ over the range of $k_\\perp = 0.003 - 0.009$ $h$ cMpc$^{-1}$."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:45:06Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    45,
                    6,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "27 pages, 12 figures, Under review in MNRAS. Comments are welcome!",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "S. Ghosh"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "C. Brinkerink"
                    },
                    {
                        "name": "A. R. Offringa"
                    },
                    {
                        "name": "A. J. Boonstra"
                    },
                    {
                        "name": "S. A. Brackenhoff"
                    },
                    {
                        "name": "E. Ceccotti"
                    },
                    {
                        "name": "J. K. Chege"
                    },
                    {
                        "name": "L. Y. Gao"
                    },
                    {
                        "name": "B. K. Gehlot"
                    },
                    {
                        "name": "L. I. Gurvits"
                    },
                    {
                        "name": "C. Hfer"
                    },
                    {
                        "name": "F. G. Mertens"
                    },
                    {
                        "name": "M. Mevius"
                    },
                    {
                        "name": "S. Munshi"
                    },
                    {
                        "name": "A. Saxena"
                    },
                    {
                        "name": "J. A. Tauber"
                    },
                    {
                        "name": "H. Vedantham"
                    },
                    {
                        "name": "S. Yatawatta"
                    },
                    {
                        "name": "S. Zaroubi"
                    }
                ],
                "author_detail": {
                    "name": "S. Zaroubi"
                },
                "author": "S. Zaroubi"
            },
            {
                "id": "http://arxiv.org/abs/2511.21667v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21667v3",
                "title": "Escaping the Verifier: Learning to Reason via Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escaping the Verifier: Learning to Reason via Demonstrations"
                },
                "updated": "2025-12-09T18:37:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    37,
                    56,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21667v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T18:42:52Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    18,
                    42,
                    52,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Locke Cai"
                    },
                    {
                        "name": "Ivan Provilkov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Provilkov"
                },
                "author": "Ivan Provilkov"
            },
            {
                "id": "http://arxiv.org/abs/2509.13316v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.13316v3",
                "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?"
                },
                "updated": "2025-12-09T18:35:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    35,
                    28,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.13316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.13316v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-16T17:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    59,
                    4,
                    1,
                    259,
                    0
                ],
                "arxiv_comment": "40 pages, 6 figures. Updated and added content",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Millicent Li"
                    },
                    {
                        "name": "Alberto Mario Ceballos Arroyo"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Naomi Saphra"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace"
            },
            {
                "id": "http://arxiv.org/abs/2512.08894v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08894v1",
                "title": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training"
                },
                "updated": "2025-12-09T18:33:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08894v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:33:48Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    48,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jakub Krajewski"
                    },
                    {
                        "name": "Amitis Shidani"
                    },
                    {
                        "name": "Dan Busbridge"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Jason Ramapuram"
                    }
                ],
                "author_detail": {
                    "name": "Jason Ramapuram"
                },
                "author": "Jason Ramapuram"
            },
            {
                "id": "http://arxiv.org/abs/2512.08892v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08892v1",
                "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders"
                },
                "updated": "2025-12-09T18:33:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    22,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08892v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:33:22Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    33,
                    22,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Zhenghao He"
                    },
                    {
                        "name": "Bohan Liu"
                    },
                    {
                        "name": "Sanchit Sinha"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08889v1",
                "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers"
                },
                "updated": "2025-12-09T18:30:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    30,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:30:23Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    30,
                    23,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Project webpage: https://glab-caltech.github.io/valor/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Damiano Marsili"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gkioxari"
                },
                "author": "Georgia Gkioxari"
            },
            {
                "id": "http://arxiv.org/abs/2504.02800v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.02800v3",
                "title": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents"
                },
                "updated": "2025-12-09T18:29:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    29,
                    25,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.02800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.02800v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ICDEW67478.2025.00027",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. Large Language Models (LLMs) offer stronger semantic understanding and reasoning than traditional deep learning, but their use in high-stakes clinical settings is limited by hallucinations and the lack of persistent memory. However, existing literature has not sufficiently investigated how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pretrained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. Large Language Models (LLMs) offer stronger semantic understanding and reasoning than traditional deep learning, but their use in high-stakes clinical settings is limited by hallucinations and the lack of persistent memory. However, existing literature has not sufficiently investigated how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pretrained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-03T17:43:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    14,
                    3,
                    93,
                    0
                ],
                "arxiv_comment": "20 pages, 10 figures. This is an extension of ICDEW 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhuohan Ge"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Xinyi Zhu"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Mingtao Zhang"
                    },
                    {
                        "name": "Shihao Qi"
                    },
                    {
                        "name": "Yuming Xu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_doi": "10.1109/ICDEW67478.2025.00027"
            },
            {
                "id": "http://arxiv.org/abs/2512.08884v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08884v1",
                "title": "AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike"
                },
                "updated": "2025-12-09T18:19:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    19,
                    42,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08884v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:19:42Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    19,
                    42,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Yiwei Wu"
                    },
                    {
                        "name": "Leah Ajmani"
                    },
                    {
                        "name": "Nathan TeBlunthuis"
                    },
                    {
                        "name": "Hanlin Li"
                    }
                ],
                "author_detail": {
                    "name": "Hanlin Li"
                },
                "author": "Hanlin Li"
            },
            {
                "id": "http://arxiv.org/abs/2508.04652v7",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04652v7",
                "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Collaboration With Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-12-09T18:12:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    12,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04652v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04652v7",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges. Our code is available at https://github.com/OpenMLRL/CoMLRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges. Our code is available at https://github.com/OpenMLRL/CoMLRL."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T17:18:25Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    18,
                    25,
                    2,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Xueguang Lyu"
                    },
                    {
                        "name": "Christopher Amato"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Amato"
                },
                "author": "Christopher Amato"
            },
            {
                "id": "http://arxiv.org/abs/2508.06457v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.06457v2",
                "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"
                },
                "updated": "2025-12-09T18:09:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    9,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.06457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.06457v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T17:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for Information Security. 19 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "Proceedings of Machine Learning Research 299, 2025 Conference on Applied Machine Learning for Information Security",
                "authors": [
                    {
                        "name": "Sanket Badhe"
                    }
                ],
                "author_detail": {
                    "name": "Sanket Badhe"
                },
                "author": "Sanket Badhe"
            },
            {
                "id": "http://arxiv.org/abs/2512.08875v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08875v1",
                "title": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation"
                },
                "updated": "2025-12-09T18:06:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    6,
                    31,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08875v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:06:31Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    6,
                    31,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Joshua Ward"
                    },
                    {
                        "name": "Bochao Gu"
                    },
                    {
                        "name": "Chi-Hua Wang"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.08870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08870v1",
                "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents"
                },
                "updated": "2025-12-09T18:04:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    4,
                    41,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:04:41Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    4,
                    41,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Qizhen Lan"
                    },
                    {
                        "name": "Yuchao Qiu"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08867v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08867v1",
                "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA"
                },
                "updated": "2025-12-09T17:58:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    58,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08867v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:58:36Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    58,
                    36,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Lianghong Guo"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.08864v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08864v1",
                "title": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse"
                },
                "updated": "2025-12-09T17:54:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    54,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08864v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:54:17Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    54,
                    17,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matthew Smith"
                    },
                    {
                        "name": "Jakub Kry"
                    },
                    {
                        "name": "Simon Campos"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Sevan Hayrapet"
                    },
                    {
                        "name": "Fred Heiding"
                    },
                    {
                        "name": "Omer Nevo"
                    },
                    {
                        "name": "Adam Swanda"
                    },
                    {
                        "name": "Jair Aguirre"
                    },
                    {
                        "name": "Asher Brass Gershovich"
                    },
                    {
                        "name": "Eric Clay"
                    },
                    {
                        "name": "Ryan Fetterman"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Marc Juarez"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Henry Papadatos"
                    }
                ],
                "author_detail": {
                    "name": "Henry Papadatos"
                },
                "author": "Henry Papadatos"
            },
            {
                "id": "http://arxiv.org/abs/2512.08860v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08860v1",
                "title": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference"
                },
                "updated": "2025-12-09T17:52:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    52,
                    57,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08860v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:52:57Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    52,
                    57,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "6 pages, 3 figures. Code and data: https://github.com/Amiton7/Tri-Bench. Accepted to the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Amit Bendkhale"
                    }
                ],
                "author_detail": {
                    "name": "Amit Bendkhale"
                },
                "author": "Amit Bendkhale"
            },
            {
                "id": "http://arxiv.org/abs/2512.08844v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08844v1",
                "title": "A Methodology for Quantitative AI Risk Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Quantitative AI Risk Modeling"
                },
                "updated": "2025-12-09T17:34:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08844v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:34:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matt Smith"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Simon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Simon Campos"
                },
                "author": "Simon Campos"
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17238v2",
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "updated": "2025-12-09T17:34:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen"
            },
            {
                "id": "http://arxiv.org/abs/2510.05526v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05526v2",
                "title": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment"
                },
                "updated": "2025-12-09T17:10:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    10,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05526v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T02:32:47Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    32,
                    47,
                    1,
                    280,
                    0
                ],
                "arxiv_comment": "Edited a few incorrect numbers in Tables 2 and 3",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Peiran Yu"
                    },
                    {
                        "name": "Heng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Huang"
                },
                "author": "Heng Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08814v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08814v1",
                "title": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts"
                },
                "updated": "2025-12-09T17:07:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    7,
                    54,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08814v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -> user vector -> labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -> user vector -> labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:07:54Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    7,
                    54,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yifan Lyu"
                    },
                    {
                        "name": "Liang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhang"
                },
                "author": "Liang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08813v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08813v1",
                "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks"
                },
                "updated": "2025-12-09T17:06:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    6,
                    21,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08813v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:06:21Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    6,
                    21,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted to SAC '26. To appear, DOI: https://doi.org/10.1145/3748522.3779970",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Connor York"
                    },
                    {
                        "name": "Zachary R Madin"
                    },
                    {
                        "name": "Paul O'Dowd"
                    },
                    {
                        "name": "Edmund R Hunt"
                    }
                ],
                "author_detail": {
                    "name": "Edmund R Hunt"
                },
                "author": "Edmund R Hunt"
            },
            {
                "id": "http://arxiv.org/abs/2511.15817v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.15817v3",
                "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code"
                },
                "updated": "2025-12-09T17:06:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    6,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.15817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.15817v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773164",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-19T19:18:28Z",
                "published_parsed": [
                    2025,
                    11,
                    19,
                    19,
                    18,
                    28,
                    2,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Alejandro Velasco"
                    },
                    {
                        "name": "Daniel Rodriguez-Cardenas"
                    },
                    {
                        "name": "Dipin Khati"
                    },
                    {
                        "name": "David N. Palacio"
                    },
                    {
                        "name": "Luftar Rahman Alif"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    }
                ],
                "author_detail": {
                    "name": "Denys Poshyvanyk"
                },
                "author": "Denys Poshyvanyk",
                "arxiv_doi": "10.1145/3744916.3773164"
            },
            {
                "id": "http://arxiv.org/abs/2512.08812v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08812v1",
                "title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation"
                },
                "updated": "2025-12-09T17:05:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    5,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08812v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:05:36Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    5,
                    36,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025). https://www.intellisky.org/workshops/AIMG2025/workshop_AIMG2025.html",
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Anna Jordanous"
                    }
                ],
                "author_detail": {
                    "name": "Anna Jordanous"
                },
                "author": "Anna Jordanous"
            },
            {
                "id": "http://arxiv.org/abs/2512.08810v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08810v1",
                "title": "Multicalibration for LLM-based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multicalibration for LLM-based Code Generation"
                },
                "updated": "2025-12-09T17:04:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    4,
                    1,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08810v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:04:01Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    4,
                    1,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted at AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Viola Campos"
                    },
                    {
                        "name": "Robin Kuschnereit"
                    },
                    {
                        "name": "Adrian Ulges"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Ulges"
                },
                "author": "Adrian Ulges"
            },
            {
                "id": "http://arxiv.org/abs/2512.08798v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08798v1",
                "title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?"
                },
                "updated": "2025-12-09T16:51:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    51,
                    30,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08798v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:51:30Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    51,
                    30,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Rejected from LoG 2025 (submitted August 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jeongwhan Choi"
                    },
                    {
                        "name": "Woosung Kang"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Jongwoo Kim"
                    },
                    {
                        "name": "Noseong Park"
                    }
                ],
                "author_detail": {
                    "name": "Noseong Park"
                },
                "author": "Noseong Park"
            },
            {
                "id": "http://arxiv.org/abs/2512.08786v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08786v1",
                "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs"
                },
                "updated": "2025-12-09T16:39:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    39,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08786v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:39:32Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    39,
                    32,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mahmoud Srewa"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Salma Elmalaki"
                    }
                ],
                "author_detail": {
                    "name": "Salma Elmalaki"
                },
                "author": "Salma Elmalaki"
            },
            {
                "id": "http://arxiv.org/abs/2504.15129v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.15129v2",
                "title": "Towards Task-Oriented Flying: Framework, Infrastructure, and Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Task-Oriented Flying: Framework, Infrastructure, and Principles"
                },
                "updated": "2025-12-09T16:30:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    30,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.15129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.15129v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-21T14:25:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    25,
                    23,
                    0,
                    111,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kangyao Huang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Jingyu Chen"
                    },
                    {
                        "name": "Jintao Chen"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Di Guo"
                    },
                    {
                        "name": "Xiangkui Zhang"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08769v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08769v1",
                "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows"
                },
                "updated": "2025-12-09T16:23:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    23,
                    5,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08769v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:23:05Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    23,
                    5,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Eranga Bandara"
                    },
                    {
                        "name": "Ross Gore"
                    },
                    {
                        "name": "Peter Foytik"
                    },
                    {
                        "name": "Sachin Shetty"
                    },
                    {
                        "name": "Ravi Mukkamala"
                    },
                    {
                        "name": "Abdul Rahman"
                    },
                    {
                        "name": "Xueping Liang"
                    },
                    {
                        "name": "Safdar H. Bouk"
                    },
                    {
                        "name": "Amin Hass"
                    },
                    {
                        "name": "Sachini Rajapakse"
                    },
                    {
                        "name": "Ng Wee Keong"
                    },
                    {
                        "name": "Kasun De Zoysa"
                    },
                    {
                        "name": "Aruna Withanage"
                    },
                    {
                        "name": "Nilaan Loganathan"
                    }
                ],
                "author_detail": {
                    "name": "Nilaan Loganathan"
                },
                "author": "Nilaan Loganathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.08767v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08767v1",
                "title": "Data-Driven Dynamic Parameter Learning of manipulator robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Dynamic Parameter Learning of manipulator robots"
                },
                "updated": "2025-12-09T16:15:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    15,
                    58,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08767v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:15:58Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    15,
                    58,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Mohammed Elseiagy"
                    },
                    {
                        "name": "Tsige Tadesse Alemayoh"
                    },
                    {
                        "name": "Ranulfo Bezerra"
                    },
                    {
                        "name": "Shotaro Kojima"
                    },
                    {
                        "name": "Kazunori Ohno"
                    }
                ],
                "author_detail": {
                    "name": "Kazunori Ohno"
                },
                "author": "Kazunori Ohno"
            },
            {
                "id": "http://arxiv.org/abs/2512.08764v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08764v1",
                "title": "Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?"
                },
                "updated": "2025-12-09T16:13:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    13,
                    27,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08764v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/978-3-032-09037-9",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:13:27Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    13,
                    27,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "8 pages, 5 tables",
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "arxiv_journal_ref": "Advances in Soft Computing. MICAI 2025. Lecture Notes in Computer Science, vol 16221. Springer",
                "authors": [
                    {
                        "name": "Nicolas Reche"
                    },
                    {
                        "name": "Elvys Linhares-Pontes"
                    },
                    {
                        "name": "Juan-Manuel Torres-Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Juan-Manuel Torres-Moreno"
                },
                "author": "Juan-Manuel Torres-Moreno",
                "arxiv_doi": "10.1007/978-3-032-09037-9"
            },
            {
                "id": "http://arxiv.org/abs/2512.08755v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08755v1",
                "title": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments"
                },
                "updated": "2025-12-09T16:06:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    6,
                    9,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08755v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:06:09Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    6,
                    9,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Dongdong Yang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Jiguang He"
                    }
                ],
                "author_detail": {
                    "name": "Jiguang He"
                },
                "author": "Jiguang He"
            },
            {
                "id": "http://arxiv.org/abs/2511.10240v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10240v2",
                "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs"
                },
                "updated": "2025-12-09T16:01:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    1,
                    50,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10240v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:14:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minbae Park"
                    },
                    {
                        "name": "Hyemin Yang"
                    },
                    {
                        "name": "Jeonghyun Kim"
                    },
                    {
                        "name": "Kunsoo Park"
                    },
                    {
                        "name": "Hyunjoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjoon Kim"
                },
                "author": "Hyunjoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.08751v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08751v1",
                "title": "Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices"
                },
                "updated": "2025-12-09T16:01:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    1,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08751v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T16:01:48Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    16,
                    1,
                    48,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kuniko Paxton"
                    },
                    {
                        "name": "Koorosh Aslansefat"
                    },
                    {
                        "name": "Dhavalkumar Thakker"
                    },
                    {
                        "name": "Yiannis Papadopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Papadopoulos"
                },
                "author": "Yiannis Papadopoulos"
            },
            {
                "id": "http://arxiv.org/abs/2512.08746v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08746v1",
                "title": "RF sensing with dense IoT network graphs: An EM-informed analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RF sensing with dense IoT network graphs: An EM-informed analysis"
                },
                "updated": "2025-12-09T15:57:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    57,
                    1,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08746v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Radio Frequency (RF) sensing is attracting interest in research, standardization, and industry, especially for its potential in Internet of Things (IoT) applications. By leveraging the properties of the ElectroMagnetic (EM) waves used in wireless networks, RF sensing captures environmental information such as the presence and movement of people and objects, enabling passive localization and vision applications. This paper investigates the theoretical bounds on accuracy and resolution for RF sensing systems within dense networks. It employs an EM model to predict the effects of body blockage in various scenarios. To detect human movements, the paper proposes a deep graph neural network, trained on Received Signal Strength (RSS) samples generated from the EM model. These samples are structured as dense graphs, with nodes representing antennas and edges as radio links. Focusing on the problem of identifying the number of human subjects co-present in a monitored area over time, the paper analyzes the theoretical limits on the number of distinguishable subjects, exploring how these limits depend on factors such as the number of radio links, the size of the monitored area and the subjects physical dimensions. These bounds enable the prediction of the system performance during network pre-deployment stages. The paper also presents the results of an indoor case study, which demonstrate the effectiveness of the approach and confirm the model's predictive potential in the network design stages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio Frequency (RF) sensing is attracting interest in research, standardization, and industry, especially for its potential in Internet of Things (IoT) applications. By leveraging the properties of the ElectroMagnetic (EM) waves used in wireless networks, RF sensing captures environmental information such as the presence and movement of people and objects, enabling passive localization and vision applications. This paper investigates the theoretical bounds on accuracy and resolution for RF sensing systems within dense networks. It employs an EM model to predict the effects of body blockage in various scenarios. To detect human movements, the paper proposes a deep graph neural network, trained on Received Signal Strength (RSS) samples generated from the EM model. These samples are structured as dense graphs, with nodes representing antennas and edges as radio links. Focusing on the problem of identifying the number of human subjects co-present in a monitored area over time, the paper analyzes the theoretical limits on the number of distinguishable subjects, exploring how these limits depend on factors such as the number of radio links, the size of the monitored area and the subjects physical dimensions. These bounds enable the prediction of the system performance during network pre-deployment stages. The paper also presents the results of an indoor case study, which demonstrate the effectiveness of the approach and confirm the model's predictive potential in the network design stages."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:57:01Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    57,
                    1,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "accepted to IEEE Internet of Things Journal",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Federica Fieramosca"
                    },
                    {
                        "name": "Vittorio Rampa"
                    },
                    {
                        "name": "Michele D'Amico"
                    },
                    {
                        "name": "Stefano Savazzi"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Savazzi"
                },
                "author": "Stefano Savazzi"
            },
            {
                "id": "http://arxiv.org/abs/2512.08737v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08737v1",
                "title": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy"
                },
                "updated": "2025-12-09T15:47:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    47,
                    16,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08737v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:47:16Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    47,
                    16,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Submitted to AAMAS 2026",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Botao 'Amber' Hu"
                    },
                    {
                        "name": "Bangdao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bangdao Chen"
                },
                "author": "Bangdao Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.08731v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08731v1",
                "title": "LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design"
                },
                "updated": "2025-12-09T15:43:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    43,
                    24,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08731v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:43:24Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    43,
                    24,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Qipan Wang"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Shuangchen Li"
                    },
                    {
                        "name": "Hongzhong Zheng"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.18545v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18545v2",
                "title": "SlicePilot: Demystifying Network Slice Placement in Heterogeneous Cloud Infrastructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlicePilot: Demystifying Network Slice Placement in Heterogeneous Cloud Infrastructures"
                },
                "updated": "2025-12-09T15:43:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    43,
                    13,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18545v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cellular networks are comprised of software-based entities, with main functions encapsulated as Virtual Network Functions (VNFs) deployed on Commercial-off-the-Shelf (COTS) hardware. As a key enabler of 5G, network slicing offers logically isolated Quality of Service (QoS) for diverse use cases. With the transition to cloud-native infrastructures, optimizing network slice placement across multi-cloud environments remains challenging due to heterogeneous resource capabilities and varying slice-specific demands. This paper presents SlicePilot, a modular framework that enables autonomous and near-optimal VNF placement using a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. SlicePilot collects real-world traffic profiles to estimate resource needs for each slice type. These estimates guide a MARL-based scheduler that minimizes deployment costs while satisfying QoS constraints. We evaluate SlicePilot on a multi-cloud testbed and demonstrate a 19x speed-up over combinatorial optimization methods, while keeping deployment costs within 7.8% of the optimal. Although SlicePilot results in 2.42x more QoS violations under high-load conditions, this trade-off is offset by faster decision-making and reduced computational overhead. Overall, SlicePilot delivers a scalable, cost-efficient solution for network slice placement, making it suitable for real-time deployments where responsiveness and efficiency are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular networks are comprised of software-based entities, with main functions encapsulated as Virtual Network Functions (VNFs) deployed on Commercial-off-the-Shelf (COTS) hardware. As a key enabler of 5G, network slicing offers logically isolated Quality of Service (QoS) for diverse use cases. With the transition to cloud-native infrastructures, optimizing network slice placement across multi-cloud environments remains challenging due to heterogeneous resource capabilities and varying slice-specific demands. This paper presents SlicePilot, a modular framework that enables autonomous and near-optimal VNF placement using a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. SlicePilot collects real-world traffic profiles to estimate resource needs for each slice type. These estimates guide a MARL-based scheduler that minimizes deployment costs while satisfying QoS constraints. We evaluate SlicePilot on a multi-cloud testbed and demonstrate a 19x speed-up over combinatorial optimization methods, while keeping deployment costs within 7.8% of the optimal. Although SlicePilot results in 2.42x more QoS violations under high-load conditions, this trade-off is offset by faster decision-making and reduced computational overhead. Overall, SlicePilot delivers a scalable, cost-efficient solution for network slice placement, making it suitable for real-time deployments where responsiveness and efficiency are critical."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T02:08:11Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    2,
                    8,
                    11,
                    1,
                    266,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Ioannis Panitsas"
                    },
                    {
                        "name": "Tolga O. Atalay"
                    },
                    {
                        "name": "Dragoslav Stojadinovic"
                    },
                    {
                        "name": "Angelos Stavrou"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas"
            },
            {
                "id": "http://arxiv.org/abs/2512.08724v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08724v1",
                "title": "Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search"
                },
                "updated": "2025-12-09T15:39:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    39,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08724v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:39:04Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    39,
                    4,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Manos Plitsis"
                    },
                    {
                        "name": "Giorgos Bouritsas"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    },
                    {
                        "name": "Yannis Panagakis"
                    }
                ],
                "author_detail": {
                    "name": "Yannis Panagakis"
                },
                "author": "Yannis Panagakis"
            },
            {
                "id": "http://arxiv.org/abs/2508.09369v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.09369v2",
                "title": "FedJam: Multimodal Federated Learning Framework for Jamming Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedJam: Multimodal Federated Learning Framework for Jamming Detection"
                },
                "updated": "2025-12-09T15:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    27,
                    16,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.09369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.09369v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Jamming attacks pose a critical threat to wireless networks, yet existing detection methods remain largely unimodal, centralized and resource-intensive, limiting their performance, scalability, and deployment feasibility, respectively. To address these limitations, we present FedJam, a multimodal Federated Learning (FL) framework for on-device jamming detection and classification. FedJam locally fuses spectrograms and cross-layer network Key Performance Indicators (KPIs) using a lightweight dual-encoder architecture with an integrated fusion module and multimodal projection head, that enables privacy-preserving training and inference without transmitting raw data. We prototype and deploy FedJam on a wireless experimental testbed and evaluate it using the first, over-the-air multimodal dataset comprising synchronized samples across benign and three distinct jamming attack types. FedJam outperforms state-of-the-art unimodal baselines by up to 15% in accuracy, while requiring 60% fewer communication rounds to converge, and maintains low resource utilization. Its advantage is especially pronounced in realistic scenarios, where it remains extremely robust under heterogeneous data distributions across devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jamming attacks pose a critical threat to wireless networks, yet existing detection methods remain largely unimodal, centralized and resource-intensive, limiting their performance, scalability, and deployment feasibility, respectively. To address these limitations, we present FedJam, a multimodal Federated Learning (FL) framework for on-device jamming detection and classification. FedJam locally fuses spectrograms and cross-layer network Key Performance Indicators (KPIs) using a lightweight dual-encoder architecture with an integrated fusion module and multimodal projection head, that enables privacy-preserving training and inference without transmitting raw data. We prototype and deploy FedJam on a wireless experimental testbed and evaluate it using the first, over-the-air multimodal dataset comprising synchronized samples across benign and three distinct jamming attack types. FedJam outperforms state-of-the-art unimodal baselines by up to 15% in accuracy, while requiring 60% fewer communication rounds to converge, and maintains low resource utilization. Its advantage is especially pronounced in realistic scenarios, where it remains extremely robust under heterogeneous data distributions across devices."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-12T21:52:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    21,
                    52,
                    0,
                    1,
                    224,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Ioannis Panitsas"
                    },
                    {
                        "name": "Iason Ofeidis"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas"
            },
            {
                "id": "http://arxiv.org/abs/2512.08706v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08706v1",
                "title": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing"
                },
                "updated": "2025-12-09T15:21:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    21,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08706v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:21:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    21,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted for ICSE 2026 Demonstration track",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Leon Kogler"
                    },
                    {
                        "name": "Maximilian Ehrhart"
                    },
                    {
                        "name": "Benedikt Dornauer"
                    },
                    {
                        "name": "Eduard Paul Enoiu"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Paul Enoiu"
                },
                "author": "Eduard Paul Enoiu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08702v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08702v1",
                "title": "VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation"
                },
                "updated": "2025-12-09T15:18:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    18,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08702v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T15:18:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    18,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted by KDD 2026",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Zitong Wan"
                    },
                    {
                        "name": "Hewei Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Yijie Li"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. H. Ngai"
                },
                "author": "Edith C. H. Ngai"
            },
            {
                "id": "http://arxiv.org/abs/2507.21704v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.21704v3",
                "title": "Affine Frequency Division Multiplexing (AFDM) for 6G: Properties, Features, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Frequency Division Multiplexing (AFDM) for 6G: Properties, Features, and Challenges"
                },
                "updated": "2025-12-09T15:09:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    9,
                    53,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.21704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.21704v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Affine frequency division multiplexing (AFDM) is an emerging waveform candidate for future sixth generation (6G) systems offering a range of promising features, such as enhanced robustness in heterogeneous and high-mobility environments, as well as inherent suitability for integrated sensing and communications (ISAC) applications. In addition, unlike other candidates such as orthogonal time-frequency space (OTFS) modulation, AFDM provides several unique advantages that strengthen its relevance to practical deployment and standardization in 6G. Notably, as a natural generalization of orthogonal frequency division multiplexing (OFDM), strong backward compatibility with existing conventional systems is guaranteed, while also offering novel possibilities in waveform design, for example to enable physical-layer security through its inherent chirp parametrization. In all, this article provides an overview of AFDM, emphasizing its suitability as a candidate waveform for 6G standardization. First, we provide a concise introduction to the fundamental properties and unique characteristics of AFDM, followed by highlights of its advantageous features, and finally a discussion of its potential and challenges in 6G standardization efforts and representative requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine frequency division multiplexing (AFDM) is an emerging waveform candidate for future sixth generation (6G) systems offering a range of promising features, such as enhanced robustness in heterogeneous and high-mobility environments, as well as inherent suitability for integrated sensing and communications (ISAC) applications. In addition, unlike other candidates such as orthogonal time-frequency space (OTFS) modulation, AFDM provides several unique advantages that strengthen its relevance to practical deployment and standardization in 6G. Notably, as a natural generalization of orthogonal frequency division multiplexing (OFDM), strong backward compatibility with existing conventional systems is guaranteed, while also offering novel possibilities in waveform design, for example to enable physical-layer security through its inherent chirp parametrization. In all, this article provides an overview of AFDM, emphasizing its suitability as a candidate waveform for 6G standardization. First, we provide a concise introduction to the fundamental properties and unique characteristics of AFDM, followed by highlights of its advantageous features, and finally a discussion of its potential and challenges in 6G standardization efforts and representative requirements."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-29T11:33:16Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    11,
                    33,
                    16,
                    1,
                    210,
                    0
                ],
                "arxiv_comment": "Accepted for publication in IEEE Communications Standards Magazine, Special Issue on \"Waveforms for 6G and Beyond\"",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Hyeon Seok Rou"
                    },
                    {
                        "name": "Kuranage Roche Rayan Ranasinghe"
                    },
                    {
                        "name": "Vincent Savaux"
                    },
                    {
                        "name": "Giuseppe Thadeu Freitas de Abreu"
                    },
                    {
                        "name": "David Gonzlez G."
                    },
                    {
                        "name": "Christos Masouros"
                    }
                ],
                "author_detail": {
                    "name": "Christos Masouros"
                },
                "author": "Christos Masouros"
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02943v6",
                "title": "Hallucination to Consensus: Multi-Agent LLMs for End-to-End Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination to Consensus: Multi-Agent LLMs for End-to-End Test Generation"
                },
                "updated": "2025-12-09T15:07:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    7,
                    34,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02943v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02943v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Unit testing plays a critical role in ensuring software correctness. However, writing unit tests manually is labor-intensive, especially for strongly typed languages like Java, motivating the need for automated approaches. Traditional methods primarily rely on search-based or randomized algorithms to achieve high code coverage and produce regression oracles, which are derived from the program's current behavior rather than its intended functionality. Recent advances in LLMs have enabled oracle generation from natural language descriptions, aligning better with user requirements. However, existing LLM-based methods often require fine-tuning or rely on external tools such as EvoSuite for test prefix generation, making them costly or cumbersome to apply in practice.\n  In this work, we propose CANDOR, a novel prompt engineering-based LLM framework for automated unit test generation in Java. CANDOR orchestrates multiple specialized LLM agents to collaboratively generate complete tests. To mitigate the notorious hallucinations in LLMs and improve oracle correctness, we introduce a novel strategy that engages multiple reasoning LLMs in a panel discussion and generates accurate oracles based on consensus. Additionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a novel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments show that CANDOR is comparable with EvoSuite in generating tests with high code coverage and clearly superior in terms of mutation score. Moreover, our prompt engineering-based approach CANDOR significantly outperforms the SOTA fine-tuning-based oracle generator TOGLL by at least 21.1 percentage points in oracle correctness on both correct and faulty source code. Further ablation studies confirm the critical contributions of key agents in generating high-quality tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However, writing unit tests manually is labor-intensive, especially for strongly typed languages like Java, motivating the need for automated approaches. Traditional methods primarily rely on search-based or randomized algorithms to achieve high code coverage and produce regression oracles, which are derived from the program's current behavior rather than its intended functionality. Recent advances in LLMs have enabled oracle generation from natural language descriptions, aligning better with user requirements. However, existing LLM-based methods often require fine-tuning or rely on external tools such as EvoSuite for test prefix generation, making them costly or cumbersome to apply in practice.\n  In this work, we propose CANDOR, a novel prompt engineering-based LLM framework for automated unit test generation in Java. CANDOR orchestrates multiple specialized LLM agents to collaboratively generate complete tests. To mitigate the notorious hallucinations in LLMs and improve oracle correctness, we introduce a novel strategy that engages multiple reasoning LLMs in a panel discussion and generates accurate oracles based on consensus. Additionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a novel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments show that CANDOR is comparable with EvoSuite in generating tests with high code coverage and clearly superior in terms of mutation score. Moreover, our prompt engineering-based approach CANDOR significantly outperforms the SOTA fine-tuning-based oracle generator TOGLL by at least 21.1 percentage points in oracle correctness on both correct and faulty source code. Further ablation studies confirm the critical contributions of key agents in generating high-quality tests."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu"
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26699v2",
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment"
                },
                "updated": "2025-12-09T15:06:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    6,
                    55,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26699v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median)."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "Accepted at 1st International Workshop on Agentic Engineering (AGENT 2026, colocated with ICSE), pages 1-5",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente"
            },
            {
                "id": "http://arxiv.org/abs/2511.11599v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11599v2",
                "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection"
                },
                "updated": "2025-12-09T14:42:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    42,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11599v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T09:27:36Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    9,
                    27,
                    36,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Arefeh Kazemi"
                    },
                    {
                        "name": "Hamza Qadeer"
                    },
                    {
                        "name": "Joachim Wagner"
                    },
                    {
                        "name": "Hossein Hosseini"
                    },
                    {
                        "name": "Sri Balaaji Natarajan Kalaivendan"
                    },
                    {
                        "name": "Brian Davis"
                    }
                ],
                "author_detail": {
                    "name": "Brian Davis"
                },
                "author": "Brian Davis"
            },
            {
                "id": "http://arxiv.org/abs/2512.08656v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08656v1",
                "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes"
                },
                "updated": "2025-12-09T14:42:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    42,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08656v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:42:26Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    42,
                    26,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "7 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Lauritz Rismark Fosso"
                    },
                    {
                        "name": "Herman Birn Amundsen"
                    },
                    {
                        "name": "Marios Xanthidis"
                    },
                    {
                        "name": "Sveinung Johan Ohrem"
                    }
                ],
                "author_detail": {
                    "name": "Sveinung Johan Ohrem"
                },
                "author": "Sveinung Johan Ohrem"
            },
            {
                "id": "http://arxiv.org/abs/2512.08646v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08646v1",
                "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models"
                },
                "updated": "2025-12-09T14:35:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    26,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08646v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:35:26Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    26,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The Python package is available at https://github.com/dess-mannheim/QSTN/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maximilian Kreutner"
                    },
                    {
                        "name": "Jens Rupprecht"
                    },
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier"
            },
            {
                "id": "http://arxiv.org/abs/2512.08645v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08645v1",
                "title": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation"
                },
                "updated": "2025-12-09T14:35:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    12,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08645v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:35:12Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    35,
                    12,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "19 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Young Kyung Kim"
                    },
                    {
                        "name": "Oded Schlesinger"
                    },
                    {
                        "name": "Yuzhou Zhao"
                    },
                    {
                        "name": "J. Matias Di Martino"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Sapiro"
                },
                "author": "Guillermo Sapiro"
            },
            {
                "id": "http://arxiv.org/abs/2512.08639v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08639v1",
                "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning"
                },
                "updated": "2025-12-09T14:25:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    25,
                    24,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08639v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:25:24Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    25,
                    24,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Under Review, 12 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhuoyang Liu"
                    },
                    {
                        "name": "Yixiang Luomei"
                    },
                    {
                        "name": "Feng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xu"
                },
                "author": "Feng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2509.25643v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25643v3",
                "title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models"
                },
                "updated": "2025-12-09T14:21:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    21,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25643v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T01:27:46Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    1,
                    27,
                    46,
                    1,
                    273,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Justin Chavarria"
                    },
                    {
                        "name": "Rohan Raizada"
                    },
                    {
                        "name": "Justin White"
                    },
                    {
                        "name": "Eyad Alhetairshi"
                    }
                ],
                "author_detail": {
                    "name": "Eyad Alhetairshi"
                },
                "author": "Eyad Alhetairshi"
            },
            {
                "id": "http://arxiv.org/abs/2506.11180v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.11180v2",
                "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing"
                },
                "updated": "2025-12-09T14:19:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    19,
                    21,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.11180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.11180v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ETFA65518.2025.11205601",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-12T13:02:16Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    2,
                    16,
                    3,
                    163,
                    0
                ],
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Kcher"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    }
                ],
                "author_detail": {
                    "name": "Felix Gehlhoff"
                },
                "author": "Felix Gehlhoff",
                "arxiv_doi": "10.1109/ETFA65518.2025.11205601"
            },
            {
                "id": "http://arxiv.org/abs/2512.08617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08617v1",
                "title": "HealthcareNLP: where are we and what is next?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthcareNLP: where are we and what is next?"
                },
                "updated": "2025-12-09T14:01:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    1,
                    51,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is \"Introductory to CL/NLP topics (HealthcareNLP)\" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is \"Introductory to CL/NLP topics (HealthcareNLP)\" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:01:51Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    1,
                    51,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted Tutorial by LREC 2026 https://lrec2026.info/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lifeng Han"
                    },
                    {
                        "name": "Paul Rayson"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Andrew Moore"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic"
            },
            {
                "id": "http://arxiv.org/abs/2512.08609v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08609v1",
                "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models"
                },
                "updated": "2025-12-09T13:54:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    54,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08609v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:54:18Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    54,
                    18,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Chaoxu Mu"
                    }
                ],
                "author_detail": {
                    "name": "Chaoxu Mu"
                },
                "author": "Chaoxu Mu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08608v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08608v1",
                "title": "NLoS Localization with Single Base Station Based on Radio Map",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLoS Localization with Single Base Station Based on Radio Map"
                },
                "updated": "2025-12-09T13:53:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    53,
                    28,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08608v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate outdoor localization in Non-Line-of-Sight (NLoS) environments remains a critical challenge for wireless communication and sensing systems. Existing methods, including positioning based on the Global Navigation Satellite System (GNSS) and triple Base Stations (BSs) techniques, cannot provide reliable performance under NLoS conditions, particularly in dense urban areas with strong multipath effects. To address this limitation, we propose a single BS localization framework that integrates sequential signal measurements with prior radio information embedded in the Radio Map (RM). Using temporal measurement features and matching them with radio maps, the proposed method effectively mitigates the adverse impact of multipath propagation and reduces the dependence on LoS paths. Simulation experiments further evaluate the impact of different radio map construction strategies and the varying lengths of the measurement sequence on localization accuracy. Results demonstrate that the proposed scheme achieves sub-meter positioning accuracy in typical NLoS environments, highlighting its potential as a practical and robust solution for single-base-station deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate outdoor localization in Non-Line-of-Sight (NLoS) environments remains a critical challenge for wireless communication and sensing systems. Existing methods, including positioning based on the Global Navigation Satellite System (GNSS) and triple Base Stations (BSs) techniques, cannot provide reliable performance under NLoS conditions, particularly in dense urban areas with strong multipath effects. To address this limitation, we propose a single BS localization framework that integrates sequential signal measurements with prior radio information embedded in the Radio Map (RM). Using temporal measurement features and matching them with radio maps, the proposed method effectively mitigates the adverse impact of multipath propagation and reduces the dependence on LoS paths. Simulation experiments further evaluate the impact of different radio map construction strategies and the varying lengths of the measurement sequence on localization accuracy. Results demonstrate that the proposed scheme achieves sub-meter positioning accuracy in typical NLoS environments, highlighting its potential as a practical and robust solution for single-base-station deployment."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:53:28Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    53,
                    28,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Yifan Guo"
                    },
                    {
                        "name": "Xiucheng Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang"
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v23",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2312.00326v23",
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "updated": "2025-12-09T13:31:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    31,
                    53,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2312.00326v23",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2312.00326v23",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "arxiv_comment": "31 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor"
            },
            {
                "id": "http://arxiv.org/abs/2412.08179v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.08179v3",
                "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis -- A Case Study for the Semiconductor Sector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis -- A Case Study for the Semiconductor Sector"
                },
                "updated": "2025-12-09T13:27:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    27,
                    48,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.08179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.08179v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyzes require significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analysis through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a sector-specific financial instruction dataset derived from semiconductor industry documents to guide the LLM adaptation to specialized financial reasoning. Using NVIDIA, AMD, and Broadcom as representative companies, our case study demonstrates that RAG-IT substantially improves a general-purpose open-source LLM and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyzes require significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analysis through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a sector-specific financial instruction dataset derived from semiconductor industry documents to guide the LLM adaptation to specialized financial reasoning. Using NVIDIA, AMD, and Broadcom as representative companies, our case study demonstrates that RAG-IT substantially improves a general-purpose open-source LLM and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting."
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-11T08:09:42Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "arxiv_comment": "We updated title, abstract and added more details in experiment section. We also updated the list of authors",
                "arxiv_primary_category": {
                    "term": "q-fin.ST"
                },
                "authors": [
                    {
                        "name": "Hai-Thien To"
                    },
                    {
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "name": "Van-Duc Le"
                    }
                ],
                "author_detail": {
                    "name": "Van-Duc Le"
                },
                "author": "Van-Duc Le"
            },
            {
                "id": "http://arxiv.org/abs/2401.15378v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2401.15378v6",
                "title": "Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reliability with RAG in Religious Question-Answering: MufassirQAS"
                },
                "updated": "2025-12-09T13:24:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    24,
                    4,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2401.15378v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2401.15378v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Religious teachings can sometimes be complex and challenging to grasp, but chatbots can serve as effective assistants in this domain. Large Language Model (LLM) based chatbots, powered by Natural Language Processing (NLP), can connect related topics and provide well-supported responses to intricate questions, making them valuable tools for religious education. However, LLMs are prone to hallucinations as they can generate inaccurate or irrelevant information, and these can include sensitive content that could be offensive, inappropriate, or controversial. Addressing such topics without inadvertently promoting hate speech or disrespecting certain beliefs remains a significant challenge. As a solution to these issues, we introduce MufassirQAS, a system that enhances LLM accuracy and transparency using a vector database-driven Retrieval-Augmented Generation (RAG) approach. We built a dataset comprising fundamental books containing Turkish translations and interpretations of Islamic texts. This database is leveraged to answer religious inquiries while ensuring that responses remain reliable and contextually grounded. Our system also presents the relevant dataset sections alongside the LLM-generated answers, reinforcing transparency. We carefully designed system prompts to prevent harmful, offensive, or disrespectful outputs, ensuring that responses align with ethical and respectful discourse. Moreover, MufassirQAS provides supplementary details, such as source page numbers and referenced articles, to enhance credibility. To evaluate its effectiveness, we tested MufassirQAS against ChatGPT with sensitive questions, and our system demonstrated superior performance in maintaining accuracy and reliability. Future work will focus on improving accuracy and refining prompt engineering techniques to further minimize biases and ensure even more reliable responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Religious teachings can sometimes be complex and challenging to grasp, but chatbots can serve as effective assistants in this domain. Large Language Model (LLM) based chatbots, powered by Natural Language Processing (NLP), can connect related topics and provide well-supported responses to intricate questions, making them valuable tools for religious education. However, LLMs are prone to hallucinations as they can generate inaccurate or irrelevant information, and these can include sensitive content that could be offensive, inappropriate, or controversial. Addressing such topics without inadvertently promoting hate speech or disrespecting certain beliefs remains a significant challenge. As a solution to these issues, we introduce MufassirQAS, a system that enhances LLM accuracy and transparency using a vector database-driven Retrieval-Augmented Generation (RAG) approach. We built a dataset comprising fundamental books containing Turkish translations and interpretations of Islamic texts. This database is leveraged to answer religious inquiries while ensuring that responses remain reliable and contextually grounded. Our system also presents the relevant dataset sections alongside the LLM-generated answers, reinforcing transparency. We carefully designed system prompts to prevent harmful, offensive, or disrespectful outputs, ensuring that responses align with ethical and respectful discourse. Moreover, MufassirQAS provides supplementary details, such as source page numbers and referenced articles, to enhance credibility. To evaluate its effectiveness, we tested MufassirQAS against ChatGPT with sensitive questions, and our system demonstrated superior performance in maintaining accuracy and reliability. Future work will focus on improving accuracy and refining prompt engineering techniques to further minimize biases and ensure even more reliable responses."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-01-27T10:50:11Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    10,
                    50,
                    11,
                    5,
                    27,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ahmet Yusuf Alan"
                    },
                    {
                        "name": "Enis Karaarslan"
                    },
                    {
                        "name": "Omer Aydin"
                    }
                ],
                "author_detail": {
                    "name": "Omer Aydin"
                },
                "author": "Omer Aydin"
            },
            {
                "id": "http://arxiv.org/abs/2512.06749v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06749v2",
                "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems"
                },
                "updated": "2025-12-09T13:22:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    22,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06749v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T09:23:48Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    9,
                    23,
                    48,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ming Ma"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Tianming Yang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.04415v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04415v2",
                "title": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation"
                },
                "updated": "2025-12-09T13:17:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    17,
                    23,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04415v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io)."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T03:24:03Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    3,
                    24,
                    3,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "Under review at the International Journal of Robotics Research (IJRR)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhoufeng Wang"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Juzhan Xu"
                    },
                    {
                        "name": "Shishun Zhang"
                    },
                    {
                        "name": "Zeyu Xiong"
                    },
                    {
                        "name": "Ruizhen Hu"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08558v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08558v1",
                "title": "Labeled Delegated PSI and its Applications in the Public Sector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labeled Delegated PSI and its Applications in the Public Sector"
                },
                "updated": "2025-12-09T12:59:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    59,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08558v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sensitive citizen data, such as social, medical, and fiscal data, is heavily fragmented across\n  public bodies and the private domain. Mining the combined data sets allows for new insights that otherwise remain hidden.\n  Examples are improved healthcare, fraud detection, and evidence-based policy making.\n  (Multi-party) delegated private set intersection (D-PSI) is a privacy-enhancing technology to link data across multiple data providers using a data collector.\n  However, before it can be deployed in these use cases, it needs to be enhanced with additional functions, e.g., securely delivering payload only for elements in the intersection.\n  Although there has been recent progress in the communication and computation requirements of D-PSI, these practical obstacles have not yet been addressed.\n  This paper is the result of a collaboration with a governmental organization responsible for collecting, linking, and pseudonymizing data.\n  Based on their requirements, we design a new D-PSI protocol with composable output functions, including encrypted payload and pseudonymized identifiers.\n  We show that our protocol is secure in the standard model against colluding semi-honest data providers and against a non-colluding, possibly malicious independent party, the data collector.\n  It, hence, allows to privately link and collect data from multiple data providers suitable for deployment in these use cases in the public sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive citizen data, such as social, medical, and fiscal data, is heavily fragmented across\n  public bodies and the private domain. Mining the combined data sets allows for new insights that otherwise remain hidden.\n  Examples are improved healthcare, fraud detection, and evidence-based policy making.\n  (Multi-party) delegated private set intersection (D-PSI) is a privacy-enhancing technology to link data across multiple data providers using a data collector.\n  However, before it can be deployed in these use cases, it needs to be enhanced with additional functions, e.g., securely delivering payload only for elements in the intersection.\n  Although there has been recent progress in the communication and computation requirements of D-PSI, these practical obstacles have not yet been addressed.\n  This paper is the result of a collaboration with a governmental organization responsible for collecting, linking, and pseudonymizing data.\n  Based on their requirements, we design a new D-PSI protocol with composable output functions, including encrypted payload and pseudonymized identifiers.\n  We show that our protocol is secure in the standard model against colluding semi-honest data providers and against a non-colluding, possibly malicious independent party, the data collector.\n  It, hence, allows to privately link and collect data from multiple data providers suitable for deployment in these use cases in the public sector."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:59:00Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    59,
                    0,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kristof Verslype"
                    },
                    {
                        "name": "Florian Kerschbaum"
                    },
                    {
                        "name": "Cyprien Delpech de Saint Guilhem"
                    },
                    {
                        "name": "Bart De Decker"
                    },
                    {
                        "name": "Jorn Lapon"
                    }
                ],
                "author_detail": {
                    "name": "Jorn Lapon"
                },
                "author": "Jorn Lapon"
            },
            {
                "id": "http://arxiv.org/abs/2510.03291v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03291v2",
                "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs"
                },
                "updated": "2025-12-09T12:42:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    42,
                    46,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03291v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T13:38:28Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    38,
                    28,
                    0,
                    272,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yizhuo Ding"
                    },
                    {
                        "name": "Wanying Qu"
                    },
                    {
                        "name": "Jiawei Geng"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08536v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08536v1",
                "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans"
                },
                "updated": "2025-12-09T12:34:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    34,
                    54,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08536v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T12:34:54Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    34,
                    54,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tammy Zhong"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Maurice Pagnucco"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Pagnucco"
                },
                "author": "Maurice Pagnucco"
            },
            {
                "id": "http://arxiv.org/abs/2510.16809v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.16809v2",
                "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation"
                },
                "updated": "2025-12-09T12:20:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    20,
                    58,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.16809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.16809v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-19T12:29:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    29,
                    13,
                    6,
                    292,
                    0
                ],
                "arxiv_comment": "Accepted to ICSE 2026 (RECODE workshop)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Amirkia Rafiei Oskooei"
                    },
                    {
                        "name": "Kaan Baturalp Cosdan"
                    },
                    {
                        "name": "Husamettin Isiktas"
                    },
                    {
                        "name": "Mehmet S. Aktas"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet S. Aktas"
                },
                "author": "Mehmet S. Aktas"
            },
            {
                "id": "http://arxiv.org/abs/2511.22334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22334v2",
                "title": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends"
                },
                "updated": "2025-12-09T12:19:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    19,
                    18,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:11:01Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    11,
                    1,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "8 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Pablo Prieto"
                    },
                    {
                        "name": "Pablo Abad"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Abad"
                },
                "author": "Pablo Abad"
            },
            {
                "id": "http://arxiv.org/abs/2509.23823v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23823v2",
                "title": "Control Your Robot: A Unified System for Robot Control and Policy Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Your Robot: A Unified System for Robot Control and Policy Deployment"
                },
                "updated": "2025-12-09T12:08:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    12,
                    8,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23823v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T11:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    51,
                    29,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/Tian-Nian/control_your_robot",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Tian Nian"
                    },
                    {
                        "name": "Weijie Ke"
                    },
                    {
                        "name": "Shaolong Zhu"
                    },
                    {
                        "name": "Bingshan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bingshan Hu"
                },
                "author": "Bingshan Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.11218v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11218v2",
                "title": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning"
                },
                "updated": "2025-12-09T11:59:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    59,
                    44,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11218v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiments in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both prediction and controller modules exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 19.1 m/s with a mean return landing distance of 4 m. These experimental results show that our proposed training scheme can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamics-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiments in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both prediction and controller modules exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 19.1 m/s with a mean return landing distance of 4 m. These experimental results show that our proposed training scheme can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamics-critical domains."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T12:22:19Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    12,
                    22,
                    19,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "Project Page: https://humanoid-badminton.github.io/Humanoid-Whole-Body-Badminton-via-Multi-Stage-Reinforcement-Learning",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chenhao Liu"
                    },
                    {
                        "name": "Leyun Jiang"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Kairan Yao"
                    },
                    {
                        "name": "Jinchen Fu"
                    },
                    {
                        "name": "Xiaoyu Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Ren"
                },
                "author": "Xiaoyu Ren"
            },
            {
                "id": "http://arxiv.org/abs/2507.06277v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.06277v3",
                "title": "The Prompt War: How AI Decides on a Military Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt War: How AI Decides on a Military Intervention"
                },
                "updated": "2025-12-09T11:25:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    25,
                    22,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.06277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.06277v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Which factors determine AI's propensity to support military intervention? While the use of AI in high-stakes decision-making is growing exponentially, we still lack systematic analysis of the key drivers embedded in these models. This paper conducts a conjoint experiment in which large language models (LLMs) from leading providers (OpenAI, Anthropic, Google) are asked to decide on military intervention across 128 vignettes, with each vignette run 10 times. This design enables a systematic assessment of AI decision-making in military contexts. The results are remarkably consistent across models: all models place substantial weight on the probability of success and domestic support, prioritizing these factors over civilian casualties, economic shock, or international sanctions. The paper then tests whether LLMs are sensitive to context by introducing different motivations for intervention. The scoring is indeed context-dependent; however, probability of victory remains the most important factor in all scenarios. Finally, the paper evaluates numerical sensitivity and finds that models display some responsiveness to the scale of civilian casualties but no detectable sensitivity to the size of the economic shock.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which factors determine AI's propensity to support military intervention? While the use of AI in high-stakes decision-making is growing exponentially, we still lack systematic analysis of the key drivers embedded in these models. This paper conducts a conjoint experiment in which large language models (LLMs) from leading providers (OpenAI, Anthropic, Google) are asked to decide on military intervention across 128 vignettes, with each vignette run 10 times. This design enables a systematic assessment of AI decision-making in military contexts. The results are remarkably consistent across models: all models place substantial weight on the probability of success and domestic support, prioritizing these factors over civilian casualties, economic shock, or international sanctions. The paper then tests whether LLMs are sensitive to context by introducing different motivations for intervention. The scoring is indeed context-dependent; however, probability of victory remains the most important factor in all scenarios. Finally, the paper evaluates numerical sensitivity and finds that models display some responsiveness to the scale of civilian casualties but no detectable sensitivity to the size of the economic shock."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-08T12:52:08Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    52,
                    8,
                    1,
                    189,
                    0
                ],
                "arxiv_comment": "22 pages, 4 tables, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2507.15771v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.15771v2",
                "title": "Left Leaning Models: How AI Evaluates Economic Policy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left Leaning Models: How AI Evaluates Economic Policy?"
                },
                "updated": "2025-12-09T11:17:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    17,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.15771v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Would artificial intelligence (AI) cut interest rates or adopt conservative monetary policy? Would it deregulate or opt for a more controlled economy? As AI use by economic policymakers, academics, and market participants grows exponentially, it is becoming critical to understand AI preferences over economic policy. However, these preferences are not yet systematically evaluated and remain a black box. This paper makes a conjoint experiment on leading large language models (LLMs) from OpenAI, Anthropic, and Google, asking them to evaluate economic policy under multi-factor constraints. The results are remarkably consistent across models: most LLMs exhibit a strong preference for high growth, low unemployment, and low inequality over traditional macroeconomic concerns such as low inflation and low public debt. Scenario-specific experiments show that LLMs are sensitive to context but still display strong preferences for low unemployment and low inequality even in monetary-policy settings. Numerical sensitivity tests reveal intuitive responses to quantitative changes but also uncover non-linear patterns such as loss aversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would artificial intelligence (AI) cut interest rates or adopt conservative monetary policy? Would it deregulate or opt for a more controlled economy? As AI use by economic policymakers, academics, and market participants grows exponentially, it is becoming critical to understand AI preferences over economic policy. However, these preferences are not yet systematically evaluated and remain a black box. This paper makes a conjoint experiment on leading large language models (LLMs) from OpenAI, Anthropic, and Google, asking them to evaluate economic policy under multi-factor constraints. The results are remarkably consistent across models: most LLMs exhibit a strong preference for high growth, low unemployment, and low inequality over traditional macroeconomic concerns such as low inflation and low public debt. Scenario-specific experiments show that LLMs are sensitive to context but still display strong preferences for low unemployment and low inequality even in monetary-policy settings. Numerical sensitivity tests reveal intuitive responses to quantitative changes but also uncover non-linear patterns such as loss aversion."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-21T16:27:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "arxiv_comment": "16 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2512.08493v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08493v1",
                "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?"
                },
                "updated": "2025-12-09T11:15:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    15,
                    13,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08493v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T11:15:13Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    15,
                    13,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "6 pages, Submitted to ESAAN 2026, Under pier review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Dyna Soumhane Ouchebara"
                    },
                    {
                        "name": "Stphane Dupont"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Dupont"
                },
                "author": "Stphane Dupont"
            },
            {
                "id": "http://arxiv.org/abs/2511.14295v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14295v2",
                "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models"
                },
                "updated": "2025-12-09T11:10:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    10,
                    7,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14295v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T09:47:01Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    9,
                    47,
                    1,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Zbeeb"
                    },
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Sina Mukalled"
                    },
                    {
                        "name": "Nadine Rizk"
                    },
                    {
                        "name": "Fatima Karnib"
                    },
                    {
                        "name": "Issam Lakkis"
                    },
                    {
                        "name": "Ammar Mohanna"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem"
            },
            {
                "id": "http://arxiv.org/abs/2509.24319v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.24319v2",
                "title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs"
                },
                "updated": "2025-12-09T11:05:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    5,
                    45,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.24319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.24319v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T05:57:00Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    57,
                    0,
                    0,
                    272,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Jongwon Lim"
                    },
                    {
                        "name": "Injin Kong"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo"
            },
            {
                "id": "http://arxiv.org/abs/2512.08483v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08483v1",
                "title": "NeurIDA: Dynamic Modeling for Effective In-Database Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeurIDA: Dynamic Modeling for Effective In-Database Analytics"
                },
                "updated": "2025-12-09T11:01:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    1,
                    6,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08483v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.\n  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically \"tweaks\" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.\n  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically \"tweaks\" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T11:01:06Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    11,
                    1,
                    6,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Lingze Zeng"
                    },
                    {
                        "name": "Naili Xing"
                    },
                    {
                        "name": "Shaofeng Cai"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Jian Pei"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi"
            },
            {
                "id": "http://arxiv.org/abs/2512.08478v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08478v1",
                "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform"
                },
                "updated": "2025-12-09T10:54:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    54,
                    58,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08478v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:54:58Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    54,
                    58,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Project page: https://visionary-laboratory.github.io/visionary",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuning Gong"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Yifan Zhan"
                    },
                    {
                        "name": "Muyao Niu"
                    },
                    {
                        "name": "Xueying Li"
                    },
                    {
                        "name": "Yuanjun Liao"
                    },
                    {
                        "name": "Jiaming Chen"
                    },
                    {
                        "name": "Yuanyuan Gao"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Minming Chen"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xiaoqing Hou"
                    },
                    {
                        "name": "Huaxi Huang"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Le Ma"
                    },
                    {
                        "name": "Dingwen Zhang"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Yanchi Zhang"
                    },
                    {
                        "name": "Yinqiang Zheng"
                    },
                    {
                        "name": "Xiao Sun"
                    },
                    {
                        "name": "Zhihang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhihang Zhong"
                },
                "author": "Zhihang Zhong"
            },
            {
                "id": "http://arxiv.org/abs/2505.24554v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24554v3",
                "title": "Bench4KE: Benchmarking Automated Competency Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench4KE: Benchmarking Automated Competency Question Generation"
                },
                "updated": "2025-12-09T10:54:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    54,
                    56,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24554v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24554v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T13:03:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    3,
                    42,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Minh Davide Ragagni"
                    },
                    {
                        "name": "Paolo Ciancarini"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    },
                    {
                        "name": "Valentina Presutti"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Presutti"
                },
                "author": "Valentina Presutti"
            },
            {
                "id": "http://arxiv.org/abs/2512.08476v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08476v1",
                "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems"
                },
                "updated": "2025-12-09T10:50:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    50,
                    19,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08476v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:50:19Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    50,
                    19,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Po-An Shih"
                    },
                    {
                        "name": "Shao-Hua Wang"
                    },
                    {
                        "name": "Yung-Che Li"
                    },
                    {
                        "name": "Chia-Heng Tu"
                    },
                    {
                        "name": "Chih-Han Chang"
                    }
                ],
                "author_detail": {
                    "name": "Chih-Han Chang"
                },
                "author": "Chih-Han Chang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08459v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08459v1",
                "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset"
                },
                "updated": "2025-12-09T10:31:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    31,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08459v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:31:02Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    31,
                    2,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "19 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gary Ackerman"
                    },
                    {
                        "name": "Theodore Wilson"
                    },
                    {
                        "name": "Zachary Kallenborn"
                    },
                    {
                        "name": "Olivia Shoemaker"
                    },
                    {
                        "name": "Anna Wetzel"
                    },
                    {
                        "name": "Hayley Peterson"
                    },
                    {
                        "name": "Abigail Danfora"
                    },
                    {
                        "name": "Jenna LaTourette"
                    },
                    {
                        "name": "Brandon Behlendorf"
                    },
                    {
                        "name": "Douglas Clifford"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Clifford"
                },
                "author": "Douglas Clifford"
            },
            {
                "id": "http://arxiv.org/abs/2512.08451v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08451v1",
                "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process"
                },
                "updated": "2025-12-09T10:24:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    24,
                    25,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08451v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:24:25Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    24,
                    25,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "18 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gary Ackerman"
                    },
                    {
                        "name": "Zachary Kallenborn"
                    },
                    {
                        "name": "Anna Wetzel"
                    },
                    {
                        "name": "Hayley Peterson"
                    },
                    {
                        "name": "Jenna LaTourette"
                    },
                    {
                        "name": "Olivia Shoemaker"
                    },
                    {
                        "name": "Brandon Behlendorf"
                    },
                    {
                        "name": "Sheriff Almakki"
                    },
                    {
                        "name": "Doug Clifford"
                    },
                    {
                        "name": "Noah Sheinbaum"
                    }
                ],
                "author_detail": {
                    "name": "Noah Sheinbaum"
                },
                "author": "Noah Sheinbaum"
            },
            {
                "id": "http://arxiv.org/abs/2512.08449v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08449v1",
                "title": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change"
                },
                "updated": "2025-12-09T10:21:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    21,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08449v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:21:02Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    21,
                    2,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yong-Woon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Woon Kim"
                },
                "author": "Yong-Woon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2504.20781v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.20781v3",
                "title": "Using LLMs in Generating Design Rationale for Software Architecture Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in Generating Design Rationale for Software Architecture Decisions"
                },
                "updated": "2025-12-09T10:14:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    14,
                    35,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.20781v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.20781v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. To further understand the trustworthiness and applicability of LLM-generated DR in practice, we conducted semi-structured interviews with six practitioners. Based on the experimental and interview results, we discussed the pros and cons of the three prompting strategies, the strengths and limitations of LLM-generated DR, and the implications for the practical use of LLM-generated DR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. To further understand the trustworthiness and applicability of LLM-generated DR in practice, we conducted semi-structured interviews with six practitioners. Based on the experimental and interview results, we discussed the pros and cons of the three prompting strategies, the strengths and limitations of LLM-generated DR, and the implications for the practical use of LLM-generated DR."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T14:00:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "arxiv_comment": "Preprint accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM), 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.08440v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08440v1",
                "title": "What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models"
                },
                "updated": "2025-12-09T10:14:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    14,
                    10,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08440v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T10:14:10Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    14,
                    10,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jania Hackenbuchner"
                    },
                    {
                        "name": "Arda Tezcan"
                    },
                    {
                        "name": "Joke Daems"
                    }
                ],
                "author_detail": {
                    "name": "Joke Daems"
                },
                "author": "Joke Daems"
            },
            {
                "id": "http://arxiv.org/abs/2510.14885v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.14885v2",
                "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction"
                },
                "updated": "2025-12-09T10:07:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    10,
                    7,
                    37,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.14885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.14885v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-16T17:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    4,
                    25,
                    3,
                    289,
                    0
                ],
                "arxiv_comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Logan Lawrence"
                    },
                    {
                        "name": "Oindrila Saha"
                    },
                    {
                        "name": "Megan Wei"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Grant Van Horn"
                    }
                ],
                "author_detail": {
                    "name": "Grant Van Horn"
                },
                "author": "Grant Van Horn"
            },
            {
                "id": "http://arxiv.org/abs/2510.02350v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02350v2",
                "title": "LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL"
                },
                "updated": "2025-12-09T09:53:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    53,
                    54,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02350v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Converting natural language questions into SQL queries enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early text-to-SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the large language model era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models, including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek-R1, and others. Notably, DeepSeek-R1 achieves 88.40% accuracy in a zero-shot setting, and models under 10B parameters surpass 90% accuracy after fine-tuning. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark. Unlike the original WikiSQL, which was tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural-language-to-SQL models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting natural language questions into SQL queries enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early text-to-SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the large language model era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models, including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek-R1, and others. Notably, DeepSeek-R1 achieves 88.40% accuracy in a zero-shot setting, and models under 10B parameters surpass 90% accuracy after fine-tuning. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark. Unlike the original WikiSQL, which was tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural-language-to-SQL models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-27T15:08:43Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    15,
                    8,
                    43,
                    5,
                    270,
                    0
                ],
                "arxiv_comment": "To appear in the Proceedings of the IEEE International Conference on Data Mining Workshops (ICDMW)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dzmitry Pihulski"
                    },
                    {
                        "name": "Karol Charchut"
                    },
                    {
                        "name": "Viktoria Novogrodskaia"
                    },
                    {
                        "name": "Jan Koco"
                    }
                ],
                "author_detail": {
                    "name": "Jan Koco"
                },
                "author": "Jan Koco"
            },
            {
                "id": "http://arxiv.org/abs/2512.08417v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08417v1",
                "title": "Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs"
                },
                "updated": "2025-12-09T09:44:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    44,
                    13,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08417v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:44:13Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    44,
                    13,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "Accepted by Network and Distributed System Security (NDSS) Symposium 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yinan Zhong"
                    },
                    {
                        "name": "Qianhao Miao"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Yushi Cheng"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.08403v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08403v1",
                "title": "DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components"
                },
                "updated": "2025-12-09T09:36:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    36,
                    38,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08403v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:36:38Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    36,
                    38,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yupei Li"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Rizhao Cai"
                    },
                    {
                        "name": "Jie Shi"
                    },
                    {
                        "name": "Bjrn W. Schuller"
                    },
                    {
                        "name": "Zhizheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhizheng Wu"
                },
                "author": "Zhizheng Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.07801v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07801v2",
                "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support"
                },
                "updated": "2025-12-09T09:33:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    33,
                    10,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07801v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents are increasingly deployed for expert decision support, yet human-AI teams in high-stakes settings do not yet reliably outperform the best individual. We argue this complementarity gap reflects a fundamental mismatch: current agents are trained as answer engines, not as partners in the collaborative sensemaking through which experts actually make decisions. Sensemaking (the ability to co-construct causal explanations, surface uncertainties, and adapt goals) is the key capability that current training pipelines do not explicitly develop or evaluate. We propose Collaborative Causal Sensemaking (CCS) as a research agenda to develop this capability from the ground up, spanning new training environments that reward collaborative thinking, representations for shared human-AI mental models, and evaluation centred on trust and complementarity. These directions can advance MAS research toward agents that think with their human partners rather than for them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents are increasingly deployed for expert decision support, yet human-AI teams in high-stakes settings do not yet reliably outperform the best individual. We argue this complementarity gap reflects a fundamental mismatch: current agents are trained as answer engines, not as partners in the collaborative sensemaking through which experts actually make decisions. Sensemaking (the ability to co-construct causal explanations, surface uncertainties, and adapt goals) is the key capability that current training pipelines do not explicitly develop or evaluate. We propose Collaborative Causal Sensemaking (CCS) as a research agenda to develop this capability from the ground up, spanning new training environments that reward collaborative thinking, representations for shared human-AI mental models, and evaluation centred on trust and complementarity. These directions can advance MAS research toward agents that think with their human partners rather than for them."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T18:30:41Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    18,
                    30,
                    41,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Raunak Jain"
                    },
                    {
                        "name": "Mudita Khurana"
                    }
                ],
                "author_detail": {
                    "name": "Mudita Khurana"
                },
                "author": "Mudita Khurana"
            },
            {
                "id": "http://arxiv.org/abs/2502.04964v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.04964v6",
                "title": "Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency"
                },
                "updated": "2025-12-09T09:26:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    26,
                    56,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.04964v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.04964v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompass a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches to boost UQ performance. However, they sometimes fail to outperform much simpler baseline methods. Our work discusses the fundamental approach to constructing uncertainty measures that directly links uncertainty with the minimum Bayes risks achieved by LLM decoding. Building on these findings, we propose a novel approach to integrating model confidence with output consistency, resulting in a family of efficient and robust UQ methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency, leading to a family of efficient and robust UQ methods. We evaluate our approach across various tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompass a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches to boost UQ performance. However, they sometimes fail to outperform much simpler baseline methods. Our work discusses the fundamental approach to constructing uncertainty measures that directly links uncertainty with the minimum Bayes risks achieved by LLM decoding. Building on these findings, we propose a novel approach to integrating model confidence with output consistency, resulting in a family of efficient and robust UQ methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency, leading to a family of efficient and robust UQ methods. We evaluate our approach across various tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-07T14:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Albina Ilina"
                    },
                    {
                        "name": "Aleksandr Rubashevskii"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov"
            },
            {
                "id": "http://arxiv.org/abs/2512.08398v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08398v1",
                "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring"
                },
                "updated": "2025-12-09T09:26:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    26,
                    37,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08398v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:26:37Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    26,
                    37,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jiin Park"
                    },
                    {
                        "name": "Hyuna Jeon"
                    },
                    {
                        "name": "Yoonseo Lee"
                    },
                    {
                        "name": "Jisu Hong"
                    },
                    {
                        "name": "Misuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Misuk Kim"
                },
                "author": "Misuk Kim"
            },
            {
                "id": "http://arxiv.org/abs/2510.07172v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.07172v2",
                "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents"
                },
                "updated": "2025-12-09T09:13:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    13,
                    44,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.07172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.07172v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using counterfactual law shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using counterfactual law shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T16:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    12,
                    11,
                    2,
                    281,
                    0
                ],
                "arxiv_comment": "71 pages, 21 figures, 21 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Kelvin Kiu-Wai Tam"
                    },
                    {
                        "name": "Newt Hue-Nam K. Nguyen"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See"
            },
            {
                "id": "http://arxiv.org/abs/2512.08379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08379v1",
                "title": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals"
                },
                "updated": "2025-12-09T09:06:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    6,
                    17,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T09:06:17Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    9,
                    6,
                    17,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kaiwei Liu"
                    },
                    {
                        "name": "Yuting He"
                    },
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Chun Man Victor Wong"
                    },
                    {
                        "name": "Ho Pong Andrew Sze"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Hongkai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Chen"
                },
                "author": "Hongkai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.07497v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07497v2",
                "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations"
                },
                "updated": "2025-12-09T08:55:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    55,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07497v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T12:27:15Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    12,
                    27,
                    15,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "48 pages, 3 tables, 2 listings",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "JV Roig"
                    }
                ],
                "author_detail": {
                    "name": "JV Roig"
                },
                "author": "JV Roig"
            },
            {
                "id": "http://arxiv.org/abs/2508.18321v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.18321v3",
                "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions"
                },
                "updated": "2025-12-09T08:45:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    45,
                    36,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.18321v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.18321v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly integrated into multi-agent systems (MAS), where peer interactions shape individual decisions. While prior work has mainly examined conformity bias, we broaden the view to include how LLMs build rapport from prior interactions, discern and integrate high-quality peer information, and resist misleading inputs-abilities essential for achieving collective intelligence under complex social dynamics. We introduce KAIROS, a benchmark that simulates quiz-style collaboration with peer agents whose rapport levels and behaviours can be precisely controlled in both historical interactions and the current round. This unified setup enables systematic analysis of how rapport, peer actions, and the model's self-confidence jointly influence decision-making. Using KAIROS, we evaluate prompting, supervised fine-tuning, and reinforcement learning via Group Relative Policy Optimisation (GRPO). Results show that model scale is a primary factor moderating susceptibility to social influence: larger models are more resilient and benefit from prompting-based mitigation, whereas smaller models remain vulnerable. Only carefully configured GRPO training yields consistent robustness and performance gains for small models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into multi-agent systems (MAS), where peer interactions shape individual decisions. While prior work has mainly examined conformity bias, we broaden the view to include how LLMs build rapport from prior interactions, discern and integrate high-quality peer information, and resist misleading inputs-abilities essential for achieving collective intelligence under complex social dynamics. We introduce KAIROS, a benchmark that simulates quiz-style collaboration with peer agents whose rapport levels and behaviours can be precisely controlled in both historical interactions and the current round. This unified setup enables systematic analysis of how rapport, peer actions, and the model's self-confidence jointly influence decision-making. Using KAIROS, we evaluate prompting, supervised fine-tuning, and reinforcement learning via Group Relative Policy Optimisation (GRPO). Results show that model scale is a primary factor moderating susceptibility to social influence: larger models are more resilient and benefit from prompting-based mitigation, whereas smaller models remain vulnerable. Only carefully configured GRPO training yields consistent robustness and performance gains for small models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-24T09:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    9,
                    58,
                    10,
                    6,
                    236,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Tej Deep Pala"
                    },
                    {
                        "name": "Ruiwen Zhou"
                    },
                    {
                        "name": "Weisheng Jin"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Dorien Herremans"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria"
            },
            {
                "id": "http://arxiv.org/abs/2512.08366v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08366v1",
                "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making"
                },
                "updated": "2025-12-09T08:44:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    44,
                    59,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08366v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T08:44:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    44,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Qunbo Wang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Junsheng Wu"
                    },
                    {
                        "name": "Hongping Gan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ling Dai"
                    },
                    {
                        "name": "Shizhuang Deng"
                    },
                    {
                        "name": "Shuntong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shuntong Sun"
                },
                "author": "Shuntong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.08365v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08365v1",
                "title": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging"
                },
                "updated": "2025-12-09T08:41:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    41,
                    16,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08365v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.\n  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.\n  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T08:41:16Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    41,
                    16,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "12 pages, 10 fi",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Wenbo Qian"
                    },
                    {
                        "name": "Dedong Xie"
                    },
                    {
                        "name": "Ruiyan Hu"
                    },
                    {
                        "name": "Yigong Hu"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci"
            },
            {
                "id": "http://arxiv.org/abs/2505.24511v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24511v3",
                "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting"
                },
                "updated": "2025-12-09T08:24:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    24,
                    11,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24511v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T12:19:02Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    19,
                    2,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Daoyu Wang"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2505.03295v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.03295v2",
                "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces"
                },
                "updated": "2025-12-09T08:21:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    21,
                    0,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.03295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.03295v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ETFA65518.2025.11205724",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-06T08:27:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    4,
                    1,
                    126,
                    0
                ],
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Kcher"
                    },
                    {
                        "name": "Nicolas Knig"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "arxiv_doi": "10.1109/ETFA65518.2025.11205724"
            },
            {
                "id": "http://arxiv.org/abs/2512.08345v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08345v1",
                "title": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations"
                },
                "updated": "2025-12-09T08:17:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    17,
                    35,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08345v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T08:17:35Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    17,
                    35,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "8 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Benedikt Mangold"
                    }
                ],
                "author_detail": {
                    "name": "Benedikt Mangold"
                },
                "author": "Benedikt Mangold"
            },
            {
                "id": "http://arxiv.org/abs/2512.08341v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08341v1",
                "title": "Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks"
                },
                "updated": "2025-12-09T08:11:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    11,
                    21,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08341v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T08:11:21Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    8,
                    11,
                    21,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "IEEE ICC 2026",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Thai Duong Nguyen"
                    },
                    {
                        "name": "Ngoc-Tan Nguyen"
                    },
                    {
                        "name": "Thanh-Dao Nguyen"
                    },
                    {
                        "name": "Nguyen Van Huynh"
                    },
                    {
                        "name": "Dinh-Hieu Tran"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas"
            },
            {
                "id": "http://arxiv.org/abs/2512.08329v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08329v1",
                "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models"
                },
                "updated": "2025-12-09T07:55:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    55,
                    11,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08329v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T07:55:11Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    55,
                    11,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "32 pages, 17 figures, 1 table, 5 algorithms, preprint",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Michael R. Martin"
                    },
                    {
                        "name": "Garrick Chan"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.06915v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06915v2",
                "title": "Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering"
                },
                "updated": "2025-12-09T07:46:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    46,
                    29,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06915v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T16:43:45Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    16,
                    43,
                    45,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Kelin Fu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zeyu Shang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Kaigui Bian"
                    }
                ],
                "author_detail": {
                    "name": "Kaigui Bian"
                },
                "author": "Kaigui Bian"
            },
            {
                "id": "http://arxiv.org/abs/2512.08326v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08326v1",
                "title": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships"
                },
                "updated": "2025-12-09T07:42:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    42,
                    10,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08326v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773208",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T07:42:10Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    42,
                    10,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures, 8 tables;Accepted to ICSE 2026 Research Track",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Liyang Zhang"
                    },
                    {
                        "name": "Qijia Zhuang"
                    },
                    {
                        "name": "Ao Yang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Xijun Luo"
                    },
                    {
                        "name": "Bing Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Lin"
                },
                "author": "Bing Lin",
                "arxiv_doi": "10.1145/3744916.3773208"
            }
        ]
    }
]